{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14525fd7"
      },
      "source": [
        "# 0.0 Import Libraries"
      ],
      "id": "14525fd7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuX4sZpytah9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086a97d4-5e1d-40c7-9e68-d13bbb2c3948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mat73\n",
            "  Downloading mat73-0.62-py3-none-any.whl (19 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.23.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mat73) (3.9.0)\n",
            "Installing collected packages: spectral, einops, mat73\n",
            "Successfully installed einops-0.7.0 mat73-0.62 spectral-0.23.1\n"
          ]
        }
      ],
      "source": [
        "pip install spectral mat73  einops"
      ],
      "id": "vuX4sZpytah9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12826627"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "\n",
        "from einops import rearrange\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import io\n",
        "import torch.utils.data\n",
        "import scipy.io as sio\n",
        "import mat73\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "12826627"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dRG3WratmCN"
      },
      "source": [
        "# 1.0 Upload Data"
      ],
      "id": "6dRG3WratmCN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpwq4Yi-tgjs",
        "outputId": "ba00a640-3d68-4bfd-a22c-31f5d9ebed65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Vpwq4Yi-tgjs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMin2GFJtorP",
        "outputId": "7c501f7f-9850-4b27-d634-0b58f80580ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencodermodel_trento.pth  HSI_TeSet.mat  LiDAR_TeSet.mat  TeLabel.mat      TrLabel.mat\n",
            "best_model_weights.pth\t     HSI_TrSet.mat  LiDAR_TrSet.mat  trento_data.mat\n"
          ]
        }
      ],
      "source": [
        "! ls '/content/drive/MyDrive/A02_RemoteSensingData/TrentoDataSet/'"
      ],
      "id": "vMin2GFJtorP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NMT_vf3t3Bg"
      },
      "outputs": [],
      "source": [
        "# path\n",
        "path ='/content/drive/MyDrive/A02_RemoteSensingData/TrentoDataSet/'"
      ],
      "id": "4NMT_vf3t3Bg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loader HSI TR_map_2018\n",
        "trento_data=sio.loadmat(path+'trento_data.mat')['HSI_data']\n",
        "trento_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VebGxkrY2sbp",
        "outputId": "9c35db4f-2791-437f-ede2-9b8f08d6cdb3"
      },
      "id": "VebGxkrY2sbp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(166, 600, 63)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loader HSI TR_map_2018\n",
        "trento_data=sio.loadmat(path+'trento_data.mat')\n",
        "\n",
        "# Extract the HSI_data and LiDAR_data arrays\n",
        "trento_hsi_data = trento_data['HSI_data']\n",
        "trento_lidar_data = trento_data['LiDAR_data']\n",
        "trento_gt=trento_data['ground']\n",
        "print('trento_hsi_data shape:', trento_hsi_data.shape)\n",
        "print('trento_lidar_data shape:', trento_lidar_data.shape)\n",
        "print('trento_gt shape:', trento_gt.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-bNzvYHnxS1",
        "outputId": "d433c0b7-0f79-4c1b-b2b2-b05ac9ed9a0d"
      },
      "id": "D-bNzvYHnxS1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trento_hsi_data shape: (166, 600, 63)\n",
            "trento_lidar_data shape: (166, 600)\n",
            "trento_gt shape: (166, 600)\n",
            "trento_hsi_trset shape: (2832, 144)\n",
            "trento_trlabel shape: (2832, 1)\n",
            "trento_hsi_teset shape: (12197, 144)\n",
            "trento_telabel shape: (12197, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "690eba9c"
      },
      "outputs": [],
      "source": [
        "# 2.1 Define the class information\n",
        "class_info = [\n",
        "    (1, \"Forest\", 443, 54511, 54954),\n",
        "    (2, \"Residential Area\", 423, 268219, 268642),\n",
        "    (3, \"Industrail Area\", 499, 19067, 19566),\n",
        "    (4, \"Low Plants\", 376, 58906, 59282),\n",
        "    (5, \"Soil\", 331, 17095, 17426),\n",
        "    (6, \"Allotment\", 280, 13025, 13305),\n",
        "    (7, \"Commenercial Area\", 298, 24526, 24824),\n",
        "    (8, \"Water\", 170, 6502, 6672)\n",
        "]\n",
        "\n",
        "class_dict = {class_number: {\"class_name\": class_name, \"training\": training, \"test\": test, \"samples\": samples} for class_number, class_name, training, test, samples in class_info}\n",
        "\n",
        "# for class_number, class_info in class_dict.items():\n",
        "#     print(f\"Class {class_number}: {class_info['class_name']}\")\n",
        "#     print(f\"Training Samples: {class_info['training']}\")\n",
        "#     print(f\"Test Samples: {class_info['test']}\")\n",
        "#     print(f\"Total Samples: {class_info['samples']}\")\n",
        "#     print()\n",
        "\n",
        "\n",
        "# print(class_dict)"
      ],
      "id": "690eba9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "002fcb4c"
      },
      "source": [
        "# 2.0 Data Preprocessing & Dataloader Preparation"
      ],
      "id": "002fcb4c"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a mask with all class labels\n",
        "# mask = np.copy(trento_gt)\n",
        "\n",
        "# # Set the background class to 0\n",
        "# mask[mask == 0] = 0\n",
        "\n",
        "# Define patch size and stride\n",
        "patch_size = 9\n",
        "stride = 1\n",
        "\n",
        "# Create an empty list to store patches and labels\n",
        "hsi_samples = []\n",
        "lidar_samples = []\n",
        "labels = []\n",
        "\n",
        "# Initialize a dictionary to store class count\n",
        "class_count = {i: 0 for i in class_dict.keys()}\n",
        "\n",
        "# Function to check if all classes have the required number of samples\n",
        "def all_classes_completed(class_count, class_dict):\n",
        "    return all(class_count[class_num] == class_dict[class_num][\"samples\"] for class_num in class_dict.keys())\n",
        "\n",
        "while not all_classes_completed(class_count, class_dict):\n",
        "    # Loop through the ground truth data\n",
        "    for label in class_dict.keys():\n",
        "        # Get the coordinates of the ground truth pixels\n",
        "        #coords = np.argwhere((trento_gt == label) & (mask > 0))\n",
        "        coords = np.argwhere(trento_gt == label)\n",
        "\n",
        "        # Shuffle the coordinates to randomize the patch extraction\n",
        "        np.random.shuffle(coords)\n",
        "\n",
        "        for coord in coords:\n",
        "            i, j = coord\n",
        "            # Calculate the patch indices\n",
        "            i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
        "            j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
        "\n",
        "            # Check if the indices are within the bounds of the HSI data\n",
        "            if i_start >= 0 and i_end <= trento_hsi_data.shape[0] and j_start >= 0 and j_end <= trento_hsi_data.shape[1]:\n",
        "                # Extract the patch\n",
        "                hsi_patch = trento_hsi_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # Extract the LiDAR patch\n",
        "                lidar_patch = trento_lidar_data[i_start:i_end, j_start:j_end]\n",
        "\n",
        "                # If the class count is less than the required samples\n",
        "                if class_count[label] < class_dict[label][\"samples\"]:\n",
        "                    # Append the patch and its label to the list\n",
        "                    hsi_samples.append(hsi_patch)\n",
        "                    lidar_samples.append(lidar_patch)\n",
        "                    labels.append(label)\n",
        "                    class_count[label] += 1\n",
        "\n",
        "                    # If all classes have the required number of samples, exit the loop\n",
        "                    if all_classes_completed(class_count, class_dict):\n",
        "                        break\n",
        "\n",
        "# Convert the list of patches and labels into arrays\n",
        "hsi_samples = np.array(hsi_samples)\n",
        "lidar_samples = np.array(lidar_samples)\n",
        "labels = np.array(labels)\n",
        "print('hsi_samples shape:', hsi_samples.shape)\n",
        "print('lidar_samples shape:', lidar_samples.shape)\n",
        "print('labels shape:', labels.shape)"
      ],
      "metadata": {
        "id": "6kbQtgR2DSsT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "a284b4a7-cd15-45e6-817b-c447af659a26"
      },
      "id": "6kbQtgR2DSsT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0909d31a9aa6>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# If the class count is less than the required samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mclass_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mclass_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0;31m# Append the patch and its label to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mhsi_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhsi_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1  Samples Extraction"
      ],
      "metadata": {
        "id": "8D-fNrGQIQFT"
      },
      "id": "8D-fNrGQIQFT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Training samples extraction"
      ],
      "metadata": {
        "id": "ifIwfC-C9ON9"
      },
      "id": "ifIwfC-C9ON9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training_samples_dict based on class_dict\n",
        "training_samples_dict = {class_num: class_info[\"training\"] for class_num, class_info in class_dict.items()}\n",
        "\n",
        "# Assuming `hsi_samples`, `lidar_samples`, and `labels` have been previously defined\n",
        "# Convert the list of patches and labels into arrays if they aren't already\n",
        "hsi_samples = np.array(hsi_samples)\n",
        "lidar_samples = np.array(lidar_samples)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Create lists to store training and test samples and labels\n",
        "hsi_training_samples, lidar_training_samples, training_labels = [], [], []\n",
        "hsi_test_samples, lidar_test_samples, test_labels = [], [], []\n",
        "\n",
        "# Split samples into training and test sets based on the desired number of training samples\n",
        "for label, train_samples in training_samples_dict.items():\n",
        "    # Get indices of the current class\n",
        "    class_indices = np.where(labels == label)[0]\n",
        "\n",
        "    # Randomly shuffle the indices\n",
        "    np.random.shuffle(class_indices)\n",
        "\n",
        "    # Split the indices into training and test set indices\n",
        "    train_indices = class_indices[:train_samples]\n",
        "    test_indices = class_indices[train_samples:]\n",
        "\n",
        "    # Add training samples and labels for the current class\n",
        "    hsi_training_samples.extend(hsi_samples[train_indices])\n",
        "    lidar_training_samples.extend(lidar_samples[train_indices])\n",
        "    training_labels.extend(labels[train_indices])\n",
        "\n",
        "    # Add test samples and labels for the current class\n",
        "    hsi_test_samples.extend(hsi_samples[test_indices])\n",
        "    lidar_test_samples.extend(lidar_samples[test_indices])\n",
        "    test_labels.extend(labels[test_indices])\n",
        "\n",
        "# Convert lists back to numpy arrays\n",
        "hsi_training_samples = np.array(hsi_training_samples)\n",
        "lidar_training_samples = np.array(lidar_training_samples)\n",
        "training_labels = np.array(training_labels)\n",
        "\n",
        "hsi_test_samples = np.array(hsi_test_samples)\n",
        "lidar_test_samples = np.array(lidar_test_samples)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Print shapes to verify\n",
        "print('hsi_training_samples shape:', hsi_training_samples.shape)\n",
        "print('lidar_training_samples shape:', lidar_training_samples.shape)\n",
        "print('training_labels shape:', training_labels.shape)\n",
        "\n",
        "print('hsi_test_samples shape:', hsi_test_samples.shape)\n",
        "print('lidar_test_samples shape:', lidar_test_samples.shape)\n",
        "print('test_labels shape:', test_labels.shape)"
      ],
      "metadata": {
        "id": "uap9kqCRHTfW"
      },
      "id": "uap9kqCRHTfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Augmented training samples"
      ],
      "metadata": {
        "id": "nl8xDxUvG8CO"
      },
      "id": "nl8xDxUvG8CO"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "def augment_training_data(hsi_training_data, lidar_training_data, training_labels, rotations=[45, 90, 135], flip_up_down=True, flip_left_right=True):\n",
        "    augmented_hsi = []\n",
        "    augmented_lidar = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for hsi, lidar, label in zip(hsi_training_data, lidar_training_data, training_labels):\n",
        "        # Original data\n",
        "        augmented_hsi.append(hsi)\n",
        "        augmented_lidar.append(lidar)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        # Rotations\n",
        "        for angle in rotations:\n",
        "            hsi_rotated = rotate(hsi, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "            lidar_rotated = rotate(lidar, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "\n",
        "            augmented_hsi.append(hsi_rotated)\n",
        "            augmented_lidar.append(lidar_rotated)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip up-down\n",
        "        if flip_up_down:\n",
        "            hsi_flipped_ud = np.flipud(hsi)\n",
        "            lidar_flipped_ud = np.flipud(lidar)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_ud)\n",
        "            augmented_lidar.append(lidar_flipped_ud)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip left-right\n",
        "        if flip_left_right:\n",
        "            hsi_flipped_lr = np.fliplr(hsi)\n",
        "            lidar_flipped_lr = np.fliplr(lidar)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_lr)\n",
        "            augmented_lidar.append(lidar_flipped_lr)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    return np.array(augmented_hsi), np.array(augmented_lidar), np.array(augmented_labels)\n",
        "\n",
        "# Augmenting the training samples\n",
        "augmented_hsi_training_samples, augmented_lidar_training_samples, augmented_training_labels = augment_training_data(hsi_training_samples, lidar_training_samples, training_labels)\n",
        "\n",
        "# Print shapes to verify the augmented training data\n",
        "print('Augmented HSI training samples shape:', augmented_hsi_training_samples.shape)\n",
        "print('Augmented LiDAR training samples shape:', augmented_lidar_training_samples.shape)\n",
        "print('Augmented training labels shape:', augmented_training_labels.shape)\n"
      ],
      "metadata": {
        "id": "bfyUlRo8G_lg"
      },
      "id": "bfyUlRo8G_lg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.0 Model Building"
      ],
      "metadata": {
        "id": "J7Ko9uYgIf2D"
      },
      "id": "J7Ko9uYgIf2D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Configuration"
      ],
      "metadata": {
        "id": "u1rQcEYmVP_W"
      },
      "id": "u1rQcEYmVP_W"
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Configuration\n",
        "class Config:\n",
        "    def __init__(self,in_channels,num_patches,kernel_size,patch_size,emb_size, dim,depth,heads,dim_head,mlp_dim,num_classes,dropout,pos_emb_size,class_emb_size,stride, ):\n",
        "        self.in_channels = in_channels\n",
        "        self.num_patches = num_patches\n",
        "        self.kernel_size = kernel_size\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "        self.pos_emb_size = pos_emb_size\n",
        "        self.class_emb_size = class_emb_size\n",
        "        self.stride = stride\n"
      ],
      "metadata": {
        "id": "35qAdvCxIPJ8"
      },
      "id": "35qAdvCxIPJ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 EmbeddingPatches"
      ],
      "metadata": {
        "id": "U5T_YmiQespq"
      },
      "id": "U5T_YmiQespq"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(\n",
        "            config.in_channels,\n",
        "            config.emb_size,\n",
        "            kernel_size=config.patch_size,\n",
        "            stride=config.stride,\n",
        "        )\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, config.num_patches + 1, config.emb_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.emb_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n",
        "        x = x + self.pos_embedding[:, :x.size(1)]\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "H0MaMR75xcKE"
      },
      "id": "H0MaMR75xcKE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Optianl Adding bandoutput"
      ],
      "metadata": {
        "id": "7DyoAhpwF1WO"
      },
      "id": "7DyoAhpwF1WO"
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, lidar_config, hsi_config):\n",
        "        super(CrossAttention, self).__init__()\n",
        "\n",
        "        # Define module parameters\n",
        "        self.dim_head = lidar_config.dim_head\n",
        "        self.num_patches=hsi_config.num_patches\n",
        "        self.num_heads = lidar_config.heads\n",
        "        self.sqrt_dim_head = math.sqrt(self.dim_head)\n",
        "\n",
        "        # Define linear layers for transforming Q, K, and V\n",
        "        self.to_q = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "        self.to_k = nn.Linear(hsi_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "        self.to_v = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "\n",
        "        self.to_out = nn.Linear( self.num_heads* self.dim_head,self.num_patches )  # added\n",
        "\n",
        "\n",
        "    def forward(self, lidar, hsi):\n",
        "        B, N_lidar, _ = lidar.size()\n",
        "        _, N_hsi, _ = hsi.size()\n",
        "\n",
        "        outputs = []\n",
        "        attn_scores = []  # List to store attention scores\n",
        "\n",
        "       # Iterate over lidar and hsi patches\n",
        "        for i in range(1, N_lidar):\n",
        "        #for i in range(N_lidar):\n",
        "\n",
        "            lidar_patch = lidar[:, i].unsqueeze(1)  # Add a dimension for number of patches\n",
        "            for j in range(1, N_hsi):\n",
        "            #for j in range(N_hsi):\n",
        "\n",
        "                hsi_patch = hsi[:, j].unsqueeze(1)  # Add a dimension for number of patches\n",
        "                Q = self.to_q(lidar_patch)\n",
        "                K = self.to_k(hsi_patch)\n",
        "                V = self.to_v(lidar_patch)\n",
        "\n",
        "                Q = Q / self.sqrt_dim_head\n",
        "                attn_weights = F.softmax(Q @ K.transpose(-2, -1), dim=-1)\n",
        "\n",
        "                attn_output = attn_weights @ V\n",
        "                attn_score = self.to_out(attn_output)  # added\n",
        "                outputs.append(attn_output)\n",
        "                attn_scores.append(attn_score)  # Store the attention scores\n",
        "\n",
        "         # Concatenate all the outputs\n",
        "        output = torch.cat(outputs, dim=1)\n",
        "        attn_scores = torch.cat(attn_scores, dim=1)  # Concatenate all the attention scores\n",
        "\n",
        "        return output, attn_scores  # Return both output and attention scores\n"
      ],
      "metadata": {
        "id": "7ZwPxVLX4uwk"
      },
      "id": "7ZwPxVLX4uwk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionModel(nn.Module):\n",
        "    def __init__(self, hsi_config, lidar_config):\n",
        "        super().__init__()\n",
        "        self.hsi_patch_embedding = PatchEmbedding(hsi_config)\n",
        "        self.lidar_patch_embedding = PatchEmbedding(lidar_config)\n",
        "        self.cross_attention = CrossAttention(lidar_config, hsi_config)\n",
        "\n",
        "    def forward(self, lidar_data, hsi_data):\n",
        "        # Apply PatchEmbedding\n",
        "        lidar_emb = self.lidar_patch_embedding(lidar_data)\n",
        "        hsi_emb = self.hsi_patch_embedding(hsi_data)\n",
        "\n",
        "        # Apply CrossAttention\n",
        "        output, attn_scores = self.cross_attention(lidar_emb, hsi_emb)\n",
        "\n",
        "        return output, attn_scores  # Return both output and attention scores\n"
      ],
      "metadata": {
        "id": "at2xTwGv91ng"
      },
      "id": "at2xTwGv91ng",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1.1 Parameters Setting\n",
        "# Hsi configuration\n",
        "hsi_config = Config(\n",
        "    in_channels=25,  # Each sample covers 144 bands\n",
        "    num_patches=144,  # 25*1*144 bands are grouped into 3600 groups\n",
        "    kernel_size=1,  # Adjusted to match new patch size\n",
        "    patch_size=1,  # Adjusted to match new patch size (5*5, 144/24=6)\n",
        "    emb_size=128,  # Embedding size, this can be changed\n",
        "    dim=128,  # Dimension of the transformer, this can be changed\n",
        "    depth=6,  # Number of transformer layers, this can be changed\n",
        "    heads=8,  # Number of attention heads, this can be changed\n",
        "    dim_head=64,  # Dimension of each attention head, this can be changed\n",
        "    mlp_dim=256,  # Dimension of the MLP layer, this can be changed\n",
        "    num_classes=15,  # Number of classes, this can be changed\n",
        "    dropout=0.4,  # Dropout rate, this can be changed\n",
        "    pos_emb_size=128,  # Position embedding size, this can be changed\n",
        "    class_emb_size=128,  # Class embedding size, this can be changed\n",
        "    stride=1  # Stride for the convolution, this can be changed\n",
        ")\n",
        "\n",
        "\n",
        "# Lidara configuration\n",
        "lidar_config = Config(\n",
        "    in_channels=25,  # lidar group has 1 channels\n",
        "    num_patches=1,  # 1 band for Lidar\n",
        "    kernel_size=1,  # Adjusted to match new patch size\n",
        "    patch_size=1, # Adjusted to match new patch size\n",
        "    emb_size=128,  # Embedding size, this can be changed\n",
        "    dim=128,  # Dimension of the transformer, this can be changed\n",
        "    depth=6,  # Number of transformer layers, this can be changed\n",
        "    heads=8,  # Number of attention heads, this can be changed\n",
        "    dim_head=64,  # Dimension of each attention head, this can be changed\n",
        "    mlp_dim=256,  # Dimension of the MLP layer, this can be changed\n",
        "    num_classes=15,  # Number of classes, this can be changed\n",
        "    dropout=0.4,  # Dropout rate, this can be changed\n",
        "    pos_emb_size=128,  # Position embedding size, this can be changed\n",
        "    class_emb_size=128,  # Class embedding size, this can be changed\n",
        "    stride=1  # Stride for the convolution, this can be changed\n",
        ")\n"
      ],
      "metadata": {
        "id": "b8oC00YgYsSS"
      },
      "id": "b8oC00YgYsSS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch = training_hsi_samples  # Shape: (2832, 5, 5, 144)\n",
        "lidar_batch = training_lidar_samples  # Shape: (2832, 5, 5, 1)\n",
        "print('hsi_batch sahpe before transpose:', hsi_batch.shape)\n",
        "print('lidar_batch sahpe before transpose:', lidar_batch.shape)\n",
        "# Transpose the data to have the channel dimension at the correct place\n",
        "hsi_batch = hsi_batch.transpose(0, 3, 1, 2)  # New shape: (2832, 144, 5, 5)\n",
        "lidar_batch = lidar_batch.transpose(0, 3, 1, 2)  # New shape: (2832, 1, 5, 5)\n",
        "print('hsi_batch sahpe after transpose:', hsi_batch.shape)\n",
        "print('lidar_batch shape after transpose:', lidar_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_WjbDjBeODf",
        "outputId": "75c38836-91e9-4f32-d877-9c75d98908f0"
      },
      "id": "b_WjbDjBeODf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch sahpe before transpose: (2832, 5, 5, 144)\n",
            "lidar_batch sahpe before transpose: (2832, 5, 5, 1)\n",
            "hsi_batch sahpe after transpose: (2832, 144, 5, 5)\n",
            "lidar_batch shape after transpose: (2832, 1, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "one_hsi_batch = training_hsi_samples[:1]  # Shape: (1, 5, 5, 144)\n",
        "one_lidar_batch = training_lidar_samples[:1]  # Shape: (1, 5, 5, 1)\n",
        "print('one_hsi_batch sahpe before transpose:', one_hsi_batch.shape)\n",
        "print('one_lidar_batch sahpe before transpose:', one_lidar_batch.shape)\n",
        "# Transpose the data to have the channel dimension at the correct place\n",
        "one_hsi_batch = one_hsi_batch.transpose(0, 3, 1, 2)  # New shape: (1, 144, 5, 5)\n",
        "one_lidar_batch = one_lidar_batch.transpose(0, 3, 1, 2)  # New shape: (1, 1, 5, 5)\n",
        "print('hsi_batch sahpe after transpose:', one_hsi_batch.shape)\n",
        "print('lidar_batch shape after transpose:', one_lidar_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWEESEK3a-hX",
        "outputId": "bca8fc48-633c-4810-b784-4558bf9a1e2e"
      },
      "id": "GWEESEK3a-hX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one_hsi_batch sahpe before transpose: (1, 5, 5, 144)\n",
            "one_lidar_batch sahpe before transpose: (1, 5, 5, 1)\n",
            "hsi_batch sahpe after transpose: (1, 144, 5, 5)\n",
            "lidar_batch shape after transpose: (1, 1, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "one_hsi_batch = training_hsi_samples[:1]  # Shape: (1, 5, 5, 144)\n",
        "one_lidar_batch = training_lidar_samples[:1]  # Shape: (1, 5, 5, 1)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "one_hsi_batch_flat = torch.from_numpy(one_hsi_batch.astype(np.float32).reshape(1, 25, 144, 1)).to(device)\n",
        "one_lidar_batch_flat = torch.from_numpy(one_lidar_batch.astype(np.float32).reshape(1, 25, 1, 1)).to(device)\n",
        "\n",
        "# Initialize the patch embedding module\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "one_hsi_batch_embedded = hsi_patch_embedding(one_hsi_batch_flat).to(device)\n",
        "one_lidar_batch_embedded = lidar_patch_embedding(one_lidar_batch_flat).to(device)\n",
        "\n",
        "print('one_hsi_batch_embedded shape:', one_hsi_batch_embedded.shape)\n",
        "print('one_lidar_batch_embedded shape:', one_lidar_batch_embedded.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjXtH2S9Q81f",
        "outputId": "c6304643-1748-467b-8bcd-52efb9955f1c"
      },
      "id": "qjXtH2S9Q81f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one_hsi_batch_embedded shape: torch.Size([1, 145, 128])\n",
            "one_lidar_batch_embedded shape: torch.Size([1, 2, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intialisation CrossAttention Class"
      ],
      "metadata": {
        "id": "gpa3oU5ndrhn"
      },
      "id": "gpa3oU5ndrhn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch_6 = training_hsi_samples[:6]  # Shape: (6, 5, 5, 144)\n",
        "lidar_batch_6 = training_lidar_samples[:6]  # Shape: (6, 5, 5, 1)\n",
        "\n",
        "print(\"hsi_batch_6  shape:\", hsi_batch_6 .shape)\n",
        "print(\"lidar_batch_6 shape:\", lidar_batch_6.shape)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "hsi_batch_6_flat = torch.from_numpy(hsi_batch_6.astype(np.float32).reshape(6, 25, 144, 1))\n",
        "lidar_batch_6_flat = torch.from_numpy(lidar_batch_6.astype(np.float32).reshape(6, 25, 1, 1))\n",
        "\n",
        "print(\"hsi_batch_6_flat  shape:\", hsi_batch_6_flat .shape)\n",
        "print(\"lidar_batch_6_flat shape:\", lidar_batch_6_flat.shape)\n",
        "\n",
        "hsi_batch_6_flat = hsi_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_6_flat = lidar_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "hsi_batch_6_embedded  = hsi_patch_embedding(hsi_batch_6_flat)\n",
        "lidar_batch_6_embedded = lidar_patch_embedding(lidar_batch_6_flat)\n",
        "\n",
        "device = torch.device(\"cuda:0\")  # Define the device (GPU)\n",
        "\n",
        "# Move the tensors to the desired device\n",
        "hsi_batch_6_embedded = hsi_batch_6_embedded.to(device)\n",
        "lidar_batch_6_embedded = lidar_batch_6_embedded.to(device)\n",
        "print(\"hsi_batch_6_embedded  shape:\", hsi_batch_6_embedded .shape)\n",
        "print(\"lidar_batch_6_embedded shape:\", lidar_batch_6_embedded.shape)\n",
        "\n",
        "# Define the dimension of the model and the number of heads\n",
        "d_model = hsi_config.emb_size  # the output dimension of PatchEmbedding\n",
        "num_heads = hsi_config.heads  # the number of attention heads in the transformer\n",
        "\n",
        "# Initialize CrossAttention module\n",
        "cross_attention = CrossAttention(lidar_config, hsi_config).to(device)\n",
        "\n",
        "# Apply the cross attention\n",
        "output,attn_scores = cross_attention(lidar_batch_6_embedded, hsi_batch_6_embedded)\n",
        "\n",
        "\n",
        "print(\"Cross attention output shape:\", attn_scores.shape)\n",
        "print(\"Output shape:\", output.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYlhrA99ncUq",
        "outputId": "d6f95d96-0c40-4120-96d2-9eed2b10b5d9"
      },
      "id": "XYlhrA99ncUq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch_6  shape: (6, 5, 5, 144)\n",
            "lidar_batch_6 shape: (6, 5, 5, 1)\n",
            "hsi_batch_6_flat  shape: torch.Size([6, 25, 144, 1])\n",
            "lidar_batch_6_flat shape: torch.Size([6, 25, 1, 1])\n",
            "hsi_batch_6_embedded  shape: torch.Size([6, 145, 128])\n",
            "lidar_batch_6_embedded shape: torch.Size([6, 2, 128])\n",
            "Cross attention output shape: torch.Size([6, 144, 144])\n",
            "Output shape: torch.Size([6, 144, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cross_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDySwOVzY7AE",
        "outputId": "f04c1c8f-2de1-4ec2-f26a-677ca34e38c0"
      },
      "id": "dDySwOVzY7AE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossAttention(\n",
              "  (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_out): Linear(in_features=512, out_features=144, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJTi3xzCYzd1",
        "outputId": "c96fd7da-874b-4448-ddcd-35a789fea053"
      },
      "id": "nJTi3xzCYzd1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossAttentionModel(\n",
              "  (hsi_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(25, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (lidar_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(25, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (cross_attention): CrossAttention(\n",
              "    (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_out): Linear(in_features=512, out_features=144, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1  Training DataLoader for Cross Attention Module"
      ],
      "metadata": {
        "id": "yem48RAvJSLH"
      },
      "id": "yem48RAvJSLH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ba361a-2273-46b1-fe3d-c2d8e0f23401",
        "id": "m4OFRwz9z71C"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training_hsi_samples shape: (2832, 5, 5, 144)\n",
            "training_lidar_samples shape: (2832, 5, 5, 1)\n",
            "training_labels shape: (2832,)\n"
          ]
        }
      ],
      "source": [
        "# 2.5 Training samples extraction\n",
        "# The desired number of samples for each class\n",
        "training_samples = [198, 190, 192, 188, 186, 182, 196, 191, 193, 191, 181, 192, 184, 181, 187]\n",
        "training_samples_dict = {i+1: size for i, size in enumerate(training_samples)}\n",
        "\n",
        "# Initialize a dictionary to store class count\n",
        "class_count = {i: 0 for i in class_dict.keys()}\n",
        "\n",
        "# Function to check if all classes have the required number of samples\n",
        "def all_classes_completed(class_count, training_samples_dict):\n",
        "    return all(class_count[class_num] == training_samples_dict[class_num] for class_num in class_dict.keys())\n",
        "\n",
        "# Create an empty list to store patches and labels\n",
        "hsi_samples = []\n",
        "lidar_samples = []\n",
        "labels = []\n",
        "\n",
        "while not all_classes_completed(class_count, training_samples_dict):\n",
        "    # Loop through the ground truth data\n",
        "    for label in class_dict.keys():\n",
        "        # Get the coordinates of the ground truth pixels\n",
        "        coords = np.argwhere((gt_2013_data == label) & (mask > 0))\n",
        "\n",
        "        # Shuffle the coordinates to randomize the patch extraction\n",
        "        np.random.shuffle(coords)\n",
        "\n",
        "        for coord in coords:\n",
        "            i, j = coord\n",
        "            # Calculate the patch indices\n",
        "            i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
        "            j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
        "\n",
        "            # Check if the indices are within the bounds of the HSI data\n",
        "            if i_start >= 0 and i_end <= hsi_2013_data.shape[0] and j_start >= 0 and j_end <= hsi_2013_data.shape[1]:\n",
        "                # Extract the patch\n",
        "                hsi_patch = hsi_2013_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # Extract the LiDAR patch\n",
        "                lidar_patch = lidar_2013_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # If the class count is less than the required samples\n",
        "                if class_count[label] < training_samples_dict[label]:\n",
        "                    # Append the patch and its label to the list\n",
        "                    hsi_samples.append(hsi_patch)\n",
        "                    lidar_samples.append(lidar_patch)\n",
        "                    labels.append(label)\n",
        "                    class_count[label] += 1\n",
        "\n",
        "                    # If all classes have the required number of samples, exit the loop\n",
        "                    if all_classes_completed(class_count, training_samples_dict):\n",
        "                        break\n",
        "\n",
        "training_hsi_samples = np.array(hsi_samples)\n",
        "training_lidar_samples = np.array(lidar_samples)\n",
        "training_labels = np.array(labels)\n",
        "print('training_hsi_samples shape:', training_hsi_samples.shape)\n",
        "print('training_lidar_samples shape:', training_lidar_samples.shape)\n",
        "print('training_labels shape:', training_labels.shape)"
      ],
      "id": "m4OFRwz9z71C"
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperspectralDataset(Dataset):\n",
        "    def __init__(self, hsi_samples, lidar_samples, labels):\n",
        "        self.hsi_samples = hsi_samples\n",
        "        self.lidar_samples = lidar_samples\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hsi_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hsi_patch = self.hsi_samples[idx].float().to(device)\n",
        "        lidar_patch = self.lidar_samples[idx].float().to(device)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert label to a tensor and reshape to match the model's output shape\n",
        "        label = torch.tensor(label).repeat(hsi_patch.shape[1], 1).to(device)\n",
        "\n",
        "        return hsi_patch, lidar_patch, label\n"
      ],
      "metadata": {
        "id": "1Sw5JbJ23X3n"
      },
      "id": "1Sw5JbJ23X3n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_tensor = torch.tensor(training_labels)\n",
        "labels_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "print('labels_tensor:',labels_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-1Ahw-bTP9T",
        "outputId": "fcb06b51-d50b-4f6b-e9aa-64ecf154ce57"
      },
      "id": "V-1Ahw-bTP9T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels_tensor: torch.Size([2832])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = torch.unique(labels_tensor)\n",
        "print(\"Unique classes:\", unique_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-72dBlA1TbBj",
        "outputId": "2d0df72f-dcd9-4e6d-e8d9-12d05650dd80"
      },
      "id": "-72dBlA1TbBj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Resahpe the data\n",
        "\n",
        "# Move the input data to the desired device\n",
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch = training_hsi_samples  # Shape: (2832, 5, 5, 144)\n",
        "lidar_batch= training_lidar_samples # Shape: (2832, 5, 5, 1)\n",
        "label_batch=training_labels\n",
        "print(\"hsi_batch shape:\", hsi_batch.shape)\n",
        "print(\"lidar_batch shape:\", lidar_batch.shape)\n",
        "print(\"label_batch shape:\", label_batch.shape)\n",
        "\n",
        "## Reshape the data\n",
        "hsi_batch_flat = torch.from_numpy(hsi_batch.astype(np.float32).reshape(2832, 25, 144, 1))\n",
        "lidar_batch_flat = torch.from_numpy(lidar_batch.astype(np.float32).reshape(2832, 25, 1, 1))\n",
        "\n",
        "print(\"hsi_batch shape:\", hsi_batch_flat .shape)\n",
        "print(\"lidar_batch shape:\", lidar_batch_flat.shape)\n",
        "\n",
        "hsi_batch_flat = hsi_batch_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_flat = lidar_batch_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "# Split into train and test\n",
        "hsi_samples_train, hsi_samples_test, lidar_samples_train, lidar_samples_test, labels_train, labels_test = train_test_split(\n",
        "    hsi_batch_flat, lidar_batch_flat, label_batch, test_size=0.1, random_state=42)\n",
        "print('labels_train shape:',labels_train.shape)\n",
        "\n",
        "# Split train into train and validation\n",
        "hsi_samples_train, hsi_samples_val, lidar_samples_train, lidar_samples_val, labels_train, labels_val = train_test_split(\n",
        "    hsi_samples_train, lidar_samples_train, labels_train, test_size=0.60, random_state=42)\n",
        "print('labels_train2 shape:',labels_train.shape)\n",
        "\n",
        "# Now you have training, validation, and test data.\n",
        "# Create Datasets\n",
        "train_dataset = HyperspectralDataset(hsi_samples_train, lidar_samples_train, labels_train)\n",
        "val_dataset = HyperspectralDataset(hsi_samples_val, lidar_samples_val, labels_val)\n",
        "test_dataset = HyperspectralDataset(hsi_samples_test, lidar_samples_test, labels_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CO7xDsCfcjDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3444de-4667-4613-d5b8-6d76509b4883"
      },
      "id": "CO7xDsCfcjDe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch shape: (2832, 5, 5, 144)\n",
            "lidar_batch shape: (2832, 5, 5, 1)\n",
            "label_batch shape: (2832,)\n",
            "hsi_batch shape: torch.Size([2832, 25, 144, 1])\n",
            "lidar_batch shape: torch.Size([2832, 25, 1, 1])\n",
            "labels_train shape: (2548,)\n",
            "labels_train2 shape: (1019,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Training CorssAttentionMode ltraining"
      ],
      "metadata": {
        "id": "_MBeu13kDLc6"
      },
      "id": "_MBeu13kDLc6"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create the data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1.00) #loss 2.3866\n",
        "\n",
        "# Initialize the best validation loss and best_model_wts before the training loop\n",
        "best_val_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "# Instantiate the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize the lowest_loss and best_model_wts before the training loop\n",
        "lowest_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (hsi_batch, lidar_batch, label_batch) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        hsi_batch = hsi_batch.to(device)\n",
        "        lidar_batch = lidar_batch.to(device)\n",
        "        label_batch = label_batch.to(device)  # Reshape labels\n",
        "        # print('hsi_batch shape:', hsi_batch.shape)\n",
        "        # print('lidar_batch:', lidar_batch.shape)\n",
        "        # print('label_batch shape:', label_batch.shape)\n",
        "\n",
        "        # Forward pass\n",
        "        output,attn_scores  = model(lidar_batch, hsi_batch)\n",
        "        #print('output shape:', output.shape)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.transpose(1, 2), label_batch.squeeze(2))\n",
        "\n",
        "        # Print loss every 10 batches\n",
        "        #if (batch_idx + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "        # If the loss is lower than the current lowest, save the model's state\n",
        "        if loss.item() < lowest_loss:\n",
        "            lowest_loss = loss.item()\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "# Print finishing training\n",
        "print('Finishing training')\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_wts, 'best_model_weights.pth')\n"
      ],
      "metadata": {
        "id": "w2D4wFzJ5ams",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f057e62-b3ca-4c39-d3a8-fa1fd9b14987"
      },
      "id": "w2D4wFzJ5ams",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Batch [1/32], Loss: 13.3595\n",
            "Epoch [1/100], Batch [2/32], Loss: 16.4099\n",
            "Epoch [1/100], Batch [3/32], Loss: 15.5711\n",
            "Epoch [1/100], Batch [4/32], Loss: 14.2605\n",
            "Epoch [1/100], Batch [5/32], Loss: 14.4543\n",
            "Epoch [1/100], Batch [6/32], Loss: 13.0786\n",
            "Epoch [1/100], Batch [7/32], Loss: 13.1088\n",
            "Epoch [1/100], Batch [8/32], Loss: 12.6681\n",
            "Epoch [1/100], Batch [9/32], Loss: 11.5521\n",
            "Epoch [1/100], Batch [10/32], Loss: 13.9038\n",
            "Epoch [1/100], Batch [11/32], Loss: 11.8375\n",
            "Epoch [1/100], Batch [12/32], Loss: 12.4341\n",
            "Epoch [1/100], Batch [13/32], Loss: 10.0448\n",
            "Epoch [1/100], Batch [14/32], Loss: 10.0942\n",
            "Epoch [1/100], Batch [15/32], Loss: 11.1172\n",
            "Epoch [1/100], Batch [16/32], Loss: 12.2169\n",
            "Epoch [1/100], Batch [17/32], Loss: 10.8048\n",
            "Epoch [1/100], Batch [18/32], Loss: 9.7726\n",
            "Epoch [1/100], Batch [19/32], Loss: 10.0019\n",
            "Epoch [1/100], Batch [20/32], Loss: 9.5897\n",
            "Epoch [1/100], Batch [21/32], Loss: 8.2813\n",
            "Epoch [1/100], Batch [22/32], Loss: 9.1455\n",
            "Epoch [1/100], Batch [23/32], Loss: 11.0644\n",
            "Epoch [1/100], Batch [24/32], Loss: 8.9259\n",
            "Epoch [1/100], Batch [25/32], Loss: 9.9950\n",
            "Epoch [1/100], Batch [26/32], Loss: 10.1250\n",
            "Epoch [1/100], Batch [27/32], Loss: 9.4085\n",
            "Epoch [1/100], Batch [28/32], Loss: 8.3543\n",
            "Epoch [1/100], Batch [29/32], Loss: 9.2827\n",
            "Epoch [1/100], Batch [30/32], Loss: 9.1529\n",
            "Epoch [1/100], Batch [31/32], Loss: 6.5660\n",
            "Epoch [1/100], Batch [32/32], Loss: 7.2505\n",
            "Epoch [2/100], Batch [1/32], Loss: 8.6205\n",
            "Epoch [2/100], Batch [2/32], Loss: 7.4061\n",
            "Epoch [2/100], Batch [3/32], Loss: 5.9805\n",
            "Epoch [2/100], Batch [4/32], Loss: 7.3869\n",
            "Epoch [2/100], Batch [5/32], Loss: 7.5100\n",
            "Epoch [2/100], Batch [6/32], Loss: 4.9680\n",
            "Epoch [2/100], Batch [7/32], Loss: 7.5049\n",
            "Epoch [2/100], Batch [8/32], Loss: 6.8307\n",
            "Epoch [2/100], Batch [9/32], Loss: 6.5536\n",
            "Epoch [2/100], Batch [10/32], Loss: 6.4341\n",
            "Epoch [2/100], Batch [11/32], Loss: 4.3650\n",
            "Epoch [2/100], Batch [12/32], Loss: 6.3825\n",
            "Epoch [2/100], Batch [13/32], Loss: 6.7946\n",
            "Epoch [2/100], Batch [14/32], Loss: 7.2170\n",
            "Epoch [2/100], Batch [15/32], Loss: 6.9935\n",
            "Epoch [2/100], Batch [16/32], Loss: 5.1093\n",
            "Epoch [2/100], Batch [17/32], Loss: 5.6070\n",
            "Epoch [2/100], Batch [18/32], Loss: 5.1750\n",
            "Epoch [2/100], Batch [19/32], Loss: 5.1378\n",
            "Epoch [2/100], Batch [20/32], Loss: 5.4545\n",
            "Epoch [2/100], Batch [21/32], Loss: 5.1728\n",
            "Epoch [2/100], Batch [22/32], Loss: 4.2168\n",
            "Epoch [2/100], Batch [23/32], Loss: 3.8416\n",
            "Epoch [2/100], Batch [24/32], Loss: 4.5553\n",
            "Epoch [2/100], Batch [25/32], Loss: 4.9890\n",
            "Epoch [2/100], Batch [26/32], Loss: 5.1695\n",
            "Epoch [2/100], Batch [27/32], Loss: 4.8475\n",
            "Epoch [2/100], Batch [28/32], Loss: 4.9223\n",
            "Epoch [2/100], Batch [29/32], Loss: 4.2952\n",
            "Epoch [2/100], Batch [30/32], Loss: 3.7848\n",
            "Epoch [2/100], Batch [31/32], Loss: 5.2268\n",
            "Epoch [2/100], Batch [32/32], Loss: 3.7465\n",
            "Epoch [3/100], Batch [1/32], Loss: 4.1684\n",
            "Epoch [3/100], Batch [2/32], Loss: 4.5452\n",
            "Epoch [3/100], Batch [3/32], Loss: 4.2843\n",
            "Epoch [3/100], Batch [4/32], Loss: 4.3435\n",
            "Epoch [3/100], Batch [5/32], Loss: 3.9638\n",
            "Epoch [3/100], Batch [6/32], Loss: 4.0593\n",
            "Epoch [3/100], Batch [7/32], Loss: 3.7744\n",
            "Epoch [3/100], Batch [8/32], Loss: 4.3832\n",
            "Epoch [3/100], Batch [9/32], Loss: 3.2782\n",
            "Epoch [3/100], Batch [10/32], Loss: 4.3519\n",
            "Epoch [3/100], Batch [11/32], Loss: 4.3853\n",
            "Epoch [3/100], Batch [12/32], Loss: 4.4270\n",
            "Epoch [3/100], Batch [13/32], Loss: 3.3954\n",
            "Epoch [3/100], Batch [14/32], Loss: 4.4418\n",
            "Epoch [3/100], Batch [15/32], Loss: 3.6852\n",
            "Epoch [3/100], Batch [16/32], Loss: 3.9191\n",
            "Epoch [3/100], Batch [17/32], Loss: 4.5237\n",
            "Epoch [3/100], Batch [18/32], Loss: 3.6500\n",
            "Epoch [3/100], Batch [19/32], Loss: 4.1272\n",
            "Epoch [3/100], Batch [20/32], Loss: 4.0009\n",
            "Epoch [3/100], Batch [21/32], Loss: 3.7967\n",
            "Epoch [3/100], Batch [22/32], Loss: 3.5833\n",
            "Epoch [3/100], Batch [23/32], Loss: 3.8648\n",
            "Epoch [3/100], Batch [24/32], Loss: 3.5008\n",
            "Epoch [3/100], Batch [25/32], Loss: 3.6924\n",
            "Epoch [3/100], Batch [26/32], Loss: 3.3415\n",
            "Epoch [3/100], Batch [27/32], Loss: 3.4701\n",
            "Epoch [3/100], Batch [28/32], Loss: 3.6662\n",
            "Epoch [3/100], Batch [29/32], Loss: 3.3876\n",
            "Epoch [3/100], Batch [30/32], Loss: 3.2937\n",
            "Epoch [3/100], Batch [31/32], Loss: 3.9597\n",
            "Epoch [3/100], Batch [32/32], Loss: 3.1408\n",
            "Epoch [4/100], Batch [1/32], Loss: 3.0272\n",
            "Epoch [4/100], Batch [2/32], Loss: 3.0670\n",
            "Epoch [4/100], Batch [3/32], Loss: 3.4401\n",
            "Epoch [4/100], Batch [4/32], Loss: 3.2120\n",
            "Epoch [4/100], Batch [5/32], Loss: 3.1246\n",
            "Epoch [4/100], Batch [6/32], Loss: 3.0196\n",
            "Epoch [4/100], Batch [7/32], Loss: 3.3089\n",
            "Epoch [4/100], Batch [8/32], Loss: 2.9633\n",
            "Epoch [4/100], Batch [9/32], Loss: 2.9514\n",
            "Epoch [4/100], Batch [10/32], Loss: 2.9212\n",
            "Epoch [4/100], Batch [11/32], Loss: 2.9970\n",
            "Epoch [4/100], Batch [12/32], Loss: 3.2124\n",
            "Epoch [4/100], Batch [13/32], Loss: 3.2247\n",
            "Epoch [4/100], Batch [14/32], Loss: 3.1184\n",
            "Epoch [4/100], Batch [15/32], Loss: 2.9732\n",
            "Epoch [4/100], Batch [16/32], Loss: 2.9292\n",
            "Epoch [4/100], Batch [17/32], Loss: 2.7876\n",
            "Epoch [4/100], Batch [18/32], Loss: 2.8954\n",
            "Epoch [4/100], Batch [19/32], Loss: 2.9027\n",
            "Epoch [4/100], Batch [20/32], Loss: 2.8882\n",
            "Epoch [4/100], Batch [21/32], Loss: 2.8738\n",
            "Epoch [4/100], Batch [22/32], Loss: 2.8728\n",
            "Epoch [4/100], Batch [23/32], Loss: 2.8694\n",
            "Epoch [4/100], Batch [24/32], Loss: 2.9688\n",
            "Epoch [4/100], Batch [25/32], Loss: 2.9580\n",
            "Epoch [4/100], Batch [26/32], Loss: 3.0288\n",
            "Epoch [4/100], Batch [27/32], Loss: 2.8929\n",
            "Epoch [4/100], Batch [28/32], Loss: 2.9597\n",
            "Epoch [4/100], Batch [29/32], Loss: 2.9979\n",
            "Epoch [4/100], Batch [30/32], Loss: 2.9551\n",
            "Epoch [4/100], Batch [31/32], Loss: 2.8975\n",
            "Epoch [4/100], Batch [32/32], Loss: 2.8857\n",
            "Epoch [5/100], Batch [1/32], Loss: 3.0753\n",
            "Epoch [5/100], Batch [2/32], Loss: 2.8262\n",
            "Epoch [5/100], Batch [3/32], Loss: 2.8809\n",
            "Epoch [5/100], Batch [4/32], Loss: 2.7379\n",
            "Epoch [5/100], Batch [5/32], Loss: 2.8631\n",
            "Epoch [5/100], Batch [6/32], Loss: 2.8468\n",
            "Epoch [5/100], Batch [7/32], Loss: 2.7910\n",
            "Epoch [5/100], Batch [8/32], Loss: 2.9097\n",
            "Epoch [5/100], Batch [9/32], Loss: 2.7997\n",
            "Epoch [5/100], Batch [10/32], Loss: 2.8181\n",
            "Epoch [5/100], Batch [11/32], Loss: 2.9884\n",
            "Epoch [5/100], Batch [12/32], Loss: 2.9469\n",
            "Epoch [5/100], Batch [13/32], Loss: 2.9758\n",
            "Epoch [5/100], Batch [14/32], Loss: 2.8845\n",
            "Epoch [5/100], Batch [15/32], Loss: 2.9570\n",
            "Epoch [5/100], Batch [16/32], Loss: 2.7015\n",
            "Epoch [5/100], Batch [17/32], Loss: 2.8049\n",
            "Epoch [5/100], Batch [18/32], Loss: 2.9226\n",
            "Epoch [5/100], Batch [19/32], Loss: 2.7057\n",
            "Epoch [5/100], Batch [20/32], Loss: 2.9736\n",
            "Epoch [5/100], Batch [21/32], Loss: 2.7740\n",
            "Epoch [5/100], Batch [22/32], Loss: 2.9265\n",
            "Epoch [5/100], Batch [23/32], Loss: 2.8487\n",
            "Epoch [5/100], Batch [24/32], Loss: 2.8446\n",
            "Epoch [5/100], Batch [25/32], Loss: 2.8524\n",
            "Epoch [5/100], Batch [26/32], Loss: 2.8119\n",
            "Epoch [5/100], Batch [27/32], Loss: 2.7505\n",
            "Epoch [5/100], Batch [28/32], Loss: 3.0097\n",
            "Epoch [5/100], Batch [29/32], Loss: 2.8627\n",
            "Epoch [5/100], Batch [30/32], Loss: 2.7144\n",
            "Epoch [5/100], Batch [31/32], Loss: 2.6741\n",
            "Epoch [5/100], Batch [32/32], Loss: 2.8155\n",
            "Epoch [6/100], Batch [1/32], Loss: 2.7445\n",
            "Epoch [6/100], Batch [2/32], Loss: 2.7289\n",
            "Epoch [6/100], Batch [3/32], Loss: 2.7120\n",
            "Epoch [6/100], Batch [4/32], Loss: 2.8445\n",
            "Epoch [6/100], Batch [5/32], Loss: 2.8782\n",
            "Epoch [6/100], Batch [6/32], Loss: 2.9735\n",
            "Epoch [6/100], Batch [7/32], Loss: 2.7157\n",
            "Epoch [6/100], Batch [8/32], Loss: 2.7924\n",
            "Epoch [6/100], Batch [9/32], Loss: 2.7682\n",
            "Epoch [6/100], Batch [10/32], Loss: 2.7514\n",
            "Epoch [6/100], Batch [11/32], Loss: 2.9353\n",
            "Epoch [6/100], Batch [12/32], Loss: 2.8540\n",
            "Epoch [6/100], Batch [13/32], Loss: 2.9604\n",
            "Epoch [6/100], Batch [14/32], Loss: 2.8342\n",
            "Epoch [6/100], Batch [15/32], Loss: 2.7791\n",
            "Epoch [6/100], Batch [16/32], Loss: 2.8575\n",
            "Epoch [6/100], Batch [17/32], Loss: 2.8952\n",
            "Epoch [6/100], Batch [18/32], Loss: 2.7442\n",
            "Epoch [6/100], Batch [19/32], Loss: 2.8091\n",
            "Epoch [6/100], Batch [20/32], Loss: 3.0102\n",
            "Epoch [6/100], Batch [21/32], Loss: 2.8373\n",
            "Epoch [6/100], Batch [22/32], Loss: 2.9389\n",
            "Epoch [6/100], Batch [23/32], Loss: 2.8711\n",
            "Epoch [6/100], Batch [24/32], Loss: 2.8143\n",
            "Epoch [6/100], Batch [25/32], Loss: 2.8472\n",
            "Epoch [6/100], Batch [26/32], Loss: 2.8493\n",
            "Epoch [6/100], Batch [27/32], Loss: 2.7561\n",
            "Epoch [6/100], Batch [28/32], Loss: 2.8456\n",
            "Epoch [6/100], Batch [29/32], Loss: 2.8510\n",
            "Epoch [6/100], Batch [30/32], Loss: 2.8150\n",
            "Epoch [6/100], Batch [31/32], Loss: 2.8013\n",
            "Epoch [6/100], Batch [32/32], Loss: 2.8948\n",
            "Epoch [7/100], Batch [1/32], Loss: 2.8067\n",
            "Epoch [7/100], Batch [2/32], Loss: 2.8204\n",
            "Epoch [7/100], Batch [3/32], Loss: 2.7902\n",
            "Epoch [7/100], Batch [4/32], Loss: 2.7974\n",
            "Epoch [7/100], Batch [5/32], Loss: 2.7834\n",
            "Epoch [7/100], Batch [6/32], Loss: 2.9239\n",
            "Epoch [7/100], Batch [7/32], Loss: 2.8302\n",
            "Epoch [7/100], Batch [8/32], Loss: 2.8362\n",
            "Epoch [7/100], Batch [9/32], Loss: 2.8680\n",
            "Epoch [7/100], Batch [10/32], Loss: 2.6510\n",
            "Epoch [7/100], Batch [11/32], Loss: 2.8094\n",
            "Epoch [7/100], Batch [12/32], Loss: 2.8436\n",
            "Epoch [7/100], Batch [13/32], Loss: 2.9824\n",
            "Epoch [7/100], Batch [14/32], Loss: 2.9093\n",
            "Epoch [7/100], Batch [15/32], Loss: 2.7762\n",
            "Epoch [7/100], Batch [16/32], Loss: 2.9858\n",
            "Epoch [7/100], Batch [17/32], Loss: 2.8228\n",
            "Epoch [7/100], Batch [18/32], Loss: 2.8089\n",
            "Epoch [7/100], Batch [19/32], Loss: 2.9821\n",
            "Epoch [7/100], Batch [20/32], Loss: 2.8402\n",
            "Epoch [7/100], Batch [21/32], Loss: 2.7324\n",
            "Epoch [7/100], Batch [22/32], Loss: 2.8045\n",
            "Epoch [7/100], Batch [23/32], Loss: 2.7537\n",
            "Epoch [7/100], Batch [24/32], Loss: 2.8520\n",
            "Epoch [7/100], Batch [25/32], Loss: 2.7327\n",
            "Epoch [7/100], Batch [26/32], Loss: 2.8045\n",
            "Epoch [7/100], Batch [27/32], Loss: 2.8341\n",
            "Epoch [7/100], Batch [28/32], Loss: 2.7742\n",
            "Epoch [7/100], Batch [29/32], Loss: 2.6938\n",
            "Epoch [7/100], Batch [30/32], Loss: 2.8022\n",
            "Epoch [7/100], Batch [31/32], Loss: 2.7936\n",
            "Epoch [7/100], Batch [32/32], Loss: 2.9623\n",
            "Epoch [8/100], Batch [1/32], Loss: 3.0344\n",
            "Epoch [8/100], Batch [2/32], Loss: 2.9114\n",
            "Epoch [8/100], Batch [3/32], Loss: 2.8185\n",
            "Epoch [8/100], Batch [4/32], Loss: 2.7313\n",
            "Epoch [8/100], Batch [5/32], Loss: 2.7573\n",
            "Epoch [8/100], Batch [6/32], Loss: 2.8329\n",
            "Epoch [8/100], Batch [7/32], Loss: 2.8333\n",
            "Epoch [8/100], Batch [8/32], Loss: 2.8137\n",
            "Epoch [8/100], Batch [9/32], Loss: 2.7909\n",
            "Epoch [8/100], Batch [10/32], Loss: 2.8463\n",
            "Epoch [8/100], Batch [11/32], Loss: 2.8142\n",
            "Epoch [8/100], Batch [12/32], Loss: 2.7905\n",
            "Epoch [8/100], Batch [13/32], Loss: 2.6919\n",
            "Epoch [8/100], Batch [14/32], Loss: 2.8622\n",
            "Epoch [8/100], Batch [15/32], Loss: 2.8332\n",
            "Epoch [8/100], Batch [16/32], Loss: 2.7810\n",
            "Epoch [8/100], Batch [17/32], Loss: 2.8661\n",
            "Epoch [8/100], Batch [18/32], Loss: 2.8063\n",
            "Epoch [8/100], Batch [19/32], Loss: 2.7187\n",
            "Epoch [8/100], Batch [20/32], Loss: 2.7556\n",
            "Epoch [8/100], Batch [21/32], Loss: 2.8211\n",
            "Epoch [8/100], Batch [22/32], Loss: 2.9589\n",
            "Epoch [8/100], Batch [23/32], Loss: 2.7739\n",
            "Epoch [8/100], Batch [24/32], Loss: 2.8080\n",
            "Epoch [8/100], Batch [25/32], Loss: 2.7003\n",
            "Epoch [8/100], Batch [26/32], Loss: 2.7720\n",
            "Epoch [8/100], Batch [27/32], Loss: 2.8784\n",
            "Epoch [8/100], Batch [28/32], Loss: 2.8271\n",
            "Epoch [8/100], Batch [29/32], Loss: 2.8170\n",
            "Epoch [8/100], Batch [30/32], Loss: 2.8128\n",
            "Epoch [8/100], Batch [31/32], Loss: 2.6805\n",
            "Epoch [8/100], Batch [32/32], Loss: 2.9030\n",
            "Epoch [9/100], Batch [1/32], Loss: 2.7758\n",
            "Epoch [9/100], Batch [2/32], Loss: 2.7824\n",
            "Epoch [9/100], Batch [3/32], Loss: 2.8115\n",
            "Epoch [9/100], Batch [4/32], Loss: 2.6986\n",
            "Epoch [9/100], Batch [5/32], Loss: 2.8777\n",
            "Epoch [9/100], Batch [6/32], Loss: 2.8371\n",
            "Epoch [9/100], Batch [7/32], Loss: 2.8306\n",
            "Epoch [9/100], Batch [8/32], Loss: 2.8511\n",
            "Epoch [9/100], Batch [9/32], Loss: 2.8650\n",
            "Epoch [9/100], Batch [10/32], Loss: 2.9367\n",
            "Epoch [9/100], Batch [11/32], Loss: 2.7209\n",
            "Epoch [9/100], Batch [12/32], Loss: 2.8912\n",
            "Epoch [9/100], Batch [13/32], Loss: 2.7891\n",
            "Epoch [9/100], Batch [14/32], Loss: 2.7555\n",
            "Epoch [9/100], Batch [15/32], Loss: 2.9178\n",
            "Epoch [9/100], Batch [16/32], Loss: 2.7515\n",
            "Epoch [9/100], Batch [17/32], Loss: 2.6563\n",
            "Epoch [9/100], Batch [18/32], Loss: 2.8975\n",
            "Epoch [9/100], Batch [19/32], Loss: 2.7365\n",
            "Epoch [9/100], Batch [20/32], Loss: 2.8430\n",
            "Epoch [9/100], Batch [21/32], Loss: 2.7693\n",
            "Epoch [9/100], Batch [22/32], Loss: 2.7119\n",
            "Epoch [9/100], Batch [23/32], Loss: 2.7944\n",
            "Epoch [9/100], Batch [24/32], Loss: 2.7887\n",
            "Epoch [9/100], Batch [25/32], Loss: 2.8253\n",
            "Epoch [9/100], Batch [26/32], Loss: 2.8575\n",
            "Epoch [9/100], Batch [27/32], Loss: 2.9119\n",
            "Epoch [9/100], Batch [28/32], Loss: 2.8078\n",
            "Epoch [9/100], Batch [29/32], Loss: 2.7507\n",
            "Epoch [9/100], Batch [30/32], Loss: 2.8147\n",
            "Epoch [9/100], Batch [31/32], Loss: 2.8330\n",
            "Epoch [9/100], Batch [32/32], Loss: 2.8344\n",
            "Epoch [10/100], Batch [1/32], Loss: 2.8969\n",
            "Epoch [10/100], Batch [2/32], Loss: 2.7710\n",
            "Epoch [10/100], Batch [3/32], Loss: 2.8662\n",
            "Epoch [10/100], Batch [4/32], Loss: 2.7956\n",
            "Epoch [10/100], Batch [5/32], Loss: 2.8985\n",
            "Epoch [10/100], Batch [6/32], Loss: 2.8317\n",
            "Epoch [10/100], Batch [7/32], Loss: 2.7326\n",
            "Epoch [10/100], Batch [8/32], Loss: 2.7735\n",
            "Epoch [10/100], Batch [9/32], Loss: 2.7619\n",
            "Epoch [10/100], Batch [10/32], Loss: 2.8143\n",
            "Epoch [10/100], Batch [11/32], Loss: 2.8487\n",
            "Epoch [10/100], Batch [12/32], Loss: 2.8555\n",
            "Epoch [10/100], Batch [13/32], Loss: 2.9640\n",
            "Epoch [10/100], Batch [14/32], Loss: 2.7897\n",
            "Epoch [10/100], Batch [15/32], Loss: 2.6995\n",
            "Epoch [10/100], Batch [16/32], Loss: 2.8104\n",
            "Epoch [10/100], Batch [17/32], Loss: 2.8688\n",
            "Epoch [10/100], Batch [18/32], Loss: 2.8018\n",
            "Epoch [10/100], Batch [19/32], Loss: 2.7749\n",
            "Epoch [10/100], Batch [20/32], Loss: 2.7811\n",
            "Epoch [10/100], Batch [21/32], Loss: 2.7849\n",
            "Epoch [10/100], Batch [22/32], Loss: 2.7519\n",
            "Epoch [10/100], Batch [23/32], Loss: 2.9062\n",
            "Epoch [10/100], Batch [24/32], Loss: 2.7192\n",
            "Epoch [10/100], Batch [25/32], Loss: 2.7332\n",
            "Epoch [10/100], Batch [26/32], Loss: 2.7689\n",
            "Epoch [10/100], Batch [27/32], Loss: 2.6829\n",
            "Epoch [10/100], Batch [28/32], Loss: 2.7132\n",
            "Epoch [10/100], Batch [29/32], Loss: 2.9362\n",
            "Epoch [10/100], Batch [30/32], Loss: 2.7704\n",
            "Epoch [10/100], Batch [31/32], Loss: 2.8413\n",
            "Epoch [10/100], Batch [32/32], Loss: 2.7276\n",
            "Epoch [11/100], Batch [1/32], Loss: 2.7741\n",
            "Epoch [11/100], Batch [2/32], Loss: 2.6739\n",
            "Epoch [11/100], Batch [3/32], Loss: 2.9069\n",
            "Epoch [11/100], Batch [4/32], Loss: 2.7932\n",
            "Epoch [11/100], Batch [5/32], Loss: 2.7390\n",
            "Epoch [11/100], Batch [6/32], Loss: 2.9807\n",
            "Epoch [11/100], Batch [7/32], Loss: 2.6562\n",
            "Epoch [11/100], Batch [8/32], Loss: 2.7554\n",
            "Epoch [11/100], Batch [9/32], Loss: 2.8680\n",
            "Epoch [11/100], Batch [10/32], Loss: 2.7875\n",
            "Epoch [11/100], Batch [11/32], Loss: 2.8385\n",
            "Epoch [11/100], Batch [12/32], Loss: 2.7878\n",
            "Epoch [11/100], Batch [13/32], Loss: 2.8360\n",
            "Epoch [11/100], Batch [14/32], Loss: 2.9304\n",
            "Epoch [11/100], Batch [15/32], Loss: 2.8864\n",
            "Epoch [11/100], Batch [16/32], Loss: 2.7118\n",
            "Epoch [11/100], Batch [17/32], Loss: 2.7108\n",
            "Epoch [11/100], Batch [18/32], Loss: 2.7090\n",
            "Epoch [11/100], Batch [19/32], Loss: 2.8685\n",
            "Epoch [11/100], Batch [20/32], Loss: 2.9948\n",
            "Epoch [11/100], Batch [21/32], Loss: 2.7513\n",
            "Epoch [11/100], Batch [22/32], Loss: 2.7847\n",
            "Epoch [11/100], Batch [23/32], Loss: 2.7714\n",
            "Epoch [11/100], Batch [24/32], Loss: 2.7979\n",
            "Epoch [11/100], Batch [25/32], Loss: 2.8399\n",
            "Epoch [11/100], Batch [26/32], Loss: 2.8020\n",
            "Epoch [11/100], Batch [27/32], Loss: 2.6977\n",
            "Epoch [11/100], Batch [28/32], Loss: 2.8919\n",
            "Epoch [11/100], Batch [29/32], Loss: 2.6941\n",
            "Epoch [11/100], Batch [30/32], Loss: 2.8595\n",
            "Epoch [11/100], Batch [31/32], Loss: 2.6037\n",
            "Epoch [11/100], Batch [32/32], Loss: 2.7223\n",
            "Epoch [12/100], Batch [1/32], Loss: 2.6992\n",
            "Epoch [12/100], Batch [2/32], Loss: 2.8566\n",
            "Epoch [12/100], Batch [3/32], Loss: 2.6529\n",
            "Epoch [12/100], Batch [4/32], Loss: 2.7944\n",
            "Epoch [12/100], Batch [5/32], Loss: 2.7603\n",
            "Epoch [12/100], Batch [6/32], Loss: 2.7906\n",
            "Epoch [12/100], Batch [7/32], Loss: 2.5886\n",
            "Epoch [12/100], Batch [8/32], Loss: 2.8632\n",
            "Epoch [12/100], Batch [9/32], Loss: 2.7963\n",
            "Epoch [12/100], Batch [10/32], Loss: 2.8488\n",
            "Epoch [12/100], Batch [11/32], Loss: 2.9213\n",
            "Epoch [12/100], Batch [12/32], Loss: 2.7382\n",
            "Epoch [12/100], Batch [13/32], Loss: 2.9099\n",
            "Epoch [12/100], Batch [14/32], Loss: 2.8123\n",
            "Epoch [12/100], Batch [15/32], Loss: 2.8777\n",
            "Epoch [12/100], Batch [16/32], Loss: 2.7866\n",
            "Epoch [12/100], Batch [17/32], Loss: 2.8962\n",
            "Epoch [12/100], Batch [18/32], Loss: 2.9249\n",
            "Epoch [12/100], Batch [19/32], Loss: 2.7637\n",
            "Epoch [12/100], Batch [20/32], Loss: 2.8624\n",
            "Epoch [12/100], Batch [21/32], Loss: 2.7273\n",
            "Epoch [12/100], Batch [22/32], Loss: 2.6932\n",
            "Epoch [12/100], Batch [23/32], Loss: 2.8158\n",
            "Epoch [12/100], Batch [24/32], Loss: 2.7630\n",
            "Epoch [12/100], Batch [25/32], Loss: 2.7230\n",
            "Epoch [12/100], Batch [26/32], Loss: 2.7698\n",
            "Epoch [12/100], Batch [27/32], Loss: 2.7078\n",
            "Epoch [12/100], Batch [28/32], Loss: 2.8538\n",
            "Epoch [12/100], Batch [29/32], Loss: 2.8097\n",
            "Epoch [12/100], Batch [30/32], Loss: 2.8508\n",
            "Epoch [12/100], Batch [31/32], Loss: 2.7476\n",
            "Epoch [12/100], Batch [32/32], Loss: 2.7577\n",
            "Epoch [13/100], Batch [1/32], Loss: 2.7740\n",
            "Epoch [13/100], Batch [2/32], Loss: 2.6969\n",
            "Epoch [13/100], Batch [3/32], Loss: 2.8041\n",
            "Epoch [13/100], Batch [4/32], Loss: 2.7388\n",
            "Epoch [13/100], Batch [5/32], Loss: 2.7972\n",
            "Epoch [13/100], Batch [6/32], Loss: 2.7478\n",
            "Epoch [13/100], Batch [7/32], Loss: 2.7143\n",
            "Epoch [13/100], Batch [8/32], Loss: 2.8594\n",
            "Epoch [13/100], Batch [9/32], Loss: 2.7583\n",
            "Epoch [13/100], Batch [10/32], Loss: 2.7995\n",
            "Epoch [13/100], Batch [11/32], Loss: 2.6846\n",
            "Epoch [13/100], Batch [12/32], Loss: 2.8331\n",
            "Epoch [13/100], Batch [13/32], Loss: 2.8288\n",
            "Epoch [13/100], Batch [14/32], Loss: 2.7667\n",
            "Epoch [13/100], Batch [15/32], Loss: 2.8349\n",
            "Epoch [13/100], Batch [16/32], Loss: 2.9814\n",
            "Epoch [13/100], Batch [17/32], Loss: 2.6824\n",
            "Epoch [13/100], Batch [18/32], Loss: 2.8092\n",
            "Epoch [13/100], Batch [19/32], Loss: 2.7782\n",
            "Epoch [13/100], Batch [20/32], Loss: 2.8243\n",
            "Epoch [13/100], Batch [21/32], Loss: 2.7266\n",
            "Epoch [13/100], Batch [22/32], Loss: 2.9237\n",
            "Epoch [13/100], Batch [23/32], Loss: 2.7834\n",
            "Epoch [13/100], Batch [24/32], Loss: 2.6391\n",
            "Epoch [13/100], Batch [25/32], Loss: 2.7970\n",
            "Epoch [13/100], Batch [26/32], Loss: 2.8257\n",
            "Epoch [13/100], Batch [27/32], Loss: 2.6729\n",
            "Epoch [13/100], Batch [28/32], Loss: 2.8679\n",
            "Epoch [13/100], Batch [29/32], Loss: 2.7743\n",
            "Epoch [13/100], Batch [30/32], Loss: 2.8605\n",
            "Epoch [13/100], Batch [31/32], Loss: 2.7219\n",
            "Epoch [13/100], Batch [32/32], Loss: 2.8011\n",
            "Epoch [14/100], Batch [1/32], Loss: 2.8418\n",
            "Epoch [14/100], Batch [2/32], Loss: 2.7113\n",
            "Epoch [14/100], Batch [3/32], Loss: 2.7427\n",
            "Epoch [14/100], Batch [4/32], Loss: 2.8196\n",
            "Epoch [14/100], Batch [5/32], Loss: 2.8853\n",
            "Epoch [14/100], Batch [6/32], Loss: 2.8305\n",
            "Epoch [14/100], Batch [7/32], Loss: 2.7189\n",
            "Epoch [14/100], Batch [8/32], Loss: 2.6964\n",
            "Epoch [14/100], Batch [9/32], Loss: 2.6706\n",
            "Epoch [14/100], Batch [10/32], Loss: 2.7903\n",
            "Epoch [14/100], Batch [11/32], Loss: 2.7688\n",
            "Epoch [14/100], Batch [12/32], Loss: 2.8945\n",
            "Epoch [14/100], Batch [13/32], Loss: 2.8459\n",
            "Epoch [14/100], Batch [14/32], Loss: 2.7812\n",
            "Epoch [14/100], Batch [15/32], Loss: 2.7859\n",
            "Epoch [14/100], Batch [16/32], Loss: 2.8086\n",
            "Epoch [14/100], Batch [17/32], Loss: 2.9437\n",
            "Epoch [14/100], Batch [18/32], Loss: 2.7222\n",
            "Epoch [14/100], Batch [19/32], Loss: 2.6631\n",
            "Epoch [14/100], Batch [20/32], Loss: 2.7758\n",
            "Epoch [14/100], Batch [21/32], Loss: 2.8129\n",
            "Epoch [14/100], Batch [22/32], Loss: 2.7324\n",
            "Epoch [14/100], Batch [23/32], Loss: 2.7590\n",
            "Epoch [14/100], Batch [24/32], Loss: 2.6539\n",
            "Epoch [14/100], Batch [25/32], Loss: 2.8891\n",
            "Epoch [14/100], Batch [26/32], Loss: 2.7113\n",
            "Epoch [14/100], Batch [27/32], Loss: 2.6996\n",
            "Epoch [14/100], Batch [28/32], Loss: 2.8967\n",
            "Epoch [14/100], Batch [29/32], Loss: 2.8480\n",
            "Epoch [14/100], Batch [30/32], Loss: 2.7421\n",
            "Epoch [14/100], Batch [31/32], Loss: 2.9177\n",
            "Epoch [14/100], Batch [32/32], Loss: 2.8127\n",
            "Epoch [15/100], Batch [1/32], Loss: 2.8036\n",
            "Epoch [15/100], Batch [2/32], Loss: 2.8206\n",
            "Epoch [15/100], Batch [3/32], Loss: 2.7982\n",
            "Epoch [15/100], Batch [4/32], Loss: 2.8355\n",
            "Epoch [15/100], Batch [5/32], Loss: 2.8661\n",
            "Epoch [15/100], Batch [6/32], Loss: 2.7874\n",
            "Epoch [15/100], Batch [7/32], Loss: 2.7579\n",
            "Epoch [15/100], Batch [8/32], Loss: 2.8028\n",
            "Epoch [15/100], Batch [9/32], Loss: 2.7524\n",
            "Epoch [15/100], Batch [10/32], Loss: 2.7202\n",
            "Epoch [15/100], Batch [11/32], Loss: 2.6882\n",
            "Epoch [15/100], Batch [12/32], Loss: 2.7353\n",
            "Epoch [15/100], Batch [13/32], Loss: 2.9056\n",
            "Epoch [15/100], Batch [14/32], Loss: 2.8349\n",
            "Epoch [15/100], Batch [15/32], Loss: 2.6877\n",
            "Epoch [15/100], Batch [16/32], Loss: 2.8269\n",
            "Epoch [15/100], Batch [17/32], Loss: 2.7339\n",
            "Epoch [15/100], Batch [18/32], Loss: 2.8184\n",
            "Epoch [15/100], Batch [19/32], Loss: 2.7242\n",
            "Epoch [15/100], Batch [20/32], Loss: 2.8054\n",
            "Epoch [15/100], Batch [21/32], Loss: 2.6604\n",
            "Epoch [15/100], Batch [22/32], Loss: 2.7800\n",
            "Epoch [15/100], Batch [23/32], Loss: 2.7738\n",
            "Epoch [15/100], Batch [24/32], Loss: 2.6849\n",
            "Epoch [15/100], Batch [25/32], Loss: 2.7834\n",
            "Epoch [15/100], Batch [26/32], Loss: 2.6223\n",
            "Epoch [15/100], Batch [27/32], Loss: 2.8843\n",
            "Epoch [15/100], Batch [28/32], Loss: 2.7971\n",
            "Epoch [15/100], Batch [29/32], Loss: 2.7822\n",
            "Epoch [15/100], Batch [30/32], Loss: 2.7072\n",
            "Epoch [15/100], Batch [31/32], Loss: 2.7282\n",
            "Epoch [15/100], Batch [32/32], Loss: 2.9020\n",
            "Epoch [16/100], Batch [1/32], Loss: 2.8169\n",
            "Epoch [16/100], Batch [2/32], Loss: 2.5815\n",
            "Epoch [16/100], Batch [3/32], Loss: 2.7966\n",
            "Epoch [16/100], Batch [4/32], Loss: 2.6836\n",
            "Epoch [16/100], Batch [5/32], Loss: 2.8597\n",
            "Epoch [16/100], Batch [6/32], Loss: 2.7843\n",
            "Epoch [16/100], Batch [7/32], Loss: 2.7188\n",
            "Epoch [16/100], Batch [8/32], Loss: 2.8665\n",
            "Epoch [16/100], Batch [9/32], Loss: 2.8534\n",
            "Epoch [16/100], Batch [10/32], Loss: 2.7328\n",
            "Epoch [16/100], Batch [11/32], Loss: 2.8200\n",
            "Epoch [16/100], Batch [12/32], Loss: 2.7478\n",
            "Epoch [16/100], Batch [13/32], Loss: 2.8004\n",
            "Epoch [16/100], Batch [14/32], Loss: 2.8647\n",
            "Epoch [16/100], Batch [15/32], Loss: 2.6415\n",
            "Epoch [16/100], Batch [16/32], Loss: 2.7089\n",
            "Epoch [16/100], Batch [17/32], Loss: 2.7777\n",
            "Epoch [16/100], Batch [18/32], Loss: 2.8000\n",
            "Epoch [16/100], Batch [19/32], Loss: 2.7437\n",
            "Epoch [16/100], Batch [20/32], Loss: 2.7453\n",
            "Epoch [16/100], Batch [21/32], Loss: 2.7179\n",
            "Epoch [16/100], Batch [22/32], Loss: 2.7889\n",
            "Epoch [16/100], Batch [23/32], Loss: 2.8974\n",
            "Epoch [16/100], Batch [24/32], Loss: 2.8267\n",
            "Epoch [16/100], Batch [25/32], Loss: 2.6510\n",
            "Epoch [16/100], Batch [26/32], Loss: 2.8013\n",
            "Epoch [16/100], Batch [27/32], Loss: 2.6759\n",
            "Epoch [16/100], Batch [28/32], Loss: 2.8138\n",
            "Epoch [16/100], Batch [29/32], Loss: 2.8203\n",
            "Epoch [16/100], Batch [30/32], Loss: 2.8293\n",
            "Epoch [16/100], Batch [31/32], Loss: 2.7894\n",
            "Epoch [16/100], Batch [32/32], Loss: 2.7221\n",
            "Epoch [17/100], Batch [1/32], Loss: 2.8593\n",
            "Epoch [17/100], Batch [2/32], Loss: 2.7231\n",
            "Epoch [17/100], Batch [3/32], Loss: 2.7691\n",
            "Epoch [17/100], Batch [4/32], Loss: 2.7821\n",
            "Epoch [17/100], Batch [5/32], Loss: 2.7786\n",
            "Epoch [17/100], Batch [6/32], Loss: 2.7049\n",
            "Epoch [17/100], Batch [7/32], Loss: 2.7245\n",
            "Epoch [17/100], Batch [8/32], Loss: 2.6376\n",
            "Epoch [17/100], Batch [9/32], Loss: 2.7487\n",
            "Epoch [17/100], Batch [10/32], Loss: 2.6768\n",
            "Epoch [17/100], Batch [11/32], Loss: 2.5615\n",
            "Epoch [17/100], Batch [12/32], Loss: 2.7172\n",
            "Epoch [17/100], Batch [13/32], Loss: 2.6890\n",
            "Epoch [17/100], Batch [14/32], Loss: 2.8814\n",
            "Epoch [17/100], Batch [15/32], Loss: 2.6473\n",
            "Epoch [17/100], Batch [16/32], Loss: 2.8742\n",
            "Epoch [17/100], Batch [17/32], Loss: 2.7505\n",
            "Epoch [17/100], Batch [18/32], Loss: 2.7950\n",
            "Epoch [17/100], Batch [19/32], Loss: 2.7678\n",
            "Epoch [17/100], Batch [20/32], Loss: 2.7620\n",
            "Epoch [17/100], Batch [21/32], Loss: 2.9278\n",
            "Epoch [17/100], Batch [22/32], Loss: 2.9149\n",
            "Epoch [17/100], Batch [23/32], Loss: 2.7339\n",
            "Epoch [17/100], Batch [24/32], Loss: 2.7517\n",
            "Epoch [17/100], Batch [25/32], Loss: 2.8185\n",
            "Epoch [17/100], Batch [26/32], Loss: 2.7734\n",
            "Epoch [17/100], Batch [27/32], Loss: 2.7060\n",
            "Epoch [17/100], Batch [28/32], Loss: 2.7820\n",
            "Epoch [17/100], Batch [29/32], Loss: 2.8151\n",
            "Epoch [17/100], Batch [30/32], Loss: 2.6965\n",
            "Epoch [17/100], Batch [31/32], Loss: 2.8601\n",
            "Epoch [17/100], Batch [32/32], Loss: 2.7942\n",
            "Epoch [18/100], Batch [1/32], Loss: 2.8943\n",
            "Epoch [18/100], Batch [2/32], Loss: 2.7650\n",
            "Epoch [18/100], Batch [3/32], Loss: 2.7256\n",
            "Epoch [18/100], Batch [4/32], Loss: 2.7510\n",
            "Epoch [18/100], Batch [5/32], Loss: 2.7159\n",
            "Epoch [18/100], Batch [6/32], Loss: 2.7473\n",
            "Epoch [18/100], Batch [7/32], Loss: 2.7103\n",
            "Epoch [18/100], Batch [8/32], Loss: 2.7719\n",
            "Epoch [18/100], Batch [9/32], Loss: 2.7707\n",
            "Epoch [18/100], Batch [10/32], Loss: 2.7179\n",
            "Epoch [18/100], Batch [11/32], Loss: 2.8844\n",
            "Epoch [18/100], Batch [12/32], Loss: 2.7414\n",
            "Epoch [18/100], Batch [13/32], Loss: 2.7722\n",
            "Epoch [18/100], Batch [14/32], Loss: 2.7801\n",
            "Epoch [18/100], Batch [15/32], Loss: 2.7889\n",
            "Epoch [18/100], Batch [16/32], Loss: 2.7636\n",
            "Epoch [18/100], Batch [17/32], Loss: 2.8252\n",
            "Epoch [18/100], Batch [18/32], Loss: 2.7667\n",
            "Epoch [18/100], Batch [19/32], Loss: 2.6693\n",
            "Epoch [18/100], Batch [20/32], Loss: 2.7431\n",
            "Epoch [18/100], Batch [21/32], Loss: 2.6822\n",
            "Epoch [18/100], Batch [22/32], Loss: 2.6889\n",
            "Epoch [18/100], Batch [23/32], Loss: 2.6948\n",
            "Epoch [18/100], Batch [24/32], Loss: 2.8296\n",
            "Epoch [18/100], Batch [25/32], Loss: 2.7591\n",
            "Epoch [18/100], Batch [26/32], Loss: 2.6731\n",
            "Epoch [18/100], Batch [27/32], Loss: 2.8817\n",
            "Epoch [18/100], Batch [28/32], Loss: 2.6817\n",
            "Epoch [18/100], Batch [29/32], Loss: 2.8997\n",
            "Epoch [18/100], Batch [30/32], Loss: 2.8000\n",
            "Epoch [18/100], Batch [31/32], Loss: 2.8698\n",
            "Epoch [18/100], Batch [32/32], Loss: 2.7648\n",
            "Epoch [19/100], Batch [1/32], Loss: 2.5647\n",
            "Epoch [19/100], Batch [2/32], Loss: 2.7993\n",
            "Epoch [19/100], Batch [3/32], Loss: 2.7894\n",
            "Epoch [19/100], Batch [4/32], Loss: 2.6913\n",
            "Epoch [19/100], Batch [5/32], Loss: 2.6692\n",
            "Epoch [19/100], Batch [6/32], Loss: 2.8681\n",
            "Epoch [19/100], Batch [7/32], Loss: 2.6686\n",
            "Epoch [19/100], Batch [8/32], Loss: 2.6460\n",
            "Epoch [19/100], Batch [9/32], Loss: 2.7762\n",
            "Epoch [19/100], Batch [10/32], Loss: 2.7077\n",
            "Epoch [19/100], Batch [11/32], Loss: 2.6589\n",
            "Epoch [19/100], Batch [12/32], Loss: 2.7175\n",
            "Epoch [19/100], Batch [13/32], Loss: 2.7206\n",
            "Epoch [19/100], Batch [14/32], Loss: 2.7316\n",
            "Epoch [19/100], Batch [15/32], Loss: 2.7490\n",
            "Epoch [19/100], Batch [16/32], Loss: 2.7077\n",
            "Epoch [19/100], Batch [17/32], Loss: 2.8034\n",
            "Epoch [19/100], Batch [18/32], Loss: 2.8684\n",
            "Epoch [19/100], Batch [19/32], Loss: 2.5953\n",
            "Epoch [19/100], Batch [20/32], Loss: 2.8593\n",
            "Epoch [19/100], Batch [21/32], Loss: 2.7205\n",
            "Epoch [19/100], Batch [22/32], Loss: 2.8511\n",
            "Epoch [19/100], Batch [23/32], Loss: 2.7983\n",
            "Epoch [19/100], Batch [24/32], Loss: 2.8258\n",
            "Epoch [19/100], Batch [25/32], Loss: 2.7746\n",
            "Epoch [19/100], Batch [26/32], Loss: 2.7789\n",
            "Epoch [19/100], Batch [27/32], Loss: 2.7887\n",
            "Epoch [19/100], Batch [28/32], Loss: 2.8037\n",
            "Epoch [19/100], Batch [29/32], Loss: 2.7742\n",
            "Epoch [19/100], Batch [30/32], Loss: 2.7442\n",
            "Epoch [19/100], Batch [31/32], Loss: 2.7978\n",
            "Epoch [19/100], Batch [32/32], Loss: 2.8151\n",
            "Epoch [20/100], Batch [1/32], Loss: 2.8297\n",
            "Epoch [20/100], Batch [2/32], Loss: 2.6461\n",
            "Epoch [20/100], Batch [3/32], Loss: 2.6942\n",
            "Epoch [20/100], Batch [4/32], Loss: 2.7378\n",
            "Epoch [20/100], Batch [5/32], Loss: 2.7016\n",
            "Epoch [20/100], Batch [6/32], Loss: 2.7009\n",
            "Epoch [20/100], Batch [7/32], Loss: 2.6887\n",
            "Epoch [20/100], Batch [8/32], Loss: 2.7267\n",
            "Epoch [20/100], Batch [9/32], Loss: 2.8449\n",
            "Epoch [20/100], Batch [10/32], Loss: 2.7325\n",
            "Epoch [20/100], Batch [11/32], Loss: 2.6991\n",
            "Epoch [20/100], Batch [12/32], Loss: 2.7302\n",
            "Epoch [20/100], Batch [13/32], Loss: 2.7863\n",
            "Epoch [20/100], Batch [14/32], Loss: 2.7617\n",
            "Epoch [20/100], Batch [15/32], Loss: 2.7395\n",
            "Epoch [20/100], Batch [16/32], Loss: 2.7084\n",
            "Epoch [20/100], Batch [17/32], Loss: 2.8136\n",
            "Epoch [20/100], Batch [18/32], Loss: 2.6877\n",
            "Epoch [20/100], Batch [19/32], Loss: 2.7307\n",
            "Epoch [20/100], Batch [20/32], Loss: 2.6565\n",
            "Epoch [20/100], Batch [21/32], Loss: 2.7563\n",
            "Epoch [20/100], Batch [22/32], Loss: 2.7555\n",
            "Epoch [20/100], Batch [23/32], Loss: 2.6868\n",
            "Epoch [20/100], Batch [24/32], Loss: 2.8944\n",
            "Epoch [20/100], Batch [25/32], Loss: 2.8523\n",
            "Epoch [20/100], Batch [26/32], Loss: 2.7356\n",
            "Epoch [20/100], Batch [27/32], Loss: 2.7331\n",
            "Epoch [20/100], Batch [28/32], Loss: 2.8219\n",
            "Epoch [20/100], Batch [29/32], Loss: 2.7054\n",
            "Epoch [20/100], Batch [30/32], Loss: 2.6894\n",
            "Epoch [20/100], Batch [31/32], Loss: 2.8320\n",
            "Epoch [20/100], Batch [32/32], Loss: 2.8763\n",
            "Epoch [21/100], Batch [1/32], Loss: 2.6278\n",
            "Epoch [21/100], Batch [2/32], Loss: 2.7592\n",
            "Epoch [21/100], Batch [3/32], Loss: 2.7505\n",
            "Epoch [21/100], Batch [4/32], Loss: 2.6227\n",
            "Epoch [21/100], Batch [5/32], Loss: 2.8511\n",
            "Epoch [21/100], Batch [6/32], Loss: 2.6673\n",
            "Epoch [21/100], Batch [7/32], Loss: 2.7819\n",
            "Epoch [21/100], Batch [8/32], Loss: 2.8447\n",
            "Epoch [21/100], Batch [9/32], Loss: 2.7528\n",
            "Epoch [21/100], Batch [10/32], Loss: 2.7394\n",
            "Epoch [21/100], Batch [11/32], Loss: 2.8040\n",
            "Epoch [21/100], Batch [12/32], Loss: 2.7913\n",
            "Epoch [21/100], Batch [13/32], Loss: 2.7876\n",
            "Epoch [21/100], Batch [14/32], Loss: 2.7127\n",
            "Epoch [21/100], Batch [15/32], Loss: 2.8016\n",
            "Epoch [21/100], Batch [16/32], Loss: 2.7194\n",
            "Epoch [21/100], Batch [17/32], Loss: 2.7387\n",
            "Epoch [21/100], Batch [18/32], Loss: 2.6748\n",
            "Epoch [21/100], Batch [19/32], Loss: 2.8285\n",
            "Epoch [21/100], Batch [20/32], Loss: 2.7459\n",
            "Epoch [21/100], Batch [21/32], Loss: 2.6964\n",
            "Epoch [21/100], Batch [22/32], Loss: 2.7029\n",
            "Epoch [21/100], Batch [23/32], Loss: 2.8484\n",
            "Epoch [21/100], Batch [24/32], Loss: 2.7228\n",
            "Epoch [21/100], Batch [25/32], Loss: 2.6769\n",
            "Epoch [21/100], Batch [26/32], Loss: 2.6595\n",
            "Epoch [21/100], Batch [27/32], Loss: 2.7648\n",
            "Epoch [21/100], Batch [28/32], Loss: 2.8108\n",
            "Epoch [21/100], Batch [29/32], Loss: 2.7721\n",
            "Epoch [21/100], Batch [30/32], Loss: 2.8930\n",
            "Epoch [21/100], Batch [31/32], Loss: 2.6943\n",
            "Epoch [21/100], Batch [32/32], Loss: 2.7642\n",
            "Epoch [22/100], Batch [1/32], Loss: 2.7491\n",
            "Epoch [22/100], Batch [2/32], Loss: 2.6802\n",
            "Epoch [22/100], Batch [3/32], Loss: 2.8130\n",
            "Epoch [22/100], Batch [4/32], Loss: 2.7406\n",
            "Epoch [22/100], Batch [5/32], Loss: 2.6508\n",
            "Epoch [22/100], Batch [6/32], Loss: 2.7372\n",
            "Epoch [22/100], Batch [7/32], Loss: 2.7863\n",
            "Epoch [22/100], Batch [8/32], Loss: 2.6497\n",
            "Epoch [22/100], Batch [9/32], Loss: 2.7993\n",
            "Epoch [22/100], Batch [10/32], Loss: 2.7635\n",
            "Epoch [22/100], Batch [11/32], Loss: 2.8101\n",
            "Epoch [22/100], Batch [12/32], Loss: 2.8048\n",
            "Epoch [22/100], Batch [13/32], Loss: 2.7504\n",
            "Epoch [22/100], Batch [14/32], Loss: 2.7080\n",
            "Epoch [22/100], Batch [15/32], Loss: 2.7787\n",
            "Epoch [22/100], Batch [16/32], Loss: 2.6805\n",
            "Epoch [22/100], Batch [17/32], Loss: 2.7827\n",
            "Epoch [22/100], Batch [18/32], Loss: 2.5960\n",
            "Epoch [22/100], Batch [19/32], Loss: 2.8944\n",
            "Epoch [22/100], Batch [20/32], Loss: 2.8222\n",
            "Epoch [22/100], Batch [21/32], Loss: 2.8825\n",
            "Epoch [22/100], Batch [22/32], Loss: 2.7672\n",
            "Epoch [22/100], Batch [23/32], Loss: 2.6776\n",
            "Epoch [22/100], Batch [24/32], Loss: 2.8022\n",
            "Epoch [22/100], Batch [25/32], Loss: 2.5488\n",
            "Epoch [22/100], Batch [26/32], Loss: 2.7097\n",
            "Epoch [22/100], Batch [27/32], Loss: 2.7100\n",
            "Epoch [22/100], Batch [28/32], Loss: 2.8174\n",
            "Epoch [22/100], Batch [29/32], Loss: 2.7390\n",
            "Epoch [22/100], Batch [30/32], Loss: 2.6740\n",
            "Epoch [22/100], Batch [31/32], Loss: 2.7376\n",
            "Epoch [22/100], Batch [32/32], Loss: 2.8189\n",
            "Epoch [23/100], Batch [1/32], Loss: 2.8117\n",
            "Epoch [23/100], Batch [2/32], Loss: 2.6806\n",
            "Epoch [23/100], Batch [3/32], Loss: 2.8440\n",
            "Epoch [23/100], Batch [4/32], Loss: 2.7368\n",
            "Epoch [23/100], Batch [5/32], Loss: 2.6690\n",
            "Epoch [23/100], Batch [6/32], Loss: 2.7096\n",
            "Epoch [23/100], Batch [7/32], Loss: 2.6773\n",
            "Epoch [23/100], Batch [8/32], Loss: 2.6891\n",
            "Epoch [23/100], Batch [9/32], Loss: 2.7445\n",
            "Epoch [23/100], Batch [10/32], Loss: 2.7555\n",
            "Epoch [23/100], Batch [11/32], Loss: 2.7744\n",
            "Epoch [23/100], Batch [12/32], Loss: 2.7965\n",
            "Epoch [23/100], Batch [13/32], Loss: 2.7775\n",
            "Epoch [23/100], Batch [14/32], Loss: 2.7002\n",
            "Epoch [23/100], Batch [15/32], Loss: 2.6707\n",
            "Epoch [23/100], Batch [16/32], Loss: 2.7483\n",
            "Epoch [23/100], Batch [17/32], Loss: 2.5622\n",
            "Epoch [23/100], Batch [18/32], Loss: 2.8057\n",
            "Epoch [23/100], Batch [19/32], Loss: 2.7086\n",
            "Epoch [23/100], Batch [20/32], Loss: 2.8069\n",
            "Epoch [23/100], Batch [21/32], Loss: 2.5035\n",
            "Epoch [23/100], Batch [22/32], Loss: 2.9430\n",
            "Epoch [23/100], Batch [23/32], Loss: 2.7685\n",
            "Epoch [23/100], Batch [24/32], Loss: 2.7335\n",
            "Epoch [23/100], Batch [25/32], Loss: 2.7546\n",
            "Epoch [23/100], Batch [26/32], Loss: 2.7611\n",
            "Epoch [23/100], Batch [27/32], Loss: 2.7255\n",
            "Epoch [23/100], Batch [28/32], Loss: 2.7072\n",
            "Epoch [23/100], Batch [29/32], Loss: 2.8167\n",
            "Epoch [23/100], Batch [30/32], Loss: 2.7237\n",
            "Epoch [23/100], Batch [31/32], Loss: 2.7068\n",
            "Epoch [23/100], Batch [32/32], Loss: 2.7650\n",
            "Epoch [24/100], Batch [1/32], Loss: 2.6894\n",
            "Epoch [24/100], Batch [2/32], Loss: 2.6999\n",
            "Epoch [24/100], Batch [3/32], Loss: 2.7382\n",
            "Epoch [24/100], Batch [4/32], Loss: 2.7437\n",
            "Epoch [24/100], Batch [5/32], Loss: 2.6916\n",
            "Epoch [24/100], Batch [6/32], Loss: 2.7766\n",
            "Epoch [24/100], Batch [7/32], Loss: 2.6975\n",
            "Epoch [24/100], Batch [8/32], Loss: 2.6679\n",
            "Epoch [24/100], Batch [9/32], Loss: 2.8336\n",
            "Epoch [24/100], Batch [10/32], Loss: 2.6713\n",
            "Epoch [24/100], Batch [11/32], Loss: 2.6741\n",
            "Epoch [24/100], Batch [12/32], Loss: 2.7849\n",
            "Epoch [24/100], Batch [13/32], Loss: 2.6400\n",
            "Epoch [24/100], Batch [14/32], Loss: 2.7497\n",
            "Epoch [24/100], Batch [15/32], Loss: 2.7659\n",
            "Epoch [24/100], Batch [16/32], Loss: 2.6892\n",
            "Epoch [24/100], Batch [17/32], Loss: 2.7881\n",
            "Epoch [24/100], Batch [18/32], Loss: 2.7842\n",
            "Epoch [24/100], Batch [19/32], Loss: 2.7940\n",
            "Epoch [24/100], Batch [20/32], Loss: 2.7177\n",
            "Epoch [24/100], Batch [21/32], Loss: 2.7787\n",
            "Epoch [24/100], Batch [22/32], Loss: 2.7417\n",
            "Epoch [24/100], Batch [23/32], Loss: 2.6683\n",
            "Epoch [24/100], Batch [24/32], Loss: 2.7820\n",
            "Epoch [24/100], Batch [25/32], Loss: 2.8011\n",
            "Epoch [24/100], Batch [26/32], Loss: 2.7741\n",
            "Epoch [24/100], Batch [27/32], Loss: 2.6436\n",
            "Epoch [24/100], Batch [28/32], Loss: 2.8490\n",
            "Epoch [24/100], Batch [29/32], Loss: 2.6583\n",
            "Epoch [24/100], Batch [30/32], Loss: 2.7768\n",
            "Epoch [24/100], Batch [31/32], Loss: 2.8084\n",
            "Epoch [24/100], Batch [32/32], Loss: 2.7097\n",
            "Epoch [25/100], Batch [1/32], Loss: 2.7087\n",
            "Epoch [25/100], Batch [2/32], Loss: 2.7380\n",
            "Epoch [25/100], Batch [3/32], Loss: 2.7503\n",
            "Epoch [25/100], Batch [4/32], Loss: 2.7893\n",
            "Epoch [25/100], Batch [5/32], Loss: 2.7880\n",
            "Epoch [25/100], Batch [6/32], Loss: 2.6932\n",
            "Epoch [25/100], Batch [7/32], Loss: 2.8772\n",
            "Epoch [25/100], Batch [8/32], Loss: 2.6174\n",
            "Epoch [25/100], Batch [9/32], Loss: 2.8163\n",
            "Epoch [25/100], Batch [10/32], Loss: 2.7918\n",
            "Epoch [25/100], Batch [11/32], Loss: 2.6740\n",
            "Epoch [25/100], Batch [12/32], Loss: 2.6801\n",
            "Epoch [25/100], Batch [13/32], Loss: 2.7255\n",
            "Epoch [25/100], Batch [14/32], Loss: 2.8236\n",
            "Epoch [25/100], Batch [15/32], Loss: 2.7275\n",
            "Epoch [25/100], Batch [16/32], Loss: 2.8255\n",
            "Epoch [25/100], Batch [17/32], Loss: 2.5604\n",
            "Epoch [25/100], Batch [18/32], Loss: 2.8437\n",
            "Epoch [25/100], Batch [19/32], Loss: 2.7940\n",
            "Epoch [25/100], Batch [20/32], Loss: 2.7953\n",
            "Epoch [25/100], Batch [21/32], Loss: 2.6983\n",
            "Epoch [25/100], Batch [22/32], Loss: 2.7507\n",
            "Epoch [25/100], Batch [23/32], Loss: 2.6415\n",
            "Epoch [25/100], Batch [24/32], Loss: 2.7232\n",
            "Epoch [25/100], Batch [25/32], Loss: 2.7063\n",
            "Epoch [25/100], Batch [26/32], Loss: 2.8630\n",
            "Epoch [25/100], Batch [27/32], Loss: 2.6863\n",
            "Epoch [25/100], Batch [28/32], Loss: 2.7087\n",
            "Epoch [25/100], Batch [29/32], Loss: 2.7635\n",
            "Epoch [25/100], Batch [30/32], Loss: 2.6230\n",
            "Epoch [25/100], Batch [31/32], Loss: 2.7004\n",
            "Epoch [25/100], Batch [32/32], Loss: 2.6743\n",
            "Epoch [26/100], Batch [1/32], Loss: 2.7113\n",
            "Epoch [26/100], Batch [2/32], Loss: 2.7701\n",
            "Epoch [26/100], Batch [3/32], Loss: 2.7381\n",
            "Epoch [26/100], Batch [4/32], Loss: 2.7390\n",
            "Epoch [26/100], Batch [5/32], Loss: 2.5117\n",
            "Epoch [26/100], Batch [6/32], Loss: 2.6625\n",
            "Epoch [26/100], Batch [7/32], Loss: 2.7898\n",
            "Epoch [26/100], Batch [8/32], Loss: 2.9660\n",
            "Epoch [26/100], Batch [9/32], Loss: 2.8315\n",
            "Epoch [26/100], Batch [10/32], Loss: 2.7160\n",
            "Epoch [26/100], Batch [11/32], Loss: 2.5861\n",
            "Epoch [26/100], Batch [12/32], Loss: 2.7319\n",
            "Epoch [26/100], Batch [13/32], Loss: 2.7663\n",
            "Epoch [26/100], Batch [14/32], Loss: 2.6950\n",
            "Epoch [26/100], Batch [15/32], Loss: 2.7307\n",
            "Epoch [26/100], Batch [16/32], Loss: 2.7475\n",
            "Epoch [26/100], Batch [17/32], Loss: 2.6562\n",
            "Epoch [26/100], Batch [18/32], Loss: 2.7079\n",
            "Epoch [26/100], Batch [19/32], Loss: 2.7616\n",
            "Epoch [26/100], Batch [20/32], Loss: 2.6613\n",
            "Epoch [26/100], Batch [21/32], Loss: 2.7295\n",
            "Epoch [26/100], Batch [22/32], Loss: 2.7375\n",
            "Epoch [26/100], Batch [23/32], Loss: 2.8106\n",
            "Epoch [26/100], Batch [24/32], Loss: 2.7123\n",
            "Epoch [26/100], Batch [25/32], Loss: 2.7593\n",
            "Epoch [26/100], Batch [26/32], Loss: 2.7641\n",
            "Epoch [26/100], Batch [27/32], Loss: 2.6445\n",
            "Epoch [26/100], Batch [28/32], Loss: 2.7332\n",
            "Epoch [26/100], Batch [29/32], Loss: 2.7177\n",
            "Epoch [26/100], Batch [30/32], Loss: 2.7343\n",
            "Epoch [26/100], Batch [31/32], Loss: 2.7793\n",
            "Epoch [26/100], Batch [32/32], Loss: 2.6693\n",
            "Epoch [27/100], Batch [1/32], Loss: 2.7001\n",
            "Epoch [27/100], Batch [2/32], Loss: 2.6757\n",
            "Epoch [27/100], Batch [3/32], Loss: 2.6528\n",
            "Epoch [27/100], Batch [4/32], Loss: 2.6166\n",
            "Epoch [27/100], Batch [5/32], Loss: 2.7529\n",
            "Epoch [27/100], Batch [6/32], Loss: 2.6775\n",
            "Epoch [27/100], Batch [7/32], Loss: 2.7014\n",
            "Epoch [27/100], Batch [8/32], Loss: 2.6720\n",
            "Epoch [27/100], Batch [9/32], Loss: 2.6732\n",
            "Epoch [27/100], Batch [10/32], Loss: 2.8662\n",
            "Epoch [27/100], Batch [11/32], Loss: 2.7308\n",
            "Epoch [27/100], Batch [12/32], Loss: 2.8079\n",
            "Epoch [27/100], Batch [13/32], Loss: 2.8077\n",
            "Epoch [27/100], Batch [14/32], Loss: 2.7110\n",
            "Epoch [27/100], Batch [15/32], Loss: 2.6624\n",
            "Epoch [27/100], Batch [16/32], Loss: 2.6465\n",
            "Epoch [27/100], Batch [17/32], Loss: 2.8543\n",
            "Epoch [27/100], Batch [18/32], Loss: 2.6642\n",
            "Epoch [27/100], Batch [19/32], Loss: 2.7528\n",
            "Epoch [27/100], Batch [20/32], Loss: 2.6718\n",
            "Epoch [27/100], Batch [21/32], Loss: 2.7549\n",
            "Epoch [27/100], Batch [22/32], Loss: 2.6024\n",
            "Epoch [27/100], Batch [23/32], Loss: 2.7128\n",
            "Epoch [27/100], Batch [24/32], Loss: 2.8564\n",
            "Epoch [27/100], Batch [25/32], Loss: 2.7546\n",
            "Epoch [27/100], Batch [26/32], Loss: 2.8514\n",
            "Epoch [27/100], Batch [27/32], Loss: 2.7597\n",
            "Epoch [27/100], Batch [28/32], Loss: 2.6391\n",
            "Epoch [27/100], Batch [29/32], Loss: 2.6145\n",
            "Epoch [27/100], Batch [30/32], Loss: 2.6553\n",
            "Epoch [27/100], Batch [31/32], Loss: 2.8062\n",
            "Epoch [27/100], Batch [32/32], Loss: 2.8549\n",
            "Epoch [28/100], Batch [1/32], Loss: 2.6798\n",
            "Epoch [28/100], Batch [2/32], Loss: 2.6274\n",
            "Epoch [28/100], Batch [3/32], Loss: 2.8022\n",
            "Epoch [28/100], Batch [4/32], Loss: 2.8514\n",
            "Epoch [28/100], Batch [5/32], Loss: 2.7043\n",
            "Epoch [28/100], Batch [6/32], Loss: 2.6116\n",
            "Epoch [28/100], Batch [7/32], Loss: 2.7775\n",
            "Epoch [28/100], Batch [8/32], Loss: 2.5499\n",
            "Epoch [28/100], Batch [9/32], Loss: 2.7052\n",
            "Epoch [28/100], Batch [10/32], Loss: 2.6857\n",
            "Epoch [28/100], Batch [11/32], Loss: 2.7568\n",
            "Epoch [28/100], Batch [12/32], Loss: 2.8018\n",
            "Epoch [28/100], Batch [13/32], Loss: 2.7719\n",
            "Epoch [28/100], Batch [14/32], Loss: 2.7774\n",
            "Epoch [28/100], Batch [15/32], Loss: 2.8192\n",
            "Epoch [28/100], Batch [16/32], Loss: 2.7181\n",
            "Epoch [28/100], Batch [17/32], Loss: 2.5900\n",
            "Epoch [28/100], Batch [18/32], Loss: 2.7742\n",
            "Epoch [28/100], Batch [19/32], Loss: 2.6974\n",
            "Epoch [28/100], Batch [20/32], Loss: 2.8168\n",
            "Epoch [28/100], Batch [21/32], Loss: 2.8747\n",
            "Epoch [28/100], Batch [22/32], Loss: 2.6600\n",
            "Epoch [28/100], Batch [23/32], Loss: 2.5774\n",
            "Epoch [28/100], Batch [24/32], Loss: 2.7408\n",
            "Epoch [28/100], Batch [25/32], Loss: 2.6481\n",
            "Epoch [28/100], Batch [26/32], Loss: 2.6369\n",
            "Epoch [28/100], Batch [27/32], Loss: 2.8361\n",
            "Epoch [28/100], Batch [28/32], Loss: 2.6647\n",
            "Epoch [28/100], Batch [29/32], Loss: 2.6744\n",
            "Epoch [28/100], Batch [30/32], Loss: 2.8029\n",
            "Epoch [28/100], Batch [31/32], Loss: 2.7131\n",
            "Epoch [28/100], Batch [32/32], Loss: 2.6236\n",
            "Epoch [29/100], Batch [1/32], Loss: 2.7707\n",
            "Epoch [29/100], Batch [2/32], Loss: 2.7165\n",
            "Epoch [29/100], Batch [3/32], Loss: 2.6335\n",
            "Epoch [29/100], Batch [4/32], Loss: 2.7751\n",
            "Epoch [29/100], Batch [5/32], Loss: 2.7297\n",
            "Epoch [29/100], Batch [6/32], Loss: 2.6190\n",
            "Epoch [29/100], Batch [7/32], Loss: 2.6019\n",
            "Epoch [29/100], Batch [8/32], Loss: 2.7094\n",
            "Epoch [29/100], Batch [9/32], Loss: 2.7073\n",
            "Epoch [29/100], Batch [10/32], Loss: 2.7596\n",
            "Epoch [29/100], Batch [11/32], Loss: 2.7127\n",
            "Epoch [29/100], Batch [12/32], Loss: 2.8088\n",
            "Epoch [29/100], Batch [13/32], Loss: 2.7497\n",
            "Epoch [29/100], Batch [14/32], Loss: 2.7280\n",
            "Epoch [29/100], Batch [15/32], Loss: 2.6973\n",
            "Epoch [29/100], Batch [16/32], Loss: 2.6091\n",
            "Epoch [29/100], Batch [17/32], Loss: 2.7571\n",
            "Epoch [29/100], Batch [18/32], Loss: 2.7583\n",
            "Epoch [29/100], Batch [19/32], Loss: 2.7592\n",
            "Epoch [29/100], Batch [20/32], Loss: 2.6797\n",
            "Epoch [29/100], Batch [21/32], Loss: 2.8845\n",
            "Epoch [29/100], Batch [22/32], Loss: 2.7506\n",
            "Epoch [29/100], Batch [23/32], Loss: 2.7298\n",
            "Epoch [29/100], Batch [24/32], Loss: 2.7744\n",
            "Epoch [29/100], Batch [25/32], Loss: 2.6289\n",
            "Epoch [29/100], Batch [26/32], Loss: 2.7447\n",
            "Epoch [29/100], Batch [27/32], Loss: 2.6587\n",
            "Epoch [29/100], Batch [28/32], Loss: 2.6397\n",
            "Epoch [29/100], Batch [29/32], Loss: 2.8421\n",
            "Epoch [29/100], Batch [30/32], Loss: 2.5995\n",
            "Epoch [29/100], Batch [31/32], Loss: 2.5332\n",
            "Epoch [29/100], Batch [32/32], Loss: 2.6800\n",
            "Epoch [30/100], Batch [1/32], Loss: 2.7540\n",
            "Epoch [30/100], Batch [2/32], Loss: 2.8755\n",
            "Epoch [30/100], Batch [3/32], Loss: 2.7817\n",
            "Epoch [30/100], Batch [4/32], Loss: 2.6728\n",
            "Epoch [30/100], Batch [5/32], Loss: 2.8080\n",
            "Epoch [30/100], Batch [6/32], Loss: 2.5273\n",
            "Epoch [30/100], Batch [7/32], Loss: 2.5829\n",
            "Epoch [30/100], Batch [8/32], Loss: 2.7018\n",
            "Epoch [30/100], Batch [9/32], Loss: 2.6749\n",
            "Epoch [30/100], Batch [10/32], Loss: 2.7503\n",
            "Epoch [30/100], Batch [11/32], Loss: 2.8312\n",
            "Epoch [30/100], Batch [12/32], Loss: 2.5684\n",
            "Epoch [30/100], Batch [13/32], Loss: 2.7714\n",
            "Epoch [30/100], Batch [14/32], Loss: 2.7935\n",
            "Epoch [30/100], Batch [15/32], Loss: 2.5497\n",
            "Epoch [30/100], Batch [16/32], Loss: 2.7892\n",
            "Epoch [30/100], Batch [17/32], Loss: 2.6934\n",
            "Epoch [30/100], Batch [18/32], Loss: 2.8011\n",
            "Epoch [30/100], Batch [19/32], Loss: 2.7939\n",
            "Epoch [30/100], Batch [20/32], Loss: 2.7519\n",
            "Epoch [30/100], Batch [21/32], Loss: 2.7249\n",
            "Epoch [30/100], Batch [22/32], Loss: 2.7084\n",
            "Epoch [30/100], Batch [23/32], Loss: 2.6214\n",
            "Epoch [30/100], Batch [24/32], Loss: 2.7094\n",
            "Epoch [30/100], Batch [25/32], Loss: 2.6544\n",
            "Epoch [30/100], Batch [26/32], Loss: 2.5834\n",
            "Epoch [30/100], Batch [27/32], Loss: 2.8412\n",
            "Epoch [30/100], Batch [28/32], Loss: 2.7779\n",
            "Epoch [30/100], Batch [29/32], Loss: 2.6008\n",
            "Epoch [30/100], Batch [30/32], Loss: 2.8031\n",
            "Epoch [30/100], Batch [31/32], Loss: 2.7940\n",
            "Epoch [30/100], Batch [32/32], Loss: 2.6990\n",
            "Epoch [31/100], Batch [1/32], Loss: 2.7476\n",
            "Epoch [31/100], Batch [2/32], Loss: 2.7693\n",
            "Epoch [31/100], Batch [3/32], Loss: 2.6287\n",
            "Epoch [31/100], Batch [4/32], Loss: 2.7767\n",
            "Epoch [31/100], Batch [5/32], Loss: 2.6500\n",
            "Epoch [31/100], Batch [6/32], Loss: 2.6819\n",
            "Epoch [31/100], Batch [7/32], Loss: 2.7716\n",
            "Epoch [31/100], Batch [8/32], Loss: 2.6755\n",
            "Epoch [31/100], Batch [9/32], Loss: 2.7231\n",
            "Epoch [31/100], Batch [10/32], Loss: 2.7963\n",
            "Epoch [31/100], Batch [11/32], Loss: 2.8195\n",
            "Epoch [31/100], Batch [12/32], Loss: 2.5755\n",
            "Epoch [31/100], Batch [13/32], Loss: 2.7504\n",
            "Epoch [31/100], Batch [14/32], Loss: 2.7169\n",
            "Epoch [31/100], Batch [15/32], Loss: 2.6380\n",
            "Epoch [31/100], Batch [16/32], Loss: 2.8205\n",
            "Epoch [31/100], Batch [17/32], Loss: 2.6415\n",
            "Epoch [31/100], Batch [18/32], Loss: 2.6420\n",
            "Epoch [31/100], Batch [19/32], Loss: 2.8265\n",
            "Epoch [31/100], Batch [20/32], Loss: 2.7791\n",
            "Epoch [31/100], Batch [21/32], Loss: 2.6878\n",
            "Epoch [31/100], Batch [22/32], Loss: 2.6819\n",
            "Epoch [31/100], Batch [23/32], Loss: 2.8473\n",
            "Epoch [31/100], Batch [24/32], Loss: 2.7517\n",
            "Epoch [31/100], Batch [25/32], Loss: 2.6568\n",
            "Epoch [31/100], Batch [26/32], Loss: 2.7437\n",
            "Epoch [31/100], Batch [27/32], Loss: 2.6401\n",
            "Epoch [31/100], Batch [28/32], Loss: 2.6818\n",
            "Epoch [31/100], Batch [29/32], Loss: 2.4982\n",
            "Epoch [31/100], Batch [30/32], Loss: 2.6574\n",
            "Epoch [31/100], Batch [31/32], Loss: 2.7401\n",
            "Epoch [31/100], Batch [32/32], Loss: 2.7284\n",
            "Epoch [32/100], Batch [1/32], Loss: 2.7663\n",
            "Epoch [32/100], Batch [2/32], Loss: 2.6996\n",
            "Epoch [32/100], Batch [3/32], Loss: 2.7350\n",
            "Epoch [32/100], Batch [4/32], Loss: 2.8003\n",
            "Epoch [32/100], Batch [5/32], Loss: 2.7530\n",
            "Epoch [32/100], Batch [6/32], Loss: 2.6965\n",
            "Epoch [32/100], Batch [7/32], Loss: 2.8226\n",
            "Epoch [32/100], Batch [8/32], Loss: 2.8051\n",
            "Epoch [32/100], Batch [9/32], Loss: 2.7818\n",
            "Epoch [32/100], Batch [10/32], Loss: 2.7904\n",
            "Epoch [32/100], Batch [11/32], Loss: 2.6820\n",
            "Epoch [32/100], Batch [12/32], Loss: 2.7601\n",
            "Epoch [32/100], Batch [13/32], Loss: 2.7346\n",
            "Epoch [32/100], Batch [14/32], Loss: 2.5290\n",
            "Epoch [32/100], Batch [15/32], Loss: 2.5992\n",
            "Epoch [32/100], Batch [16/32], Loss: 2.6486\n",
            "Epoch [32/100], Batch [17/32], Loss: 2.6926\n",
            "Epoch [32/100], Batch [18/32], Loss: 2.7992\n",
            "Epoch [32/100], Batch [19/32], Loss: 2.7057\n",
            "Epoch [32/100], Batch [20/32], Loss: 2.7503\n",
            "Epoch [32/100], Batch [21/32], Loss: 2.6226\n",
            "Epoch [32/100], Batch [22/32], Loss: 2.7131\n",
            "Epoch [32/100], Batch [23/32], Loss: 2.7771\n",
            "Epoch [32/100], Batch [24/32], Loss: 2.7247\n",
            "Epoch [32/100], Batch [25/32], Loss: 2.6421\n",
            "Epoch [32/100], Batch [26/32], Loss: 2.6920\n",
            "Epoch [32/100], Batch [27/32], Loss: 2.8381\n",
            "Epoch [32/100], Batch [28/32], Loss: 2.8298\n",
            "Epoch [32/100], Batch [29/32], Loss: 2.7815\n",
            "Epoch [32/100], Batch [30/32], Loss: 2.4699\n",
            "Epoch [32/100], Batch [31/32], Loss: 2.6189\n",
            "Epoch [32/100], Batch [32/32], Loss: 2.6732\n",
            "Epoch [33/100], Batch [1/32], Loss: 2.6019\n",
            "Epoch [33/100], Batch [2/32], Loss: 2.5463\n",
            "Epoch [33/100], Batch [3/32], Loss: 2.7305\n",
            "Epoch [33/100], Batch [4/32], Loss: 2.6616\n",
            "Epoch [33/100], Batch [5/32], Loss: 2.8410\n",
            "Epoch [33/100], Batch [6/32], Loss: 2.6721\n",
            "Epoch [33/100], Batch [7/32], Loss: 2.7512\n",
            "Epoch [33/100], Batch [8/32], Loss: 2.5991\n",
            "Epoch [33/100], Batch [9/32], Loss: 2.6999\n",
            "Epoch [33/100], Batch [10/32], Loss: 2.7115\n",
            "Epoch [33/100], Batch [11/32], Loss: 2.6161\n",
            "Epoch [33/100], Batch [12/32], Loss: 2.7281\n",
            "Epoch [33/100], Batch [13/32], Loss: 2.7516\n",
            "Epoch [33/100], Batch [14/32], Loss: 2.8393\n",
            "Epoch [33/100], Batch [15/32], Loss: 2.8175\n",
            "Epoch [33/100], Batch [16/32], Loss: 2.7439\n",
            "Epoch [33/100], Batch [17/32], Loss: 2.8143\n",
            "Epoch [33/100], Batch [18/32], Loss: 2.5973\n",
            "Epoch [33/100], Batch [19/32], Loss: 2.6508\n",
            "Epoch [33/100], Batch [20/32], Loss: 2.5786\n",
            "Epoch [33/100], Batch [21/32], Loss: 2.6432\n",
            "Epoch [33/100], Batch [22/32], Loss: 2.6476\n",
            "Epoch [33/100], Batch [23/32], Loss: 2.8274\n",
            "Epoch [33/100], Batch [24/32], Loss: 2.7525\n",
            "Epoch [33/100], Batch [25/32], Loss: 2.7244\n",
            "Epoch [33/100], Batch [26/32], Loss: 2.6131\n",
            "Epoch [33/100], Batch [27/32], Loss: 2.7820\n",
            "Epoch [33/100], Batch [28/32], Loss: 2.7439\n",
            "Epoch [33/100], Batch [29/32], Loss: 2.7547\n",
            "Epoch [33/100], Batch [30/32], Loss: 2.7604\n",
            "Epoch [33/100], Batch [31/32], Loss: 2.7327\n",
            "Epoch [33/100], Batch [32/32], Loss: 2.7121\n",
            "Epoch [34/100], Batch [1/32], Loss: 2.6665\n",
            "Epoch [34/100], Batch [2/32], Loss: 2.6790\n",
            "Epoch [34/100], Batch [3/32], Loss: 2.6787\n",
            "Epoch [34/100], Batch [4/32], Loss: 2.6659\n",
            "Epoch [34/100], Batch [5/32], Loss: 2.6069\n",
            "Epoch [34/100], Batch [6/32], Loss: 2.6486\n",
            "Epoch [34/100], Batch [7/32], Loss: 2.6852\n",
            "Epoch [34/100], Batch [8/32], Loss: 2.8072\n",
            "Epoch [34/100], Batch [9/32], Loss: 2.6738\n",
            "Epoch [34/100], Batch [10/32], Loss: 2.6618\n",
            "Epoch [34/100], Batch [11/32], Loss: 2.7582\n",
            "Epoch [34/100], Batch [12/32], Loss: 2.7337\n",
            "Epoch [34/100], Batch [13/32], Loss: 2.7983\n",
            "Epoch [34/100], Batch [14/32], Loss: 2.6987\n",
            "Epoch [34/100], Batch [15/32], Loss: 2.8258\n",
            "Epoch [34/100], Batch [16/32], Loss: 2.6965\n",
            "Epoch [34/100], Batch [17/32], Loss: 2.7153\n",
            "Epoch [34/100], Batch [18/32], Loss: 2.7961\n",
            "Epoch [34/100], Batch [19/32], Loss: 2.6992\n",
            "Epoch [34/100], Batch [20/32], Loss: 2.7811\n",
            "Epoch [34/100], Batch [21/32], Loss: 2.6935\n",
            "Epoch [34/100], Batch [22/32], Loss: 2.7616\n",
            "Epoch [34/100], Batch [23/32], Loss: 2.7383\n",
            "Epoch [34/100], Batch [24/32], Loss: 2.7449\n",
            "Epoch [34/100], Batch [25/32], Loss: 2.6626\n",
            "Epoch [34/100], Batch [26/32], Loss: 2.6168\n",
            "Epoch [34/100], Batch [27/32], Loss: 2.5937\n",
            "Epoch [34/100], Batch [28/32], Loss: 2.5663\n",
            "Epoch [34/100], Batch [29/32], Loss: 2.7660\n",
            "Epoch [34/100], Batch [30/32], Loss: 2.6849\n",
            "Epoch [34/100], Batch [31/32], Loss: 2.7573\n",
            "Epoch [34/100], Batch [32/32], Loss: 2.5929\n",
            "Epoch [35/100], Batch [1/32], Loss: 2.6805\n",
            "Epoch [35/100], Batch [2/32], Loss: 2.6846\n",
            "Epoch [35/100], Batch [3/32], Loss: 2.5441\n",
            "Epoch [35/100], Batch [4/32], Loss: 2.5658\n",
            "Epoch [35/100], Batch [5/32], Loss: 2.6658\n",
            "Epoch [35/100], Batch [6/32], Loss: 2.6803\n",
            "Epoch [35/100], Batch [7/32], Loss: 2.7837\n",
            "Epoch [35/100], Batch [8/32], Loss: 2.8183\n",
            "Epoch [35/100], Batch [9/32], Loss: 2.6811\n",
            "Epoch [35/100], Batch [10/32], Loss: 2.6368\n",
            "Epoch [35/100], Batch [11/32], Loss: 2.7166\n",
            "Epoch [35/100], Batch [12/32], Loss: 2.7095\n",
            "Epoch [35/100], Batch [13/32], Loss: 2.7621\n",
            "Epoch [35/100], Batch [14/32], Loss: 2.7589\n",
            "Epoch [35/100], Batch [15/32], Loss: 2.5900\n",
            "Epoch [35/100], Batch [16/32], Loss: 2.6434\n",
            "Epoch [35/100], Batch [17/32], Loss: 2.5947\n",
            "Epoch [35/100], Batch [18/32], Loss: 2.7821\n",
            "Epoch [35/100], Batch [19/32], Loss: 2.7299\n",
            "Epoch [35/100], Batch [20/32], Loss: 2.8110\n",
            "Epoch [35/100], Batch [21/32], Loss: 2.7935\n",
            "Epoch [35/100], Batch [22/32], Loss: 2.6897\n",
            "Epoch [35/100], Batch [23/32], Loss: 2.7894\n",
            "Epoch [35/100], Batch [24/32], Loss: 2.6938\n",
            "Epoch [35/100], Batch [25/32], Loss: 2.8367\n",
            "Epoch [35/100], Batch [26/32], Loss: 2.6572\n",
            "Epoch [35/100], Batch [27/32], Loss: 2.7155\n",
            "Epoch [35/100], Batch [28/32], Loss: 2.8721\n",
            "Epoch [35/100], Batch [29/32], Loss: 2.7816\n",
            "Epoch [35/100], Batch [30/32], Loss: 2.6238\n",
            "Epoch [35/100], Batch [31/32], Loss: 2.5723\n",
            "Epoch [35/100], Batch [32/32], Loss: 2.6406\n",
            "Epoch [36/100], Batch [1/32], Loss: 2.4711\n",
            "Epoch [36/100], Batch [2/32], Loss: 2.8348\n",
            "Epoch [36/100], Batch [3/32], Loss: 2.6595\n",
            "Epoch [36/100], Batch [4/32], Loss: 2.6429\n",
            "Epoch [36/100], Batch [5/32], Loss: 2.7466\n",
            "Epoch [36/100], Batch [6/32], Loss: 2.7632\n",
            "Epoch [36/100], Batch [7/32], Loss: 2.7619\n",
            "Epoch [36/100], Batch [8/32], Loss: 2.6615\n",
            "Epoch [36/100], Batch [9/32], Loss: 2.6698\n",
            "Epoch [36/100], Batch [10/32], Loss: 2.5719\n",
            "Epoch [36/100], Batch [11/32], Loss: 2.7274\n",
            "Epoch [36/100], Batch [12/32], Loss: 2.6276\n",
            "Epoch [36/100], Batch [13/32], Loss: 2.7549\n",
            "Epoch [36/100], Batch [14/32], Loss: 2.8716\n",
            "Epoch [36/100], Batch [15/32], Loss: 2.7388\n",
            "Epoch [36/100], Batch [16/32], Loss: 2.7352\n",
            "Epoch [36/100], Batch [17/32], Loss: 2.8225\n",
            "Epoch [36/100], Batch [18/32], Loss: 2.8352\n",
            "Epoch [36/100], Batch [19/32], Loss: 2.7068\n",
            "Epoch [36/100], Batch [20/32], Loss: 2.5988\n",
            "Epoch [36/100], Batch [21/32], Loss: 2.7187\n",
            "Epoch [36/100], Batch [22/32], Loss: 2.6998\n",
            "Epoch [36/100], Batch [23/32], Loss: 2.5938\n",
            "Epoch [36/100], Batch [24/32], Loss: 2.6363\n",
            "Epoch [36/100], Batch [25/32], Loss: 2.7339\n",
            "Epoch [36/100], Batch [26/32], Loss: 2.8607\n",
            "Epoch [36/100], Batch [27/32], Loss: 2.6627\n",
            "Epoch [36/100], Batch [28/32], Loss: 2.6550\n",
            "Epoch [36/100], Batch [29/32], Loss: 2.7139\n",
            "Epoch [36/100], Batch [30/32], Loss: 2.5908\n",
            "Epoch [36/100], Batch [31/32], Loss: 2.7431\n",
            "Epoch [36/100], Batch [32/32], Loss: 2.6280\n",
            "Epoch [37/100], Batch [1/32], Loss: 2.7711\n",
            "Epoch [37/100], Batch [2/32], Loss: 2.6864\n",
            "Epoch [37/100], Batch [3/32], Loss: 2.7443\n",
            "Epoch [37/100], Batch [4/32], Loss: 2.5957\n",
            "Epoch [37/100], Batch [5/32], Loss: 2.6878\n",
            "Epoch [37/100], Batch [6/32], Loss: 2.7299\n",
            "Epoch [37/100], Batch [7/32], Loss: 2.7471\n",
            "Epoch [37/100], Batch [8/32], Loss: 2.8026\n",
            "Epoch [37/100], Batch [9/32], Loss: 2.6007\n",
            "Epoch [37/100], Batch [10/32], Loss: 2.7242\n",
            "Epoch [37/100], Batch [11/32], Loss: 2.7066\n",
            "Epoch [37/100], Batch [12/32], Loss: 2.6043\n",
            "Epoch [37/100], Batch [13/32], Loss: 2.6874\n",
            "Epoch [37/100], Batch [14/32], Loss: 2.5862\n",
            "Epoch [37/100], Batch [15/32], Loss: 2.6328\n",
            "Epoch [37/100], Batch [16/32], Loss: 2.7100\n",
            "Epoch [37/100], Batch [17/32], Loss: 2.6986\n",
            "Epoch [37/100], Batch [18/32], Loss: 2.7635\n",
            "Epoch [37/100], Batch [19/32], Loss: 2.8253\n",
            "Epoch [37/100], Batch [20/32], Loss: 2.7450\n",
            "Epoch [37/100], Batch [21/32], Loss: 2.6497\n",
            "Epoch [37/100], Batch [22/32], Loss: 2.7240\n",
            "Epoch [37/100], Batch [23/32], Loss: 2.6491\n",
            "Epoch [37/100], Batch [24/32], Loss: 2.7409\n",
            "Epoch [37/100], Batch [25/32], Loss: 2.6977\n",
            "Epoch [37/100], Batch [26/32], Loss: 2.8511\n",
            "Epoch [37/100], Batch [27/32], Loss: 2.6985\n",
            "Epoch [37/100], Batch [28/32], Loss: 2.6644\n",
            "Epoch [37/100], Batch [29/32], Loss: 2.6523\n",
            "Epoch [37/100], Batch [30/32], Loss: 2.7350\n",
            "Epoch [37/100], Batch [31/32], Loss: 2.6254\n",
            "Epoch [37/100], Batch [32/32], Loss: 2.6843\n",
            "Epoch [38/100], Batch [1/32], Loss: 2.5914\n",
            "Epoch [38/100], Batch [2/32], Loss: 2.7457\n",
            "Epoch [38/100], Batch [3/32], Loss: 2.6182\n",
            "Epoch [38/100], Batch [4/32], Loss: 2.6368\n",
            "Epoch [38/100], Batch [5/32], Loss: 2.7924\n",
            "Epoch [38/100], Batch [6/32], Loss: 2.8135\n",
            "Epoch [38/100], Batch [7/32], Loss: 2.6139\n",
            "Epoch [38/100], Batch [8/32], Loss: 2.8382\n",
            "Epoch [38/100], Batch [9/32], Loss: 2.6709\n",
            "Epoch [38/100], Batch [10/32], Loss: 2.7370\n",
            "Epoch [38/100], Batch [11/32], Loss: 2.6693\n",
            "Epoch [38/100], Batch [12/32], Loss: 2.5335\n",
            "Epoch [38/100], Batch [13/32], Loss: 2.6058\n",
            "Epoch [38/100], Batch [14/32], Loss: 2.7093\n",
            "Epoch [38/100], Batch [15/32], Loss: 2.7529\n",
            "Epoch [38/100], Batch [16/32], Loss: 2.7272\n",
            "Epoch [38/100], Batch [17/32], Loss: 2.9058\n",
            "Epoch [38/100], Batch [18/32], Loss: 2.6694\n",
            "Epoch [38/100], Batch [19/32], Loss: 2.5265\n",
            "Epoch [38/100], Batch [20/32], Loss: 2.7721\n",
            "Epoch [38/100], Batch [21/32], Loss: 2.7235\n",
            "Epoch [38/100], Batch [22/32], Loss: 2.7591\n",
            "Epoch [38/100], Batch [23/32], Loss: 2.7481\n",
            "Epoch [38/100], Batch [24/32], Loss: 2.6203\n",
            "Epoch [38/100], Batch [25/32], Loss: 2.6907\n",
            "Epoch [38/100], Batch [26/32], Loss: 2.6790\n",
            "Epoch [38/100], Batch [27/32], Loss: 2.7464\n",
            "Epoch [38/100], Batch [28/32], Loss: 2.6174\n",
            "Epoch [38/100], Batch [29/32], Loss: 2.8425\n",
            "Epoch [38/100], Batch [30/32], Loss: 2.6566\n",
            "Epoch [38/100], Batch [31/32], Loss: 2.5957\n",
            "Epoch [38/100], Batch [32/32], Loss: 2.6892\n",
            "Epoch [39/100], Batch [1/32], Loss: 2.6061\n",
            "Epoch [39/100], Batch [2/32], Loss: 2.7470\n",
            "Epoch [39/100], Batch [3/32], Loss: 2.7439\n",
            "Epoch [39/100], Batch [4/32], Loss: 2.6765\n",
            "Epoch [39/100], Batch [5/32], Loss: 2.6672\n",
            "Epoch [39/100], Batch [6/32], Loss: 2.6875\n",
            "Epoch [39/100], Batch [7/32], Loss: 2.8282\n",
            "Epoch [39/100], Batch [8/32], Loss: 2.6809\n",
            "Epoch [39/100], Batch [9/32], Loss: 2.7259\n",
            "Epoch [39/100], Batch [10/32], Loss: 2.5658\n",
            "Epoch [39/100], Batch [11/32], Loss: 2.8062\n",
            "Epoch [39/100], Batch [12/32], Loss: 2.6692\n",
            "Epoch [39/100], Batch [13/32], Loss: 2.6382\n",
            "Epoch [39/100], Batch [14/32], Loss: 2.7073\n",
            "Epoch [39/100], Batch [15/32], Loss: 2.7433\n",
            "Epoch [39/100], Batch [16/32], Loss: 2.7049\n",
            "Epoch [39/100], Batch [17/32], Loss: 2.8023\n",
            "Epoch [39/100], Batch [18/32], Loss: 2.8003\n",
            "Epoch [39/100], Batch [19/32], Loss: 2.7003\n",
            "Epoch [39/100], Batch [20/32], Loss: 2.7777\n",
            "Epoch [39/100], Batch [21/32], Loss: 2.6409\n",
            "Epoch [39/100], Batch [22/32], Loss: 2.5588\n",
            "Epoch [39/100], Batch [23/32], Loss: 2.4890\n",
            "Epoch [39/100], Batch [24/32], Loss: 2.6924\n",
            "Epoch [39/100], Batch [25/32], Loss: 2.6490\n",
            "Epoch [39/100], Batch [26/32], Loss: 2.6474\n",
            "Epoch [39/100], Batch [27/32], Loss: 2.6937\n",
            "Epoch [39/100], Batch [28/32], Loss: 2.6192\n",
            "Epoch [39/100], Batch [29/32], Loss: 2.5156\n",
            "Epoch [39/100], Batch [30/32], Loss: 2.7332\n",
            "Epoch [39/100], Batch [31/32], Loss: 2.8053\n",
            "Epoch [39/100], Batch [32/32], Loss: 2.8482\n",
            "Epoch [40/100], Batch [1/32], Loss: 2.6471\n",
            "Epoch [40/100], Batch [2/32], Loss: 2.7630\n",
            "Epoch [40/100], Batch [3/32], Loss: 2.6865\n",
            "Epoch [40/100], Batch [4/32], Loss: 2.7645\n",
            "Epoch [40/100], Batch [5/32], Loss: 2.5588\n",
            "Epoch [40/100], Batch [6/32], Loss: 2.6722\n",
            "Epoch [40/100], Batch [7/32], Loss: 2.6999\n",
            "Epoch [40/100], Batch [8/32], Loss: 2.6436\n",
            "Epoch [40/100], Batch [9/32], Loss: 2.7461\n",
            "Epoch [40/100], Batch [10/32], Loss: 2.5852\n",
            "Epoch [40/100], Batch [11/32], Loss: 2.7183\n",
            "Epoch [40/100], Batch [12/32], Loss: 2.6691\n",
            "Epoch [40/100], Batch [13/32], Loss: 2.6524\n",
            "Epoch [40/100], Batch [14/32], Loss: 2.6945\n",
            "Epoch [40/100], Batch [15/32], Loss: 2.7444\n",
            "Epoch [40/100], Batch [16/32], Loss: 2.7960\n",
            "Epoch [40/100], Batch [17/32], Loss: 2.5691\n",
            "Epoch [40/100], Batch [18/32], Loss: 2.8362\n",
            "Epoch [40/100], Batch [19/32], Loss: 2.7916\n",
            "Epoch [40/100], Batch [20/32], Loss: 2.4933\n",
            "Epoch [40/100], Batch [21/32], Loss: 2.7530\n",
            "Epoch [40/100], Batch [22/32], Loss: 2.6796\n",
            "Epoch [40/100], Batch [23/32], Loss: 2.7584\n",
            "Epoch [40/100], Batch [24/32], Loss: 2.6822\n",
            "Epoch [40/100], Batch [25/32], Loss: 2.6040\n",
            "Epoch [40/100], Batch [26/32], Loss: 2.8013\n",
            "Epoch [40/100], Batch [27/32], Loss: 2.5606\n",
            "Epoch [40/100], Batch [28/32], Loss: 2.7635\n",
            "Epoch [40/100], Batch [29/32], Loss: 2.7041\n",
            "Epoch [40/100], Batch [30/32], Loss: 2.6216\n",
            "Epoch [40/100], Batch [31/32], Loss: 2.7025\n",
            "Epoch [40/100], Batch [32/32], Loss: 2.8149\n",
            "Epoch [41/100], Batch [1/32], Loss: 2.6070\n",
            "Epoch [41/100], Batch [2/32], Loss: 2.7672\n",
            "Epoch [41/100], Batch [3/32], Loss: 2.7674\n",
            "Epoch [41/100], Batch [4/32], Loss: 2.6176\n",
            "Epoch [41/100], Batch [5/32], Loss: 2.6382\n",
            "Epoch [41/100], Batch [6/32], Loss: 2.6786\n",
            "Epoch [41/100], Batch [7/32], Loss: 2.7309\n",
            "Epoch [41/100], Batch [8/32], Loss: 2.5397\n",
            "Epoch [41/100], Batch [9/32], Loss: 2.6570\n",
            "Epoch [41/100], Batch [10/32], Loss: 2.6455\n",
            "Epoch [41/100], Batch [11/32], Loss: 2.5981\n",
            "Epoch [41/100], Batch [12/32], Loss: 2.7264\n",
            "Epoch [41/100], Batch [13/32], Loss: 2.7867\n",
            "Epoch [41/100], Batch [14/32], Loss: 2.7665\n",
            "Epoch [41/100], Batch [15/32], Loss: 2.7129\n",
            "Epoch [41/100], Batch [16/32], Loss: 2.6909\n",
            "Epoch [41/100], Batch [17/32], Loss: 2.5649\n",
            "Epoch [41/100], Batch [18/32], Loss: 2.6695\n",
            "Epoch [41/100], Batch [19/32], Loss: 2.6482\n",
            "Epoch [41/100], Batch [20/32], Loss: 2.6732\n",
            "Epoch [41/100], Batch [21/32], Loss: 2.7460\n",
            "Epoch [41/100], Batch [22/32], Loss: 2.5987\n",
            "Epoch [41/100], Batch [23/32], Loss: 2.6916\n",
            "Epoch [41/100], Batch [24/32], Loss: 2.7019\n",
            "Epoch [41/100], Batch [25/32], Loss: 2.7369\n",
            "Epoch [41/100], Batch [26/32], Loss: 2.8057\n",
            "Epoch [41/100], Batch [27/32], Loss: 2.6170\n",
            "Epoch [41/100], Batch [28/32], Loss: 2.5702\n",
            "Epoch [41/100], Batch [29/32], Loss: 2.6105\n",
            "Epoch [41/100], Batch [30/32], Loss: 2.7378\n",
            "Epoch [41/100], Batch [31/32], Loss: 2.7051\n",
            "Epoch [41/100], Batch [32/32], Loss: 2.8480\n",
            "Epoch [42/100], Batch [1/32], Loss: 2.7817\n",
            "Epoch [42/100], Batch [2/32], Loss: 2.7535\n",
            "Epoch [42/100], Batch [3/32], Loss: 2.6060\n",
            "Epoch [42/100], Batch [4/32], Loss: 2.6483\n",
            "Epoch [42/100], Batch [5/32], Loss: 2.6037\n",
            "Epoch [42/100], Batch [6/32], Loss: 2.5888\n",
            "Epoch [42/100], Batch [7/32], Loss: 2.7234\n",
            "Epoch [42/100], Batch [8/32], Loss: 2.6597\n",
            "Epoch [42/100], Batch [9/32], Loss: 2.6698\n",
            "Epoch [42/100], Batch [10/32], Loss: 2.6353\n",
            "Epoch [42/100], Batch [11/32], Loss: 2.6631\n",
            "Epoch [42/100], Batch [12/32], Loss: 2.7184\n",
            "Epoch [42/100], Batch [13/32], Loss: 2.6368\n",
            "Epoch [42/100], Batch [14/32], Loss: 2.7224\n",
            "Epoch [42/100], Batch [15/32], Loss: 2.7370\n",
            "Epoch [42/100], Batch [16/32], Loss: 2.8818\n",
            "Epoch [42/100], Batch [17/32], Loss: 2.6398\n",
            "Epoch [42/100], Batch [18/32], Loss: 2.6368\n",
            "Epoch [42/100], Batch [19/32], Loss: 2.4678\n",
            "Epoch [42/100], Batch [20/32], Loss: 2.7404\n",
            "Epoch [42/100], Batch [21/32], Loss: 2.8245\n",
            "Epoch [42/100], Batch [22/32], Loss: 2.6312\n",
            "Epoch [42/100], Batch [23/32], Loss: 2.6589\n",
            "Epoch [42/100], Batch [24/32], Loss: 2.6989\n",
            "Epoch [42/100], Batch [25/32], Loss: 2.6832\n",
            "Epoch [42/100], Batch [26/32], Loss: 2.8477\n",
            "Epoch [42/100], Batch [27/32], Loss: 2.6331\n",
            "Epoch [42/100], Batch [28/32], Loss: 2.6766\n",
            "Epoch [42/100], Batch [29/32], Loss: 2.6854\n",
            "Epoch [42/100], Batch [30/32], Loss: 2.5136\n",
            "Epoch [42/100], Batch [31/32], Loss: 2.6822\n",
            "Epoch [42/100], Batch [32/32], Loss: 2.9138\n",
            "Epoch [43/100], Batch [1/32], Loss: 2.6489\n",
            "Epoch [43/100], Batch [2/32], Loss: 2.5279\n",
            "Epoch [43/100], Batch [3/32], Loss: 2.6657\n",
            "Epoch [43/100], Batch [4/32], Loss: 2.6421\n",
            "Epoch [43/100], Batch [5/32], Loss: 2.7402\n",
            "Epoch [43/100], Batch [6/32], Loss: 2.7334\n",
            "Epoch [43/100], Batch [7/32], Loss: 2.5954\n",
            "Epoch [43/100], Batch [8/32], Loss: 2.8147\n",
            "Epoch [43/100], Batch [9/32], Loss: 2.5709\n",
            "Epoch [43/100], Batch [10/32], Loss: 2.6154\n",
            "Epoch [43/100], Batch [11/32], Loss: 2.7789\n",
            "Epoch [43/100], Batch [12/32], Loss: 2.7768\n",
            "Epoch [43/100], Batch [13/32], Loss: 2.6998\n",
            "Epoch [43/100], Batch [14/32], Loss: 2.6792\n",
            "Epoch [43/100], Batch [15/32], Loss: 2.7333\n",
            "Epoch [43/100], Batch [16/32], Loss: 2.6607\n",
            "Epoch [43/100], Batch [17/32], Loss: 2.5837\n",
            "Epoch [43/100], Batch [18/32], Loss: 2.7664\n",
            "Epoch [43/100], Batch [19/32], Loss: 2.5812\n",
            "Epoch [43/100], Batch [20/32], Loss: 2.6196\n",
            "Epoch [43/100], Batch [21/32], Loss: 2.7843\n",
            "Epoch [43/100], Batch [22/32], Loss: 2.7121\n",
            "Epoch [43/100], Batch [23/32], Loss: 2.7100\n",
            "Epoch [43/100], Batch [24/32], Loss: 2.5418\n",
            "Epoch [43/100], Batch [25/32], Loss: 2.7183\n",
            "Epoch [43/100], Batch [26/32], Loss: 2.6378\n",
            "Epoch [43/100], Batch [27/32], Loss: 2.7949\n",
            "Epoch [43/100], Batch [28/32], Loss: 2.5482\n",
            "Epoch [43/100], Batch [29/32], Loss: 2.5904\n",
            "Epoch [43/100], Batch [30/32], Loss: 2.6982\n",
            "Epoch [43/100], Batch [31/32], Loss: 2.7584\n",
            "Epoch [43/100], Batch [32/32], Loss: 2.6710\n",
            "Epoch [44/100], Batch [1/32], Loss: 2.7863\n",
            "Epoch [44/100], Batch [2/32], Loss: 2.8029\n",
            "Epoch [44/100], Batch [3/32], Loss: 2.6282\n",
            "Epoch [44/100], Batch [4/32], Loss: 2.7931\n",
            "Epoch [44/100], Batch [5/32], Loss: 2.6191\n",
            "Epoch [44/100], Batch [6/32], Loss: 2.5495\n",
            "Epoch [44/100], Batch [7/32], Loss: 2.7673\n",
            "Epoch [44/100], Batch [8/32], Loss: 2.6314\n",
            "Epoch [44/100], Batch [9/32], Loss: 2.5810\n",
            "Epoch [44/100], Batch [10/32], Loss: 2.6959\n",
            "Epoch [44/100], Batch [11/32], Loss: 2.6246\n",
            "Epoch [44/100], Batch [12/32], Loss: 2.6360\n",
            "Epoch [44/100], Batch [13/32], Loss: 2.6337\n",
            "Epoch [44/100], Batch [14/32], Loss: 2.7684\n",
            "Epoch [44/100], Batch [15/32], Loss: 2.7971\n",
            "Epoch [44/100], Batch [16/32], Loss: 2.5988\n",
            "Epoch [44/100], Batch [17/32], Loss: 2.6894\n",
            "Epoch [44/100], Batch [18/32], Loss: 2.7938\n",
            "Epoch [44/100], Batch [19/32], Loss: 2.9048\n",
            "Epoch [44/100], Batch [20/32], Loss: 2.8274\n",
            "Epoch [44/100], Batch [21/32], Loss: 2.7036\n",
            "Epoch [44/100], Batch [22/32], Loss: 2.7561\n",
            "Epoch [44/100], Batch [23/32], Loss: 2.6140\n",
            "Epoch [44/100], Batch [24/32], Loss: 2.6613\n",
            "Epoch [44/100], Batch [25/32], Loss: 2.6463\n",
            "Epoch [44/100], Batch [26/32], Loss: 2.5674\n",
            "Epoch [44/100], Batch [27/32], Loss: 2.6346\n",
            "Epoch [44/100], Batch [28/32], Loss: 2.6639\n",
            "Epoch [44/100], Batch [29/32], Loss: 2.7314\n",
            "Epoch [44/100], Batch [30/32], Loss: 2.7228\n",
            "Epoch [44/100], Batch [31/32], Loss: 2.4307\n",
            "Epoch [44/100], Batch [32/32], Loss: 2.6987\n",
            "Epoch [45/100], Batch [1/32], Loss: 2.6093\n",
            "Epoch [45/100], Batch [2/32], Loss: 2.6169\n",
            "Epoch [45/100], Batch [3/32], Loss: 2.7137\n",
            "Epoch [45/100], Batch [4/32], Loss: 2.5776\n",
            "Epoch [45/100], Batch [5/32], Loss: 2.7562\n",
            "Epoch [45/100], Batch [6/32], Loss: 2.7285\n",
            "Epoch [45/100], Batch [7/32], Loss: 2.5529\n",
            "Epoch [45/100], Batch [8/32], Loss: 2.6400\n",
            "Epoch [45/100], Batch [9/32], Loss: 2.6121\n",
            "Epoch [45/100], Batch [10/32], Loss: 2.6390\n",
            "Epoch [45/100], Batch [11/32], Loss: 2.6955\n",
            "Epoch [45/100], Batch [12/32], Loss: 2.5955\n",
            "Epoch [45/100], Batch [13/32], Loss: 2.7637\n",
            "Epoch [45/100], Batch [14/32], Loss: 2.7092\n",
            "Epoch [45/100], Batch [15/32], Loss: 2.5760\n",
            "Epoch [45/100], Batch [16/32], Loss: 2.7420\n",
            "Epoch [45/100], Batch [17/32], Loss: 2.8022\n",
            "Epoch [45/100], Batch [18/32], Loss: 2.7702\n",
            "Epoch [45/100], Batch [19/32], Loss: 2.7056\n",
            "Epoch [45/100], Batch [20/32], Loss: 2.7225\n",
            "Epoch [45/100], Batch [21/32], Loss: 2.7354\n",
            "Epoch [45/100], Batch [22/32], Loss: 2.6025\n",
            "Epoch [45/100], Batch [23/32], Loss: 2.7390\n",
            "Epoch [45/100], Batch [24/32], Loss: 2.7782\n",
            "Epoch [45/100], Batch [25/32], Loss: 2.8088\n",
            "Epoch [45/100], Batch [26/32], Loss: 2.5271\n",
            "Epoch [45/100], Batch [27/32], Loss: 2.7094\n",
            "Epoch [45/100], Batch [28/32], Loss: 2.6792\n",
            "Epoch [45/100], Batch [29/32], Loss: 2.7574\n",
            "Epoch [45/100], Batch [30/32], Loss: 2.7086\n",
            "Epoch [45/100], Batch [31/32], Loss: 2.7151\n",
            "Epoch [45/100], Batch [32/32], Loss: 2.7310\n",
            "Epoch [46/100], Batch [1/32], Loss: 2.6502\n",
            "Epoch [46/100], Batch [2/32], Loss: 2.6695\n",
            "Epoch [46/100], Batch [3/32], Loss: 2.6923\n",
            "Epoch [46/100], Batch [4/32], Loss: 2.9127\n",
            "Epoch [46/100], Batch [5/32], Loss: 2.6665\n",
            "Epoch [46/100], Batch [6/32], Loss: 2.6475\n",
            "Epoch [46/100], Batch [7/32], Loss: 2.6745\n",
            "Epoch [46/100], Batch [8/32], Loss: 2.6485\n",
            "Epoch [46/100], Batch [9/32], Loss: 2.6342\n",
            "Epoch [46/100], Batch [10/32], Loss: 2.6730\n",
            "Epoch [46/100], Batch [11/32], Loss: 2.6821\n",
            "Epoch [46/100], Batch [12/32], Loss: 2.5802\n",
            "Epoch [46/100], Batch [13/32], Loss: 2.6439\n",
            "Epoch [46/100], Batch [14/32], Loss: 2.7279\n",
            "Epoch [46/100], Batch [15/32], Loss: 2.4625\n",
            "Epoch [46/100], Batch [16/32], Loss: 2.4831\n",
            "Epoch [46/100], Batch [17/32], Loss: 2.7086\n",
            "Epoch [46/100], Batch [18/32], Loss: 2.6962\n",
            "Epoch [46/100], Batch [19/32], Loss: 2.5743\n",
            "Epoch [46/100], Batch [20/32], Loss: 2.8159\n",
            "Epoch [46/100], Batch [21/32], Loss: 2.7204\n",
            "Epoch [46/100], Batch [22/32], Loss: 2.7091\n",
            "Epoch [46/100], Batch [23/32], Loss: 2.6998\n",
            "Epoch [46/100], Batch [24/32], Loss: 2.6511\n",
            "Epoch [46/100], Batch [25/32], Loss: 2.7325\n",
            "Epoch [46/100], Batch [26/32], Loss: 2.6817\n",
            "Epoch [46/100], Batch [27/32], Loss: 2.6347\n",
            "Epoch [46/100], Batch [28/32], Loss: 2.6613\n",
            "Epoch [46/100], Batch [29/32], Loss: 2.7106\n",
            "Epoch [46/100], Batch [30/32], Loss: 2.6904\n",
            "Epoch [46/100], Batch [31/32], Loss: 2.7040\n",
            "Epoch [46/100], Batch [32/32], Loss: 2.7171\n",
            "Epoch [47/100], Batch [1/32], Loss: 2.6857\n",
            "Epoch [47/100], Batch [2/32], Loss: 2.6223\n",
            "Epoch [47/100], Batch [3/32], Loss: 2.7537\n",
            "Epoch [47/100], Batch [4/32], Loss: 2.7803\n",
            "Epoch [47/100], Batch [5/32], Loss: 2.6322\n",
            "Epoch [47/100], Batch [6/32], Loss: 2.4924\n",
            "Epoch [47/100], Batch [7/32], Loss: 2.7478\n",
            "Epoch [47/100], Batch [8/32], Loss: 2.6469\n",
            "Epoch [47/100], Batch [9/32], Loss: 2.6302\n",
            "Epoch [47/100], Batch [10/32], Loss: 2.5601\n",
            "Epoch [47/100], Batch [11/32], Loss: 2.6448\n",
            "Epoch [47/100], Batch [12/32], Loss: 2.7070\n",
            "Epoch [47/100], Batch [13/32], Loss: 2.7208\n",
            "Epoch [47/100], Batch [14/32], Loss: 2.7478\n",
            "Epoch [47/100], Batch [15/32], Loss: 2.6088\n",
            "Epoch [47/100], Batch [16/32], Loss: 2.6573\n",
            "Epoch [47/100], Batch [17/32], Loss: 2.6349\n",
            "Epoch [47/100], Batch [18/32], Loss: 2.8152\n",
            "Epoch [47/100], Batch [19/32], Loss: 2.6713\n",
            "Epoch [47/100], Batch [20/32], Loss: 2.6964\n",
            "Epoch [47/100], Batch [21/32], Loss: 2.6696\n",
            "Epoch [47/100], Batch [22/32], Loss: 2.7450\n",
            "Epoch [47/100], Batch [23/32], Loss: 2.6657\n",
            "Epoch [47/100], Batch [24/32], Loss: 2.6561\n",
            "Epoch [47/100], Batch [25/32], Loss: 2.6914\n",
            "Epoch [47/100], Batch [26/32], Loss: 2.6100\n",
            "Epoch [47/100], Batch [27/32], Loss: 2.6502\n",
            "Epoch [47/100], Batch [28/32], Loss: 2.6465\n",
            "Epoch [47/100], Batch [29/32], Loss: 2.5994\n",
            "Epoch [47/100], Batch [30/32], Loss: 2.6518\n",
            "Epoch [47/100], Batch [31/32], Loss: 2.6772\n",
            "Epoch [47/100], Batch [32/32], Loss: 2.6999\n",
            "Epoch [48/100], Batch [1/32], Loss: 2.7129\n",
            "Epoch [48/100], Batch [2/32], Loss: 2.7611\n",
            "Epoch [48/100], Batch [3/32], Loss: 2.6140\n",
            "Epoch [48/100], Batch [4/32], Loss: 2.4438\n",
            "Epoch [48/100], Batch [5/32], Loss: 2.7038\n",
            "Epoch [48/100], Batch [6/32], Loss: 2.7931\n",
            "Epoch [48/100], Batch [7/32], Loss: 2.5851\n",
            "Epoch [48/100], Batch [8/32], Loss: 2.7007\n",
            "Epoch [48/100], Batch [9/32], Loss: 2.5758\n",
            "Epoch [48/100], Batch [10/32], Loss: 2.6837\n",
            "Epoch [48/100], Batch [11/32], Loss: 2.7790\n",
            "Epoch [48/100], Batch [12/32], Loss: 2.5827\n",
            "Epoch [48/100], Batch [13/32], Loss: 2.5372\n",
            "Epoch [48/100], Batch [14/32], Loss: 2.7418\n",
            "Epoch [48/100], Batch [15/32], Loss: 2.6642\n",
            "Epoch [48/100], Batch [16/32], Loss: 2.6425\n",
            "Epoch [48/100], Batch [17/32], Loss: 2.7644\n",
            "Epoch [48/100], Batch [18/32], Loss: 2.7679\n",
            "Epoch [48/100], Batch [19/32], Loss: 2.5409\n",
            "Epoch [48/100], Batch [20/32], Loss: 2.7711\n",
            "Epoch [48/100], Batch [21/32], Loss: 2.6121\n",
            "Epoch [48/100], Batch [22/32], Loss: 2.7046\n",
            "Epoch [48/100], Batch [23/32], Loss: 2.5495\n",
            "Epoch [48/100], Batch [24/32], Loss: 2.5731\n",
            "Epoch [48/100], Batch [25/32], Loss: 2.6595\n",
            "Epoch [48/100], Batch [26/32], Loss: 2.7012\n",
            "Epoch [48/100], Batch [27/32], Loss: 2.8043\n",
            "Epoch [48/100], Batch [28/32], Loss: 2.7080\n",
            "Epoch [48/100], Batch [29/32], Loss: 2.7387\n",
            "Epoch [48/100], Batch [30/32], Loss: 2.6374\n",
            "Epoch [48/100], Batch [31/32], Loss: 2.5544\n",
            "Epoch [48/100], Batch [32/32], Loss: 2.7200\n",
            "Epoch [49/100], Batch [1/32], Loss: 2.6894\n",
            "Epoch [49/100], Batch [2/32], Loss: 2.5745\n",
            "Epoch [49/100], Batch [3/32], Loss: 2.6048\n",
            "Epoch [49/100], Batch [4/32], Loss: 2.5528\n",
            "Epoch [49/100], Batch [5/32], Loss: 2.5708\n",
            "Epoch [49/100], Batch [6/32], Loss: 2.6404\n",
            "Epoch [49/100], Batch [7/32], Loss: 2.6538\n",
            "Epoch [49/100], Batch [8/32], Loss: 2.7520\n",
            "Epoch [49/100], Batch [9/32], Loss: 2.7546\n",
            "Epoch [49/100], Batch [10/32], Loss: 2.5606\n",
            "Epoch [49/100], Batch [11/32], Loss: 2.6062\n",
            "Epoch [49/100], Batch [12/32], Loss: 2.7899\n",
            "Epoch [49/100], Batch [13/32], Loss: 2.7040\n",
            "Epoch [49/100], Batch [14/32], Loss: 2.5721\n",
            "Epoch [49/100], Batch [15/32], Loss: 2.8137\n",
            "Epoch [49/100], Batch [16/32], Loss: 2.7183\n",
            "Epoch [49/100], Batch [17/32], Loss: 2.6033\n",
            "Epoch [49/100], Batch [18/32], Loss: 2.6989\n",
            "Epoch [49/100], Batch [19/32], Loss: 2.6807\n",
            "Epoch [49/100], Batch [20/32], Loss: 2.7277\n",
            "Epoch [49/100], Batch [21/32], Loss: 2.5937\n",
            "Epoch [49/100], Batch [22/32], Loss: 2.7610\n",
            "Epoch [49/100], Batch [23/32], Loss: 2.7379\n",
            "Epoch [49/100], Batch [24/32], Loss: 2.4318\n",
            "Epoch [49/100], Batch [25/32], Loss: 2.6925\n",
            "Epoch [49/100], Batch [26/32], Loss: 2.7887\n",
            "Epoch [49/100], Batch [27/32], Loss: 2.6872\n",
            "Epoch [49/100], Batch [28/32], Loss: 2.5853\n",
            "Epoch [49/100], Batch [29/32], Loss: 2.6816\n",
            "Epoch [49/100], Batch [30/32], Loss: 2.5677\n",
            "Epoch [49/100], Batch [31/32], Loss: 2.6620\n",
            "Epoch [49/100], Batch [32/32], Loss: 2.7089\n",
            "Epoch [50/100], Batch [1/32], Loss: 2.6354\n",
            "Epoch [50/100], Batch [2/32], Loss: 2.6983\n",
            "Epoch [50/100], Batch [3/32], Loss: 2.5645\n",
            "Epoch [50/100], Batch [4/32], Loss: 2.7176\n",
            "Epoch [50/100], Batch [5/32], Loss: 2.6709\n",
            "Epoch [50/100], Batch [6/32], Loss: 2.7118\n",
            "Epoch [50/100], Batch [7/32], Loss: 2.6631\n",
            "Epoch [50/100], Batch [8/32], Loss: 2.6367\n",
            "Epoch [50/100], Batch [9/32], Loss: 2.8359\n",
            "Epoch [50/100], Batch [10/32], Loss: 2.6849\n",
            "Epoch [50/100], Batch [11/32], Loss: 2.7809\n",
            "Epoch [50/100], Batch [12/32], Loss: 2.6413\n",
            "Epoch [50/100], Batch [13/32], Loss: 2.5677\n",
            "Epoch [50/100], Batch [14/32], Loss: 2.6303\n",
            "Epoch [50/100], Batch [15/32], Loss: 2.5331\n",
            "Epoch [50/100], Batch [16/32], Loss: 2.7803\n",
            "Epoch [50/100], Batch [17/32], Loss: 2.6470\n",
            "Epoch [50/100], Batch [18/32], Loss: 2.7781\n",
            "Epoch [50/100], Batch [19/32], Loss: 2.6863\n",
            "Epoch [50/100], Batch [20/32], Loss: 2.7949\n",
            "Epoch [50/100], Batch [21/32], Loss: 2.5266\n",
            "Epoch [50/100], Batch [22/32], Loss: 2.6553\n",
            "Epoch [50/100], Batch [23/32], Loss: 2.5105\n",
            "Epoch [50/100], Batch [24/32], Loss: 2.7629\n",
            "Epoch [50/100], Batch [25/32], Loss: 2.5934\n",
            "Epoch [50/100], Batch [26/32], Loss: 2.6921\n",
            "Epoch [50/100], Batch [27/32], Loss: 2.6055\n",
            "Epoch [50/100], Batch [28/32], Loss: 2.7084\n",
            "Epoch [50/100], Batch [29/32], Loss: 2.8381\n",
            "Epoch [50/100], Batch [30/32], Loss: 2.6174\n",
            "Epoch [50/100], Batch [31/32], Loss: 2.7411\n",
            "Epoch [50/100], Batch [32/32], Loss: 2.7210\n",
            "Epoch [51/100], Batch [1/32], Loss: 2.7191\n",
            "Epoch [51/100], Batch [2/32], Loss: 2.5912\n",
            "Epoch [51/100], Batch [3/32], Loss: 2.6700\n",
            "Epoch [51/100], Batch [4/32], Loss: 2.4560\n",
            "Epoch [51/100], Batch [5/32], Loss: 2.7196\n",
            "Epoch [51/100], Batch [6/32], Loss: 2.6412\n",
            "Epoch [51/100], Batch [7/32], Loss: 2.7498\n",
            "Epoch [51/100], Batch [8/32], Loss: 2.7916\n",
            "Epoch [51/100], Batch [9/32], Loss: 2.6905\n",
            "Epoch [51/100], Batch [10/32], Loss: 2.6692\n",
            "Epoch [51/100], Batch [11/32], Loss: 2.7197\n",
            "Epoch [51/100], Batch [12/32], Loss: 2.5920\n",
            "Epoch [51/100], Batch [13/32], Loss: 2.6204\n",
            "Epoch [51/100], Batch [14/32], Loss: 2.6790\n",
            "Epoch [51/100], Batch [15/32], Loss: 2.7903\n",
            "Epoch [51/100], Batch [16/32], Loss: 2.5537\n",
            "Epoch [51/100], Batch [17/32], Loss: 2.5881\n",
            "Epoch [51/100], Batch [18/32], Loss: 2.6512\n",
            "Epoch [51/100], Batch [19/32], Loss: 2.5851\n",
            "Epoch [51/100], Batch [20/32], Loss: 2.6687\n",
            "Epoch [51/100], Batch [21/32], Loss: 2.6222\n",
            "Epoch [51/100], Batch [22/32], Loss: 2.6786\n",
            "Epoch [51/100], Batch [23/32], Loss: 2.7208\n",
            "Epoch [51/100], Batch [24/32], Loss: 2.5446\n",
            "Epoch [51/100], Batch [25/32], Loss: 2.7392\n",
            "Epoch [51/100], Batch [26/32], Loss: 2.6175\n",
            "Epoch [51/100], Batch [27/32], Loss: 2.6602\n",
            "Epoch [51/100], Batch [28/32], Loss: 2.6425\n",
            "Epoch [51/100], Batch [29/32], Loss: 2.6482\n",
            "Epoch [51/100], Batch [30/32], Loss: 2.6659\n",
            "Epoch [51/100], Batch [31/32], Loss: 2.7913\n",
            "Epoch [51/100], Batch [32/32], Loss: 2.5524\n",
            "Epoch [52/100], Batch [1/32], Loss: 2.6233\n",
            "Epoch [52/100], Batch [2/32], Loss: 2.6652\n",
            "Epoch [52/100], Batch [3/32], Loss: 2.7343\n",
            "Epoch [52/100], Batch [4/32], Loss: 2.5839\n",
            "Epoch [52/100], Batch [5/32], Loss: 2.6536\n",
            "Epoch [52/100], Batch [6/32], Loss: 2.6102\n",
            "Epoch [52/100], Batch [7/32], Loss: 2.6522\n",
            "Epoch [52/100], Batch [8/32], Loss: 2.5147\n",
            "Epoch [52/100], Batch [9/32], Loss: 2.6575\n",
            "Epoch [52/100], Batch [10/32], Loss: 2.7511\n",
            "Epoch [52/100], Batch [11/32], Loss: 2.5448\n",
            "Epoch [52/100], Batch [12/32], Loss: 2.5789\n",
            "Epoch [52/100], Batch [13/32], Loss: 2.7673\n",
            "Epoch [52/100], Batch [14/32], Loss: 2.8011\n",
            "Epoch [52/100], Batch [15/32], Loss: 2.5414\n",
            "Epoch [52/100], Batch [16/32], Loss: 2.5978\n",
            "Epoch [52/100], Batch [17/32], Loss: 2.7035\n",
            "Epoch [52/100], Batch [18/32], Loss: 2.7796\n",
            "Epoch [52/100], Batch [19/32], Loss: 2.6594\n",
            "Epoch [52/100], Batch [20/32], Loss: 2.5811\n",
            "Epoch [52/100], Batch [21/32], Loss: 2.5872\n",
            "Epoch [52/100], Batch [22/32], Loss: 2.5970\n",
            "Epoch [52/100], Batch [23/32], Loss: 2.5973\n",
            "Epoch [52/100], Batch [24/32], Loss: 2.6764\n",
            "Epoch [52/100], Batch [25/32], Loss: 2.7430\n",
            "Epoch [52/100], Batch [26/32], Loss: 2.7650\n",
            "Epoch [52/100], Batch [27/32], Loss: 2.7532\n",
            "Epoch [52/100], Batch [28/32], Loss: 2.5488\n",
            "Epoch [52/100], Batch [29/32], Loss: 2.6223\n",
            "Epoch [52/100], Batch [30/32], Loss: 2.6119\n",
            "Epoch [52/100], Batch [31/32], Loss: 2.6652\n",
            "Epoch [52/100], Batch [32/32], Loss: 2.6640\n",
            "Epoch [53/100], Batch [1/32], Loss: 2.5959\n",
            "Epoch [53/100], Batch [2/32], Loss: 2.5385\n",
            "Epoch [53/100], Batch [3/32], Loss: 2.7020\n",
            "Epoch [53/100], Batch [4/32], Loss: 2.6713\n",
            "Epoch [53/100], Batch [5/32], Loss: 2.5808\n",
            "Epoch [53/100], Batch [6/32], Loss: 2.6134\n",
            "Epoch [53/100], Batch [7/32], Loss: 2.6430\n",
            "Epoch [53/100], Batch [8/32], Loss: 2.7582\n",
            "Epoch [53/100], Batch [9/32], Loss: 2.6746\n",
            "Epoch [53/100], Batch [10/32], Loss: 2.7376\n",
            "Epoch [53/100], Batch [11/32], Loss: 2.6527\n",
            "Epoch [53/100], Batch [12/32], Loss: 2.6459\n",
            "Epoch [53/100], Batch [13/32], Loss: 2.7493\n",
            "Epoch [53/100], Batch [14/32], Loss: 2.6431\n",
            "Epoch [53/100], Batch [15/32], Loss: 2.6414\n",
            "Epoch [53/100], Batch [16/32], Loss: 2.7179\n",
            "Epoch [53/100], Batch [17/32], Loss: 2.6790\n",
            "Epoch [53/100], Batch [18/32], Loss: 2.6537\n",
            "Epoch [53/100], Batch [19/32], Loss: 2.8995\n",
            "Epoch [53/100], Batch [20/32], Loss: 2.4569\n",
            "Epoch [53/100], Batch [21/32], Loss: 2.6890\n",
            "Epoch [53/100], Batch [22/32], Loss: 2.6589\n",
            "Epoch [53/100], Batch [23/32], Loss: 2.6816\n",
            "Epoch [53/100], Batch [24/32], Loss: 2.7333\n",
            "Epoch [53/100], Batch [25/32], Loss: 2.5513\n",
            "Epoch [53/100], Batch [26/32], Loss: 2.5007\n",
            "Epoch [53/100], Batch [27/32], Loss: 2.5092\n",
            "Epoch [53/100], Batch [28/32], Loss: 2.6128\n",
            "Epoch [53/100], Batch [29/32], Loss: 2.7034\n",
            "Epoch [53/100], Batch [30/32], Loss: 2.7724\n",
            "Epoch [53/100], Batch [31/32], Loss: 2.6665\n",
            "Epoch [53/100], Batch [32/32], Loss: 2.5733\n",
            "Epoch [54/100], Batch [1/32], Loss: 2.6199\n",
            "Epoch [54/100], Batch [2/32], Loss: 2.5433\n",
            "Epoch [54/100], Batch [3/32], Loss: 2.5816\n",
            "Epoch [54/100], Batch [4/32], Loss: 2.7351\n",
            "Epoch [54/100], Batch [5/32], Loss: 2.6022\n",
            "Epoch [54/100], Batch [6/32], Loss: 2.6370\n",
            "Epoch [54/100], Batch [7/32], Loss: 2.6894\n",
            "Epoch [54/100], Batch [8/32], Loss: 2.5451\n",
            "Epoch [54/100], Batch [9/32], Loss: 2.6148\n",
            "Epoch [54/100], Batch [10/32], Loss: 2.6328\n",
            "Epoch [54/100], Batch [11/32], Loss: 2.7211\n",
            "Epoch [54/100], Batch [12/32], Loss: 2.6537\n",
            "Epoch [54/100], Batch [13/32], Loss: 2.5997\n",
            "Epoch [54/100], Batch [14/32], Loss: 2.6375\n",
            "Epoch [54/100], Batch [15/32], Loss: 2.7241\n",
            "Epoch [54/100], Batch [16/32], Loss: 2.6092\n",
            "Epoch [54/100], Batch [17/32], Loss: 2.6018\n",
            "Epoch [54/100], Batch [18/32], Loss: 2.6502\n",
            "Epoch [54/100], Batch [19/32], Loss: 2.6549\n",
            "Epoch [54/100], Batch [20/32], Loss: 2.7023\n",
            "Epoch [54/100], Batch [21/32], Loss: 2.7316\n",
            "Epoch [54/100], Batch [22/32], Loss: 2.6029\n",
            "Epoch [54/100], Batch [23/32], Loss: 2.6503\n",
            "Epoch [54/100], Batch [24/32], Loss: 2.5369\n",
            "Epoch [54/100], Batch [25/32], Loss: 2.7588\n",
            "Epoch [54/100], Batch [26/32], Loss: 2.5354\n",
            "Epoch [54/100], Batch [27/32], Loss: 2.7413\n",
            "Epoch [54/100], Batch [28/32], Loss: 2.6904\n",
            "Epoch [54/100], Batch [29/32], Loss: 2.7627\n",
            "Epoch [54/100], Batch [30/32], Loss: 2.6753\n",
            "Epoch [54/100], Batch [31/32], Loss: 2.5559\n",
            "Epoch [54/100], Batch [32/32], Loss: 2.8334\n",
            "Epoch [55/100], Batch [1/32], Loss: 2.4946\n",
            "Epoch [55/100], Batch [2/32], Loss: 2.3961\n",
            "Epoch [55/100], Batch [3/32], Loss: 2.6141\n",
            "Epoch [55/100], Batch [4/32], Loss: 2.5981\n",
            "Epoch [55/100], Batch [5/32], Loss: 2.6805\n",
            "Epoch [55/100], Batch [6/32], Loss: 2.6113\n",
            "Epoch [55/100], Batch [7/32], Loss: 2.7346\n",
            "Epoch [55/100], Batch [8/32], Loss: 2.6679\n",
            "Epoch [55/100], Batch [9/32], Loss: 2.6082\n",
            "Epoch [55/100], Batch [10/32], Loss: 2.7023\n",
            "Epoch [55/100], Batch [11/32], Loss: 2.6789\n",
            "Epoch [55/100], Batch [12/32], Loss: 2.7384\n",
            "Epoch [55/100], Batch [13/32], Loss: 2.6531\n",
            "Epoch [55/100], Batch [14/32], Loss: 2.5898\n",
            "Epoch [55/100], Batch [15/32], Loss: 2.6958\n",
            "Epoch [55/100], Batch [16/32], Loss: 2.6888\n",
            "Epoch [55/100], Batch [17/32], Loss: 2.7115\n",
            "Epoch [55/100], Batch [18/32], Loss: 2.6029\n",
            "Epoch [55/100], Batch [19/32], Loss: 2.6016\n",
            "Epoch [55/100], Batch [20/32], Loss: 2.7454\n",
            "Epoch [55/100], Batch [21/32], Loss: 2.6159\n",
            "Epoch [55/100], Batch [22/32], Loss: 2.7541\n",
            "Epoch [55/100], Batch [23/32], Loss: 2.7494\n",
            "Epoch [55/100], Batch [24/32], Loss: 2.5348\n",
            "Epoch [55/100], Batch [25/32], Loss: 2.7345\n",
            "Epoch [55/100], Batch [26/32], Loss: 2.5763\n",
            "Epoch [55/100], Batch [27/32], Loss: 2.6725\n",
            "Epoch [55/100], Batch [28/32], Loss: 2.6945\n",
            "Epoch [55/100], Batch [29/32], Loss: 2.7413\n",
            "Epoch [55/100], Batch [30/32], Loss: 2.6502\n",
            "Epoch [55/100], Batch [31/32], Loss: 2.7112\n",
            "Epoch [55/100], Batch [32/32], Loss: 2.5893\n",
            "Epoch [56/100], Batch [1/32], Loss: 2.5464\n",
            "Epoch [56/100], Batch [2/32], Loss: 2.6850\n",
            "Epoch [56/100], Batch [3/32], Loss: 2.6849\n",
            "Epoch [56/100], Batch [4/32], Loss: 2.7518\n",
            "Epoch [56/100], Batch [5/32], Loss: 2.7342\n",
            "Epoch [56/100], Batch [6/32], Loss: 2.5345\n",
            "Epoch [56/100], Batch [7/32], Loss: 2.6589\n",
            "Epoch [56/100], Batch [8/32], Loss: 2.5773\n",
            "Epoch [56/100], Batch [9/32], Loss: 2.6955\n",
            "Epoch [56/100], Batch [10/32], Loss: 2.5889\n",
            "Epoch [56/100], Batch [11/32], Loss: 2.6058\n",
            "Epoch [56/100], Batch [12/32], Loss: 2.7146\n",
            "Epoch [56/100], Batch [13/32], Loss: 2.6489\n",
            "Epoch [56/100], Batch [14/32], Loss: 2.6124\n",
            "Epoch [56/100], Batch [15/32], Loss: 2.5123\n",
            "Epoch [56/100], Batch [16/32], Loss: 2.5969\n",
            "Epoch [56/100], Batch [17/32], Loss: 2.8169\n",
            "Epoch [56/100], Batch [18/32], Loss: 2.6188\n",
            "Epoch [56/100], Batch [19/32], Loss: 2.7067\n",
            "Epoch [56/100], Batch [20/32], Loss: 2.7088\n",
            "Epoch [56/100], Batch [21/32], Loss: 2.6938\n",
            "Epoch [56/100], Batch [22/32], Loss: 2.5278\n",
            "Epoch [56/100], Batch [23/32], Loss: 2.4376\n",
            "Epoch [56/100], Batch [24/32], Loss: 2.5709\n",
            "Epoch [56/100], Batch [25/32], Loss: 2.6418\n",
            "Epoch [56/100], Batch [26/32], Loss: 2.7847\n",
            "Epoch [56/100], Batch [27/32], Loss: 2.7218\n",
            "Epoch [56/100], Batch [28/32], Loss: 2.6387\n",
            "Epoch [56/100], Batch [29/32], Loss: 2.5110\n",
            "Epoch [56/100], Batch [30/32], Loss: 2.7027\n",
            "Epoch [56/100], Batch [31/32], Loss: 2.6979\n",
            "Epoch [56/100], Batch [32/32], Loss: 2.6419\n",
            "Epoch [57/100], Batch [1/32], Loss: 2.5499\n",
            "Epoch [57/100], Batch [2/32], Loss: 2.5997\n",
            "Epoch [57/100], Batch [3/32], Loss: 2.6544\n",
            "Epoch [57/100], Batch [4/32], Loss: 2.4144\n",
            "Epoch [57/100], Batch [5/32], Loss: 2.5369\n",
            "Epoch [57/100], Batch [6/32], Loss: 2.6362\n",
            "Epoch [57/100], Batch [7/32], Loss: 2.8059\n",
            "Epoch [57/100], Batch [8/32], Loss: 2.6407\n",
            "Epoch [57/100], Batch [9/32], Loss: 2.6995\n",
            "Epoch [57/100], Batch [10/32], Loss: 2.6521\n",
            "Epoch [57/100], Batch [11/32], Loss: 2.6586\n",
            "Epoch [57/100], Batch [12/32], Loss: 2.6946\n",
            "Epoch [57/100], Batch [13/32], Loss: 2.5813\n",
            "Epoch [57/100], Batch [14/32], Loss: 2.5797\n",
            "Epoch [57/100], Batch [15/32], Loss: 2.6932\n",
            "Epoch [57/100], Batch [16/32], Loss: 2.6670\n",
            "Epoch [57/100], Batch [17/32], Loss: 2.6066\n",
            "Epoch [57/100], Batch [18/32], Loss: 2.7717\n",
            "Epoch [57/100], Batch [19/32], Loss: 2.7093\n",
            "Epoch [57/100], Batch [20/32], Loss: 2.7282\n",
            "Epoch [57/100], Batch [21/32], Loss: 2.7242\n",
            "Epoch [57/100], Batch [22/32], Loss: 2.6735\n",
            "Epoch [57/100], Batch [23/32], Loss: 2.7144\n",
            "Epoch [57/100], Batch [24/32], Loss: 2.6870\n",
            "Epoch [57/100], Batch [25/32], Loss: 2.4601\n",
            "Epoch [57/100], Batch [26/32], Loss: 2.5934\n",
            "Epoch [57/100], Batch [27/32], Loss: 2.7640\n",
            "Epoch [57/100], Batch [28/32], Loss: 2.7483\n",
            "Epoch [57/100], Batch [29/32], Loss: 2.6925\n",
            "Epoch [57/100], Batch [30/32], Loss: 2.5383\n",
            "Epoch [57/100], Batch [31/32], Loss: 2.6457\n",
            "Epoch [57/100], Batch [32/32], Loss: 2.5991\n",
            "Epoch [58/100], Batch [1/32], Loss: 2.6069\n",
            "Epoch [58/100], Batch [2/32], Loss: 2.6856\n",
            "Epoch [58/100], Batch [3/32], Loss: 2.6488\n",
            "Epoch [58/100], Batch [4/32], Loss: 2.5133\n",
            "Epoch [58/100], Batch [5/32], Loss: 2.6518\n",
            "Epoch [58/100], Batch [6/32], Loss: 2.6329\n",
            "Epoch [58/100], Batch [7/32], Loss: 2.6657\n",
            "Epoch [58/100], Batch [8/32], Loss: 2.6575\n",
            "Epoch [58/100], Batch [9/32], Loss: 2.6149\n",
            "Epoch [58/100], Batch [10/32], Loss: 2.6997\n",
            "Epoch [58/100], Batch [11/32], Loss: 2.5976\n",
            "Epoch [58/100], Batch [12/32], Loss: 2.5768\n",
            "Epoch [58/100], Batch [13/32], Loss: 2.8267\n",
            "Epoch [58/100], Batch [14/32], Loss: 2.6248\n",
            "Epoch [58/100], Batch [15/32], Loss: 2.6327\n",
            "Epoch [58/100], Batch [16/32], Loss: 2.5699\n",
            "Epoch [58/100], Batch [17/32], Loss: 2.5732\n",
            "Epoch [58/100], Batch [18/32], Loss: 2.5500\n",
            "Epoch [58/100], Batch [19/32], Loss: 2.5716\n",
            "Epoch [58/100], Batch [20/32], Loss: 2.6153\n",
            "Epoch [58/100], Batch [21/32], Loss: 2.6589\n",
            "Epoch [58/100], Batch [22/32], Loss: 2.6553\n",
            "Epoch [58/100], Batch [23/32], Loss: 2.6748\n",
            "Epoch [58/100], Batch [24/32], Loss: 2.5672\n",
            "Epoch [58/100], Batch [25/32], Loss: 2.8133\n",
            "Epoch [58/100], Batch [26/32], Loss: 2.6017\n",
            "Epoch [58/100], Batch [27/32], Loss: 2.6492\n",
            "Epoch [58/100], Batch [28/32], Loss: 2.6275\n",
            "Epoch [58/100], Batch [29/32], Loss: 2.5717\n",
            "Epoch [58/100], Batch [30/32], Loss: 2.6912\n",
            "Epoch [58/100], Batch [31/32], Loss: 2.7745\n",
            "Epoch [58/100], Batch [32/32], Loss: 2.6161\n",
            "Epoch [59/100], Batch [1/32], Loss: 2.6603\n",
            "Epoch [59/100], Batch [2/32], Loss: 2.4581\n",
            "Epoch [59/100], Batch [3/32], Loss: 2.6763\n",
            "Epoch [59/100], Batch [4/32], Loss: 2.5923\n",
            "Epoch [59/100], Batch [5/32], Loss: 2.5981\n",
            "Epoch [59/100], Batch [6/32], Loss: 2.6571\n",
            "Epoch [59/100], Batch [7/32], Loss: 2.6488\n",
            "Epoch [59/100], Batch [8/32], Loss: 2.5617\n",
            "Epoch [59/100], Batch [9/32], Loss: 2.6682\n",
            "Epoch [59/100], Batch [10/32], Loss: 2.5987\n",
            "Epoch [59/100], Batch [11/32], Loss: 2.7696\n",
            "Epoch [59/100], Batch [12/32], Loss: 2.6244\n",
            "Epoch [59/100], Batch [13/32], Loss: 2.5866\n",
            "Epoch [59/100], Batch [14/32], Loss: 2.5583\n",
            "Epoch [59/100], Batch [15/32], Loss: 2.6743\n",
            "Epoch [59/100], Batch [16/32], Loss: 2.5859\n",
            "Epoch [59/100], Batch [17/32], Loss: 2.6370\n",
            "Epoch [59/100], Batch [18/32], Loss: 2.7873\n",
            "Epoch [59/100], Batch [19/32], Loss: 2.6507\n",
            "Epoch [59/100], Batch [20/32], Loss: 2.5645\n",
            "Epoch [59/100], Batch [21/32], Loss: 2.6601\n",
            "Epoch [59/100], Batch [22/32], Loss: 2.7419\n",
            "Epoch [59/100], Batch [23/32], Loss: 2.6165\n",
            "Epoch [59/100], Batch [24/32], Loss: 2.6796\n",
            "Epoch [59/100], Batch [25/32], Loss: 2.5406\n",
            "Epoch [59/100], Batch [26/32], Loss: 2.7220\n",
            "Epoch [59/100], Batch [27/32], Loss: 2.5702\n",
            "Epoch [59/100], Batch [28/32], Loss: 2.5678\n",
            "Epoch [59/100], Batch [29/32], Loss: 2.6843\n",
            "Epoch [59/100], Batch [30/32], Loss: 2.7754\n",
            "Epoch [59/100], Batch [31/32], Loss: 2.7584\n",
            "Epoch [59/100], Batch [32/32], Loss: 2.4818\n",
            "Epoch [60/100], Batch [1/32], Loss: 2.6114\n",
            "Epoch [60/100], Batch [2/32], Loss: 2.6812\n",
            "Epoch [60/100], Batch [3/32], Loss: 2.6695\n",
            "Epoch [60/100], Batch [4/32], Loss: 2.6893\n",
            "Epoch [60/100], Batch [5/32], Loss: 2.6743\n",
            "Epoch [60/100], Batch [6/32], Loss: 2.5856\n",
            "Epoch [60/100], Batch [7/32], Loss: 2.7873\n",
            "Epoch [60/100], Batch [8/32], Loss: 2.8063\n",
            "Epoch [60/100], Batch [9/32], Loss: 2.5800\n",
            "Epoch [60/100], Batch [10/32], Loss: 2.5393\n",
            "Epoch [60/100], Batch [11/32], Loss: 2.5379\n",
            "Epoch [60/100], Batch [12/32], Loss: 2.5065\n",
            "Epoch [60/100], Batch [13/32], Loss: 2.6645\n",
            "Epoch [60/100], Batch [14/32], Loss: 2.5839\n",
            "Epoch [60/100], Batch [15/32], Loss: 2.7574\n",
            "Epoch [60/100], Batch [16/32], Loss: 2.6040\n",
            "Epoch [60/100], Batch [17/32], Loss: 2.6897\n",
            "Epoch [60/100], Batch [18/32], Loss: 2.7108\n",
            "Epoch [60/100], Batch [19/32], Loss: 2.6626\n",
            "Epoch [60/100], Batch [20/32], Loss: 2.5789\n",
            "Epoch [60/100], Batch [21/32], Loss: 2.6291\n",
            "Epoch [60/100], Batch [22/32], Loss: 2.7464\n",
            "Epoch [60/100], Batch [23/32], Loss: 2.6930\n",
            "Epoch [60/100], Batch [24/32], Loss: 2.4269\n",
            "Epoch [60/100], Batch [25/32], Loss: 2.6536\n",
            "Epoch [60/100], Batch [26/32], Loss: 2.6832\n",
            "Epoch [60/100], Batch [27/32], Loss: 2.6207\n",
            "Epoch [60/100], Batch [28/32], Loss: 2.6423\n",
            "Epoch [60/100], Batch [29/32], Loss: 2.6458\n",
            "Epoch [60/100], Batch [30/32], Loss: 2.6982\n",
            "Epoch [60/100], Batch [31/32], Loss: 2.5415\n",
            "Epoch [60/100], Batch [32/32], Loss: 2.5238\n",
            "Epoch [61/100], Batch [1/32], Loss: 2.6548\n",
            "Epoch [61/100], Batch [2/32], Loss: 2.6360\n",
            "Epoch [61/100], Batch [3/32], Loss: 2.6818\n",
            "Epoch [61/100], Batch [4/32], Loss: 2.6728\n",
            "Epoch [61/100], Batch [5/32], Loss: 2.6369\n",
            "Epoch [61/100], Batch [6/32], Loss: 2.6338\n",
            "Epoch [61/100], Batch [7/32], Loss: 2.5743\n",
            "Epoch [61/100], Batch [8/32], Loss: 2.4960\n",
            "Epoch [61/100], Batch [9/32], Loss: 2.6784\n",
            "Epoch [61/100], Batch [10/32], Loss: 2.5084\n",
            "Epoch [61/100], Batch [11/32], Loss: 2.5829\n",
            "Epoch [61/100], Batch [12/32], Loss: 2.4912\n",
            "Epoch [61/100], Batch [13/32], Loss: 2.6545\n",
            "Epoch [61/100], Batch [14/32], Loss: 2.5441\n",
            "Epoch [61/100], Batch [15/32], Loss: 2.6857\n",
            "Epoch [61/100], Batch [16/32], Loss: 2.5776\n",
            "Epoch [61/100], Batch [17/32], Loss: 2.6078\n",
            "Epoch [61/100], Batch [18/32], Loss: 2.7568\n",
            "Epoch [61/100], Batch [19/32], Loss: 2.6095\n",
            "Epoch [61/100], Batch [20/32], Loss: 2.7177\n",
            "Epoch [61/100], Batch [21/32], Loss: 2.6375\n",
            "Epoch [61/100], Batch [22/32], Loss: 2.5366\n",
            "Epoch [61/100], Batch [23/32], Loss: 2.6665\n",
            "Epoch [61/100], Batch [24/32], Loss: 2.6515\n",
            "Epoch [61/100], Batch [25/32], Loss: 2.7345\n",
            "Epoch [61/100], Batch [26/32], Loss: 2.6000\n",
            "Epoch [61/100], Batch [27/32], Loss: 2.6668\n",
            "Epoch [61/100], Batch [28/32], Loss: 2.7498\n",
            "Epoch [61/100], Batch [29/32], Loss: 2.7337\n",
            "Epoch [61/100], Batch [30/32], Loss: 2.6384\n",
            "Epoch [61/100], Batch [31/32], Loss: 2.5286\n",
            "Epoch [61/100], Batch [32/32], Loss: 2.7255\n",
            "Epoch [62/100], Batch [1/32], Loss: 2.4896\n",
            "Epoch [62/100], Batch [2/32], Loss: 2.6268\n",
            "Epoch [62/100], Batch [3/32], Loss: 2.5446\n",
            "Epoch [62/100], Batch [4/32], Loss: 2.6765\n",
            "Epoch [62/100], Batch [5/32], Loss: 2.5372\n",
            "Epoch [62/100], Batch [6/32], Loss: 2.6079\n",
            "Epoch [62/100], Batch [7/32], Loss: 2.7200\n",
            "Epoch [62/100], Batch [8/32], Loss: 2.5662\n",
            "Epoch [62/100], Batch [9/32], Loss: 2.6279\n",
            "Epoch [62/100], Batch [10/32], Loss: 2.6452\n",
            "Epoch [62/100], Batch [11/32], Loss: 2.6089\n",
            "Epoch [62/100], Batch [12/32], Loss: 2.6889\n",
            "Epoch [62/100], Batch [13/32], Loss: 2.5255\n",
            "Epoch [62/100], Batch [14/32], Loss: 2.5444\n",
            "Epoch [62/100], Batch [15/32], Loss: 2.6638\n",
            "Epoch [62/100], Batch [16/32], Loss: 2.5668\n",
            "Epoch [62/100], Batch [17/32], Loss: 2.6968\n",
            "Epoch [62/100], Batch [18/32], Loss: 2.6192\n",
            "Epoch [62/100], Batch [19/32], Loss: 2.5344\n",
            "Epoch [62/100], Batch [20/32], Loss: 2.7503\n",
            "Epoch [62/100], Batch [21/32], Loss: 2.6665\n",
            "Epoch [62/100], Batch [22/32], Loss: 2.6154\n",
            "Epoch [62/100], Batch [23/32], Loss: 2.6610\n",
            "Epoch [62/100], Batch [24/32], Loss: 2.8925\n",
            "Epoch [62/100], Batch [25/32], Loss: 2.6655\n",
            "Epoch [62/100], Batch [26/32], Loss: 2.6439\n",
            "Epoch [62/100], Batch [27/32], Loss: 2.6302\n",
            "Epoch [62/100], Batch [28/32], Loss: 2.6404\n",
            "Epoch [62/100], Batch [29/32], Loss: 2.6415\n",
            "Epoch [62/100], Batch [30/32], Loss: 2.5783\n",
            "Epoch [62/100], Batch [31/32], Loss: 2.6803\n",
            "Epoch [62/100], Batch [32/32], Loss: 2.5722\n",
            "Epoch [63/100], Batch [1/32], Loss: 2.5766\n",
            "Epoch [63/100], Batch [2/32], Loss: 2.7079\n",
            "Epoch [63/100], Batch [3/32], Loss: 2.4485\n",
            "Epoch [63/100], Batch [4/32], Loss: 2.4899\n",
            "Epoch [63/100], Batch [5/32], Loss: 2.6470\n",
            "Epoch [63/100], Batch [6/32], Loss: 2.6307\n",
            "Epoch [63/100], Batch [7/32], Loss: 2.7614\n",
            "Epoch [63/100], Batch [8/32], Loss: 2.6560\n",
            "Epoch [63/100], Batch [9/32], Loss: 2.5384\n",
            "Epoch [63/100], Batch [10/32], Loss: 2.7249\n",
            "Epoch [63/100], Batch [11/32], Loss: 2.7204\n",
            "Epoch [63/100], Batch [12/32], Loss: 2.7086\n",
            "Epoch [63/100], Batch [13/32], Loss: 2.6423\n",
            "Epoch [63/100], Batch [14/32], Loss: 2.5579\n",
            "Epoch [63/100], Batch [15/32], Loss: 2.5009\n",
            "Epoch [63/100], Batch [16/32], Loss: 2.7135\n",
            "Epoch [63/100], Batch [17/32], Loss: 2.7115\n",
            "Epoch [63/100], Batch [18/32], Loss: 2.8345\n",
            "Epoch [63/100], Batch [19/32], Loss: 2.4858\n",
            "Epoch [63/100], Batch [20/32], Loss: 2.7589\n",
            "Epoch [63/100], Batch [21/32], Loss: 2.6570\n",
            "Epoch [63/100], Batch [22/32], Loss: 2.5943\n",
            "Epoch [63/100], Batch [23/32], Loss: 2.6601\n",
            "Epoch [63/100], Batch [24/32], Loss: 2.7163\n",
            "Epoch [63/100], Batch [25/32], Loss: 2.6476\n",
            "Epoch [63/100], Batch [26/32], Loss: 2.7553\n",
            "Epoch [63/100], Batch [27/32], Loss: 2.4040\n",
            "Epoch [63/100], Batch [28/32], Loss: 2.6384\n",
            "Epoch [63/100], Batch [29/32], Loss: 2.6458\n",
            "Epoch [63/100], Batch [30/32], Loss: 2.6742\n",
            "Epoch [63/100], Batch [31/32], Loss: 2.5294\n",
            "Epoch [63/100], Batch [32/32], Loss: 2.7816\n",
            "Epoch [64/100], Batch [1/32], Loss: 2.6211\n",
            "Epoch [64/100], Batch [2/32], Loss: 2.6489\n",
            "Epoch [64/100], Batch [3/32], Loss: 2.6143\n",
            "Epoch [64/100], Batch [4/32], Loss: 2.7062\n",
            "Epoch [64/100], Batch [5/32], Loss: 2.7002\n",
            "Epoch [64/100], Batch [6/32], Loss: 2.6523\n",
            "Epoch [64/100], Batch [7/32], Loss: 2.6498\n",
            "Epoch [64/100], Batch [8/32], Loss: 2.6480\n",
            "Epoch [64/100], Batch [9/32], Loss: 2.5914\n",
            "Epoch [64/100], Batch [10/32], Loss: 2.5533\n",
            "Epoch [64/100], Batch [11/32], Loss: 2.7140\n",
            "Epoch [64/100], Batch [12/32], Loss: 2.6242\n",
            "Epoch [64/100], Batch [13/32], Loss: 2.7785\n",
            "Epoch [64/100], Batch [14/32], Loss: 2.5861\n",
            "Epoch [64/100], Batch [15/32], Loss: 2.6035\n",
            "Epoch [64/100], Batch [16/32], Loss: 2.5999\n",
            "Epoch [64/100], Batch [17/32], Loss: 2.4412\n",
            "Epoch [64/100], Batch [18/32], Loss: 2.6393\n",
            "Epoch [64/100], Batch [19/32], Loss: 2.5647\n",
            "Epoch [64/100], Batch [20/32], Loss: 2.5745\n",
            "Epoch [64/100], Batch [21/32], Loss: 2.6550\n",
            "Epoch [64/100], Batch [22/32], Loss: 2.6594\n",
            "Epoch [64/100], Batch [23/32], Loss: 2.7772\n",
            "Epoch [64/100], Batch [24/32], Loss: 2.6024\n",
            "Epoch [64/100], Batch [25/32], Loss: 2.5619\n",
            "Epoch [64/100], Batch [26/32], Loss: 2.6982\n",
            "Epoch [64/100], Batch [27/32], Loss: 2.6090\n",
            "Epoch [64/100], Batch [28/32], Loss: 2.6651\n",
            "Epoch [64/100], Batch [29/32], Loss: 2.6340\n",
            "Epoch [64/100], Batch [30/32], Loss: 2.7170\n",
            "Epoch [64/100], Batch [31/32], Loss: 2.6564\n",
            "Epoch [64/100], Batch [32/32], Loss: 2.5726\n",
            "Epoch [65/100], Batch [1/32], Loss: 2.7330\n",
            "Epoch [65/100], Batch [2/32], Loss: 2.7852\n",
            "Epoch [65/100], Batch [3/32], Loss: 2.6171\n",
            "Epoch [65/100], Batch [4/32], Loss: 2.6990\n",
            "Epoch [65/100], Batch [5/32], Loss: 2.5824\n",
            "Epoch [65/100], Batch [6/32], Loss: 2.5288\n",
            "Epoch [65/100], Batch [7/32], Loss: 2.6398\n",
            "Epoch [65/100], Batch [8/32], Loss: 2.5341\n",
            "Epoch [65/100], Batch [9/32], Loss: 2.5957\n",
            "Epoch [65/100], Batch [10/32], Loss: 2.6251\n",
            "Epoch [65/100], Batch [11/32], Loss: 2.7651\n",
            "Epoch [65/100], Batch [12/32], Loss: 2.5923\n",
            "Epoch [65/100], Batch [13/32], Loss: 2.5164\n",
            "Epoch [65/100], Batch [14/32], Loss: 2.7783\n",
            "Epoch [65/100], Batch [15/32], Loss: 2.7102\n",
            "Epoch [65/100], Batch [16/32], Loss: 2.6315\n",
            "Epoch [65/100], Batch [17/32], Loss: 2.6705\n",
            "Epoch [65/100], Batch [18/32], Loss: 2.5881\n",
            "Epoch [65/100], Batch [19/32], Loss: 2.6209\n",
            "Epoch [65/100], Batch [20/32], Loss: 2.7012\n",
            "Epoch [65/100], Batch [21/32], Loss: 2.5389\n",
            "Epoch [65/100], Batch [22/32], Loss: 2.7277\n",
            "Epoch [65/100], Batch [23/32], Loss: 2.6379\n",
            "Epoch [65/100], Batch [24/32], Loss: 2.6221\n",
            "Epoch [65/100], Batch [25/32], Loss: 2.6560\n",
            "Epoch [65/100], Batch [26/32], Loss: 2.6909\n",
            "Epoch [65/100], Batch [27/32], Loss: 2.5201\n",
            "Epoch [65/100], Batch [28/32], Loss: 2.4621\n",
            "Epoch [65/100], Batch [29/32], Loss: 2.6722\n",
            "Epoch [65/100], Batch [30/32], Loss: 2.3827\n",
            "Epoch [65/100], Batch [31/32], Loss: 2.5667\n",
            "Epoch [65/100], Batch [32/32], Loss: 2.5679\n",
            "Epoch [66/100], Batch [1/32], Loss: 2.7773\n",
            "Epoch [66/100], Batch [2/32], Loss: 2.5208\n",
            "Epoch [66/100], Batch [3/32], Loss: 2.3836\n",
            "Epoch [66/100], Batch [4/32], Loss: 2.6820\n",
            "Epoch [66/100], Batch [5/32], Loss: 2.6163\n",
            "Epoch [66/100], Batch [6/32], Loss: 2.6789\n",
            "Epoch [66/100], Batch [7/32], Loss: 2.5944\n",
            "Epoch [66/100], Batch [8/32], Loss: 2.6179\n",
            "Epoch [66/100], Batch [9/32], Loss: 2.5154\n",
            "Epoch [66/100], Batch [10/32], Loss: 2.5617\n",
            "Epoch [66/100], Batch [11/32], Loss: 2.6089\n",
            "Epoch [66/100], Batch [12/32], Loss: 2.7215\n",
            "Epoch [66/100], Batch [13/32], Loss: 2.7205\n",
            "Epoch [66/100], Batch [14/32], Loss: 2.6146\n",
            "Epoch [66/100], Batch [15/32], Loss: 2.7876\n",
            "Epoch [66/100], Batch [16/32], Loss: 2.6223\n",
            "Epoch [66/100], Batch [17/32], Loss: 2.7614\n",
            "Epoch [66/100], Batch [18/32], Loss: 2.5744\n",
            "Epoch [66/100], Batch [19/32], Loss: 2.7324\n",
            "Epoch [66/100], Batch [20/32], Loss: 2.6411\n",
            "Epoch [66/100], Batch [21/32], Loss: 2.5141\n",
            "Epoch [66/100], Batch [22/32], Loss: 2.7158\n",
            "Epoch [66/100], Batch [23/32], Loss: 2.6029\n",
            "Epoch [66/100], Batch [24/32], Loss: 2.7636\n",
            "Epoch [66/100], Batch [25/32], Loss: 2.6574\n",
            "Epoch [66/100], Batch [26/32], Loss: 2.6681\n",
            "Epoch [66/100], Batch [27/32], Loss: 2.6644\n",
            "Epoch [66/100], Batch [28/32], Loss: 2.5849\n",
            "Epoch [66/100], Batch [29/32], Loss: 2.5471\n",
            "Epoch [66/100], Batch [30/32], Loss: 2.6629\n",
            "Epoch [66/100], Batch [31/32], Loss: 2.6684\n",
            "Epoch [66/100], Batch [32/32], Loss: 2.5772\n",
            "Epoch [67/100], Batch [1/32], Loss: 2.6540\n",
            "Epoch [67/100], Batch [2/32], Loss: 2.7605\n",
            "Epoch [67/100], Batch [3/32], Loss: 2.6859\n",
            "Epoch [67/100], Batch [4/32], Loss: 2.4640\n",
            "Epoch [67/100], Batch [5/32], Loss: 2.3424\n",
            "Epoch [67/100], Batch [6/32], Loss: 2.6702\n",
            "Epoch [67/100], Batch [7/32], Loss: 2.6883\n",
            "Epoch [67/100], Batch [8/32], Loss: 2.5991\n",
            "Epoch [67/100], Batch [9/32], Loss: 2.5533\n",
            "Epoch [67/100], Batch [10/32], Loss: 2.5903\n",
            "Epoch [67/100], Batch [11/32], Loss: 2.4569\n",
            "Epoch [67/100], Batch [12/32], Loss: 2.4800\n",
            "Epoch [67/100], Batch [13/32], Loss: 2.5739\n",
            "Epoch [67/100], Batch [14/32], Loss: 2.6226\n",
            "Epoch [67/100], Batch [15/32], Loss: 2.7655\n",
            "Epoch [67/100], Batch [16/32], Loss: 2.6633\n",
            "Epoch [67/100], Batch [17/32], Loss: 2.5381\n",
            "Epoch [67/100], Batch [18/32], Loss: 2.6293\n",
            "Epoch [67/100], Batch [19/32], Loss: 2.5950\n",
            "Epoch [67/100], Batch [20/32], Loss: 2.6722\n",
            "Epoch [67/100], Batch [21/32], Loss: 2.5645\n",
            "Epoch [67/100], Batch [22/32], Loss: 2.6298\n",
            "Epoch [67/100], Batch [23/32], Loss: 2.7335\n",
            "Epoch [67/100], Batch [24/32], Loss: 2.6842\n",
            "Epoch [67/100], Batch [25/32], Loss: 2.7019\n",
            "Epoch [67/100], Batch [26/32], Loss: 2.6593\n",
            "Epoch [67/100], Batch [27/32], Loss: 2.6326\n",
            "Epoch [67/100], Batch [28/32], Loss: 2.5173\n",
            "Epoch [67/100], Batch [29/32], Loss: 2.6397\n",
            "Epoch [67/100], Batch [30/32], Loss: 2.6767\n",
            "Epoch [67/100], Batch [31/32], Loss: 2.7873\n",
            "Epoch [67/100], Batch [32/32], Loss: 2.6737\n",
            "Epoch [68/100], Batch [1/32], Loss: 2.6637\n",
            "Epoch [68/100], Batch [2/32], Loss: 2.7120\n",
            "Epoch [68/100], Batch [3/32], Loss: 2.6584\n",
            "Epoch [68/100], Batch [4/32], Loss: 2.6022\n",
            "Epoch [68/100], Batch [5/32], Loss: 2.5231\n",
            "Epoch [68/100], Batch [6/32], Loss: 2.6961\n",
            "Epoch [68/100], Batch [7/32], Loss: 2.4820\n",
            "Epoch [68/100], Batch [8/32], Loss: 2.6689\n",
            "Epoch [68/100], Batch [9/32], Loss: 2.6565\n",
            "Epoch [68/100], Batch [10/32], Loss: 2.5222\n",
            "Epoch [68/100], Batch [11/32], Loss: 2.5866\n",
            "Epoch [68/100], Batch [12/32], Loss: 2.6183\n",
            "Epoch [68/100], Batch [13/32], Loss: 2.5710\n",
            "Epoch [68/100], Batch [14/32], Loss: 2.4604\n",
            "Epoch [68/100], Batch [15/32], Loss: 2.6140\n",
            "Epoch [68/100], Batch [16/32], Loss: 2.6265\n",
            "Epoch [68/100], Batch [17/32], Loss: 2.6999\n",
            "Epoch [68/100], Batch [18/32], Loss: 2.6645\n",
            "Epoch [68/100], Batch [19/32], Loss: 2.6182\n",
            "Epoch [68/100], Batch [20/32], Loss: 2.8270\n",
            "Epoch [68/100], Batch [21/32], Loss: 2.5501\n",
            "Epoch [68/100], Batch [22/32], Loss: 2.5173\n",
            "Epoch [68/100], Batch [23/32], Loss: 2.7102\n",
            "Epoch [68/100], Batch [24/32], Loss: 2.5523\n",
            "Epoch [68/100], Batch [25/32], Loss: 2.5696\n",
            "Epoch [68/100], Batch [26/32], Loss: 2.7145\n",
            "Epoch [68/100], Batch [27/32], Loss: 2.5482\n",
            "Epoch [68/100], Batch [28/32], Loss: 2.6218\n",
            "Epoch [68/100], Batch [29/32], Loss: 2.6612\n",
            "Epoch [68/100], Batch [30/32], Loss: 2.7458\n",
            "Epoch [68/100], Batch [31/32], Loss: 2.7686\n",
            "Epoch [68/100], Batch [32/32], Loss: 2.6709\n",
            "Epoch [69/100], Batch [1/32], Loss: 2.6087\n",
            "Epoch [69/100], Batch [2/32], Loss: 2.4486\n",
            "Epoch [69/100], Batch [3/32], Loss: 2.4145\n",
            "Epoch [69/100], Batch [4/32], Loss: 2.6931\n",
            "Epoch [69/100], Batch [5/32], Loss: 2.5660\n",
            "Epoch [69/100], Batch [6/32], Loss: 2.6745\n",
            "Epoch [69/100], Batch [7/32], Loss: 2.6292\n",
            "Epoch [69/100], Batch [8/32], Loss: 2.5985\n",
            "Epoch [69/100], Batch [9/32], Loss: 2.6159\n",
            "Epoch [69/100], Batch [10/32], Loss: 2.5828\n",
            "Epoch [69/100], Batch [11/32], Loss: 2.5952\n",
            "Epoch [69/100], Batch [12/32], Loss: 2.7086\n",
            "Epoch [69/100], Batch [13/32], Loss: 2.6369\n",
            "Epoch [69/100], Batch [14/32], Loss: 2.6263\n",
            "Epoch [69/100], Batch [15/32], Loss: 2.6085\n",
            "Epoch [69/100], Batch [16/32], Loss: 2.5818\n",
            "Epoch [69/100], Batch [17/32], Loss: 2.6573\n",
            "Epoch [69/100], Batch [18/32], Loss: 2.7637\n",
            "Epoch [69/100], Batch [19/32], Loss: 2.7557\n",
            "Epoch [69/100], Batch [20/32], Loss: 2.7368\n",
            "Epoch [69/100], Batch [21/32], Loss: 2.4670\n",
            "Epoch [69/100], Batch [22/32], Loss: 2.5572\n",
            "Epoch [69/100], Batch [23/32], Loss: 2.6672\n",
            "Epoch [69/100], Batch [24/32], Loss: 2.6722\n",
            "Epoch [69/100], Batch [25/32], Loss: 2.5314\n",
            "Epoch [69/100], Batch [26/32], Loss: 2.3783\n",
            "Epoch [69/100], Batch [27/32], Loss: 2.5421\n",
            "Epoch [69/100], Batch [28/32], Loss: 2.7166\n",
            "Epoch [69/100], Batch [29/32], Loss: 2.6589\n",
            "Epoch [69/100], Batch [30/32], Loss: 2.5530\n",
            "Epoch [69/100], Batch [31/32], Loss: 2.6666\n",
            "Epoch [69/100], Batch [32/32], Loss: 2.7017\n",
            "Epoch [70/100], Batch [1/32], Loss: 2.5878\n",
            "Epoch [70/100], Batch [2/32], Loss: 2.6759\n",
            "Epoch [70/100], Batch [3/32], Loss: 2.7049\n",
            "Epoch [70/100], Batch [4/32], Loss: 2.5513\n",
            "Epoch [70/100], Batch [5/32], Loss: 2.4923\n",
            "Epoch [70/100], Batch [6/32], Loss: 2.5718\n",
            "Epoch [70/100], Batch [7/32], Loss: 2.6572\n",
            "Epoch [70/100], Batch [8/32], Loss: 2.6442\n",
            "Epoch [70/100], Batch [9/32], Loss: 2.6763\n",
            "Epoch [70/100], Batch [10/32], Loss: 2.5832\n",
            "Epoch [70/100], Batch [11/32], Loss: 2.4774\n",
            "Epoch [70/100], Batch [12/32], Loss: 2.5746\n",
            "Epoch [70/100], Batch [13/32], Loss: 2.6306\n",
            "Epoch [70/100], Batch [14/32], Loss: 2.6038\n",
            "Epoch [70/100], Batch [15/32], Loss: 2.6612\n",
            "Epoch [70/100], Batch [16/32], Loss: 2.5469\n",
            "Epoch [70/100], Batch [17/32], Loss: 2.7818\n",
            "Epoch [70/100], Batch [18/32], Loss: 2.6841\n",
            "Epoch [70/100], Batch [19/32], Loss: 2.4836\n",
            "Epoch [70/100], Batch [20/32], Loss: 2.7126\n",
            "Epoch [70/100], Batch [21/32], Loss: 2.7686\n",
            "Epoch [70/100], Batch [22/32], Loss: 2.4377\n",
            "Epoch [70/100], Batch [23/32], Loss: 2.5345\n",
            "Epoch [70/100], Batch [24/32], Loss: 2.6656\n",
            "Epoch [70/100], Batch [25/32], Loss: 2.6504\n",
            "Epoch [70/100], Batch [26/32], Loss: 2.3947\n",
            "Epoch [70/100], Batch [27/32], Loss: 2.5376\n",
            "Epoch [70/100], Batch [28/32], Loss: 2.6862\n",
            "Epoch [70/100], Batch [29/32], Loss: 2.5374\n",
            "Epoch [70/100], Batch [30/32], Loss: 2.6377\n",
            "Epoch [70/100], Batch [31/32], Loss: 2.8161\n",
            "Epoch [70/100], Batch [32/32], Loss: 2.6347\n",
            "Epoch [71/100], Batch [1/32], Loss: 2.6863\n",
            "Epoch [71/100], Batch [2/32], Loss: 2.5786\n",
            "Epoch [71/100], Batch [3/32], Loss: 2.6694\n",
            "Epoch [71/100], Batch [4/32], Loss: 2.5623\n",
            "Epoch [71/100], Batch [5/32], Loss: 2.6213\n",
            "Epoch [71/100], Batch [6/32], Loss: 2.7044\n",
            "Epoch [71/100], Batch [7/32], Loss: 2.5761\n",
            "Epoch [71/100], Batch [8/32], Loss: 2.6288\n",
            "Epoch [71/100], Batch [9/32], Loss: 2.6668\n",
            "Epoch [71/100], Batch [10/32], Loss: 2.5312\n",
            "Epoch [71/100], Batch [11/32], Loss: 2.7889\n",
            "Epoch [71/100], Batch [12/32], Loss: 2.6793\n",
            "Epoch [71/100], Batch [13/32], Loss: 2.5005\n",
            "Epoch [71/100], Batch [14/32], Loss: 2.6562\n",
            "Epoch [71/100], Batch [15/32], Loss: 2.5571\n",
            "Epoch [71/100], Batch [16/32], Loss: 2.5943\n",
            "Epoch [71/100], Batch [17/32], Loss: 2.7206\n",
            "Epoch [71/100], Batch [18/32], Loss: 2.6210\n",
            "Epoch [71/100], Batch [19/32], Loss: 2.5846\n",
            "Epoch [71/100], Batch [20/32], Loss: 2.5670\n",
            "Epoch [71/100], Batch [21/32], Loss: 2.6235\n",
            "Epoch [71/100], Batch [22/32], Loss: 2.5633\n",
            "Epoch [71/100], Batch [23/32], Loss: 2.3787\n",
            "Epoch [71/100], Batch [24/32], Loss: 2.6213\n",
            "Epoch [71/100], Batch [25/32], Loss: 2.5898\n",
            "Epoch [71/100], Batch [26/32], Loss: 2.6623\n",
            "Epoch [71/100], Batch [27/32], Loss: 2.6037\n",
            "Epoch [71/100], Batch [28/32], Loss: 2.5216\n",
            "Epoch [71/100], Batch [29/32], Loss: 2.5605\n",
            "Epoch [71/100], Batch [30/32], Loss: 2.6862\n",
            "Epoch [71/100], Batch [31/32], Loss: 2.6145\n",
            "Epoch [71/100], Batch [32/32], Loss: 2.8482\n",
            "Epoch [72/100], Batch [1/32], Loss: 2.4693\n",
            "Epoch [72/100], Batch [2/32], Loss: 2.5528\n",
            "Epoch [72/100], Batch [3/32], Loss: 2.5646\n",
            "Epoch [72/100], Batch [4/32], Loss: 2.6186\n",
            "Epoch [72/100], Batch [5/32], Loss: 2.6684\n",
            "Epoch [72/100], Batch [6/32], Loss: 2.4496\n",
            "Epoch [72/100], Batch [7/32], Loss: 2.5565\n",
            "Epoch [72/100], Batch [8/32], Loss: 2.6701\n",
            "Epoch [72/100], Batch [9/32], Loss: 2.7061\n",
            "Epoch [72/100], Batch [10/32], Loss: 2.6296\n",
            "Epoch [72/100], Batch [11/32], Loss: 2.4460\n",
            "Epoch [72/100], Batch [12/32], Loss: 2.7016\n",
            "Epoch [72/100], Batch [13/32], Loss: 2.6389\n",
            "Epoch [72/100], Batch [14/32], Loss: 2.5626\n",
            "Epoch [72/100], Batch [15/32], Loss: 2.6117\n",
            "Epoch [72/100], Batch [16/32], Loss: 2.7619\n",
            "Epoch [72/100], Batch [17/32], Loss: 2.7918\n",
            "Epoch [72/100], Batch [18/32], Loss: 2.6916\n",
            "Epoch [72/100], Batch [19/32], Loss: 2.6649\n",
            "Epoch [72/100], Batch [20/32], Loss: 2.5974\n",
            "Epoch [72/100], Batch [21/32], Loss: 2.6160\n",
            "Epoch [72/100], Batch [22/32], Loss: 2.6425\n",
            "Epoch [72/100], Batch [23/32], Loss: 2.6339\n",
            "Epoch [72/100], Batch [24/32], Loss: 2.6880\n",
            "Epoch [72/100], Batch [25/32], Loss: 2.6940\n",
            "Epoch [72/100], Batch [26/32], Loss: 2.6801\n",
            "Epoch [72/100], Batch [27/32], Loss: 2.5674\n",
            "Epoch [72/100], Batch [28/32], Loss: 2.5930\n",
            "Epoch [72/100], Batch [29/32], Loss: 2.6130\n",
            "Epoch [72/100], Batch [30/32], Loss: 2.6112\n",
            "Epoch [72/100], Batch [31/32], Loss: 2.3812\n",
            "Epoch [72/100], Batch [32/32], Loss: 2.6670\n",
            "Epoch [73/100], Batch [1/32], Loss: 2.6275\n",
            "Epoch [73/100], Batch [2/32], Loss: 2.5664\n",
            "Epoch [73/100], Batch [3/32], Loss: 2.6806\n",
            "Epoch [73/100], Batch [4/32], Loss: 2.7295\n",
            "Epoch [73/100], Batch [5/32], Loss: 2.5907\n",
            "Epoch [73/100], Batch [6/32], Loss: 2.6151\n",
            "Epoch [73/100], Batch [7/32], Loss: 2.5645\n",
            "Epoch [73/100], Batch [8/32], Loss: 2.5693\n",
            "Epoch [73/100], Batch [9/32], Loss: 2.5969\n",
            "Epoch [73/100], Batch [10/32], Loss: 2.6229\n",
            "Epoch [73/100], Batch [11/32], Loss: 2.6603\n",
            "Epoch [73/100], Batch [12/32], Loss: 2.5613\n",
            "Epoch [73/100], Batch [13/32], Loss: 2.5396\n",
            "Epoch [73/100], Batch [14/32], Loss: 2.7000\n",
            "Epoch [73/100], Batch [15/32], Loss: 2.5662\n",
            "Epoch [73/100], Batch [16/32], Loss: 2.6418\n",
            "Epoch [73/100], Batch [17/32], Loss: 2.6525\n",
            "Epoch [73/100], Batch [18/32], Loss: 2.6191\n",
            "Epoch [73/100], Batch [19/32], Loss: 2.5618\n",
            "Epoch [73/100], Batch [20/32], Loss: 2.6370\n",
            "Epoch [73/100], Batch [21/32], Loss: 2.5423\n",
            "Epoch [73/100], Batch [22/32], Loss: 2.6308\n",
            "Epoch [73/100], Batch [23/32], Loss: 2.5887\n",
            "Epoch [73/100], Batch [24/32], Loss: 2.5212\n",
            "Epoch [73/100], Batch [25/32], Loss: 2.6720\n",
            "Epoch [73/100], Batch [26/32], Loss: 2.5447\n",
            "Epoch [73/100], Batch [27/32], Loss: 2.5972\n",
            "Epoch [73/100], Batch [28/32], Loss: 2.4505\n",
            "Epoch [73/100], Batch [29/32], Loss: 2.6044\n",
            "Epoch [73/100], Batch [30/32], Loss: 2.7448\n",
            "Epoch [73/100], Batch [31/32], Loss: 2.5514\n",
            "Epoch [73/100], Batch [32/32], Loss: 2.6000\n",
            "Epoch [74/100], Batch [1/32], Loss: 2.6230\n",
            "Epoch [74/100], Batch [2/32], Loss: 2.5286\n",
            "Epoch [74/100], Batch [3/32], Loss: 2.4846\n",
            "Epoch [74/100], Batch [4/32], Loss: 2.4993\n",
            "Epoch [74/100], Batch [5/32], Loss: 2.5971\n",
            "Epoch [74/100], Batch [6/32], Loss: 2.5403\n",
            "Epoch [74/100], Batch [7/32], Loss: 2.6666\n",
            "Epoch [74/100], Batch [8/32], Loss: 2.7057\n",
            "Epoch [74/100], Batch [9/32], Loss: 2.6822\n",
            "Epoch [74/100], Batch [10/32], Loss: 2.5717\n",
            "Epoch [74/100], Batch [11/32], Loss: 2.6319\n",
            "Epoch [74/100], Batch [12/32], Loss: 2.7193\n",
            "Epoch [74/100], Batch [13/32], Loss: 2.5010\n",
            "Epoch [74/100], Batch [14/32], Loss: 2.6169\n",
            "Epoch [74/100], Batch [15/32], Loss: 2.5487\n",
            "Epoch [74/100], Batch [16/32], Loss: 2.7044\n",
            "Epoch [74/100], Batch [17/32], Loss: 2.5870\n",
            "Epoch [74/100], Batch [18/32], Loss: 2.5650\n",
            "Epoch [74/100], Batch [19/32], Loss: 2.7738\n",
            "Epoch [74/100], Batch [20/32], Loss: 2.5981\n",
            "Epoch [74/100], Batch [21/32], Loss: 2.6008\n",
            "Epoch [74/100], Batch [22/32], Loss: 2.6537\n",
            "Epoch [74/100], Batch [23/32], Loss: 2.5801\n",
            "Epoch [74/100], Batch [24/32], Loss: 2.6960\n",
            "Epoch [74/100], Batch [25/32], Loss: 2.6492\n",
            "Epoch [74/100], Batch [26/32], Loss: 2.4658\n",
            "Epoch [74/100], Batch [27/32], Loss: 2.6444\n",
            "Epoch [74/100], Batch [28/32], Loss: 2.6132\n",
            "Epoch [74/100], Batch [29/32], Loss: 2.5831\n",
            "Epoch [74/100], Batch [30/32], Loss: 2.7540\n",
            "Epoch [74/100], Batch [31/32], Loss: 2.5445\n",
            "Epoch [74/100], Batch [32/32], Loss: 2.5173\n",
            "Epoch [75/100], Batch [1/32], Loss: 2.3960\n",
            "Epoch [75/100], Batch [2/32], Loss: 2.6145\n",
            "Epoch [75/100], Batch [3/32], Loss: 2.7929\n",
            "Epoch [75/100], Batch [4/32], Loss: 2.4590\n",
            "Epoch [75/100], Batch [5/32], Loss: 2.5404\n",
            "Epoch [75/100], Batch [6/32], Loss: 2.6966\n",
            "Epoch [75/100], Batch [7/32], Loss: 2.4709\n",
            "Epoch [75/100], Batch [8/32], Loss: 2.8215\n",
            "Epoch [75/100], Batch [9/32], Loss: 2.6922\n",
            "Epoch [75/100], Batch [10/32], Loss: 2.7259\n",
            "Epoch [75/100], Batch [11/32], Loss: 2.6102\n",
            "Epoch [75/100], Batch [12/32], Loss: 2.7275\n",
            "Epoch [75/100], Batch [13/32], Loss: 2.6583\n",
            "Epoch [75/100], Batch [14/32], Loss: 2.6115\n",
            "Epoch [75/100], Batch [15/32], Loss: 2.6765\n",
            "Epoch [75/100], Batch [16/32], Loss: 2.6904\n",
            "Epoch [75/100], Batch [17/32], Loss: 2.6299\n",
            "Epoch [75/100], Batch [18/32], Loss: 2.6097\n",
            "Epoch [75/100], Batch [19/32], Loss: 2.6069\n",
            "Epoch [75/100], Batch [20/32], Loss: 2.5726\n",
            "Epoch [75/100], Batch [21/32], Loss: 2.6098\n",
            "Epoch [75/100], Batch [22/32], Loss: 2.6124\n",
            "Epoch [75/100], Batch [23/32], Loss: 2.6410\n",
            "Epoch [75/100], Batch [24/32], Loss: 2.6890\n",
            "Epoch [75/100], Batch [25/32], Loss: 2.5521\n",
            "Epoch [75/100], Batch [26/32], Loss: 2.5196\n",
            "Epoch [75/100], Batch [27/32], Loss: 2.6354\n",
            "Epoch [75/100], Batch [28/32], Loss: 2.5967\n",
            "Epoch [75/100], Batch [29/32], Loss: 2.5280\n",
            "Epoch [75/100], Batch [30/32], Loss: 2.5752\n",
            "Epoch [75/100], Batch [31/32], Loss: 2.5729\n",
            "Epoch [75/100], Batch [32/32], Loss: 2.5652\n",
            "Epoch [76/100], Batch [1/32], Loss: 2.5062\n",
            "Epoch [76/100], Batch [2/32], Loss: 2.6695\n",
            "Epoch [76/100], Batch [3/32], Loss: 2.6178\n",
            "Epoch [76/100], Batch [4/32], Loss: 2.5696\n",
            "Epoch [76/100], Batch [5/32], Loss: 2.5990\n",
            "Epoch [76/100], Batch [6/32], Loss: 2.6236\n",
            "Epoch [76/100], Batch [7/32], Loss: 2.4402\n",
            "Epoch [76/100], Batch [8/32], Loss: 2.5638\n",
            "Epoch [76/100], Batch [9/32], Loss: 2.6750\n",
            "Epoch [76/100], Batch [10/32], Loss: 2.6569\n",
            "Epoch [76/100], Batch [11/32], Loss: 2.6646\n",
            "Epoch [76/100], Batch [12/32], Loss: 2.6916\n",
            "Epoch [76/100], Batch [13/32], Loss: 2.7233\n",
            "Epoch [76/100], Batch [14/32], Loss: 2.5706\n",
            "Epoch [76/100], Batch [15/32], Loss: 2.7561\n",
            "Epoch [76/100], Batch [16/32], Loss: 2.6605\n",
            "Epoch [76/100], Batch [17/32], Loss: 2.6569\n",
            "Epoch [76/100], Batch [18/32], Loss: 2.5469\n",
            "Epoch [76/100], Batch [19/32], Loss: 2.5893\n",
            "Epoch [76/100], Batch [20/32], Loss: 2.6434\n",
            "Epoch [76/100], Batch [21/32], Loss: 2.5524\n",
            "Epoch [76/100], Batch [22/32], Loss: 2.4965\n",
            "Epoch [76/100], Batch [23/32], Loss: 2.4995\n",
            "Epoch [76/100], Batch [24/32], Loss: 2.5120\n",
            "Epoch [76/100], Batch [25/32], Loss: 2.6976\n",
            "Epoch [76/100], Batch [26/32], Loss: 2.6785\n",
            "Epoch [76/100], Batch [27/32], Loss: 2.3396\n",
            "Epoch [76/100], Batch [28/32], Loss: 2.4752\n",
            "Epoch [76/100], Batch [29/32], Loss: 2.6485\n",
            "Epoch [76/100], Batch [30/32], Loss: 2.6150\n",
            "Epoch [76/100], Batch [31/32], Loss: 2.5764\n",
            "Epoch [76/100], Batch [32/32], Loss: 2.7234\n",
            "Epoch [77/100], Batch [1/32], Loss: 2.6328\n",
            "Epoch [77/100], Batch [2/32], Loss: 2.4660\n",
            "Epoch [77/100], Batch [3/32], Loss: 2.4663\n",
            "Epoch [77/100], Batch [4/32], Loss: 2.6510\n",
            "Epoch [77/100], Batch [5/32], Loss: 2.5903\n",
            "Epoch [77/100], Batch [6/32], Loss: 2.6030\n",
            "Epoch [77/100], Batch [7/32], Loss: 2.7837\n",
            "Epoch [77/100], Batch [8/32], Loss: 2.4611\n",
            "Epoch [77/100], Batch [9/32], Loss: 2.5488\n",
            "Epoch [77/100], Batch [10/32], Loss: 2.6021\n",
            "Epoch [77/100], Batch [11/32], Loss: 2.6004\n",
            "Epoch [77/100], Batch [12/32], Loss: 2.7239\n",
            "Epoch [77/100], Batch [13/32], Loss: 2.6440\n",
            "Epoch [77/100], Batch [14/32], Loss: 2.6458\n",
            "Epoch [77/100], Batch [15/32], Loss: 2.6345\n",
            "Epoch [77/100], Batch [16/32], Loss: 2.6753\n",
            "Epoch [77/100], Batch [17/32], Loss: 2.7613\n",
            "Epoch [77/100], Batch [18/32], Loss: 2.5274\n",
            "Epoch [77/100], Batch [19/32], Loss: 2.5955\n",
            "Epoch [77/100], Batch [20/32], Loss: 2.4287\n",
            "Epoch [77/100], Batch [21/32], Loss: 2.6490\n",
            "Epoch [77/100], Batch [22/32], Loss: 2.6194\n",
            "Epoch [77/100], Batch [23/32], Loss: 2.4387\n",
            "Epoch [77/100], Batch [24/32], Loss: 2.3943\n",
            "Epoch [77/100], Batch [25/32], Loss: 2.6007\n",
            "Epoch [77/100], Batch [26/32], Loss: 2.7085\n",
            "Epoch [77/100], Batch [27/32], Loss: 2.6759\n",
            "Epoch [77/100], Batch [28/32], Loss: 2.5023\n",
            "Epoch [77/100], Batch [29/32], Loss: 2.6749\n",
            "Epoch [77/100], Batch [30/32], Loss: 2.7092\n",
            "Epoch [77/100], Batch [31/32], Loss: 2.6482\n",
            "Epoch [77/100], Batch [32/32], Loss: 2.7034\n",
            "Epoch [78/100], Batch [1/32], Loss: 2.7198\n",
            "Epoch [78/100], Batch [2/32], Loss: 2.5475\n",
            "Epoch [78/100], Batch [3/32], Loss: 2.5712\n",
            "Epoch [78/100], Batch [4/32], Loss: 2.8579\n",
            "Epoch [78/100], Batch [5/32], Loss: 2.7559\n",
            "Epoch [78/100], Batch [6/32], Loss: 2.5409\n",
            "Epoch [78/100], Batch [7/32], Loss: 2.5036\n",
            "Epoch [78/100], Batch [8/32], Loss: 2.5845\n",
            "Epoch [78/100], Batch [9/32], Loss: 2.5447\n",
            "Epoch [78/100], Batch [10/32], Loss: 2.6183\n",
            "Epoch [78/100], Batch [11/32], Loss: 2.6205\n",
            "Epoch [78/100], Batch [12/32], Loss: 2.7133\n",
            "Epoch [78/100], Batch [13/32], Loss: 2.6113\n",
            "Epoch [78/100], Batch [14/32], Loss: 2.5371\n",
            "Epoch [78/100], Batch [15/32], Loss: 2.6118\n",
            "Epoch [78/100], Batch [16/32], Loss: 2.6023\n",
            "Epoch [78/100], Batch [17/32], Loss: 2.5782\n",
            "Epoch [78/100], Batch [18/32], Loss: 2.5006\n",
            "Epoch [78/100], Batch [19/32], Loss: 2.6264\n",
            "Epoch [78/100], Batch [20/32], Loss: 2.5201\n",
            "Epoch [78/100], Batch [21/32], Loss: 2.4906\n",
            "Epoch [78/100], Batch [22/32], Loss: 2.4902\n",
            "Epoch [78/100], Batch [23/32], Loss: 2.7653\n",
            "Epoch [78/100], Batch [24/32], Loss: 2.4492\n",
            "Epoch [78/100], Batch [25/32], Loss: 2.6573\n",
            "Epoch [78/100], Batch [26/32], Loss: 2.6226\n",
            "Epoch [78/100], Batch [27/32], Loss: 2.4395\n",
            "Epoch [78/100], Batch [28/32], Loss: 2.4867\n",
            "Epoch [78/100], Batch [29/32], Loss: 2.7411\n",
            "Epoch [78/100], Batch [30/32], Loss: 2.5484\n",
            "Epoch [78/100], Batch [31/32], Loss: 2.5436\n",
            "Epoch [78/100], Batch [32/32], Loss: 2.5238\n",
            "Epoch [79/100], Batch [1/32], Loss: 2.7440\n",
            "Epoch [79/100], Batch [2/32], Loss: 2.5776\n",
            "Epoch [79/100], Batch [3/32], Loss: 2.6499\n",
            "Epoch [79/100], Batch [4/32], Loss: 2.5286\n",
            "Epoch [79/100], Batch [5/32], Loss: 2.5237\n",
            "Epoch [79/100], Batch [6/32], Loss: 2.5719\n",
            "Epoch [79/100], Batch [7/32], Loss: 2.6430\n",
            "Epoch [79/100], Batch [8/32], Loss: 2.5442\n",
            "Epoch [79/100], Batch [9/32], Loss: 2.6758\n",
            "Epoch [79/100], Batch [10/32], Loss: 2.6906\n",
            "Epoch [79/100], Batch [11/32], Loss: 2.4995\n",
            "Epoch [79/100], Batch [12/32], Loss: 2.5746\n",
            "Epoch [79/100], Batch [13/32], Loss: 2.6110\n",
            "Epoch [79/100], Batch [14/32], Loss: 2.5484\n",
            "Epoch [79/100], Batch [15/32], Loss: 2.4570\n",
            "Epoch [79/100], Batch [16/32], Loss: 2.6404\n",
            "Epoch [79/100], Batch [17/32], Loss: 2.6813\n",
            "Epoch [79/100], Batch [18/32], Loss: 2.7630\n",
            "Epoch [79/100], Batch [19/32], Loss: 2.6246\n",
            "Epoch [79/100], Batch [20/32], Loss: 2.6890\n",
            "Epoch [79/100], Batch [21/32], Loss: 2.6371\n",
            "Epoch [79/100], Batch [22/32], Loss: 2.5068\n",
            "Epoch [79/100], Batch [23/32], Loss: 2.4742\n",
            "Epoch [79/100], Batch [24/32], Loss: 2.7777\n",
            "Epoch [79/100], Batch [25/32], Loss: 2.6789\n",
            "Epoch [79/100], Batch [26/32], Loss: 2.5171\n",
            "Epoch [79/100], Batch [27/32], Loss: 2.6109\n",
            "Epoch [79/100], Batch [28/32], Loss: 2.6396\n",
            "Epoch [79/100], Batch [29/32], Loss: 2.5263\n",
            "Epoch [79/100], Batch [30/32], Loss: 2.5528\n",
            "Epoch [79/100], Batch [31/32], Loss: 2.4448\n",
            "Epoch [79/100], Batch [32/32], Loss: 2.5187\n",
            "Epoch [80/100], Batch [1/32], Loss: 2.6055\n",
            "Epoch [80/100], Batch [2/32], Loss: 2.6027\n",
            "Epoch [80/100], Batch [3/32], Loss: 2.5593\n",
            "Epoch [80/100], Batch [4/32], Loss: 2.5089\n",
            "Epoch [80/100], Batch [5/32], Loss: 2.5037\n",
            "Epoch [80/100], Batch [6/32], Loss: 2.6498\n",
            "Epoch [80/100], Batch [7/32], Loss: 2.6574\n",
            "Epoch [80/100], Batch [8/32], Loss: 2.6166\n",
            "Epoch [80/100], Batch [9/32], Loss: 2.8858\n",
            "Epoch [80/100], Batch [10/32], Loss: 2.6635\n",
            "Epoch [80/100], Batch [11/32], Loss: 2.6370\n",
            "Epoch [80/100], Batch [12/32], Loss: 2.6792\n",
            "Epoch [80/100], Batch [13/32], Loss: 2.5860\n",
            "Epoch [80/100], Batch [14/32], Loss: 2.4105\n",
            "Epoch [80/100], Batch [15/32], Loss: 2.5429\n",
            "Epoch [80/100], Batch [16/32], Loss: 2.5679\n",
            "Epoch [80/100], Batch [17/32], Loss: 2.7062\n",
            "Epoch [80/100], Batch [18/32], Loss: 2.5534\n",
            "Epoch [80/100], Batch [19/32], Loss: 2.5697\n",
            "Epoch [80/100], Batch [20/32], Loss: 2.7082\n",
            "Epoch [80/100], Batch [21/32], Loss: 2.4108\n",
            "Epoch [80/100], Batch [22/32], Loss: 2.7063\n",
            "Epoch [80/100], Batch [23/32], Loss: 2.5308\n",
            "Epoch [80/100], Batch [24/32], Loss: 2.6949\n",
            "Epoch [80/100], Batch [25/32], Loss: 2.6393\n",
            "Epoch [80/100], Batch [26/32], Loss: 2.5847\n",
            "Epoch [80/100], Batch [27/32], Loss: 2.6323\n",
            "Epoch [80/100], Batch [28/32], Loss: 2.6634\n",
            "Epoch [80/100], Batch [29/32], Loss: 2.5133\n",
            "Epoch [80/100], Batch [30/32], Loss: 2.5777\n",
            "Epoch [80/100], Batch [31/32], Loss: 2.6419\n",
            "Epoch [80/100], Batch [32/32], Loss: 2.4134\n",
            "Epoch [81/100], Batch [1/32], Loss: 2.6258\n",
            "Epoch [81/100], Batch [2/32], Loss: 2.5446\n",
            "Epoch [81/100], Batch [3/32], Loss: 2.6772\n",
            "Epoch [81/100], Batch [4/32], Loss: 2.3891\n",
            "Epoch [81/100], Batch [5/32], Loss: 2.6809\n",
            "Epoch [81/100], Batch [6/32], Loss: 2.5572\n",
            "Epoch [81/100], Batch [7/32], Loss: 2.4934\n",
            "Epoch [81/100], Batch [8/32], Loss: 2.5991\n",
            "Epoch [81/100], Batch [9/32], Loss: 2.5530\n",
            "Epoch [81/100], Batch [10/32], Loss: 2.6937\n",
            "Epoch [81/100], Batch [11/32], Loss: 2.5832\n",
            "Epoch [81/100], Batch [12/32], Loss: 2.6680\n",
            "Epoch [81/100], Batch [13/32], Loss: 2.6589\n",
            "Epoch [81/100], Batch [14/32], Loss: 2.5298\n",
            "Epoch [81/100], Batch [15/32], Loss: 2.3042\n",
            "Epoch [81/100], Batch [16/32], Loss: 2.6648\n",
            "Epoch [81/100], Batch [17/32], Loss: 2.7118\n",
            "Epoch [81/100], Batch [18/32], Loss: 2.5959\n",
            "Epoch [81/100], Batch [19/32], Loss: 2.6096\n",
            "Epoch [81/100], Batch [20/32], Loss: 2.4265\n",
            "Epoch [81/100], Batch [21/32], Loss: 2.6355\n",
            "Epoch [81/100], Batch [22/32], Loss: 2.6686\n",
            "Epoch [81/100], Batch [23/32], Loss: 2.6465\n",
            "Epoch [81/100], Batch [24/32], Loss: 2.5394\n",
            "Epoch [81/100], Batch [25/32], Loss: 2.8266\n",
            "Epoch [81/100], Batch [26/32], Loss: 2.6085\n",
            "Epoch [81/100], Batch [27/32], Loss: 2.4170\n",
            "Epoch [81/100], Batch [28/32], Loss: 2.7053\n",
            "Epoch [81/100], Batch [29/32], Loss: 2.5784\n",
            "Epoch [81/100], Batch [30/32], Loss: 2.5255\n",
            "Epoch [81/100], Batch [31/32], Loss: 2.6971\n",
            "Epoch [81/100], Batch [32/32], Loss: 2.6744\n",
            "Epoch [82/100], Batch [1/32], Loss: 2.6651\n",
            "Epoch [82/100], Batch [2/32], Loss: 2.6446\n",
            "Epoch [82/100], Batch [3/32], Loss: 2.5299\n",
            "Epoch [82/100], Batch [4/32], Loss: 2.5830\n",
            "Epoch [82/100], Batch [5/32], Loss: 2.5710\n",
            "Epoch [82/100], Batch [6/32], Loss: 2.6411\n",
            "Epoch [82/100], Batch [7/32], Loss: 2.6503\n",
            "Epoch [82/100], Batch [8/32], Loss: 2.4558\n",
            "Epoch [82/100], Batch [9/32], Loss: 2.5683\n",
            "Epoch [82/100], Batch [10/32], Loss: 2.6612\n",
            "Epoch [82/100], Batch [11/32], Loss: 2.6631\n",
            "Epoch [82/100], Batch [12/32], Loss: 2.6164\n",
            "Epoch [82/100], Batch [13/32], Loss: 2.6106\n",
            "Epoch [82/100], Batch [14/32], Loss: 2.4626\n",
            "Epoch [82/100], Batch [15/32], Loss: 2.4514\n",
            "Epoch [82/100], Batch [16/32], Loss: 2.5567\n",
            "Epoch [82/100], Batch [17/32], Loss: 2.5299\n",
            "Epoch [82/100], Batch [18/32], Loss: 2.6888\n",
            "Epoch [82/100], Batch [19/32], Loss: 2.7534\n",
            "Epoch [82/100], Batch [20/32], Loss: 2.5950\n",
            "Epoch [82/100], Batch [21/32], Loss: 2.5628\n",
            "Epoch [82/100], Batch [22/32], Loss: 2.4407\n",
            "Epoch [82/100], Batch [23/32], Loss: 2.4038\n",
            "Epoch [82/100], Batch [24/32], Loss: 2.7231\n",
            "Epoch [82/100], Batch [25/32], Loss: 2.6172\n",
            "Epoch [82/100], Batch [26/32], Loss: 2.6585\n",
            "Epoch [82/100], Batch [27/32], Loss: 2.5847\n",
            "Epoch [82/100], Batch [28/32], Loss: 2.6028\n",
            "Epoch [82/100], Batch [29/32], Loss: 2.6963\n",
            "Epoch [82/100], Batch [30/32], Loss: 2.7812\n",
            "Epoch [82/100], Batch [31/32], Loss: 2.6752\n",
            "Epoch [82/100], Batch [32/32], Loss: 2.4756\n",
            "Epoch [83/100], Batch [1/32], Loss: 2.5333\n",
            "Epoch [83/100], Batch [2/32], Loss: 2.5466\n",
            "Epoch [83/100], Batch [3/32], Loss: 2.5198\n",
            "Epoch [83/100], Batch [4/32], Loss: 2.4957\n",
            "Epoch [83/100], Batch [5/32], Loss: 2.6542\n",
            "Epoch [83/100], Batch [6/32], Loss: 2.6232\n",
            "Epoch [83/100], Batch [7/32], Loss: 2.5517\n",
            "Epoch [83/100], Batch [8/32], Loss: 2.5802\n",
            "Epoch [83/100], Batch [9/32], Loss: 2.4505\n",
            "Epoch [83/100], Batch [10/32], Loss: 2.5313\n",
            "Epoch [83/100], Batch [11/32], Loss: 2.7398\n",
            "Epoch [83/100], Batch [12/32], Loss: 2.6977\n",
            "Epoch [83/100], Batch [13/32], Loss: 2.4323\n",
            "Epoch [83/100], Batch [14/32], Loss: 2.4027\n",
            "Epoch [83/100], Batch [15/32], Loss: 2.6386\n",
            "Epoch [83/100], Batch [16/32], Loss: 2.6430\n",
            "Epoch [83/100], Batch [17/32], Loss: 2.5495\n",
            "Epoch [83/100], Batch [18/32], Loss: 2.5773\n",
            "Epoch [83/100], Batch [19/32], Loss: 2.6114\n",
            "Epoch [83/100], Batch [20/32], Loss: 2.7867\n",
            "Epoch [83/100], Batch [21/32], Loss: 2.7040\n",
            "Epoch [83/100], Batch [22/32], Loss: 2.5968\n",
            "Epoch [83/100], Batch [23/32], Loss: 2.6845\n",
            "Epoch [83/100], Batch [24/32], Loss: 2.5360\n",
            "Epoch [83/100], Batch [25/32], Loss: 2.6200\n",
            "Epoch [83/100], Batch [26/32], Loss: 2.6285\n",
            "Epoch [83/100], Batch [27/32], Loss: 2.5401\n",
            "Epoch [83/100], Batch [28/32], Loss: 2.6302\n",
            "Epoch [83/100], Batch [29/32], Loss: 2.5983\n",
            "Epoch [83/100], Batch [30/32], Loss: 2.8091\n",
            "Epoch [83/100], Batch [31/32], Loss: 2.5232\n",
            "Epoch [83/100], Batch [32/32], Loss: 2.4783\n",
            "Epoch [84/100], Batch [1/32], Loss: 2.6693\n",
            "Epoch [84/100], Batch [2/32], Loss: 2.5722\n",
            "Epoch [84/100], Batch [3/32], Loss: 2.5781\n",
            "Epoch [84/100], Batch [4/32], Loss: 2.6201\n",
            "Epoch [84/100], Batch [5/32], Loss: 2.4661\n",
            "Epoch [84/100], Batch [6/32], Loss: 2.7413\n",
            "Epoch [84/100], Batch [7/32], Loss: 2.5034\n",
            "Epoch [84/100], Batch [8/32], Loss: 2.6223\n",
            "Epoch [84/100], Batch [9/32], Loss: 2.8169\n",
            "Epoch [84/100], Batch [10/32], Loss: 2.5367\n",
            "Epoch [84/100], Batch [11/32], Loss: 2.5585\n",
            "Epoch [84/100], Batch [12/32], Loss: 2.6218\n",
            "Epoch [84/100], Batch [13/32], Loss: 2.5858\n",
            "Epoch [84/100], Batch [14/32], Loss: 2.5448\n",
            "Epoch [84/100], Batch [15/32], Loss: 2.5164\n",
            "Epoch [84/100], Batch [16/32], Loss: 2.7541\n",
            "Epoch [84/100], Batch [17/32], Loss: 2.6042\n",
            "Epoch [84/100], Batch [18/32], Loss: 2.6234\n",
            "Epoch [84/100], Batch [19/32], Loss: 2.5376\n",
            "Epoch [84/100], Batch [20/32], Loss: 2.4301\n",
            "Epoch [84/100], Batch [21/32], Loss: 2.5085\n",
            "Epoch [84/100], Batch [22/32], Loss: 2.6253\n",
            "Epoch [84/100], Batch [23/32], Loss: 2.6593\n",
            "Epoch [84/100], Batch [24/32], Loss: 2.5656\n",
            "Epoch [84/100], Batch [25/32], Loss: 2.5778\n",
            "Epoch [84/100], Batch [26/32], Loss: 2.5500\n",
            "Epoch [84/100], Batch [27/32], Loss: 2.5895\n",
            "Epoch [84/100], Batch [28/32], Loss: 2.5082\n",
            "Epoch [84/100], Batch [29/32], Loss: 2.6147\n",
            "Epoch [84/100], Batch [30/32], Loss: 2.4473\n",
            "Epoch [84/100], Batch [31/32], Loss: 2.6210\n",
            "Epoch [84/100], Batch [32/32], Loss: 2.7928\n",
            "Epoch [85/100], Batch [1/32], Loss: 2.5516\n",
            "Epoch [85/100], Batch [2/32], Loss: 2.6025\n",
            "Epoch [85/100], Batch [3/32], Loss: 2.5796\n",
            "Epoch [85/100], Batch [4/32], Loss: 2.7019\n",
            "Epoch [85/100], Batch [5/32], Loss: 2.5858\n",
            "Epoch [85/100], Batch [6/32], Loss: 2.5569\n",
            "Epoch [85/100], Batch [7/32], Loss: 2.5503\n",
            "Epoch [85/100], Batch [8/32], Loss: 2.4507\n",
            "Epoch [85/100], Batch [9/32], Loss: 2.5301\n",
            "Epoch [85/100], Batch [10/32], Loss: 2.8017\n",
            "Epoch [85/100], Batch [11/32], Loss: 2.6078\n",
            "Epoch [85/100], Batch [12/32], Loss: 2.5159\n",
            "Epoch [85/100], Batch [13/32], Loss: 2.6631\n",
            "Epoch [85/100], Batch [14/32], Loss: 2.5009\n",
            "Epoch [85/100], Batch [15/32], Loss: 2.5988\n",
            "Epoch [85/100], Batch [16/32], Loss: 2.6922\n",
            "Epoch [85/100], Batch [17/32], Loss: 2.7218\n",
            "Epoch [85/100], Batch [18/32], Loss: 2.5692\n",
            "Epoch [85/100], Batch [19/32], Loss: 2.6952\n",
            "Epoch [85/100], Batch [20/32], Loss: 2.5556\n",
            "Epoch [85/100], Batch [21/32], Loss: 2.6305\n",
            "Epoch [85/100], Batch [22/32], Loss: 2.6237\n",
            "Epoch [85/100], Batch [23/32], Loss: 2.7234\n",
            "Epoch [85/100], Batch [24/32], Loss: 2.6381\n",
            "Epoch [85/100], Batch [25/32], Loss: 2.4070\n",
            "Epoch [85/100], Batch [26/32], Loss: 2.4413\n",
            "Epoch [85/100], Batch [27/32], Loss: 2.4818\n",
            "Epoch [85/100], Batch [28/32], Loss: 2.7290\n",
            "Epoch [85/100], Batch [29/32], Loss: 2.4504\n",
            "Epoch [85/100], Batch [30/32], Loss: 2.5939\n",
            "Epoch [85/100], Batch [31/32], Loss: 2.6743\n",
            "Epoch [85/100], Batch [32/32], Loss: 2.4683\n",
            "Epoch [86/100], Batch [1/32], Loss: 2.4436\n",
            "Epoch [86/100], Batch [2/32], Loss: 2.5849\n",
            "Epoch [86/100], Batch [3/32], Loss: 2.5619\n",
            "Epoch [86/100], Batch [4/32], Loss: 2.6654\n",
            "Epoch [86/100], Batch [5/32], Loss: 2.5196\n",
            "Epoch [86/100], Batch [6/32], Loss: 2.4623\n",
            "Epoch [86/100], Batch [7/32], Loss: 2.7124\n",
            "Epoch [86/100], Batch [8/32], Loss: 2.5200\n",
            "Epoch [86/100], Batch [9/32], Loss: 2.5838\n",
            "Epoch [86/100], Batch [10/32], Loss: 2.5264\n",
            "Epoch [86/100], Batch [11/32], Loss: 2.6789\n",
            "Epoch [86/100], Batch [12/32], Loss: 2.5739\n",
            "Epoch [86/100], Batch [13/32], Loss: 2.4957\n",
            "Epoch [86/100], Batch [14/32], Loss: 2.4781\n",
            "Epoch [86/100], Batch [15/32], Loss: 2.5252\n",
            "Epoch [86/100], Batch [16/32], Loss: 2.5945\n",
            "Epoch [86/100], Batch [17/32], Loss: 2.6191\n",
            "Epoch [86/100], Batch [18/32], Loss: 2.5849\n",
            "Epoch [86/100], Batch [19/32], Loss: 2.5982\n",
            "Epoch [86/100], Batch [20/32], Loss: 2.7397\n",
            "Epoch [86/100], Batch [21/32], Loss: 2.6143\n",
            "Epoch [86/100], Batch [22/32], Loss: 2.5611\n",
            "Epoch [86/100], Batch [23/32], Loss: 2.5977\n",
            "Epoch [86/100], Batch [24/32], Loss: 2.6121\n",
            "Epoch [86/100], Batch [25/32], Loss: 2.4598\n",
            "Epoch [86/100], Batch [26/32], Loss: 2.7812\n",
            "Epoch [86/100], Batch [27/32], Loss: 2.6743\n",
            "Epoch [86/100], Batch [28/32], Loss: 2.5838\n",
            "Epoch [86/100], Batch [29/32], Loss: 2.5334\n",
            "Epoch [86/100], Batch [30/32], Loss: 2.5555\n",
            "Epoch [86/100], Batch [31/32], Loss: 2.6660\n",
            "Epoch [86/100], Batch [32/32], Loss: 2.5361\n",
            "Epoch [87/100], Batch [1/32], Loss: 2.5509\n",
            "Epoch [87/100], Batch [2/32], Loss: 2.5666\n",
            "Epoch [87/100], Batch [3/32], Loss: 2.4821\n",
            "Epoch [87/100], Batch [4/32], Loss: 2.6539\n",
            "Epoch [87/100], Batch [5/32], Loss: 2.6710\n",
            "Epoch [87/100], Batch [6/32], Loss: 2.4811\n",
            "Epoch [87/100], Batch [7/32], Loss: 2.5569\n",
            "Epoch [87/100], Batch [8/32], Loss: 2.3884\n",
            "Epoch [87/100], Batch [9/32], Loss: 2.7470\n",
            "Epoch [87/100], Batch [10/32], Loss: 2.6041\n",
            "Epoch [87/100], Batch [11/32], Loss: 2.5438\n",
            "Epoch [87/100], Batch [12/32], Loss: 2.7014\n",
            "Epoch [87/100], Batch [13/32], Loss: 2.5837\n",
            "Epoch [87/100], Batch [14/32], Loss: 2.3500\n",
            "Epoch [87/100], Batch [15/32], Loss: 2.5526\n",
            "Epoch [87/100], Batch [16/32], Loss: 2.4918\n",
            "Epoch [87/100], Batch [17/32], Loss: 2.7190\n",
            "Epoch [87/100], Batch [18/32], Loss: 2.6750\n",
            "Epoch [87/100], Batch [19/32], Loss: 2.6309\n",
            "Epoch [87/100], Batch [20/32], Loss: 2.5605\n",
            "Epoch [87/100], Batch [21/32], Loss: 2.7275\n",
            "Epoch [87/100], Batch [22/32], Loss: 2.4145\n",
            "Epoch [87/100], Batch [23/32], Loss: 2.6036\n",
            "Epoch [87/100], Batch [24/32], Loss: 2.6444\n",
            "Epoch [87/100], Batch [25/32], Loss: 2.6664\n",
            "Epoch [87/100], Batch [26/32], Loss: 2.5495\n",
            "Epoch [87/100], Batch [27/32], Loss: 2.6428\n",
            "Epoch [87/100], Batch [28/32], Loss: 2.4099\n",
            "Epoch [87/100], Batch [29/32], Loss: 2.6543\n",
            "Epoch [87/100], Batch [30/32], Loss: 2.5929\n",
            "Epoch [87/100], Batch [31/32], Loss: 2.4184\n",
            "Epoch [87/100], Batch [32/32], Loss: 2.7549\n",
            "Epoch [88/100], Batch [1/32], Loss: 2.6369\n",
            "Epoch [88/100], Batch [2/32], Loss: 2.5121\n",
            "Epoch [88/100], Batch [3/32], Loss: 2.6974\n",
            "Epoch [88/100], Batch [4/32], Loss: 2.5443\n",
            "Epoch [88/100], Batch [5/32], Loss: 2.5863\n",
            "Epoch [88/100], Batch [6/32], Loss: 2.7102\n",
            "Epoch [88/100], Batch [7/32], Loss: 2.4948\n",
            "Epoch [88/100], Batch [8/32], Loss: 2.6567\n",
            "Epoch [88/100], Batch [9/32], Loss: 2.5266\n",
            "Epoch [88/100], Batch [10/32], Loss: 2.7278\n",
            "Epoch [88/100], Batch [11/32], Loss: 2.4764\n",
            "Epoch [88/100], Batch [12/32], Loss: 2.5263\n",
            "Epoch [88/100], Batch [13/32], Loss: 2.6258\n",
            "Epoch [88/100], Batch [14/32], Loss: 2.4794\n",
            "Epoch [88/100], Batch [15/32], Loss: 2.4893\n",
            "Epoch [88/100], Batch [16/32], Loss: 2.6229\n",
            "Epoch [88/100], Batch [17/32], Loss: 2.5603\n",
            "Epoch [88/100], Batch [18/32], Loss: 2.6513\n",
            "Epoch [88/100], Batch [19/32], Loss: 2.5370\n",
            "Epoch [88/100], Batch [20/32], Loss: 2.5867\n",
            "Epoch [88/100], Batch [21/32], Loss: 2.6302\n",
            "Epoch [88/100], Batch [22/32], Loss: 2.6155\n",
            "Epoch [88/100], Batch [23/32], Loss: 2.5522\n",
            "Epoch [88/100], Batch [24/32], Loss: 2.5896\n",
            "Epoch [88/100], Batch [25/32], Loss: 2.5123\n",
            "Epoch [88/100], Batch [26/32], Loss: 2.7411\n",
            "Epoch [88/100], Batch [27/32], Loss: 2.6050\n",
            "Epoch [88/100], Batch [28/32], Loss: 2.5907\n",
            "Epoch [88/100], Batch [29/32], Loss: 2.6731\n",
            "Epoch [88/100], Batch [30/32], Loss: 2.6335\n",
            "Epoch [88/100], Batch [31/32], Loss: 2.4330\n",
            "Epoch [88/100], Batch [32/32], Loss: 2.4910\n",
            "Epoch [89/100], Batch [1/32], Loss: 2.6476\n",
            "Epoch [89/100], Batch [2/32], Loss: 2.5868\n",
            "Epoch [89/100], Batch [3/32], Loss: 2.5358\n",
            "Epoch [89/100], Batch [4/32], Loss: 2.5833\n",
            "Epoch [89/100], Batch [5/32], Loss: 2.5304\n",
            "Epoch [89/100], Batch [6/32], Loss: 2.5260\n",
            "Epoch [89/100], Batch [7/32], Loss: 2.6177\n",
            "Epoch [89/100], Batch [8/32], Loss: 2.5385\n",
            "Epoch [89/100], Batch [9/32], Loss: 2.6090\n",
            "Epoch [89/100], Batch [10/32], Loss: 2.5882\n",
            "Epoch [89/100], Batch [11/32], Loss: 2.4284\n",
            "Epoch [89/100], Batch [12/32], Loss: 2.6506\n",
            "Epoch [89/100], Batch [13/32], Loss: 2.6720\n",
            "Epoch [89/100], Batch [14/32], Loss: 2.4199\n",
            "Epoch [89/100], Batch [15/32], Loss: 2.6765\n",
            "Epoch [89/100], Batch [16/32], Loss: 2.7464\n",
            "Epoch [89/100], Batch [17/32], Loss: 2.6254\n",
            "Epoch [89/100], Batch [18/32], Loss: 2.5394\n",
            "Epoch [89/100], Batch [19/32], Loss: 2.4992\n",
            "Epoch [89/100], Batch [20/32], Loss: 2.5986\n",
            "Epoch [89/100], Batch [21/32], Loss: 2.5070\n",
            "Epoch [89/100], Batch [22/32], Loss: 2.6326\n",
            "Epoch [89/100], Batch [23/32], Loss: 2.5963\n",
            "Epoch [89/100], Batch [24/32], Loss: 2.5537\n",
            "Epoch [89/100], Batch [25/32], Loss: 2.6000\n",
            "Epoch [89/100], Batch [26/32], Loss: 2.5737\n",
            "Epoch [89/100], Batch [27/32], Loss: 2.7867\n",
            "Epoch [89/100], Batch [28/32], Loss: 2.5695\n",
            "Epoch [89/100], Batch [29/32], Loss: 2.6022\n",
            "Epoch [89/100], Batch [30/32], Loss: 2.5754\n",
            "Epoch [89/100], Batch [31/32], Loss: 2.6026\n",
            "Epoch [89/100], Batch [32/32], Loss: 2.4887\n",
            "Epoch [90/100], Batch [1/32], Loss: 2.4559\n",
            "Epoch [90/100], Batch [2/32], Loss: 2.6296\n",
            "Epoch [90/100], Batch [3/32], Loss: 2.6466\n",
            "Epoch [90/100], Batch [4/32], Loss: 2.3517\n",
            "Epoch [90/100], Batch [5/32], Loss: 2.5564\n",
            "Epoch [90/100], Batch [6/32], Loss: 2.4592\n",
            "Epoch [90/100], Batch [7/32], Loss: 2.5402\n",
            "Epoch [90/100], Batch [8/32], Loss: 2.6907\n",
            "Epoch [90/100], Batch [9/32], Loss: 2.7782\n",
            "Epoch [90/100], Batch [10/32], Loss: 2.7211\n",
            "Epoch [90/100], Batch [11/32], Loss: 2.6052\n",
            "Epoch [90/100], Batch [12/32], Loss: 2.5030\n",
            "Epoch [90/100], Batch [13/32], Loss: 2.4924\n",
            "Epoch [90/100], Batch [14/32], Loss: 2.5791\n",
            "Epoch [90/100], Batch [15/32], Loss: 2.5426\n",
            "Epoch [90/100], Batch [16/32], Loss: 2.7500\n",
            "Epoch [90/100], Batch [17/32], Loss: 2.5602\n",
            "Epoch [90/100], Batch [18/32], Loss: 2.5520\n",
            "Epoch [90/100], Batch [19/32], Loss: 2.4362\n",
            "Epoch [90/100], Batch [20/32], Loss: 2.5549\n",
            "Epoch [90/100], Batch [21/32], Loss: 2.5012\n",
            "Epoch [90/100], Batch [22/32], Loss: 2.6666\n",
            "Epoch [90/100], Batch [23/32], Loss: 2.6589\n",
            "Epoch [90/100], Batch [24/32], Loss: 2.5567\n",
            "Epoch [90/100], Batch [25/32], Loss: 2.7596\n",
            "Epoch [90/100], Batch [26/32], Loss: 2.6543\n",
            "Epoch [90/100], Batch [27/32], Loss: 2.5846\n",
            "Epoch [90/100], Batch [28/32], Loss: 2.4856\n",
            "Epoch [90/100], Batch [29/32], Loss: 2.6598\n",
            "Epoch [90/100], Batch [30/32], Loss: 2.6035\n",
            "Epoch [90/100], Batch [31/32], Loss: 2.4454\n",
            "Epoch [90/100], Batch [32/32], Loss: 2.6202\n",
            "Epoch [91/100], Batch [1/32], Loss: 2.4276\n",
            "Epoch [91/100], Batch [2/32], Loss: 2.6288\n",
            "Epoch [91/100], Batch [3/32], Loss: 2.5313\n",
            "Epoch [91/100], Batch [4/32], Loss: 2.5968\n",
            "Epoch [91/100], Batch [5/32], Loss: 2.5144\n",
            "Epoch [91/100], Batch [6/32], Loss: 2.7021\n",
            "Epoch [91/100], Batch [7/32], Loss: 2.5426\n",
            "Epoch [91/100], Batch [8/32], Loss: 2.5452\n",
            "Epoch [91/100], Batch [9/32], Loss: 2.5986\n",
            "Epoch [91/100], Batch [10/32], Loss: 2.3949\n",
            "Epoch [91/100], Batch [11/32], Loss: 2.5666\n",
            "Epoch [91/100], Batch [12/32], Loss: 2.5977\n",
            "Epoch [91/100], Batch [13/32], Loss: 2.6772\n",
            "Epoch [91/100], Batch [14/32], Loss: 2.5910\n",
            "Epoch [91/100], Batch [15/32], Loss: 2.6554\n",
            "Epoch [91/100], Batch [16/32], Loss: 2.6395\n",
            "Epoch [91/100], Batch [17/32], Loss: 2.6850\n",
            "Epoch [91/100], Batch [18/32], Loss: 2.5316\n",
            "Epoch [91/100], Batch [19/32], Loss: 2.4982\n",
            "Epoch [91/100], Batch [20/32], Loss: 2.4247\n",
            "Epoch [91/100], Batch [21/32], Loss: 2.5923\n",
            "Epoch [91/100], Batch [22/32], Loss: 2.4204\n",
            "Epoch [91/100], Batch [23/32], Loss: 2.6144\n",
            "Epoch [91/100], Batch [24/32], Loss: 2.6466\n",
            "Epoch [91/100], Batch [25/32], Loss: 2.5167\n",
            "Epoch [91/100], Batch [26/32], Loss: 2.5711\n",
            "Epoch [91/100], Batch [27/32], Loss: 2.6580\n",
            "Epoch [91/100], Batch [28/32], Loss: 2.6837\n",
            "Epoch [91/100], Batch [29/32], Loss: 2.5159\n",
            "Epoch [91/100], Batch [30/32], Loss: 2.6105\n",
            "Epoch [91/100], Batch [31/32], Loss: 2.4888\n",
            "Epoch [91/100], Batch [32/32], Loss: 2.5638\n",
            "Epoch [92/100], Batch [1/32], Loss: 2.5729\n",
            "Epoch [92/100], Batch [2/32], Loss: 2.6212\n",
            "Epoch [92/100], Batch [3/32], Loss: 2.5551\n",
            "Epoch [92/100], Batch [4/32], Loss: 2.5252\n",
            "Epoch [92/100], Batch [5/32], Loss: 2.5189\n",
            "Epoch [92/100], Batch [6/32], Loss: 2.6135\n",
            "Epoch [92/100], Batch [7/32], Loss: 2.5022\n",
            "Epoch [92/100], Batch [8/32], Loss: 2.4761\n",
            "Epoch [92/100], Batch [9/32], Loss: 2.7080\n",
            "Epoch [92/100], Batch [10/32], Loss: 2.3352\n",
            "Epoch [92/100], Batch [11/32], Loss: 2.5361\n",
            "Epoch [92/100], Batch [12/32], Loss: 2.6514\n",
            "Epoch [92/100], Batch [13/32], Loss: 2.5539\n",
            "Epoch [92/100], Batch [14/32], Loss: 2.7360\n",
            "Epoch [92/100], Batch [15/32], Loss: 2.4318\n",
            "Epoch [92/100], Batch [16/32], Loss: 2.7062\n",
            "Epoch [92/100], Batch [17/32], Loss: 2.5834\n",
            "Epoch [92/100], Batch [18/32], Loss: 2.4433\n",
            "Epoch [92/100], Batch [19/32], Loss: 2.6245\n",
            "Epoch [92/100], Batch [20/32], Loss: 2.6612\n",
            "Epoch [92/100], Batch [21/32], Loss: 2.6524\n",
            "Epoch [92/100], Batch [22/32], Loss: 2.6487\n",
            "Epoch [92/100], Batch [23/32], Loss: 2.5758\n",
            "Epoch [92/100], Batch [24/32], Loss: 2.6254\n",
            "Epoch [92/100], Batch [25/32], Loss: 2.4975\n",
            "Epoch [92/100], Batch [26/32], Loss: 2.7024\n",
            "Epoch [92/100], Batch [27/32], Loss: 2.5852\n",
            "Epoch [92/100], Batch [28/32], Loss: 2.7328\n",
            "Epoch [92/100], Batch [29/32], Loss: 2.3367\n",
            "Epoch [92/100], Batch [30/32], Loss: 2.6092\n",
            "Epoch [92/100], Batch [31/32], Loss: 2.4132\n",
            "Epoch [92/100], Batch [32/32], Loss: 2.6655\n",
            "Epoch [93/100], Batch [1/32], Loss: 2.6754\n",
            "Epoch [93/100], Batch [2/32], Loss: 2.5810\n",
            "Epoch [93/100], Batch [3/32], Loss: 2.4540\n",
            "Epoch [93/100], Batch [4/32], Loss: 2.7625\n",
            "Epoch [93/100], Batch [5/32], Loss: 2.5751\n",
            "Epoch [93/100], Batch [6/32], Loss: 2.7432\n",
            "Epoch [93/100], Batch [7/32], Loss: 2.4265\n",
            "Epoch [93/100], Batch [8/32], Loss: 2.4901\n",
            "Epoch [93/100], Batch [9/32], Loss: 2.4467\n",
            "Epoch [93/100], Batch [10/32], Loss: 2.5321\n",
            "Epoch [93/100], Batch [11/32], Loss: 2.4666\n",
            "Epoch [93/100], Batch [12/32], Loss: 2.6094\n",
            "Epoch [93/100], Batch [13/32], Loss: 2.5128\n",
            "Epoch [93/100], Batch [14/32], Loss: 2.5640\n",
            "Epoch [93/100], Batch [15/32], Loss: 2.4872\n",
            "Epoch [93/100], Batch [16/32], Loss: 2.5700\n",
            "Epoch [93/100], Batch [17/32], Loss: 2.5935\n",
            "Epoch [93/100], Batch [18/32], Loss: 2.5233\n",
            "Epoch [93/100], Batch [19/32], Loss: 2.6322\n",
            "Epoch [93/100], Batch [20/32], Loss: 2.5635\n",
            "Epoch [93/100], Batch [21/32], Loss: 2.5711\n",
            "Epoch [93/100], Batch [22/32], Loss: 2.6751\n",
            "Epoch [93/100], Batch [23/32], Loss: 2.5846\n",
            "Epoch [93/100], Batch [24/32], Loss: 2.5181\n",
            "Epoch [93/100], Batch [25/32], Loss: 2.6930\n",
            "Epoch [93/100], Batch [26/32], Loss: 2.5231\n",
            "Epoch [93/100], Batch [27/32], Loss: 2.5530\n",
            "Epoch [93/100], Batch [28/32], Loss: 2.6483\n",
            "Epoch [93/100], Batch [29/32], Loss: 2.6532\n",
            "Epoch [93/100], Batch [30/32], Loss: 2.5033\n",
            "Epoch [93/100], Batch [31/32], Loss: 2.6274\n",
            "Epoch [93/100], Batch [32/32], Loss: 2.4672\n",
            "Epoch [94/100], Batch [1/32], Loss: 2.5278\n",
            "Epoch [94/100], Batch [2/32], Loss: 2.4300\n",
            "Epoch [94/100], Batch [3/32], Loss: 2.4705\n",
            "Epoch [94/100], Batch [4/32], Loss: 2.6367\n",
            "Epoch [94/100], Batch [5/32], Loss: 2.4911\n",
            "Epoch [94/100], Batch [6/32], Loss: 2.5885\n",
            "Epoch [94/100], Batch [7/32], Loss: 2.6823\n",
            "Epoch [94/100], Batch [8/32], Loss: 2.5772\n",
            "Epoch [94/100], Batch [9/32], Loss: 2.5326\n",
            "Epoch [94/100], Batch [10/32], Loss: 2.6592\n",
            "Epoch [94/100], Batch [11/32], Loss: 2.5107\n",
            "Epoch [94/100], Batch [12/32], Loss: 2.5838\n",
            "Epoch [94/100], Batch [13/32], Loss: 2.5217\n",
            "Epoch [94/100], Batch [14/32], Loss: 2.6381\n",
            "Epoch [94/100], Batch [15/32], Loss: 2.5592\n",
            "Epoch [94/100], Batch [16/32], Loss: 2.4661\n",
            "Epoch [94/100], Batch [17/32], Loss: 2.5524\n",
            "Epoch [94/100], Batch [18/32], Loss: 2.7080\n",
            "Epoch [94/100], Batch [19/32], Loss: 2.5675\n",
            "Epoch [94/100], Batch [20/32], Loss: 2.5929\n",
            "Epoch [94/100], Batch [21/32], Loss: 2.4957\n",
            "Epoch [94/100], Batch [22/32], Loss: 2.5691\n",
            "Epoch [94/100], Batch [23/32], Loss: 2.5685\n",
            "Epoch [94/100], Batch [24/32], Loss: 2.6098\n",
            "Epoch [94/100], Batch [25/32], Loss: 2.5431\n",
            "Epoch [94/100], Batch [26/32], Loss: 2.4949\n",
            "Epoch [94/100], Batch [27/32], Loss: 2.7388\n",
            "Epoch [94/100], Batch [28/32], Loss: 2.6838\n",
            "Epoch [94/100], Batch [29/32], Loss: 2.5317\n",
            "Epoch [94/100], Batch [30/32], Loss: 2.5342\n",
            "Epoch [94/100], Batch [31/32], Loss: 2.5650\n",
            "Epoch [94/100], Batch [32/32], Loss: 2.6296\n",
            "Epoch [95/100], Batch [1/32], Loss: 2.6915\n",
            "Epoch [95/100], Batch [2/32], Loss: 2.5579\n",
            "Epoch [95/100], Batch [3/32], Loss: 2.8353\n",
            "Epoch [95/100], Batch [4/32], Loss: 2.6209\n",
            "Epoch [95/100], Batch [5/32], Loss: 2.6159\n",
            "Epoch [95/100], Batch [6/32], Loss: 2.4305\n",
            "Epoch [95/100], Batch [7/32], Loss: 2.6488\n",
            "Epoch [95/100], Batch [8/32], Loss: 2.5988\n",
            "Epoch [95/100], Batch [9/32], Loss: 2.6444\n",
            "Epoch [95/100], Batch [10/32], Loss: 2.4563\n",
            "Epoch [95/100], Batch [11/32], Loss: 2.6097\n",
            "Epoch [95/100], Batch [12/32], Loss: 2.4152\n",
            "Epoch [95/100], Batch [13/32], Loss: 2.6452\n",
            "Epoch [95/100], Batch [14/32], Loss: 2.5877\n",
            "Epoch [95/100], Batch [15/32], Loss: 2.4826\n",
            "Epoch [95/100], Batch [16/32], Loss: 2.5757\n",
            "Epoch [95/100], Batch [17/32], Loss: 2.5734\n",
            "Epoch [95/100], Batch [18/32], Loss: 2.6402\n",
            "Epoch [95/100], Batch [19/32], Loss: 2.5541\n",
            "Epoch [95/100], Batch [20/32], Loss: 2.7156\n",
            "Epoch [95/100], Batch [21/32], Loss: 2.6146\n",
            "Epoch [95/100], Batch [22/32], Loss: 2.4354\n",
            "Epoch [95/100], Batch [23/32], Loss: 2.4738\n",
            "Epoch [95/100], Batch [24/32], Loss: 2.4697\n",
            "Epoch [95/100], Batch [25/32], Loss: 2.6893\n",
            "Epoch [95/100], Batch [26/32], Loss: 2.4923\n",
            "Epoch [95/100], Batch [27/32], Loss: 2.5283\n",
            "Epoch [95/100], Batch [28/32], Loss: 2.6346\n",
            "Epoch [95/100], Batch [29/32], Loss: 2.6787\n",
            "Epoch [95/100], Batch [30/32], Loss: 2.4216\n",
            "Epoch [95/100], Batch [31/32], Loss: 2.3324\n",
            "Epoch [95/100], Batch [32/32], Loss: 2.5280\n",
            "Epoch [96/100], Batch [1/32], Loss: 2.6680\n",
            "Epoch [96/100], Batch [2/32], Loss: 2.4475\n",
            "Epoch [96/100], Batch [3/32], Loss: 2.5428\n",
            "Epoch [96/100], Batch [4/32], Loss: 2.5322\n",
            "Epoch [96/100], Batch [5/32], Loss: 2.5959\n",
            "Epoch [96/100], Batch [6/32], Loss: 2.4317\n",
            "Epoch [96/100], Batch [7/32], Loss: 2.5333\n",
            "Epoch [96/100], Batch [8/32], Loss: 2.5813\n",
            "Epoch [96/100], Batch [9/32], Loss: 2.5928\n",
            "Epoch [96/100], Batch [10/32], Loss: 2.6552\n",
            "Epoch [96/100], Batch [11/32], Loss: 2.4956\n",
            "Epoch [96/100], Batch [12/32], Loss: 2.5752\n",
            "Epoch [96/100], Batch [13/32], Loss: 2.5320\n",
            "Epoch [96/100], Batch [14/32], Loss: 2.4442\n",
            "Epoch [96/100], Batch [15/32], Loss: 2.3546\n",
            "Epoch [96/100], Batch [16/32], Loss: 2.5375\n",
            "Epoch [96/100], Batch [17/32], Loss: 2.5846\n",
            "Epoch [96/100], Batch [18/32], Loss: 2.6365\n",
            "Epoch [96/100], Batch [19/32], Loss: 2.6075\n",
            "Epoch [96/100], Batch [20/32], Loss: 2.6984\n",
            "Epoch [96/100], Batch [21/32], Loss: 2.7018\n",
            "Epoch [96/100], Batch [22/32], Loss: 2.6833\n",
            "Epoch [96/100], Batch [23/32], Loss: 2.5942\n",
            "Epoch [96/100], Batch [24/32], Loss: 2.4830\n",
            "Epoch [96/100], Batch [25/32], Loss: 2.5168\n",
            "Epoch [96/100], Batch [26/32], Loss: 2.8007\n",
            "Epoch [96/100], Batch [27/32], Loss: 2.5060\n",
            "Epoch [96/100], Batch [28/32], Loss: 2.6018\n",
            "Epoch [96/100], Batch [29/32], Loss: 2.5221\n",
            "Epoch [96/100], Batch [30/32], Loss: 2.6755\n",
            "Epoch [96/100], Batch [31/32], Loss: 2.6714\n",
            "Epoch [96/100], Batch [32/32], Loss: 2.6177\n",
            "Epoch [97/100], Batch [1/32], Loss: 2.5688\n",
            "Epoch [97/100], Batch [2/32], Loss: 2.5096\n",
            "Epoch [97/100], Batch [3/32], Loss: 2.3324\n",
            "Epoch [97/100], Batch [4/32], Loss: 2.5676\n",
            "Epoch [97/100], Batch [5/32], Loss: 2.5868\n",
            "Epoch [97/100], Batch [6/32], Loss: 2.4853\n",
            "Epoch [97/100], Batch [7/32], Loss: 2.6083\n",
            "Epoch [97/100], Batch [8/32], Loss: 2.4624\n",
            "Epoch [97/100], Batch [9/32], Loss: 2.4332\n",
            "Epoch [97/100], Batch [10/32], Loss: 2.6447\n",
            "Epoch [97/100], Batch [11/32], Loss: 2.6320\n",
            "Epoch [97/100], Batch [12/32], Loss: 2.6709\n",
            "Epoch [97/100], Batch [13/32], Loss: 2.6211\n",
            "Epoch [97/100], Batch [14/32], Loss: 2.7683\n",
            "Epoch [97/100], Batch [15/32], Loss: 2.6021\n",
            "Epoch [97/100], Batch [16/32], Loss: 2.8841\n",
            "Epoch [97/100], Batch [17/32], Loss: 2.5232\n",
            "Epoch [97/100], Batch [18/32], Loss: 2.5539\n",
            "Epoch [97/100], Batch [19/32], Loss: 2.4768\n",
            "Epoch [97/100], Batch [20/32], Loss: 2.6146\n",
            "Epoch [97/100], Batch [21/32], Loss: 2.5781\n",
            "Epoch [97/100], Batch [22/32], Loss: 2.5455\n",
            "Epoch [97/100], Batch [23/32], Loss: 2.4471\n",
            "Epoch [97/100], Batch [24/32], Loss: 2.5021\n",
            "Epoch [97/100], Batch [25/32], Loss: 2.5941\n",
            "Epoch [97/100], Batch [26/32], Loss: 2.6249\n",
            "Epoch [97/100], Batch [27/32], Loss: 2.5048\n",
            "Epoch [97/100], Batch [28/32], Loss: 2.6742\n",
            "Epoch [97/100], Batch [29/32], Loss: 2.5602\n",
            "Epoch [97/100], Batch [30/32], Loss: 2.6036\n",
            "Epoch [97/100], Batch [31/32], Loss: 2.4136\n",
            "Epoch [97/100], Batch [32/32], Loss: 2.5434\n",
            "Epoch [98/100], Batch [1/32], Loss: 2.5423\n",
            "Epoch [98/100], Batch [2/32], Loss: 2.5124\n",
            "Epoch [98/100], Batch [3/32], Loss: 2.5708\n",
            "Epoch [98/100], Batch [4/32], Loss: 2.5323\n",
            "Epoch [98/100], Batch [5/32], Loss: 2.3184\n",
            "Epoch [98/100], Batch [6/32], Loss: 2.5272\n",
            "Epoch [98/100], Batch [7/32], Loss: 2.5993\n",
            "Epoch [98/100], Batch [8/32], Loss: 2.4505\n",
            "Epoch [98/100], Batch [9/32], Loss: 2.7485\n",
            "Epoch [98/100], Batch [10/32], Loss: 2.7089\n",
            "Epoch [98/100], Batch [11/32], Loss: 2.6587\n",
            "Epoch [98/100], Batch [12/32], Loss: 2.5424\n",
            "Epoch [98/100], Batch [13/32], Loss: 2.8575\n",
            "Epoch [98/100], Batch [14/32], Loss: 2.6214\n",
            "Epoch [98/100], Batch [15/32], Loss: 2.3893\n",
            "Epoch [98/100], Batch [16/32], Loss: 2.4869\n",
            "Epoch [98/100], Batch [17/32], Loss: 2.4521\n",
            "Epoch [98/100], Batch [18/32], Loss: 2.5085\n",
            "Epoch [98/100], Batch [19/32], Loss: 2.7089\n",
            "Epoch [98/100], Batch [20/32], Loss: 2.6045\n",
            "Epoch [98/100], Batch [21/32], Loss: 2.4988\n",
            "Epoch [98/100], Batch [22/32], Loss: 2.5414\n",
            "Epoch [98/100], Batch [23/32], Loss: 2.7052\n",
            "Epoch [98/100], Batch [24/32], Loss: 2.4308\n",
            "Epoch [98/100], Batch [25/32], Loss: 2.6373\n",
            "Epoch [98/100], Batch [26/32], Loss: 2.5507\n",
            "Epoch [98/100], Batch [27/32], Loss: 2.5874\n",
            "Epoch [98/100], Batch [28/32], Loss: 2.5148\n",
            "Epoch [98/100], Batch [29/32], Loss: 2.7951\n",
            "Epoch [98/100], Batch [30/32], Loss: 2.5928\n",
            "Epoch [98/100], Batch [31/32], Loss: 2.5005\n",
            "Epoch [98/100], Batch [32/32], Loss: 2.4074\n",
            "Epoch [99/100], Batch [1/32], Loss: 2.4624\n",
            "Epoch [99/100], Batch [2/32], Loss: 2.4946\n",
            "Epoch [99/100], Batch [3/32], Loss: 2.5466\n",
            "Epoch [99/100], Batch [4/32], Loss: 2.6447\n",
            "Epoch [99/100], Batch [5/32], Loss: 2.6136\n",
            "Epoch [99/100], Batch [6/32], Loss: 2.5634\n",
            "Epoch [99/100], Batch [7/32], Loss: 2.5535\n",
            "Epoch [99/100], Batch [8/32], Loss: 2.5803\n",
            "Epoch [99/100], Batch [9/32], Loss: 2.4307\n",
            "Epoch [99/100], Batch [10/32], Loss: 2.5384\n",
            "Epoch [99/100], Batch [11/32], Loss: 2.4566\n",
            "Epoch [99/100], Batch [12/32], Loss: 2.6027\n",
            "Epoch [99/100], Batch [13/32], Loss: 2.7025\n",
            "Epoch [99/100], Batch [14/32], Loss: 2.5530\n",
            "Epoch [99/100], Batch [15/32], Loss: 2.5575\n",
            "Epoch [99/100], Batch [16/32], Loss: 2.6590\n",
            "Epoch [99/100], Batch [17/32], Loss: 2.6288\n",
            "Epoch [99/100], Batch [18/32], Loss: 2.6361\n",
            "Epoch [99/100], Batch [19/32], Loss: 2.6780\n",
            "Epoch [99/100], Batch [20/32], Loss: 2.5227\n",
            "Epoch [99/100], Batch [21/32], Loss: 2.6048\n",
            "Epoch [99/100], Batch [22/32], Loss: 2.4099\n",
            "Epoch [99/100], Batch [23/32], Loss: 2.5891\n",
            "Epoch [99/100], Batch [24/32], Loss: 2.5905\n",
            "Epoch [99/100], Batch [25/32], Loss: 2.2687\n",
            "Epoch [99/100], Batch [26/32], Loss: 2.5581\n",
            "Epoch [99/100], Batch [27/32], Loss: 2.4174\n",
            "Epoch [99/100], Batch [28/32], Loss: 2.6013\n",
            "Epoch [99/100], Batch [29/32], Loss: 2.5321\n",
            "Epoch [99/100], Batch [30/32], Loss: 2.7140\n",
            "Epoch [99/100], Batch [31/32], Loss: 2.5877\n",
            "Epoch [99/100], Batch [32/32], Loss: 2.7100\n",
            "Epoch [100/100], Batch [1/32], Loss: 2.5614\n",
            "Epoch [100/100], Batch [2/32], Loss: 2.6331\n",
            "Epoch [100/100], Batch [3/32], Loss: 2.6251\n",
            "Epoch [100/100], Batch [4/32], Loss: 2.5005\n",
            "Epoch [100/100], Batch [5/32], Loss: 2.3922\n",
            "Epoch [100/100], Batch [6/32], Loss: 2.5770\n",
            "Epoch [100/100], Batch [7/32], Loss: 2.5560\n",
            "Epoch [100/100], Batch [8/32], Loss: 2.5538\n",
            "Epoch [100/100], Batch [9/32], Loss: 2.5098\n",
            "Epoch [100/100], Batch [10/32], Loss: 2.4874\n",
            "Epoch [100/100], Batch [11/32], Loss: 2.5214\n",
            "Epoch [100/100], Batch [12/32], Loss: 2.6232\n",
            "Epoch [100/100], Batch [13/32], Loss: 2.4947\n",
            "Epoch [100/100], Batch [14/32], Loss: 2.4093\n",
            "Epoch [100/100], Batch [15/32], Loss: 2.7415\n",
            "Epoch [100/100], Batch [16/32], Loss: 2.4038\n",
            "Epoch [100/100], Batch [17/32], Loss: 2.5226\n",
            "Epoch [100/100], Batch [18/32], Loss: 2.5594\n",
            "Epoch [100/100], Batch [19/32], Loss: 2.6628\n",
            "Epoch [100/100], Batch [20/32], Loss: 2.6265\n",
            "Epoch [100/100], Batch [21/32], Loss: 2.5885\n",
            "Epoch [100/100], Batch [22/32], Loss: 2.4394\n",
            "Epoch [100/100], Batch [23/32], Loss: 2.5557\n",
            "Epoch [100/100], Batch [24/32], Loss: 2.6879\n",
            "Epoch [100/100], Batch [25/32], Loss: 2.6185\n",
            "Epoch [100/100], Batch [26/32], Loss: 2.4630\n",
            "Epoch [100/100], Batch [27/32], Loss: 2.5447\n",
            "Epoch [100/100], Batch [28/32], Loss: 2.6141\n",
            "Epoch [100/100], Batch [29/32], Loss: 2.6547\n",
            "Epoch [100/100], Batch [30/32], Loss: 2.6111\n",
            "Epoch [100/100], Batch [31/32], Loss: 2.6113\n",
            "Epoch [100/100], Batch [32/32], Loss: 2.5987\n",
            "Finishing training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "\n",
        "# Instantiate the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1)  # loss 2.6101\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # 2.5928\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0) # loss 2.6425\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=1) # loss 2.5945\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1) # loss 2..5951\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)#2.6039\n",
        "\n",
        "\n",
        "# Initialize the best validation loss and best_model_wts before the training loop\n",
        "best_val_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "num_epochs = 200\n",
        "patience = 50  # Number of epochs with no improvement after which training will be stopped\n",
        "no_improve_epochs = 0  # Number of epochs with no improvement\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for batch_idx, (hsi_batch, lidar_batch, label_batch) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        hsi_batch = hsi_batch.to(device)\n",
        "        lidar_batch = lidar_batch.to(device)\n",
        "        label_batch = label_batch.to(device)  # Reshape labels\n",
        "\n",
        "        # Forward pass\n",
        "        output, attn_scores  = model(lidar_batch, hsi_batch)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.transpose(1, 2), label_batch.squeeze(2))\n",
        "        running_train_loss += loss.item() * hsi_batch.size(0)  # Multiply by batch size\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate epoch training loss\n",
        "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for hsi_batch, lidar_batch, label_batch in val_loader:\n",
        "            # Move tensors to the configured device\n",
        "            hsi_batch = hsi_batch.to(device)\n",
        "            lidar_batch = lidar_batch.to(device)\n",
        "            label_batch = label_batch.to(device)  # Reshape labels\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(lidar_batch, hsi_batch)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output[0].transpose(1, 2), label_batch.squeeze(2))\n",
        "            running_val_loss += loss.item() * hsi_batch.size(0)  # Multiply by batch size\n",
        "\n",
        "    # Calculate epoch validation loss\n",
        "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "    # If the validation loss is lower than the current best, save the model's state\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        print(f'Validation Loss Decreased({best_val_loss:.6f}--->{epoch_val_loss:.6f}) \\t Saving The Model')\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        no_improve_epochs = 0  # reset count\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "\n",
        "    # Check early stopping condition\n",
        "    if no_improve_epochs > patience:\n",
        "        print('Early stopping!')\n",
        "        model.load_state_dict(best_model_wts)  # load best model\n",
        "        break\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    #scheduler.step(epoch_val_loss)\n",
        "\n",
        "print('Finished training')\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_wts, path+'best_model_weights.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP23mD1wx63c",
        "outputId": "3a132386-8fc3-44ce-ed46-7d90dd99c9ed"
      },
      "id": "DP23mD1wx63c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Train Loss: 8.6001, Val Loss: 3.7772\n",
            "Validation Loss Decreased(inf--->3.777221) \t Saving The Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/200], Train Loss: 2.9858, Val Loss: 2.6757\n",
            "Validation Loss Decreased(3.777221--->2.675684) \t Saving The Model\n",
            "Epoch [3/200], Train Loss: 2.6624, Val Loss: 2.6493\n",
            "Validation Loss Decreased(2.675684--->2.649280) \t Saving The Model\n",
            "Epoch [4/200], Train Loss: 2.6368, Val Loss: 2.6352\n",
            "Validation Loss Decreased(2.649280--->2.635213) \t Saving The Model\n",
            "Epoch [5/200], Train Loss: 2.6296, Val Loss: 2.6467\n",
            "Epoch [6/200], Train Loss: 2.6185, Val Loss: 2.6095\n",
            "Validation Loss Decreased(2.635213--->2.609454) \t Saving The Model\n",
            "Epoch [7/200], Train Loss: 2.6028, Val Loss: 2.6024\n",
            "Validation Loss Decreased(2.609454--->2.602388) \t Saving The Model\n",
            "Epoch [8/200], Train Loss: 2.5959, Val Loss: 2.5985\n",
            "Validation Loss Decreased(2.602388--->2.598497) \t Saving The Model\n",
            "Epoch [9/200], Train Loss: 2.5901, Val Loss: 2.5860\n",
            "Validation Loss Decreased(2.598497--->2.586010) \t Saving The Model\n",
            "Epoch [10/200], Train Loss: 2.5848, Val Loss: 2.5921\n",
            "Epoch [11/200], Train Loss: 2.5802, Val Loss: 2.5911\n",
            "Epoch [12/200], Train Loss: 2.5712, Val Loss: 2.5733\n",
            "Validation Loss Decreased(2.586010--->2.573346) \t Saving The Model\n",
            "Epoch [13/200], Train Loss: 2.5611, Val Loss: 2.5682\n",
            "Validation Loss Decreased(2.573346--->2.568225) \t Saving The Model\n",
            "Epoch [14/200], Train Loss: 2.5490, Val Loss: 2.5608\n",
            "Validation Loss Decreased(2.568225--->2.560823) \t Saving The Model\n",
            "Epoch [15/200], Train Loss: 2.5497, Val Loss: 2.5552\n",
            "Validation Loss Decreased(2.560823--->2.555202) \t Saving The Model\n",
            "Epoch [16/200], Train Loss: 2.5417, Val Loss: 2.5581\n",
            "Epoch [17/200], Train Loss: 2.5422, Val Loss: 2.5587\n",
            "Epoch [18/200], Train Loss: 2.5362, Val Loss: 2.5471\n",
            "Validation Loss Decreased(2.555202--->2.547096) \t Saving The Model\n",
            "Epoch [19/200], Train Loss: 2.5272, Val Loss: 2.5469\n",
            "Validation Loss Decreased(2.547096--->2.546885) \t Saving The Model\n",
            "Epoch [20/200], Train Loss: 2.5258, Val Loss: 2.5441\n",
            "Validation Loss Decreased(2.546885--->2.544087) \t Saving The Model\n",
            "Epoch [21/200], Train Loss: 2.5146, Val Loss: 2.5286\n",
            "Validation Loss Decreased(2.544087--->2.528597) \t Saving The Model\n",
            "Epoch [22/200], Train Loss: 2.5166, Val Loss: 2.5420\n",
            "Epoch [23/200], Train Loss: 2.5030, Val Loss: 2.5211\n",
            "Validation Loss Decreased(2.528597--->2.521103) \t Saving The Model\n",
            "Epoch [24/200], Train Loss: 2.5092, Val Loss: 2.5290\n",
            "Epoch [25/200], Train Loss: 2.5039, Val Loss: 2.5228\n",
            "Epoch [26/200], Train Loss: 2.4927, Val Loss: 2.5264\n",
            "Epoch [27/200], Train Loss: 2.4970, Val Loss: 2.5095\n",
            "Validation Loss Decreased(2.521103--->2.509523) \t Saving The Model\n",
            "Epoch [28/200], Train Loss: 2.4883, Val Loss: 2.5137\n",
            "Epoch [29/200], Train Loss: 2.4803, Val Loss: 2.5091\n",
            "Validation Loss Decreased(2.509523--->2.509113) \t Saving The Model\n",
            "Epoch [30/200], Train Loss: 2.4814, Val Loss: 2.5192\n",
            "Epoch [31/200], Train Loss: 2.4800, Val Loss: 2.5033\n",
            "Validation Loss Decreased(2.509113--->2.503306) \t Saving The Model\n",
            "Epoch [32/200], Train Loss: 2.4748, Val Loss: 2.5115\n",
            "Epoch [33/200], Train Loss: 2.4728, Val Loss: 2.4928\n",
            "Validation Loss Decreased(2.503306--->2.492756) \t Saving The Model\n",
            "Epoch [34/200], Train Loss: 2.4680, Val Loss: 2.5011\n",
            "Epoch [35/200], Train Loss: 2.4641, Val Loss: 2.5087\n",
            "Epoch [36/200], Train Loss: 2.4603, Val Loss: 2.5151\n",
            "Epoch [37/200], Train Loss: 2.4640, Val Loss: 2.4984\n",
            "Epoch [38/200], Train Loss: 2.4554, Val Loss: 2.5124\n",
            "Epoch [39/200], Train Loss: 2.4623, Val Loss: 2.4824\n",
            "Validation Loss Decreased(2.492756--->2.482426) \t Saving The Model\n",
            "Epoch [40/200], Train Loss: 2.4422, Val Loss: 2.4905\n",
            "Epoch [41/200], Train Loss: 2.4550, Val Loss: 2.4635\n",
            "Validation Loss Decreased(2.482426--->2.463476) \t Saving The Model\n",
            "Epoch [42/200], Train Loss: 2.4455, Val Loss: 2.4846\n",
            "Epoch [43/200], Train Loss: 2.4415, Val Loss: 2.4623\n",
            "Validation Loss Decreased(2.463476--->2.462299) \t Saving The Model\n",
            "Epoch [44/200], Train Loss: 2.4365, Val Loss: 2.4840\n",
            "Epoch [45/200], Train Loss: 2.4296, Val Loss: 2.4753\n",
            "Epoch [46/200], Train Loss: 2.4328, Val Loss: 2.4651\n",
            "Epoch [47/200], Train Loss: 2.4302, Val Loss: 2.4590\n",
            "Validation Loss Decreased(2.462299--->2.459016) \t Saving The Model\n",
            "Epoch [48/200], Train Loss: 2.4195, Val Loss: 2.4584\n",
            "Validation Loss Decreased(2.459016--->2.458382) \t Saving The Model\n",
            "Epoch [49/200], Train Loss: 2.4229, Val Loss: 2.4536\n",
            "Validation Loss Decreased(2.458382--->2.453576) \t Saving The Model\n",
            "Epoch [50/200], Train Loss: 2.4215, Val Loss: 2.4587\n",
            "Epoch [51/200], Train Loss: 2.4299, Val Loss: 2.4529\n",
            "Validation Loss Decreased(2.453576--->2.452889) \t Saving The Model\n",
            "Epoch [52/200], Train Loss: 2.4304, Val Loss: 2.4568\n",
            "Epoch [53/200], Train Loss: 2.4129, Val Loss: 2.4590\n",
            "Epoch [54/200], Train Loss: 2.4070, Val Loss: 2.4414\n",
            "Validation Loss Decreased(2.452889--->2.441383) \t Saving The Model\n",
            "Epoch [55/200], Train Loss: 2.4046, Val Loss: 2.4398\n",
            "Validation Loss Decreased(2.441383--->2.439823) \t Saving The Model\n",
            "Epoch [56/200], Train Loss: 2.4053, Val Loss: 2.4510\n",
            "Epoch [57/200], Train Loss: 2.4076, Val Loss: 2.4579\n",
            "Epoch [58/200], Train Loss: 2.3925, Val Loss: 2.4295\n",
            "Validation Loss Decreased(2.439823--->2.429534) \t Saving The Model\n",
            "Epoch [59/200], Train Loss: 2.3911, Val Loss: 2.4378\n",
            "Epoch [60/200], Train Loss: 2.3931, Val Loss: 2.4276\n",
            "Validation Loss Decreased(2.429534--->2.427645) \t Saving The Model\n",
            "Epoch [61/200], Train Loss: 2.3938, Val Loss: 2.4291\n",
            "Epoch [62/200], Train Loss: 2.3840, Val Loss: 2.4361\n",
            "Epoch [63/200], Train Loss: 2.3827, Val Loss: 2.4242\n",
            "Validation Loss Decreased(2.427645--->2.424177) \t Saving The Model\n",
            "Epoch [64/200], Train Loss: 2.3896, Val Loss: 2.4243\n",
            "Epoch [65/200], Train Loss: 2.3765, Val Loss: 2.4165\n",
            "Validation Loss Decreased(2.424177--->2.416529) \t Saving The Model\n",
            "Epoch [66/200], Train Loss: 2.3819, Val Loss: 2.4224\n",
            "Epoch [67/200], Train Loss: 2.3738, Val Loss: 2.4241\n",
            "Epoch [68/200], Train Loss: 2.3718, Val Loss: 2.4066\n",
            "Validation Loss Decreased(2.416529--->2.406583) \t Saving The Model\n",
            "Epoch [69/200], Train Loss: 2.3694, Val Loss: 2.4300\n",
            "Epoch [70/200], Train Loss: 2.3685, Val Loss: 2.4193\n",
            "Epoch [71/200], Train Loss: 2.3705, Val Loss: 2.4061\n",
            "Validation Loss Decreased(2.406583--->2.406080) \t Saving The Model\n",
            "Epoch [72/200], Train Loss: 2.3626, Val Loss: 2.4274\n",
            "Epoch [73/200], Train Loss: 2.3572, Val Loss: 2.4095\n",
            "Epoch [74/200], Train Loss: 2.3587, Val Loss: 2.4071\n",
            "Epoch [75/200], Train Loss: 2.3571, Val Loss: 2.4061\n",
            "Epoch [76/200], Train Loss: 2.3577, Val Loss: 2.4210\n",
            "Epoch [77/200], Train Loss: 2.3530, Val Loss: 2.4005\n",
            "Validation Loss Decreased(2.406080--->2.400528) \t Saving The Model\n",
            "Epoch [78/200], Train Loss: 2.3506, Val Loss: 2.4006\n",
            "Epoch [79/200], Train Loss: 2.3489, Val Loss: 2.3910\n",
            "Validation Loss Decreased(2.400528--->2.391028) \t Saving The Model\n",
            "Epoch [80/200], Train Loss: 2.3390, Val Loss: 2.3929\n",
            "Epoch [81/200], Train Loss: 2.3415, Val Loss: 2.3973\n",
            "Epoch [82/200], Train Loss: 2.3368, Val Loss: 2.3868\n",
            "Validation Loss Decreased(2.391028--->2.386822) \t Saving The Model\n",
            "Epoch [83/200], Train Loss: 2.3435, Val Loss: 2.3903\n",
            "Epoch [84/200], Train Loss: 2.3361, Val Loss: 2.3826\n",
            "Validation Loss Decreased(2.386822--->2.382633) \t Saving The Model\n",
            "Epoch [85/200], Train Loss: 2.3368, Val Loss: 2.3784\n",
            "Validation Loss Decreased(2.382633--->2.378387) \t Saving The Model\n",
            "Epoch [86/200], Train Loss: 2.3281, Val Loss: 2.3798\n",
            "Epoch [87/200], Train Loss: 2.3403, Val Loss: 2.3908\n",
            "Epoch [88/200], Train Loss: 2.3344, Val Loss: 2.3844\n",
            "Epoch [89/200], Train Loss: 2.3319, Val Loss: 2.3784\n",
            "Epoch [90/200], Train Loss: 2.3263, Val Loss: 2.3901\n",
            "Epoch [91/200], Train Loss: 2.3223, Val Loss: 2.3670\n",
            "Validation Loss Decreased(2.378387--->2.367042) \t Saving The Model\n",
            "Epoch [92/200], Train Loss: 2.3271, Val Loss: 2.3769\n",
            "Epoch [93/200], Train Loss: 2.3155, Val Loss: 2.3984\n",
            "Epoch [94/200], Train Loss: 2.3256, Val Loss: 2.3684\n",
            "Epoch [95/200], Train Loss: 2.3191, Val Loss: 2.3674\n",
            "Epoch [96/200], Train Loss: 2.3102, Val Loss: 2.3537\n",
            "Validation Loss Decreased(2.367042--->2.353727) \t Saving The Model\n",
            "Epoch [97/200], Train Loss: 2.3136, Val Loss: 2.3544\n",
            "Epoch [98/200], Train Loss: 2.3055, Val Loss: 2.3524\n",
            "Validation Loss Decreased(2.353727--->2.352362) \t Saving The Model\n",
            "Epoch [99/200], Train Loss: 2.3104, Val Loss: 2.3560\n",
            "Epoch [100/200], Train Loss: 2.3080, Val Loss: 2.3499\n",
            "Validation Loss Decreased(2.352362--->2.349942) \t Saving The Model\n",
            "Epoch [101/200], Train Loss: 2.3013, Val Loss: 2.3633\n",
            "Epoch [102/200], Train Loss: 2.2986, Val Loss: 2.3519\n",
            "Epoch [103/200], Train Loss: 2.2972, Val Loss: 2.3464\n",
            "Validation Loss Decreased(2.349942--->2.346432) \t Saving The Model\n",
            "Epoch [104/200], Train Loss: 2.2929, Val Loss: 2.3640\n",
            "Epoch [105/200], Train Loss: 2.2996, Val Loss: 2.3441\n",
            "Validation Loss Decreased(2.346432--->2.344062) \t Saving The Model\n",
            "Epoch [106/200], Train Loss: 2.2904, Val Loss: 2.3382\n",
            "Validation Loss Decreased(2.344062--->2.338177) \t Saving The Model\n",
            "Epoch [107/200], Train Loss: 2.2906, Val Loss: 2.3629\n",
            "Epoch [108/200], Train Loss: 2.3060, Val Loss: 2.3385\n",
            "Epoch [109/200], Train Loss: 2.2895, Val Loss: 2.3371\n",
            "Validation Loss Decreased(2.338177--->2.337081) \t Saving The Model\n",
            "Epoch [110/200], Train Loss: 2.2867, Val Loss: 2.3401\n",
            "Epoch [111/200], Train Loss: 2.2826, Val Loss: 2.3459\n",
            "Epoch [112/200], Train Loss: 2.2832, Val Loss: 2.3410\n",
            "Epoch [113/200], Train Loss: 2.2807, Val Loss: 2.3399\n",
            "Epoch [114/200], Train Loss: 2.2787, Val Loss: 2.3563\n",
            "Epoch [115/200], Train Loss: 2.2736, Val Loss: 2.3338\n",
            "Validation Loss Decreased(2.337081--->2.333833) \t Saving The Model\n",
            "Epoch [116/200], Train Loss: 2.2740, Val Loss: 2.3382\n",
            "Epoch [117/200], Train Loss: 2.2767, Val Loss: 2.3330\n",
            "Validation Loss Decreased(2.333833--->2.332982) \t Saving The Model\n",
            "Epoch [118/200], Train Loss: 2.2777, Val Loss: 2.3263\n",
            "Validation Loss Decreased(2.332982--->2.326344) \t Saving The Model\n",
            "Epoch [119/200], Train Loss: 2.2788, Val Loss: 2.3318\n",
            "Epoch [120/200], Train Loss: 2.2730, Val Loss: 2.3298\n",
            "Epoch [121/200], Train Loss: 2.2676, Val Loss: 2.3323\n",
            "Epoch [122/200], Train Loss: 2.2643, Val Loss: 2.3249\n",
            "Validation Loss Decreased(2.326344--->2.324934) \t Saving The Model\n",
            "Epoch [123/200], Train Loss: 2.2666, Val Loss: 2.3282\n",
            "Epoch [124/200], Train Loss: 2.2615, Val Loss: 2.3290\n",
            "Epoch [125/200], Train Loss: 2.2632, Val Loss: 2.3205\n",
            "Validation Loss Decreased(2.324934--->2.320536) \t Saving The Model\n",
            "Epoch [126/200], Train Loss: 2.2619, Val Loss: 2.3219\n",
            "Epoch [127/200], Train Loss: 2.2593, Val Loss: 2.3096\n",
            "Validation Loss Decreased(2.320536--->2.309649) \t Saving The Model\n",
            "Epoch [128/200], Train Loss: 2.2527, Val Loss: 2.3190\n",
            "Epoch [129/200], Train Loss: 2.2518, Val Loss: 2.3111\n",
            "Epoch [130/200], Train Loss: 2.2535, Val Loss: 2.3152\n",
            "Epoch [131/200], Train Loss: 2.2435, Val Loss: 2.3132\n",
            "Epoch [132/200], Train Loss: 2.2517, Val Loss: 2.3182\n",
            "Epoch [133/200], Train Loss: 2.2572, Val Loss: 2.3361\n",
            "Epoch [134/200], Train Loss: 2.2520, Val Loss: 2.3044\n",
            "Validation Loss Decreased(2.309649--->2.304434) \t Saving The Model\n",
            "Epoch [135/200], Train Loss: 2.2477, Val Loss: 2.3201\n",
            "Epoch [136/200], Train Loss: 2.2424, Val Loss: 2.3025\n",
            "Validation Loss Decreased(2.304434--->2.302488) \t Saving The Model\n",
            "Epoch [137/200], Train Loss: 2.2426, Val Loss: 2.3040\n",
            "Epoch [138/200], Train Loss: 2.2380, Val Loss: 2.3000\n",
            "Validation Loss Decreased(2.302488--->2.299965) \t Saving The Model\n",
            "Epoch [139/200], Train Loss: 2.2379, Val Loss: 2.3195\n",
            "Epoch [140/200], Train Loss: 2.2503, Val Loss: 2.3079\n",
            "Epoch [141/200], Train Loss: 2.2366, Val Loss: 2.2962\n",
            "Validation Loss Decreased(2.299965--->2.296173) \t Saving The Model\n",
            "Epoch [142/200], Train Loss: 2.2347, Val Loss: 2.2984\n",
            "Epoch [143/200], Train Loss: 2.2351, Val Loss: 2.3191\n",
            "Epoch [144/200], Train Loss: 2.2413, Val Loss: 2.2981\n",
            "Epoch [145/200], Train Loss: 2.2277, Val Loss: 2.3081\n",
            "Epoch [146/200], Train Loss: 2.2360, Val Loss: 2.3212\n",
            "Epoch [147/200], Train Loss: 2.2269, Val Loss: 2.3260\n",
            "Epoch [148/200], Train Loss: 2.2311, Val Loss: 2.3155\n",
            "Epoch [149/200], Train Loss: 2.2236, Val Loss: 2.2982\n",
            "Epoch [150/200], Train Loss: 2.2305, Val Loss: 2.2990\n",
            "Epoch [151/200], Train Loss: 2.2209, Val Loss: 2.3026\n",
            "Epoch [152/200], Train Loss: 2.2298, Val Loss: 2.3051\n",
            "Epoch [153/200], Train Loss: 2.2279, Val Loss: 2.2905\n",
            "Validation Loss Decreased(2.296173--->2.290480) \t Saving The Model\n",
            "Epoch [154/200], Train Loss: 2.2248, Val Loss: 2.2928\n",
            "Epoch [155/200], Train Loss: 2.2244, Val Loss: 2.2860\n",
            "Validation Loss Decreased(2.290480--->2.285967) \t Saving The Model\n",
            "Epoch [156/200], Train Loss: 2.2209, Val Loss: 2.2802\n",
            "Validation Loss Decreased(2.285967--->2.280224) \t Saving The Model\n",
            "Epoch [157/200], Train Loss: 2.2180, Val Loss: 2.2875\n",
            "Epoch [158/200], Train Loss: 2.2184, Val Loss: 2.3133\n",
            "Epoch [159/200], Train Loss: 2.2248, Val Loss: 2.2817\n",
            "Epoch [160/200], Train Loss: 2.2190, Val Loss: 2.2909\n",
            "Epoch [161/200], Train Loss: 2.2096, Val Loss: 2.2946\n",
            "Epoch [162/200], Train Loss: 2.2122, Val Loss: 2.2884\n",
            "Epoch [163/200], Train Loss: 2.2161, Val Loss: 2.2758\n",
            "Validation Loss Decreased(2.280224--->2.275770) \t Saving The Model\n",
            "Epoch [164/200], Train Loss: 2.2080, Val Loss: 2.2855\n",
            "Epoch [165/200], Train Loss: 2.2165, Val Loss: 2.3090\n",
            "Epoch [166/200], Train Loss: 2.2113, Val Loss: 2.2760\n",
            "Epoch [167/200], Train Loss: 2.2035, Val Loss: 2.3011\n",
            "Epoch [168/200], Train Loss: 2.2073, Val Loss: 2.2796\n",
            "Epoch [169/200], Train Loss: 2.2028, Val Loss: 2.2729\n",
            "Validation Loss Decreased(2.275770--->2.272907) \t Saving The Model\n",
            "Epoch [170/200], Train Loss: 2.2004, Val Loss: 2.2831\n",
            "Epoch [171/200], Train Loss: 2.2035, Val Loss: 2.2779\n",
            "Epoch [172/200], Train Loss: 2.2027, Val Loss: 2.2912\n",
            "Epoch [173/200], Train Loss: 2.2015, Val Loss: 2.2759\n",
            "Epoch [174/200], Train Loss: 2.1972, Val Loss: 2.2777\n",
            "Epoch [175/200], Train Loss: 2.2034, Val Loss: 2.2940\n",
            "Epoch [176/200], Train Loss: 2.2083, Val Loss: 2.2905\n",
            "Epoch [177/200], Train Loss: 2.2042, Val Loss: 2.2659\n",
            "Validation Loss Decreased(2.272907--->2.265922) \t Saving The Model\n",
            "Epoch [178/200], Train Loss: 2.1958, Val Loss: 2.2778\n",
            "Epoch [179/200], Train Loss: 2.1996, Val Loss: 2.2791\n",
            "Epoch [180/200], Train Loss: 2.2000, Val Loss: 2.2734\n",
            "Epoch [181/200], Train Loss: 2.1969, Val Loss: 2.2801\n",
            "Epoch [182/200], Train Loss: 2.1991, Val Loss: 2.2837\n",
            "Epoch [183/200], Train Loss: 2.1983, Val Loss: 2.2610\n",
            "Validation Loss Decreased(2.265922--->2.261028) \t Saving The Model\n",
            "Epoch [184/200], Train Loss: 2.1871, Val Loss: 2.2705\n",
            "Epoch [185/200], Train Loss: 2.1881, Val Loss: 2.2727\n",
            "Epoch [186/200], Train Loss: 2.1952, Val Loss: 2.2775\n",
            "Epoch [187/200], Train Loss: 2.2017, Val Loss: 2.2720\n",
            "Epoch [188/200], Train Loss: 2.1888, Val Loss: 2.2765\n",
            "Epoch [189/200], Train Loss: 2.1916, Val Loss: 2.2614\n",
            "Epoch [190/200], Train Loss: 2.1914, Val Loss: 2.2687\n",
            "Epoch [191/200], Train Loss: 2.1887, Val Loss: 2.2998\n",
            "Epoch [192/200], Train Loss: 2.1834, Val Loss: 2.2619\n",
            "Epoch [193/200], Train Loss: 2.1836, Val Loss: 2.2584\n",
            "Validation Loss Decreased(2.261028--->2.258399) \t Saving The Model\n",
            "Epoch [194/200], Train Loss: 2.1889, Val Loss: 2.2564\n",
            "Validation Loss Decreased(2.258399--->2.256354) \t Saving The Model\n",
            "Epoch [195/200], Train Loss: 2.1818, Val Loss: 2.2570\n",
            "Epoch [196/200], Train Loss: 2.1789, Val Loss: 2.2551\n",
            "Validation Loss Decreased(2.256354--->2.255083) \t Saving The Model\n",
            "Epoch [197/200], Train Loss: 2.1774, Val Loss: 2.2712\n",
            "Epoch [198/200], Train Loss: 2.1844, Val Loss: 2.2539\n",
            "Validation Loss Decreased(2.255083--->2.253853) \t Saving The Model\n",
            "Epoch [199/200], Train Loss: 2.1736, Val Loss: 2.2563\n",
            "Epoch [200/200], Train Loss: 2.1781, Val Loss: 2.2780\n",
            "Finished training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model state\n",
        "torch.save(best_model_wts, path+'best_model_weights.pth')\n"
      ],
      "metadata": {
        "id": "8G-EKMlepL2t"
      },
      "id": "8G-EKMlepL2t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "best_model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "\n",
        "# Load the weights\n",
        "best_model.load_state_dict(torch.load(path+'best_model_weights.pth'))\n",
        "\n",
        "# Move the model to the GPU\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "# Now your model is ready for making predictions on GPU\n"
      ],
      "metadata": {
        "id": "jOc5jb92pL6V"
      },
      "id": "jOc5jb92pL6V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.0 Load Trained Model"
      ],
      "metadata": {
        "id": "oSKh2t4gJcGU"
      },
      "id": "oSKh2t4gJcGU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "model.load_state_dict(torch.load(path+'best_model_weights.pth'))\n",
        "model.eval()  # set the model to evaluation mode\n",
        "\n",
        "# Move the model to the GPU\n",
        "#\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrRolVFaKZy_",
        "outputId": "6dc95fa0-a1b7-4c43-b7ec-3719a1f0e270"
      },
      "id": "IrRolVFaKZy_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "hsi_patch_embedding.pos_embedding \t torch.Size([1, 145, 128])\n",
            "hsi_patch_embedding.cls_token \t torch.Size([1, 1, 128])\n",
            "hsi_patch_embedding.proj.weight \t torch.Size([128, 25, 1, 1])\n",
            "hsi_patch_embedding.proj.bias \t torch.Size([128])\n",
            "lidar_patch_embedding.pos_embedding \t torch.Size([1, 2, 128])\n",
            "lidar_patch_embedding.cls_token \t torch.Size([1, 1, 128])\n",
            "lidar_patch_embedding.proj.weight \t torch.Size([128, 25, 1, 1])\n",
            "lidar_patch_embedding.proj.bias \t torch.Size([128])\n",
            "cross_attention.to_q.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_k.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_v.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_out.weight \t torch.Size([144, 512])\n",
            "cross_attention.to_out.bias \t torch.Size([144])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n"
      ],
      "metadata": {
        "id": "2OAS0mPPKi9O"
      },
      "id": "2OAS0mPPKi9O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "best_model.eval()\n",
        "\n",
        "# Initialize a list to hold all the predictions and attention scores\n",
        "all_predictions = []\n",
        "all_attn_scores = []\n",
        "\n",
        "# Loop over the validation set\n",
        "for hsi_batch, lidar_batch, _ in val_loader:  # we don't need the labels for predictions\n",
        "    # Move the batch to the desired device\n",
        "    hsi_batch = hsi_batch.to(device)\n",
        "    lidar_batch = lidar_batch.to(device)\n",
        "\n",
        "    # Pass the batch through the model\n",
        "    with torch.no_grad():\n",
        "        output, attn_scores = best_model(lidar_batch, hsi_batch)\n",
        "\n",
        "        # Add the predictions and attention scores to our lists\n",
        "        all_predictions.append(output.cpu().numpy())\n",
        "        all_attn_scores.append(attn_scores.cpu().numpy())\n",
        "\n",
        "# Concatenate all predictions and attention scores into a single numpy array\n",
        "all_predictions = np.concatenate(all_predictions)\n",
        "all_attn_scores = np.concatenate(all_attn_scores)\n",
        "\n",
        "# Now you can use the predictions and attention scores as needed\n",
        "print('all_predictions shape:', all_predictions.shape )\n",
        "print('all_attn_scores shape:', all_attn_scores.shape )\n"
      ],
      "metadata": {
        "id": "S2xqXWkOndVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f0d128-583e-499a-e81b-ffb4b870522e"
      },
      "id": "S2xqXWkOndVP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_predictions shape: (1529, 144, 512)\n",
            "all_attn_scores shape: (1529, 144, 144)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Band Selection"
      ],
      "metadata": {
        "id": "7o9FUM2ILtVK"
      },
      "id": "7o9FUM2ILtVK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the variance of the feature vectors for each band across all samples and then find the bands with the maximum variance"
      ],
      "metadata": {
        "id": "oqc6nZNfXKEN"
      },
      "id": "oqc6nZNfXKEN"
    },
    {
      "cell_type": "code",
      "source": [
        "# First, take the mean across all samples and heads. This will result in a single 144-dim vector.\n",
        "mean_attention = np.mean(all_attn_scores, axis=(0, 1))\n",
        "print('mean_attention:',mean_attention.shape)\n",
        "\n",
        "# Then, sort the bands by attention. This will give you the band indices in descending order of attention.\n",
        "sorted_band_indices = np.argsort(mean_attention)[::-1]\n",
        "print('sorted_band_indices :',sorted_band_indices.shape)\n",
        "\n",
        "# If you want to see the attention values as well, you can sort the mean_attention array in the same order.\n",
        "sorted_attention_values = mean_attention[sorted_band_indices]\n",
        "print('sorted_attention_values :',sorted_attention_values.shape)\n",
        "\n",
        "N = 70  # Top N bands\n",
        "top_N_bands = sorted_band_indices[:N]\n",
        "top_N_attention_values = sorted_attention_values[:N]\n",
        "\n",
        "# Print the top N band indices with their attention scores\n",
        "print(f\"Top {N} bands based on the mean attention score:\")\n",
        "for band_index, attention_value in zip(top_N_bands, top_N_attention_values):\n",
        "    print(f\"Band index: {band_index}, Mean attention score: {attention_value}\")\n",
        "\n",
        "# Print the list of top N band indices\n",
        "print(\"\\nList of top {} band indices:\".format(N))\n",
        "print(top_N_bands.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EH-ULRdlVVW",
        "outputId": "51b7877f-0448-4f06-ab05-0c2a464e71de"
      },
      "id": "4EH-ULRdlVVW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_attention: (144,)\n",
            "sorted_band_indices : (144,)\n",
            "sorted_attention_values : (144,)\n",
            "Top 70 bands based on the mean attention score:\n",
            "Band index: 115, Mean attention score: 32.019432067871094\n",
            "Band index: 127, Mean attention score: 29.595396041870117\n",
            "Band index: 63, Mean attention score: 21.864004135131836\n",
            "Band index: 113, Mean attention score: 20.806163787841797\n",
            "Band index: 54, Mean attention score: 20.27324867248535\n",
            "Band index: 12, Mean attention score: 19.954082489013672\n",
            "Band index: 60, Mean attention score: 18.420604705810547\n",
            "Band index: 62, Mean attention score: 18.20339584350586\n",
            "Band index: 4, Mean attention score: 17.818204879760742\n",
            "Band index: 139, Mean attention score: 17.692548751831055\n",
            "Band index: 50, Mean attention score: 17.607107162475586\n",
            "Band index: 90, Mean attention score: 16.83207130432129\n",
            "Band index: 51, Mean attention score: 15.624239921569824\n",
            "Band index: 112, Mean attention score: 14.99621868133545\n",
            "Band index: 75, Mean attention score: 14.399415016174316\n",
            "Band index: 42, Mean attention score: 14.15027141571045\n",
            "Band index: 11, Mean attention score: 13.599845886230469\n",
            "Band index: 97, Mean attention score: 12.956725120544434\n",
            "Band index: 106, Mean attention score: 12.676129341125488\n",
            "Band index: 48, Mean attention score: 11.554957389831543\n",
            "Band index: 1, Mean attention score: 10.906537055969238\n",
            "Band index: 35, Mean attention score: 10.6218843460083\n",
            "Band index: 109, Mean attention score: 10.580787658691406\n",
            "Band index: 76, Mean attention score: 10.304936408996582\n",
            "Band index: 81, Mean attention score: 10.249398231506348\n",
            "Band index: 53, Mean attention score: 10.140469551086426\n",
            "Band index: 124, Mean attention score: 10.058510780334473\n",
            "Band index: 80, Mean attention score: 9.551862716674805\n",
            "Band index: 137, Mean attention score: 9.346771240234375\n",
            "Band index: 85, Mean attention score: 9.159069061279297\n",
            "Band index: 125, Mean attention score: 9.027214050292969\n",
            "Band index: 37, Mean attention score: 9.006909370422363\n",
            "Band index: 105, Mean attention score: 8.894911766052246\n",
            "Band index: 135, Mean attention score: 8.852219581604004\n",
            "Band index: 58, Mean attention score: 8.347931861877441\n",
            "Band index: 126, Mean attention score: 7.964796543121338\n",
            "Band index: 7, Mean attention score: 7.712950706481934\n",
            "Band index: 40, Mean attention score: 7.578982830047607\n",
            "Band index: 15, Mean attention score: 7.45992374420166\n",
            "Band index: 56, Mean attention score: 7.005629539489746\n",
            "Band index: 19, Mean attention score: 6.811568260192871\n",
            "Band index: 18, Mean attention score: 6.542520046234131\n",
            "Band index: 3, Mean attention score: 6.23666524887085\n",
            "Band index: 38, Mean attention score: 5.989075183868408\n",
            "Band index: 21, Mean attention score: 5.360602855682373\n",
            "Band index: 61, Mean attention score: 5.092289924621582\n",
            "Band index: 138, Mean attention score: 4.832271099090576\n",
            "Band index: 95, Mean attention score: 4.747211456298828\n",
            "Band index: 118, Mean attention score: 4.507180213928223\n",
            "Band index: 101, Mean attention score: 4.415899753570557\n",
            "Band index: 24, Mean attention score: 4.395419597625732\n",
            "Band index: 73, Mean attention score: 3.952146530151367\n",
            "Band index: 32, Mean attention score: 3.7210707664489746\n",
            "Band index: 141, Mean attention score: 3.5760905742645264\n",
            "Band index: 72, Mean attention score: 3.481074333190918\n",
            "Band index: 131, Mean attention score: 2.4656548500061035\n",
            "Band index: 70, Mean attention score: 2.4392805099487305\n",
            "Band index: 49, Mean attention score: 1.9950242042541504\n",
            "Band index: 71, Mean attention score: 0.8067755103111267\n",
            "Band index: 65, Mean attention score: 0.7489802837371826\n",
            "Band index: 66, Mean attention score: 0.7470124959945679\n",
            "Band index: 45, Mean attention score: 0.6624094247817993\n",
            "Band index: 110, Mean attention score: 0.44364115595817566\n",
            "Band index: 93, Mean attention score: 0.37206730246543884\n",
            "Band index: 22, Mean attention score: 0.21391713619232178\n",
            "Band index: 16, Mean attention score: 0.12642498314380646\n",
            "Band index: 14, Mean attention score: -0.6457018852233887\n",
            "Band index: 123, Mean attention score: -0.9863501787185669\n",
            "Band index: 107, Mean attention score: -1.197571039199829\n",
            "Band index: 100, Mean attention score: -1.2946381568908691\n",
            "\n",
            "List of top 70 band indices:\n",
            "[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72, 131, 70, 49, 71, 65, 66, 45, 110, 93, 22, 16, 14, 123, 107, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "band_50=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101]\n",
        "band_45=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21]\n",
        "band_40=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56]\n",
        "band_35=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58]\n",
        "band_30=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85]\n",
        "band_25=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81]\n",
        "band_20=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48]\n",
        "band_15=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75]\n",
        "band_10=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139]\n",
        "band_5=[115, 127, 63, 113, 54]\n",
        "band_80=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72, 131, 70, 49, 71, 65, 66, 45, 110, 93, 22, 16, 14, 123, 107, 100]\n",
        "band_75=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72, 131, 70, 49, 71, 65, 66, 45, 110, 93, 22]\n",
        "band_70=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72, 131, 70, 49, 71, 65]\n",
        "band_65=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72]\n",
        "band_60=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101]\n",
        "band_55=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21]"
      ],
      "metadata": {
        "id": "1E-TAAxz8X7e"
      },
      "id": "1E-TAAxz8X7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8TdumuTVo6PE"
      },
      "id": "8TdumuTVo6PE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNBFu0zVo6Tg"
      },
      "id": "yNBFu0zVo6Tg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}