{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "14525fd7",
      "metadata": {
        "id": "14525fd7"
      },
      "source": [
        "# 0.0 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vuX4sZpytah9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuX4sZpytah9",
        "outputId": "31bea5dd-4d9b-46e0-b64c-ad12f925d814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mat73\n",
            "  Downloading mat73-0.62-py3-none-any.whl (19 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.23.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mat73) (3.9.0)\n",
            "Installing collected packages: spectral, einops, mat73\n",
            "Successfully installed einops-0.7.0 mat73-0.62 spectral-0.23.1\n"
          ]
        }
      ],
      "source": [
        "pip install spectral mat73  einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12826627",
      "metadata": {
        "id": "12826627"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "\n",
        "from einops import rearrange\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import io\n",
        "import torch.utils.data\n",
        "import scipy.io as sio\n",
        "import mat73\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dRG3WratmCN",
      "metadata": {
        "id": "6dRG3WratmCN"
      },
      "source": [
        "# 1.0 Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vpwq4Yi-tgjs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpwq4Yi-tgjs",
        "outputId": "4fb32a3f-1380-4b6d-eb86-bb5a86f8d1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vMin2GFJtorP",
      "metadata": {
        "id": "vMin2GFJtorP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b68f4b-7f38-4098-ac9a-82fc72e8e70f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2013_DFTC\t\t\t\t\t   best_model_p7_weights.pth\n",
            " 2013_IEEE_GRSS_DF_Contest_CASI_349_1905_144.mat   best_model_p9_weights.pth\n",
            " 2013_IEEE_GRSS_DF_Contest_LiDAR.mat\t\t   best_model.pth\n",
            " Autoencodermodel.pth\t\t\t\t   best_model_weights.pth\n",
            " Autoencodermodel_uh2013_20Ksample.pth\t\t   BS_RLFCC_UH2013.ipynb\n",
            " Autoencodermodel_uh2013_adam_20Ksample.pth\t   cae_model_state_batchscore.pth\n",
            " Autoencodermodel_uh2013_adamp13_20Ksample.pth\t   cae_model_state.pth\n",
            " Autoencodermodel_uh2013_adamp9_20Ksample.pth\t  'Contrastive Best model'\n",
            " Autoencodermodel_uh2013.pth\t\t\t   cross_modal_autoencoder_best.pth\n",
            " Autoencodermodel_uh2013_rms_20Ksample.pth\t   cross_modal_autoencoder.pth\n",
            " Autoencodermodel_uh2013_sgdp13_20Ksample.pth\t   DFTC2013_Fusion_Model.h5\n",
            " Autoencodermodel_uh2013_sgdp3_20Ksample.pth\t   GRSS2013.mat\n",
            " Autoencodermodel_uh2013_sgdp5_20Ksample.pth\t   __MACOSX\n",
            " Autoencodermodel_uh2013_sgdp7_20Ksample.pth\t   model_path.pth\n",
            " Autoencodermodel_uh2013_sgdp9v1_20Ksample.pth\t   model.pth\n",
            " best_contrastive_model.pth\t\t\t   output_mask.tif\n",
            " best_model_p11_weights.pth\t\t\t   saved_model.pt\n",
            " best_model_p13_weights.pth\t\t\t   trained_model.pth\n",
            " best_model_p15_weights.pth\t\t\t   vis_attn_best_model.pt\n",
            " best_model_p3_weights.pth\n"
          ]
        }
      ],
      "source": [
        "! ls '/content/drive/MyDrive/A02_RemoteSensingData/UHS_2013_DFTC/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4NMT_vf3t3Bg",
      "metadata": {
        "id": "4NMT_vf3t3Bg"
      },
      "outputs": [],
      "source": [
        "# Define the path\n",
        "path='/content/drive/MyDrive/A02_RemoteSensingData/UHS_2013_DFTC/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d23b73f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d23b73f",
        "outputId": "9fbeac01-266a-4a7b-a125-abcf7e9c2ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_2013_data shape: (349, 1905, 144)\n",
            "Lidar_2013_data shape: (349, 1905, 1)\n",
            "gt_2013_data.shape: (349, 1905)\n"
          ]
        }
      ],
      "source": [
        "# 2.1 Loads Data\n",
        "# Load hyperpsectral data\n",
        "hsi_2013_data=sio.loadmat(path+'2013_IEEE_GRSS_DF_Contest_CASI_349_1905_144.mat')['ans']\n",
        "print('hsi_2013_data shape:', hsi_2013_data.shape)\n",
        "\n",
        "# Loader Lidar  data\n",
        "import mat73\n",
        "lidar_2013_data = sio.loadmat(path+'2013_IEEE_GRSS_DF_Contest_LiDAR.mat')['LiDAR_data']\n",
        "\n",
        "print('Lidar_2013_data shape:', lidar_2013_data.shape)\n",
        "\n",
        "#Load ground truth labels\n",
        "gt_2013_data=sio.loadmat(path+'GRSS2013.mat')['name']\n",
        "print('gt_2013_data.shape:', gt_2013_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002fcb4c",
      "metadata": {
        "id": "002fcb4c"
      },
      "source": [
        "# 2.0 Data Preprocessing & Dataloader Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Define the class information\n",
        "class_info = [(1, \"Healthy grass\", 'training_sample', 198, 'test_sample', 1053,  'total', 1251),\n",
        "    (2, \"Stressed grass\",'training_sample', 190, 'test_sample', 1064,  'total', 1254),\n",
        "    (3, \"Synthetic grass\", 'training_sample', 192, 'test_sample', 505,  'total', 697),\n",
        "    (4, \"Trees\", 'training_sample', 188, 'test_sample', 1058,  'total', 1244),\n",
        "    (5, \"Soil\",'training_sample', 186, 'test_sample', 1056,  'total', 1242),\n",
        "    (6, \"Water\", 'training_sample', 182, 'test_sample', 141,  'total', 325),\n",
        "    (7, \"Residential\", 'training_sample', 196, 'test_sample', 1072,  'total', 1268),\n",
        "    (8, \"Commercial\", 'training_sample', 191, 'test_sample', 1053,  'total', 1244),\n",
        "    (9, \"Road\", 'training_sample', 193, 'test_sample', 1059,  'total', 1252),\n",
        "    (10, \"Highway\", 'training_sample', 191, 'test_sample', 1036,  'total', 1227),\n",
        "    (11, \"Railway\", 'training_sample', 181, 'test_sample', 1054,  'total', 1235),\n",
        "    (12, \"Parking lot 1\", 'training_sample', 192, 'test_sample', 1041,  'total', 1233),\n",
        "    (13, \"Parking lot 2\", 'training_sample', 184, 'test_sample',285,  'total', 469),\n",
        "    (14, \"Tennis court\",'training_sample', 181, 'test_sample', 247,  'total', 428),\n",
        "    (15, \"Running track\", 'training_sample', 187, 'test_sample', 473,  'total', 660)]\n",
        "\n",
        "# Create a dictionary to store class number, class name, and class samples\n",
        "class_dict = {class_number: {\"class_name\": class_name,\n",
        "                             'training_sample': training_sample,\n",
        "                             'test_sample': test_sample,\n",
        "                             \"total_samples\": total}\n",
        "              for class_number, class_name, _, training_sample, _, test_sample, _, total in class_info}\n",
        "\n",
        "print(class_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPNJK4nmvTm8",
        "outputId": "47af09b2-5a7b-436a-e9ae-36668ffdba3e"
      },
      "id": "YPNJK4nmvTm8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: {'class_name': 'Healthy grass', 'training_sample': 198, 'test_sample': 1053, 'total_samples': 1251}, 2: {'class_name': 'Stressed grass', 'training_sample': 190, 'test_sample': 1064, 'total_samples': 1254}, 3: {'class_name': 'Synthetic grass', 'training_sample': 192, 'test_sample': 505, 'total_samples': 697}, 4: {'class_name': 'Trees', 'training_sample': 188, 'test_sample': 1058, 'total_samples': 1244}, 5: {'class_name': 'Soil', 'training_sample': 186, 'test_sample': 1056, 'total_samples': 1242}, 6: {'class_name': 'Water', 'training_sample': 182, 'test_sample': 141, 'total_samples': 325}, 7: {'class_name': 'Residential', 'training_sample': 196, 'test_sample': 1072, 'total_samples': 1268}, 8: {'class_name': 'Commercial', 'training_sample': 191, 'test_sample': 1053, 'total_samples': 1244}, 9: {'class_name': 'Road', 'training_sample': 193, 'test_sample': 1059, 'total_samples': 1252}, 10: {'class_name': 'Highway', 'training_sample': 191, 'test_sample': 1036, 'total_samples': 1227}, 11: {'class_name': 'Railway', 'training_sample': 181, 'test_sample': 1054, 'total_samples': 1235}, 12: {'class_name': 'Parking lot 1', 'training_sample': 192, 'test_sample': 1041, 'total_samples': 1233}, 13: {'class_name': 'Parking lot 2', 'training_sample': 184, 'test_sample': 285, 'total_samples': 469}, 14: {'class_name': 'Tennis court', 'training_sample': 181, 'test_sample': 247, 'total_samples': 428}, 15: {'class_name': 'Running track', 'training_sample': 187, 'test_sample': 473, 'total_samples': 660}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8D-fNrGQIQFT",
      "metadata": {
        "id": "8D-fNrGQIQFT"
      },
      "source": [
        "### 2.1  Samples Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9615e24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9615e24",
        "outputId": "954152d9-0c7a-4de4-8487-f379e4832f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_samples shape: (15029, 9, 9, 144)\n",
            "lidar_samples shape: (15029, 9, 9, 1)\n",
            "labels shape: (15029,)\n"
          ]
        }
      ],
      "source": [
        "# 2.2 Samples Extraction\n",
        "\n",
        "# # Create a mask with all class labels\n",
        "# mask = np.copy(gt_2013_data)\n",
        "\n",
        "# # Set the background class to 0\n",
        "# mask[mask == 0] = 0\n",
        "\n",
        "# Define patch size and stride\n",
        "patch_size = 9\n",
        "stride = 1\n",
        "\n",
        "# Create an empty list to store patches and labels\n",
        "hsi_samples = []\n",
        "lidar_samples = []\n",
        "labels = []\n",
        "\n",
        "# Initialize a dictionary to store class count\n",
        "class_count = {i: 0 for i in class_dict.keys()}\n",
        "\n",
        "# Function to check if all classes have the required number of samples\n",
        "def all_classes_completed(class_count, class_dict):\n",
        "    return all(class_count[class_num] == class_dict[class_num][\"total_samples\"] for class_num in class_dict.keys())\n",
        "\n",
        "while not all_classes_completed(class_count, class_dict):\n",
        "    # Loop through the ground truth data\n",
        "    for label in class_dict.keys():\n",
        "        # Get the coordinates of the ground truth pixels\n",
        "        #coords = np.argwhere((gt_2013_data == label) & (mask > 0))\n",
        "        coords = np.argwhere(gt_2013_data == label)\n",
        "\n",
        "        # Shuffle the coordinates to randomize the patch extraction\n",
        "        np.random.shuffle(coords)\n",
        "\n",
        "        for coord in coords:\n",
        "            i, j = coord\n",
        "            # Calculate the patch indices\n",
        "            i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
        "            j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
        "\n",
        "            # Check if the indices are within the bounds of the HSI data\n",
        "            if i_start >= 0 and i_end <= hsi_2013_data.shape[0] and j_start >= 0 and j_end <= hsi_2013_data.shape[1]:\n",
        "                # Extract the patch\n",
        "                hsi_patch = hsi_2013_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # Extract the LiDAR patch\n",
        "                lidar_patch = lidar_2013_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # If the class count is less than the required samples\n",
        "                if class_count[label] < class_dict[label][\"total_samples\"]:\n",
        "                    # Append the patch and its label to the list\n",
        "                    hsi_samples.append(hsi_patch)\n",
        "                    lidar_samples.append(lidar_patch)\n",
        "                    labels.append(label)\n",
        "                    class_count[label] += 1\n",
        "\n",
        "                    # If all classes have the required number of samples, exit the loop\n",
        "                    if all_classes_completed(class_count, class_dict):\n",
        "                        break\n",
        "\n",
        "# Convert the list of patches and labels into arrays\n",
        "hsi_samples = np.array(hsi_samples)\n",
        "lidar_samples = np.array(lidar_samples)\n",
        "labels = np.array(labels)\n",
        "print('hsi_samples shape:', hsi_samples.shape)\n",
        "print('lidar_samples shape:', lidar_samples.shape)\n",
        "print('labels shape:', labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ifIwfC-C9ON9",
      "metadata": {
        "id": "ifIwfC-C9ON9"
      },
      "source": [
        "### 2.2 Training samples extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Avoid overlap of train and test\n",
        "# Extracting training samples\n",
        "hsi_training_samples, lidar_training_samples, training_labels = [], [], []\n",
        "used_indices = []  # To keep track of indices already taken for training samples\n",
        "\n",
        "for label, class_data in class_dict.items():\n",
        "    # Get indices of the current class\n",
        "    class_indices = np.where(labels == label)[0]\n",
        "\n",
        "    # Randomly shuffle the indices\n",
        "    np.random.shuffle(class_indices)\n",
        "\n",
        "    # Take the required number of training samples\n",
        "    train_indices = class_indices[:class_data[\"training_sample\"]]\n",
        "    used_indices.extend(train_indices)  # Add these to the used_indices list\n",
        "\n",
        "    # Append training samples\n",
        "    hsi_training_samples.extend(hsi_samples[train_indices])\n",
        "    lidar_training_samples.extend(lidar_samples[train_indices])\n",
        "    training_labels.extend(labels[train_indices])\n",
        "\n",
        "# Extracting test samples\n",
        "hsi_test_samples, lidar_test_samples, test_labels = [], [], []\n",
        "\n",
        "for label, class_data in class_dict.items():\n",
        "    class_indices = np.where(labels == label)[0]\n",
        "\n",
        "    # Exclude indices which were used for training\n",
        "    test_indices = np.setdiff1d(class_indices, used_indices)\n",
        "\n",
        "    # Append test samples\n",
        "    hsi_test_samples.extend(hsi_samples[test_indices])\n",
        "    lidar_test_samples.extend(lidar_samples[test_indices])\n",
        "    test_labels.extend(labels[test_indices])\n",
        "\n",
        "# Convert lists back to numpy arrays\n",
        "hsi_training_samples = np.array(hsi_training_samples)\n",
        "lidar_training_samples = np.array(lidar_training_samples)\n",
        "training_labels = np.array(training_labels)\n",
        "\n",
        "hsi_test_samples = np.array(hsi_test_samples)\n",
        "lidar_test_samples = np.array(lidar_test_samples)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Print shapes to verify\n",
        "print('hsi_training_samples shape:', hsi_training_samples.shape)\n",
        "print('lidar_training_samples shape:', lidar_training_samples.shape)\n",
        "print('training_labels shape:', training_labels.shape)\n",
        "\n",
        "print('hsi_test_samples shape:', hsi_test_samples.shape)\n",
        "print('lidar_test_samples shape:', lidar_test_samples.shape)\n",
        "print('test_labels shape:', test_labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH4-2rJ-vpQs",
        "outputId": "33877eae-1a12-4d9d-84c9-f7c1005399cd"
      },
      "id": "MH4-2rJ-vpQs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_training_samples shape: (2832, 9, 9, 144)\n",
            "lidar_training_samples shape: (2832, 9, 9, 1)\n",
            "training_labels shape: (2832,)\n",
            "hsi_test_samples shape: (12197, 9, 9, 144)\n",
            "lidar_test_samples shape: (12197, 9, 9, 1)\n",
            "test_labels shape: (12197,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Augmented samples"
      ],
      "metadata": {
        "id": "2LB_Lx9AGvT3"
      },
      "id": "2LB_Lx9AGvT3"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "def augment_training_data(hsi_training_data, lidar_training_data, training_labels, rotations=[45, 90, 135], flip_up_down=True, flip_left_right=True):\n",
        "    augmented_hsi = []\n",
        "    augmented_lidar = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for hsi, lidar, label in zip(hsi_training_data, lidar_training_data, training_labels):\n",
        "        # Original data\n",
        "        augmented_hsi.append(hsi)\n",
        "        augmented_lidar.append(lidar)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        # Rotations\n",
        "        for angle in rotations:\n",
        "            hsi_rotated = rotate(hsi, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "            lidar_rotated = rotate(lidar, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "\n",
        "            augmented_hsi.append(hsi_rotated)\n",
        "            augmented_lidar.append(lidar_rotated)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip up-down\n",
        "        if flip_up_down:\n",
        "            hsi_flipped_ud = np.flipud(hsi)\n",
        "            lidar_flipped_ud = np.flipud(lidar)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_ud)\n",
        "            augmented_lidar.append(lidar_flipped_ud)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip left-right\n",
        "        if flip_left_right:\n",
        "            hsi_flipped_lr = np.fliplr(hsi)\n",
        "            lidar_flipped_lr = np.fliplr(lidar)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_lr)\n",
        "            augmented_lidar.append(lidar_flipped_lr)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    return np.array(augmented_hsi), np.array(augmented_lidar), np.array(augmented_labels)\n",
        "\n",
        "# Augmenting the training samples\n",
        "augmented_hsi_training_samples, augmented_lidar_training_samples, augmented_training_labels = augment_training_data(hsi_training_samples, lidar_training_samples, training_labels)\n",
        "\n",
        "# Print shapes to verify the augmented training data\n",
        "print('Augmented HSI training samples shape:', augmented_hsi_training_samples.shape)\n",
        "print('Augmented LiDAR training samples shape:', augmented_lidar_training_samples.shape)\n",
        "print('Augmented training labels shape:', augmented_training_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMHXQ9gcGx9v",
        "outputId": "8e03566f-f112-4655-eca9-09ff3ef04125"
      },
      "id": "GMHXQ9gcGx9v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmented HSI training samples shape: (16992, 9, 9, 144)\n",
            "Augmented LiDAR training samples shape: (16992, 9, 9, 1)\n",
            "Augmented training labels shape: (16992,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hsi_train=augmented_hsi_training_samples\n",
        "lidar_train=augmented_lidar_training_samples\n",
        "y_train=augmented_training_labels\n",
        "print('hsi_train_samples shape:', hsi_train.shape)\n",
        "print('lidar_train_samples shape:', lidar_train.shape)\n",
        "print('train_labels shape:', y_train.shape)\n",
        "hsi_test=hsi_test_samples\n",
        "lidar_test=lidar_test_samples\n",
        "y_test=test_labels\n",
        "print('hsi_test_samples shape:', hsi_test.shape)\n",
        "print('lidar_test_samples shape:', lidar_test.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MTS2gMxG-jf",
        "outputId": "7fdf0d2b-8204-448e-f053-58bb74245d88"
      },
      "id": "_MTS2gMxG-jf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_train_samples shape: (16992, 9, 9, 144)\n",
            "lidar_train_samples shape: (16992, 9, 9, 1)\n",
            "train_labels shape: (16992,)\n",
            "hsi_test_samples shape: (12197, 9, 9, 144)\n",
            "lidar_test_samples shape: (12197, 9, 9, 1)\n",
            "y_test shape: (12197,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# One hot encoding of labels\n",
        "# Substract 1 from labels\n",
        "y_train_adj = augmented_training_labels - 1\n",
        "y_test_adj = y_test - 1\n",
        "\n",
        "# One hot encoding of labels\n",
        "y_train = to_categorical(y_train_adj, num_classes = 15, dtype =\"int32\")\n",
        "y_test = to_categorical(y_test_adj, num_classes = 15, dtype =\"int32\")\n",
        "\n",
        "print('y_train.shape:',y_train.shape)\n",
        "print('y_test.shape:',y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riBpJLLFHa-3",
        "outputId": "90447037-7fc9-403c-dc47-8a2386caf680"
      },
      "id": "riBpJLLFHa-3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train.shape: (16992, 15)\n",
            "y_test.shape: (12197, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J7Ko9uYgIf2D",
      "metadata": {
        "id": "J7Ko9uYgIf2D"
      },
      "source": [
        "#3.0 Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u1rQcEYmVP_W",
      "metadata": {
        "id": "u1rQcEYmVP_W"
      },
      "source": [
        "### 3.1 Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35qAdvCxIPJ8",
      "metadata": {
        "id": "35qAdvCxIPJ8"
      },
      "outputs": [],
      "source": [
        "# 3.1 Configuration\n",
        "class Config:\n",
        "    def __init__(self,in_channels,num_patches,kernel_size,patch_size,emb_size, dim,depth,heads,dim_head,mlp_dim,num_classes,dropout,pos_emb_size,class_emb_size,stride, ):\n",
        "        self.in_channels = in_channels\n",
        "        self.num_patches = num_patches\n",
        "        self.kernel_size = kernel_size\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "        self.pos_emb_size = pos_emb_size\n",
        "        self.class_emb_size = class_emb_size\n",
        "        self.stride = stride\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U5T_YmiQespq",
      "metadata": {
        "id": "U5T_YmiQespq"
      },
      "source": [
        "### 3.2 EmbeddingPatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H0MaMR75xcKE",
      "metadata": {
        "id": "H0MaMR75xcKE"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(\n",
        "            config.in_channels,\n",
        "            config.emb_size,\n",
        "            kernel_size=config.kernel_size,\n",
        "            stride=config.stride,\n",
        "        )\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, config.num_patches + 1, config.emb_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.emb_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n",
        "        x = x + self.pos_embedding[:, :x.size(1)]\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7DyoAhpwF1WO",
      "metadata": {
        "id": "7DyoAhpwF1WO"
      },
      "source": [
        "### 3.2 Optianl Adding bandoutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ZwPxVLX4uwk",
      "metadata": {
        "id": "7ZwPxVLX4uwk"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, lidar_config, hsi_config):\n",
        "        super(CrossAttention, self).__init__()\n",
        "\n",
        "        # Define module parameters\n",
        "        self.dim_head = lidar_config.dim_head\n",
        "        self.num_patches=hsi_config.num_patches\n",
        "        self.num_heads = lidar_config.heads\n",
        "        self.sqrt_dim_head = math.sqrt(self.dim_head)\n",
        "\n",
        "        # Define linear layers for transforming Q, K, and V\n",
        "        self.to_q = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "        self.to_k = nn.Linear(hsi_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "        self.to_v = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "\n",
        "        self.to_out = nn.Linear( self.num_heads* self.dim_head,self.num_patches )  # added\n",
        "\n",
        "\n",
        "    def forward(self, lidar, hsi):\n",
        "        B, N_lidar, _ = lidar.size()\n",
        "        _, N_hsi, _ = hsi.size()\n",
        "\n",
        "        outputs = []\n",
        "        attn_scores = []  # List to store attention scores\n",
        "\n",
        "       # Iterate over lidar and hsi patches\n",
        "        for i in range(1, N_lidar):\n",
        "        #for i in range(N_lidar):\n",
        "\n",
        "            lidar_patch = lidar[:, i].unsqueeze(1)  # Add a dimension for number of patches\n",
        "            for j in range(1, N_hsi):\n",
        "            #for j in range(N_hsi):\n",
        "\n",
        "                hsi_patch = hsi[:, j].unsqueeze(1)  # Add a dimension for number of patches\n",
        "                Q = self.to_q(lidar_patch)\n",
        "                K = self.to_k(hsi_patch)\n",
        "                V = self.to_v(lidar_patch)\n",
        "\n",
        "                Q = Q / self.sqrt_dim_head\n",
        "                attn_weights = F.softmax(Q @ K.transpose(-2, -1), dim=-1)\n",
        "\n",
        "                attn_output = attn_weights @ V\n",
        "                attn_score = self.to_out(attn_output)  # added\n",
        "                outputs.append(attn_output)\n",
        "                attn_scores.append(attn_score)  # Store the attention scores\n",
        "\n",
        "         # Concatenate all the outputs\n",
        "        output = torch.cat(outputs, dim=1)\n",
        "        attn_scores = torch.cat(attn_scores, dim=1)  # Concatenate all the attention scores\n",
        "\n",
        "        return output, attn_scores  # Return both output and attention scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "at2xTwGv91ng",
      "metadata": {
        "id": "at2xTwGv91ng"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionModel(nn.Module):\n",
        "    def __init__(self, hsi_config, lidar_config):\n",
        "        super().__init__()\n",
        "        self.hsi_patch_embedding = PatchEmbedding(hsi_config)\n",
        "        self.lidar_patch_embedding = PatchEmbedding(lidar_config)\n",
        "        self.cross_attention = CrossAttention(lidar_config, hsi_config)\n",
        "\n",
        "    def forward(self, lidar_data, hsi_data):\n",
        "        # Apply PatchEmbedding\n",
        "        lidar_emb = self.lidar_patch_embedding(lidar_data)\n",
        "        hsi_emb = self.hsi_patch_embedding(hsi_data)\n",
        "\n",
        "        # Apply CrossAttention\n",
        "        output, attn_scores = self.cross_attention(lidar_emb, hsi_emb)\n",
        "\n",
        "        return output, attn_scores  # Return both output and attention scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8oC00YgYsSS",
      "metadata": {
        "id": "b8oC00YgYsSS"
      },
      "outputs": [],
      "source": [
        "#3.1.1 Parameters Setting\n",
        "# Hsi configuration\n",
        "hsi_config = Config(\n",
        "    in_channels=81,  # Each sample covers 144 bands\n",
        "    num_patches=144,  # 25*1*144 bands are grouped into 3600 groups\n",
        "    kernel_size=1,  # Adjusted to match new patch size\n",
        "    patch_size=9,  # Adjusted to match new patch size (5*5, 144/24=6)\n",
        "    emb_size=128,  # Embedding size, this can be changed\n",
        "    dim=128,  # Dimension of the transformer, this can be changed\n",
        "    depth=3,  # Number of transformer layers, this can be changed\n",
        "    heads=8,  # Number of attention heads, this can be changed\n",
        "    dim_head=64,  # Dimension of each attention head, this can be changed\n",
        "    mlp_dim=256,  # Dimension of the MLP layer, this can be changed\n",
        "    num_classes=15,  # Number of classes, this can be changed\n",
        "    dropout=0.4,  # Dropout rate, this can be changed\n",
        "    pos_emb_size=128,  # Position embedding size, this can be changed\n",
        "    class_emb_size=128,  # Class embedding size, this can be changed\n",
        "    stride=1  # Stride for the convolution, this can be changed\n",
        ")\n",
        "\n",
        "\n",
        "# Lidara configuration\n",
        "lidar_config = Config(\n",
        "    in_channels=81,  # lidar group has 1 channels\n",
        "    num_patches=1,  # 1 band for Lidar\n",
        "    kernel_size=1,  # Adjusted to match new patch size\n",
        "    patch_size=9, # Adjusted to match new patch size\n",
        "    emb_size=128,  # Embedding size, this can be changed\n",
        "    dim=128,  # Dimension of the transformer, this can be changed\n",
        "    depth=6,  # Number of transformer layers, this can be changed\n",
        "    heads=8,  # Number of attention heads, this can be changed\n",
        "    dim_head=64,  # Dimension of each attention head, this can be changed\n",
        "    mlp_dim=256,  # Dimension of the MLP layer, this can be changed\n",
        "    num_classes=15,  # Number of classes, this can be changed\n",
        "    dropout=0.4,  # Dropout rate, this can be changed\n",
        "    pos_emb_size=128,  # Position embedding size, this can be changed\n",
        "    class_emb_size=128,  # Class embedding size, this can be changed\n",
        "    stride=1  # Stride for the convolution, this can be changed\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b_WjbDjBeODf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_WjbDjBeODf",
        "outputId": "e14808db-2ba2-4a3e-8189-29a30e00604a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch sahpe before transpose: (16992, 9, 9, 144)\n",
            "lidar_batch sahpe before transpose: (16992, 9, 9, 1)\n",
            "hsi_batch sahpe after transpose: (16992, 144, 9, 9)\n",
            "lidar_batch shape after transpose: (16992, 1, 9, 9)\n"
          ]
        }
      ],
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch = hsi_train # Shape: (2832, 5, 5, 144)\n",
        "lidar_batch = lidar_train # Shape: (2832, 5, 5, 1)\n",
        "print('hsi_batch sahpe before transpose:', hsi_batch.shape)\n",
        "print('lidar_batch sahpe before transpose:', lidar_batch.shape)\n",
        "# Transpose the data to have the channel dimension at the correct place\n",
        "hsi_batch = hsi_batch.transpose(0, 3, 1, 2)  # New shape: (2832, 144, 5, 5)\n",
        "lidar_batch = lidar_batch.transpose(0, 3, 1, 2)  # New shape: (2832, 1, 5, 5)\n",
        "print('hsi_batch sahpe after transpose:', hsi_batch.shape)\n",
        "print('lidar_batch shape after transpose:', lidar_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GWEESEK3a-hX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWEESEK3a-hX",
        "outputId": "4c473600-7548-4c99-8ee7-b03b6d47dc07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one_hsi_batch sahpe before transpose: (1, 9, 9, 144)\n",
            "one_lidar_batch sahpe before transpose: (1, 9, 9, 1)\n",
            "hsi_batch sahpe after transpose: (1, 144, 9, 9)\n",
            "lidar_batch shape after transpose: (1, 1, 9, 9)\n"
          ]
        }
      ],
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "one_hsi_batch = hsi_train[:1]  # Shape: (1, 5, 5, 144)\n",
        "one_lidar_batch = lidar_train[:1]  # Shape: (1, 5, 5, 1)\n",
        "print('one_hsi_batch sahpe before transpose:', one_hsi_batch.shape)\n",
        "print('one_lidar_batch sahpe before transpose:', one_lidar_batch.shape)\n",
        "# Transpose the data to have the channel dimension at the correct place\n",
        "one_hsi_batch = one_hsi_batch.transpose(0, 3, 1, 2)  # New shape: (1, 144, 5, 5)\n",
        "one_lidar_batch = one_lidar_batch.transpose(0, 3, 1, 2)  # New shape: (1, 1, 5, 5)\n",
        "print('hsi_batch sahpe after transpose:', one_hsi_batch.shape)\n",
        "print('lidar_batch shape after transpose:', one_lidar_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qjXtH2S9Q81f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjXtH2S9Q81f",
        "outputId": "8e0a7dc6-5f48-426d-ddbc-8212a05bc75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one_hsi_batch_embedded shape: torch.Size([1, 145, 128])\n",
            "one_lidar_batch_embedded shape: torch.Size([1, 2, 128])\n"
          ]
        }
      ],
      "source": [
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "one_hsi_batch = hsi_train[:1]  # Shape: (1, 5, 5, 144)\n",
        "one_lidar_batch = lidar_train[:1]  # Shape: (1, 5, 5, 1)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "one_hsi_batch_flat = torch.from_numpy(one_hsi_batch.astype(np.float32).reshape(1, hsi_config.in_channels, hsi_config.num_patches, 1)).to(device)\n",
        "one_lidar_batch_flat = torch.from_numpy(one_lidar_batch.astype(np.float32).reshape(1, lidar_config.in_channels, lidar_config.num_patches, 1)).to(device)\n",
        "\n",
        "# Initialize the patch embedding module\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "one_hsi_batch_embedded = hsi_patch_embedding(one_hsi_batch_flat).to(device)\n",
        "one_lidar_batch_embedded = lidar_patch_embedding(one_lidar_batch_flat).to(device)\n",
        "\n",
        "print('one_hsi_batch_embedded shape:', one_hsi_batch_embedded.shape)\n",
        "print('one_lidar_batch_embedded shape:', one_lidar_batch_embedded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gpa3oU5ndrhn",
      "metadata": {
        "id": "gpa3oU5ndrhn"
      },
      "source": [
        "### Intialisation CrossAttention Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XYlhrA99ncUq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYlhrA99ncUq",
        "outputId": "44e835fd-d008-491a-dda3-326e351ca9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch_6  shape: (6, 9, 9, 144)\n",
            "lidar_batch_6 shape: (6, 9, 9, 1)\n",
            "hsi_batch_6_flat  shape: torch.Size([6, 81, 144, 1])\n",
            "lidar_batch_6_flat shape: torch.Size([6, 81, 1, 1])\n",
            "hsi_batch_6_embedded  shape: torch.Size([6, 145, 128])\n",
            "lidar_batch_6_embedded shape: torch.Size([6, 2, 128])\n",
            "Cross attention output shape: torch.Size([6, 144, 144])\n",
            "Output shape: torch.Size([6, 144, 512])\n"
          ]
        }
      ],
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch_6 = hsi_train[:6]  # Shape: (6, 5, 5, 144)\n",
        "lidar_batch_6 = lidar_train[:6]  # Shape: (6, 5, 5, 1)\n",
        "\n",
        "print(\"hsi_batch_6  shape:\", hsi_batch_6 .shape)\n",
        "print(\"lidar_batch_6 shape:\", lidar_batch_6.shape)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "hsi_batch_6_flat = torch.from_numpy(hsi_batch_6.astype(np.float32).reshape(6, hsi_config.in_channels, hsi_config.num_patches, 1))\n",
        "lidar_batch_6_flat = torch.from_numpy(lidar_batch_6.astype(np.float32).reshape(6, lidar_config.in_channels, lidar_config.num_patches, 1))\n",
        "\n",
        "print(\"hsi_batch_6_flat  shape:\", hsi_batch_6_flat .shape)\n",
        "print(\"lidar_batch_6_flat shape:\", lidar_batch_6_flat.shape)\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "hsi_batch_6_flat = hsi_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_6_flat = lidar_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "hsi_batch_6_embedded  = hsi_patch_embedding(hsi_batch_6_flat)\n",
        "lidar_batch_6_embedded = lidar_patch_embedding(lidar_batch_6_flat)\n",
        "\n",
        "#device = torch.device(\"cuda:0\")  # Define the device (GPU)\n",
        "device = torch.device(\"cpu\")\n",
        "# Move the tensors to the desired device\n",
        "hsi_batch_6_embedded = hsi_batch_6_embedded.to(device)\n",
        "lidar_batch_6_embedded = lidar_batch_6_embedded.to(device)\n",
        "print(\"hsi_batch_6_embedded  shape:\", hsi_batch_6_embedded .shape)\n",
        "print(\"lidar_batch_6_embedded shape:\", lidar_batch_6_embedded.shape)\n",
        "\n",
        "# Define the dimension of the model and the number of heads\n",
        "d_model = hsi_config.emb_size  # the output dimension of PatchEmbedding\n",
        "num_heads = hsi_config.heads  # the number of attention heads in the transformer\n",
        "\n",
        "# Initialize CrossAttention module\n",
        "cross_attention = CrossAttention(lidar_config, hsi_config).to(device)\n",
        "\n",
        "# Apply the cross attention\n",
        "output,attn_scores = cross_attention(lidar_batch_6_embedded, hsi_batch_6_embedded)\n",
        "\n",
        "\n",
        "print(\"Cross attention output shape:\", attn_scores.shape)\n",
        "print(\"Output shape:\", output.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dDySwOVzY7AE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDySwOVzY7AE",
        "outputId": "a64c4a4a-2f85-41d1-897e-65a472f2e023"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossAttention(\n",
              "  (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_out): Linear(in_features=512, out_features=144, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "cross_attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nJTi3xzCYzd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJTi3xzCYzd1",
        "outputId": "5e9b0f72-ee15-4236-826a-a0e1af8ee8d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossAttentionModel(\n",
              "  (hsi_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(81, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (lidar_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(81, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (cross_attention): CrossAttention(\n",
              "    (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_out): Linear(in_features=512, out_features=144, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yem48RAvJSLH",
      "metadata": {
        "id": "yem48RAvJSLH"
      },
      "source": [
        "# 4.1  Training DataLoader for Cross Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperspectralDataset(Dataset):\n",
        "    def __init__(self, hsi_samples, lidar_samples, labels):\n",
        "        self.hsi_samples = hsi_samples\n",
        "        self.lidar_samples = lidar_samples\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hsi_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hsi_patch = self.hsi_samples[idx].float().to(device)\n",
        "        lidar_patch = self.lidar_samples[idx].float().to(device)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert label to a tensor and reshape to match the model's output shape\n",
        "        label = torch.tensor(label).repeat(hsi_patch.shape[1], 1).to(device)\n",
        "\n",
        "        return hsi_patch, lidar_patch, label"
      ],
      "metadata": {
        "id": "THj2z-T8JpVQ"
      },
      "id": "THj2z-T8JpVQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_tensor = torch.tensor(y_train)\n",
        "#labels_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "print('labels_tensor:',labels_tensor.shape)\n",
        "\n",
        "unique_classes = torch.unique(labels_tensor)\n",
        "print(\"Unique classes:\", unique_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkJm2U9BJvy4",
        "outputId": "999316b6-6728-4f55-8bd6-4a82604760c8"
      },
      "id": "HkJm2U9BJvy4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels_tensor: torch.Size([16992, 15])\n",
            "Unique classes: tensor([0, 1], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a00236bf",
        "outputId": "ae27a4e7-e958-43c5-d68e-0694f7c6a768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16992])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Assuming labels_tensor is one-hot encoded or contains probabilities\n",
        "labels_tensor = torch.argmax(labels_tensor, dim=1)\n",
        "labels_tensor.shape"
      ],
      "id": "a00236bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3ec406a",
        "outputId": "d912ce46-43f3-43cb-cd0e-c88ea8734cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train shape: torch.Size([16992])\n"
          ]
        }
      ],
      "source": [
        "y_train=labels_tensor\n",
        "print('y_train shape:',y_train.shape)"
      ],
      "id": "e3ec406a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59448309",
        "outputId": "fe4895c7-1ad7-4e8b-fee3-aec74a1141c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test_tensor shape: torch.Size([12197, 15])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "y_test_tensor= torch.tensor(y_test)\n",
        "y_test_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "print('y_test_tensor shape:',y_test_tensor.shape)"
      ],
      "id": "59448309"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1db9eb8",
        "outputId": "7008bae3-260b-4546-ed6d-c216d3ce05ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_test_tensor shape: torch.Size([12197])\n"
          ]
        }
      ],
      "source": [
        "# Assuming labels_tensor is one-hot encoded or contains probabilities\n",
        "y_test_tensor = torch.argmax(y_test_tensor, dim=1)\n",
        "print('y_test_tensor shape:',y_test_tensor.shape)"
      ],
      "id": "d1db9eb8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bc61bc8"
      },
      "outputs": [],
      "source": [
        "y_test= y_test_tensor"
      ],
      "id": "9bc61bc8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-72dBlA1TbBj",
        "outputId": "8edc6fe6-23cc-4356-ff64-8c44ab0447e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
          ]
        }
      ],
      "source": [
        "unique_classes = torch.unique(labels_tensor)\n",
        "print(\"Unique classes:\", unique_classes)"
      ],
      "id": "-72dBlA1TbBj"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "## Reshape the data\n",
        "hsi_batch_flat = torch.from_numpy(hsi_train.astype(np.float32).reshape(16992, hsi_config.in_channels, 144, 1))\n",
        "lidar_batch_flat = torch.from_numpy(lidar_train.astype(np.float32).reshape(16992, lidar_config.in_channels, 1, 1))\n",
        "\n",
        "print(\"hsi_batch shape:\", hsi_batch_flat .shape)\n",
        "print(\"lidar_batch shape:\", lidar_batch_flat.shape)\n",
        "\n",
        "hsi_batch_flat = hsi_batch_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_flat = lidar_batch_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "\n",
        "# Split train into train and validation\n",
        "hsi_samples_train, hsi_samples_val, lidar_samples_train, lidar_samples_val, labels_train, labels_val = train_test_split(\n",
        "    hsi_batch_flat, lidar_batch_flat, y_train, test_size=0.20, random_state=42)\n",
        "print('hsi_samples_train shape:',hsi_samples_train.shape)\n",
        "print('hsi_samples_val shape:',hsi_samples_val.shape)\n",
        "print('lidar_samples_train shape:',lidar_samples_train.shape)\n",
        "print('lidar_samples_valshape:',lidar_samples_val.shape)\n",
        "print('labels_train, shape:',labels_train.shape)\n",
        "print('labels_val, shape:',labels_val.shape)\n",
        "\n",
        "# Now test data.\n",
        "## Reshape the data\n",
        "hsi_test_batch_flat = torch.from_numpy(hsi_test.astype(np.float32).reshape(12197, hsi_config.in_channels, 144, 1))\n",
        "lidar_test_batch_flat = torch.from_numpy(lidar_test.astype(np.float32).reshape(12197, lidar_config.in_channels, 1, 1))\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = HyperspectralDataset(hsi_samples_train, lidar_samples_train, labels_train)\n",
        "val_dataset = HyperspectralDataset(hsi_samples_val, lidar_samples_val, labels_val)\n",
        "test_dataset = HyperspectralDataset(hsi_test_batch_flat, lidar_test_batch_flat, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jNyLfweKjBA",
        "outputId": "20774172-6465-4ffa-f9e2-de0732b4b5a2"
      },
      "id": "-jNyLfweKjBA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch shape: torch.Size([16992, 81, 144, 1])\n",
            "lidar_batch shape: torch.Size([16992, 81, 1, 1])\n",
            "hsi_samples_train shape: torch.Size([13593, 81, 144, 1])\n",
            "hsi_samples_val shape: torch.Size([3399, 81, 144, 1])\n",
            "lidar_samples_train shape: torch.Size([13593, 81, 1, 1])\n",
            "lidar_samples_valshape: torch.Size([3399, 81, 1, 1])\n",
            "labels_train, shape: torch.Size([13593])\n",
            "labels_val, shape: torch.Size([3399])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_MBeu13kDLc6",
      "metadata": {
        "id": "_MBeu13kDLc6"
      },
      "source": [
        "# 4.2 Training CorssAttentionMode ltraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w2D4wFzJ5ams",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w2D4wFzJ5ams",
        "outputId": "0696934e-8946-4244-f367-0de30217bb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-b31b03d9f721>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).repeat(hsi_patch.shape[1], 1).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Batch [1/425], Loss: 13.5947\n",
            "Epoch [1/200], Batch [2/425], Loss: 13.3491\n",
            "Epoch [1/200], Batch [3/425], Loss: 12.2305\n",
            "Epoch [1/200], Batch [4/425], Loss: 12.9050\n",
            "Epoch [1/200], Batch [5/425], Loss: 12.2903\n",
            "Epoch [1/200], Batch [6/425], Loss: 11.7252\n",
            "Epoch [1/200], Batch [7/425], Loss: 12.4860\n",
            "Epoch [1/200], Batch [8/425], Loss: 11.6189\n",
            "Epoch [1/200], Batch [9/425], Loss: 11.2262\n",
            "Epoch [1/200], Batch [10/425], Loss: 10.9239\n",
            "Epoch [1/200], Batch [11/425], Loss: 10.3285\n",
            "Epoch [1/200], Batch [12/425], Loss: 10.1136\n",
            "Epoch [1/200], Batch [13/425], Loss: 8.6381\n",
            "Epoch [1/200], Batch [14/425], Loss: 9.1455\n",
            "Epoch [1/200], Batch [15/425], Loss: 8.1612\n",
            "Epoch [1/200], Batch [16/425], Loss: 8.7146\n",
            "Epoch [1/200], Batch [17/425], Loss: 8.7865\n",
            "Epoch [1/200], Batch [18/425], Loss: 8.1203\n",
            "Epoch [1/200], Batch [19/425], Loss: 8.7570\n",
            "Epoch [1/200], Batch [20/425], Loss: 8.0156\n",
            "Epoch [1/200], Batch [21/425], Loss: 8.0290\n",
            "Epoch [1/200], Batch [22/425], Loss: 7.4068\n",
            "Epoch [1/200], Batch [23/425], Loss: 6.9367\n",
            "Epoch [1/200], Batch [24/425], Loss: 6.4517\n",
            "Epoch [1/200], Batch [25/425], Loss: 6.4289\n",
            "Epoch [1/200], Batch [26/425], Loss: 6.2418\n",
            "Epoch [1/200], Batch [27/425], Loss: 6.2407\n",
            "Epoch [1/200], Batch [28/425], Loss: 5.8110\n",
            "Epoch [1/200], Batch [29/425], Loss: 5.7846\n",
            "Epoch [1/200], Batch [30/425], Loss: 6.6189\n",
            "Epoch [1/200], Batch [31/425], Loss: 5.2652\n",
            "Epoch [1/200], Batch [32/425], Loss: 5.2523\n",
            "Epoch [1/200], Batch [33/425], Loss: 4.9028\n",
            "Epoch [1/200], Batch [34/425], Loss: 4.6866\n",
            "Epoch [1/200], Batch [35/425], Loss: 4.7625\n",
            "Epoch [1/200], Batch [36/425], Loss: 4.2429\n",
            "Epoch [1/200], Batch [37/425], Loss: 4.9611\n",
            "Epoch [1/200], Batch [38/425], Loss: 5.6527\n",
            "Epoch [1/200], Batch [39/425], Loss: 4.6930\n",
            "Epoch [1/200], Batch [40/425], Loss: 4.4348\n",
            "Epoch [1/200], Batch [41/425], Loss: 4.8354\n",
            "Epoch [1/200], Batch [42/425], Loss: 4.1442\n",
            "Epoch [1/200], Batch [43/425], Loss: 4.6232\n",
            "Epoch [1/200], Batch [44/425], Loss: 4.2301\n",
            "Epoch [1/200], Batch [45/425], Loss: 3.9830\n",
            "Epoch [1/200], Batch [46/425], Loss: 3.9667\n",
            "Epoch [1/200], Batch [47/425], Loss: 3.8549\n",
            "Epoch [1/200], Batch [48/425], Loss: 3.5043\n",
            "Epoch [1/200], Batch [49/425], Loss: 3.3069\n",
            "Epoch [1/200], Batch [50/425], Loss: 3.6993\n",
            "Epoch [1/200], Batch [51/425], Loss: 3.7284\n",
            "Epoch [1/200], Batch [52/425], Loss: 3.5199\n",
            "Epoch [1/200], Batch [53/425], Loss: 3.4110\n",
            "Epoch [1/200], Batch [54/425], Loss: 3.3051\n",
            "Epoch [1/200], Batch [55/425], Loss: 3.1783\n",
            "Epoch [1/200], Batch [56/425], Loss: 2.9827\n",
            "Epoch [1/200], Batch [57/425], Loss: 3.2291\n",
            "Epoch [1/200], Batch [58/425], Loss: 2.9656\n",
            "Epoch [1/200], Batch [59/425], Loss: 3.3407\n",
            "Epoch [1/200], Batch [60/425], Loss: 2.9996\n",
            "Epoch [1/200], Batch [61/425], Loss: 3.1065\n",
            "Epoch [1/200], Batch [62/425], Loss: 2.8349\n",
            "Epoch [1/200], Batch [63/425], Loss: 3.0695\n",
            "Epoch [1/200], Batch [64/425], Loss: 3.1151\n",
            "Epoch [1/200], Batch [65/425], Loss: 3.0795\n",
            "Epoch [1/200], Batch [66/425], Loss: 3.0516\n",
            "Epoch [1/200], Batch [67/425], Loss: 3.0428\n",
            "Epoch [1/200], Batch [68/425], Loss: 3.2293\n",
            "Epoch [1/200], Batch [69/425], Loss: 3.0283\n",
            "Epoch [1/200], Batch [70/425], Loss: 3.0262\n",
            "Epoch [1/200], Batch [71/425], Loss: 3.1471\n",
            "Epoch [1/200], Batch [72/425], Loss: 3.1832\n",
            "Epoch [1/200], Batch [73/425], Loss: 2.9272\n",
            "Epoch [1/200], Batch [74/425], Loss: 3.0628\n",
            "Epoch [1/200], Batch [75/425], Loss: 2.8443\n",
            "Epoch [1/200], Batch [76/425], Loss: 2.9213\n",
            "Epoch [1/200], Batch [77/425], Loss: 2.7777\n",
            "Epoch [1/200], Batch [78/425], Loss: 2.7755\n",
            "Epoch [1/200], Batch [79/425], Loss: 2.8600\n",
            "Epoch [1/200], Batch [80/425], Loss: 2.7517\n",
            "Epoch [1/200], Batch [81/425], Loss: 2.8481\n",
            "Epoch [1/200], Batch [82/425], Loss: 2.9365\n",
            "Epoch [1/200], Batch [83/425], Loss: 2.9467\n",
            "Epoch [1/200], Batch [84/425], Loss: 2.8555\n",
            "Epoch [1/200], Batch [85/425], Loss: 3.2143\n",
            "Epoch [1/200], Batch [86/425], Loss: 2.8889\n",
            "Epoch [1/200], Batch [87/425], Loss: 2.7891\n",
            "Epoch [1/200], Batch [88/425], Loss: 2.8574\n",
            "Epoch [1/200], Batch [89/425], Loss: 3.0067\n",
            "Epoch [1/200], Batch [90/425], Loss: 2.7654\n",
            "Epoch [1/200], Batch [91/425], Loss: 2.7763\n",
            "Epoch [1/200], Batch [92/425], Loss: 2.9302\n",
            "Epoch [1/200], Batch [93/425], Loss: 2.8076\n",
            "Epoch [1/200], Batch [94/425], Loss: 2.9598\n",
            "Epoch [1/200], Batch [95/425], Loss: 3.0073\n",
            "Epoch [1/200], Batch [96/425], Loss: 2.8421\n",
            "Epoch [1/200], Batch [97/425], Loss: 3.2270\n",
            "Epoch [1/200], Batch [98/425], Loss: 2.8640\n",
            "Epoch [1/200], Batch [99/425], Loss: 2.9177\n",
            "Epoch [1/200], Batch [100/425], Loss: 2.7999\n",
            "Epoch [1/200], Batch [101/425], Loss: 3.0308\n",
            "Epoch [1/200], Batch [102/425], Loss: 2.7586\n",
            "Epoch [1/200], Batch [103/425], Loss: 3.0084\n",
            "Epoch [1/200], Batch [104/425], Loss: 3.0266\n",
            "Epoch [1/200], Batch [105/425], Loss: 2.6829\n",
            "Epoch [1/200], Batch [106/425], Loss: 2.7817\n",
            "Epoch [1/200], Batch [107/425], Loss: 2.8504\n",
            "Epoch [1/200], Batch [108/425], Loss: 2.8346\n",
            "Epoch [1/200], Batch [109/425], Loss: 2.7689\n",
            "Epoch [1/200], Batch [110/425], Loss: 2.8911\n",
            "Epoch [1/200], Batch [111/425], Loss: 2.7903\n",
            "Epoch [1/200], Batch [112/425], Loss: 3.0456\n",
            "Epoch [1/200], Batch [113/425], Loss: 2.9433\n",
            "Epoch [1/200], Batch [114/425], Loss: 2.7482\n",
            "Epoch [1/200], Batch [115/425], Loss: 2.9999\n",
            "Epoch [1/200], Batch [116/425], Loss: 2.6687\n",
            "Epoch [1/200], Batch [117/425], Loss: 2.9411\n",
            "Epoch [1/200], Batch [118/425], Loss: 2.7296\n",
            "Epoch [1/200], Batch [119/425], Loss: 2.8683\n",
            "Epoch [1/200], Batch [120/425], Loss: 2.6569\n",
            "Epoch [1/200], Batch [121/425], Loss: 2.7912\n",
            "Epoch [1/200], Batch [122/425], Loss: 2.7702\n",
            "Epoch [1/200], Batch [123/425], Loss: 2.7670\n",
            "Epoch [1/200], Batch [124/425], Loss: 2.8839\n",
            "Epoch [1/200], Batch [125/425], Loss: 3.0410\n",
            "Epoch [1/200], Batch [126/425], Loss: 2.8035\n",
            "Epoch [1/200], Batch [127/425], Loss: 2.9520\n",
            "Epoch [1/200], Batch [128/425], Loss: 2.7665\n",
            "Epoch [1/200], Batch [129/425], Loss: 2.8467\n",
            "Epoch [1/200], Batch [130/425], Loss: 2.8322\n",
            "Epoch [1/200], Batch [131/425], Loss: 2.8411\n",
            "Epoch [1/200], Batch [132/425], Loss: 2.8087\n",
            "Epoch [1/200], Batch [133/425], Loss: 2.9546\n",
            "Epoch [1/200], Batch [134/425], Loss: 3.0835\n",
            "Epoch [1/200], Batch [135/425], Loss: 2.8135\n",
            "Epoch [1/200], Batch [136/425], Loss: 2.8804\n",
            "Epoch [1/200], Batch [137/425], Loss: 2.8675\n",
            "Epoch [1/200], Batch [138/425], Loss: 3.0397\n",
            "Epoch [1/200], Batch [139/425], Loss: 2.8657\n",
            "Epoch [1/200], Batch [140/425], Loss: 2.9266\n",
            "Epoch [1/200], Batch [141/425], Loss: 2.9165\n",
            "Epoch [1/200], Batch [142/425], Loss: 2.5712\n",
            "Epoch [1/200], Batch [143/425], Loss: 2.7775\n",
            "Epoch [1/200], Batch [144/425], Loss: 2.8018\n",
            "Epoch [1/200], Batch [145/425], Loss: 2.7767\n",
            "Epoch [1/200], Batch [146/425], Loss: 2.9800\n",
            "Epoch [1/200], Batch [147/425], Loss: 2.8559\n",
            "Epoch [1/200], Batch [148/425], Loss: 2.6871\n",
            "Epoch [1/200], Batch [149/425], Loss: 2.8531\n",
            "Epoch [1/200], Batch [150/425], Loss: 2.7696\n",
            "Epoch [1/200], Batch [151/425], Loss: 2.9237\n",
            "Epoch [1/200], Batch [152/425], Loss: 2.8407\n",
            "Epoch [1/200], Batch [153/425], Loss: 2.8241\n",
            "Epoch [1/200], Batch [154/425], Loss: 2.8412\n",
            "Epoch [1/200], Batch [155/425], Loss: 2.6217\n",
            "Epoch [1/200], Batch [156/425], Loss: 2.6400\n",
            "Epoch [1/200], Batch [157/425], Loss: 2.7467\n",
            "Epoch [1/200], Batch [158/425], Loss: 2.7499\n",
            "Epoch [1/200], Batch [159/425], Loss: 2.7651\n",
            "Epoch [1/200], Batch [160/425], Loss: 2.5802\n",
            "Epoch [1/200], Batch [161/425], Loss: 2.7160\n",
            "Epoch [1/200], Batch [162/425], Loss: 2.7895\n",
            "Epoch [1/200], Batch [163/425], Loss: 2.8642\n",
            "Epoch [1/200], Batch [164/425], Loss: 2.7329\n",
            "Epoch [1/200], Batch [165/425], Loss: 2.8853\n",
            "Epoch [1/200], Batch [166/425], Loss: 2.8201\n",
            "Epoch [1/200], Batch [167/425], Loss: 2.7528\n",
            "Epoch [1/200], Batch [168/425], Loss: 2.8692\n",
            "Epoch [1/200], Batch [169/425], Loss: 2.9375\n",
            "Epoch [1/200], Batch [170/425], Loss: 2.7666\n",
            "Epoch [1/200], Batch [171/425], Loss: 2.8900\n",
            "Epoch [1/200], Batch [172/425], Loss: 3.0876\n",
            "Epoch [1/200], Batch [173/425], Loss: 2.8596\n",
            "Epoch [1/200], Batch [174/425], Loss: 3.0132\n",
            "Epoch [1/200], Batch [175/425], Loss: 2.6569\n",
            "Epoch [1/200], Batch [176/425], Loss: 2.7760\n",
            "Epoch [1/200], Batch [177/425], Loss: 2.8717\n",
            "Epoch [1/200], Batch [178/425], Loss: 2.8683\n",
            "Epoch [1/200], Batch [179/425], Loss: 2.8467\n",
            "Epoch [1/200], Batch [180/425], Loss: 2.8470\n",
            "Epoch [1/200], Batch [181/425], Loss: 2.7541\n",
            "Epoch [1/200], Batch [182/425], Loss: 3.0814\n",
            "Epoch [1/200], Batch [183/425], Loss: 2.6390\n",
            "Epoch [1/200], Batch [184/425], Loss: 3.3051\n",
            "Epoch [1/200], Batch [185/425], Loss: 2.7764\n",
            "Epoch [1/200], Batch [186/425], Loss: 2.8056\n",
            "Epoch [1/200], Batch [187/425], Loss: 3.1873\n",
            "Epoch [1/200], Batch [188/425], Loss: 2.6899\n",
            "Epoch [1/200], Batch [189/425], Loss: 2.9226\n",
            "Epoch [1/200], Batch [190/425], Loss: 2.8785\n",
            "Epoch [1/200], Batch [191/425], Loss: 3.0495\n",
            "Epoch [1/200], Batch [192/425], Loss: 2.8519\n",
            "Epoch [1/200], Batch [193/425], Loss: 2.7720\n",
            "Epoch [1/200], Batch [194/425], Loss: 3.2013\n",
            "Epoch [1/200], Batch [195/425], Loss: 2.7423\n",
            "Epoch [1/200], Batch [196/425], Loss: 2.9054\n",
            "Epoch [1/200], Batch [197/425], Loss: 2.6723\n",
            "Epoch [1/200], Batch [198/425], Loss: 2.7233\n",
            "Epoch [1/200], Batch [199/425], Loss: 2.9171\n",
            "Epoch [1/200], Batch [200/425], Loss: 2.6984\n",
            "Epoch [1/200], Batch [201/425], Loss: 2.8130\n",
            "Epoch [1/200], Batch [202/425], Loss: 2.7048\n",
            "Epoch [1/200], Batch [203/425], Loss: 2.7914\n",
            "Epoch [1/200], Batch [204/425], Loss: 2.7752\n",
            "Epoch [1/200], Batch [205/425], Loss: 2.8236\n",
            "Epoch [1/200], Batch [206/425], Loss: 2.6445\n",
            "Epoch [1/200], Batch [207/425], Loss: 3.0123\n",
            "Epoch [1/200], Batch [208/425], Loss: 2.8488\n",
            "Epoch [1/200], Batch [209/425], Loss: 2.8473\n",
            "Epoch [1/200], Batch [210/425], Loss: 2.7375\n",
            "Epoch [1/200], Batch [211/425], Loss: 2.7840\n",
            "Epoch [1/200], Batch [212/425], Loss: 2.6464\n",
            "Epoch [1/200], Batch [213/425], Loss: 2.6350\n",
            "Epoch [1/200], Batch [214/425], Loss: 2.9188\n",
            "Epoch [1/200], Batch [215/425], Loss: 2.6499\n",
            "Epoch [1/200], Batch [216/425], Loss: 2.7595\n",
            "Epoch [1/200], Batch [217/425], Loss: 2.9852\n",
            "Epoch [1/200], Batch [218/425], Loss: 2.7084\n",
            "Epoch [1/200], Batch [219/425], Loss: 2.7896\n",
            "Epoch [1/200], Batch [220/425], Loss: 2.6658\n",
            "Epoch [1/200], Batch [221/425], Loss: 2.9854\n",
            "Epoch [1/200], Batch [222/425], Loss: 2.5347\n",
            "Epoch [1/200], Batch [223/425], Loss: 2.6794\n",
            "Epoch [1/200], Batch [224/425], Loss: 2.6718\n",
            "Epoch [1/200], Batch [225/425], Loss: 2.9290\n",
            "Epoch [1/200], Batch [226/425], Loss: 2.8010\n",
            "Epoch [1/200], Batch [227/425], Loss: 2.7276\n",
            "Epoch [1/200], Batch [228/425], Loss: 2.7823\n",
            "Epoch [1/200], Batch [229/425], Loss: 3.0862\n",
            "Epoch [1/200], Batch [230/425], Loss: 2.9091\n",
            "Epoch [1/200], Batch [231/425], Loss: 2.8427\n",
            "Epoch [1/200], Batch [232/425], Loss: 2.7617\n",
            "Epoch [1/200], Batch [233/425], Loss: 2.7287\n",
            "Epoch [1/200], Batch [234/425], Loss: 2.8937\n",
            "Epoch [1/200], Batch [235/425], Loss: 2.9760\n",
            "Epoch [1/200], Batch [236/425], Loss: 2.7495\n",
            "Epoch [1/200], Batch [237/425], Loss: 2.6010\n",
            "Epoch [1/200], Batch [238/425], Loss: 2.9395\n",
            "Epoch [1/200], Batch [239/425], Loss: 2.8499\n",
            "Epoch [1/200], Batch [240/425], Loss: 2.9604\n",
            "Epoch [1/200], Batch [241/425], Loss: 2.9471\n",
            "Epoch [1/200], Batch [242/425], Loss: 3.0410\n",
            "Epoch [1/200], Batch [243/425], Loss: 2.7711\n",
            "Epoch [1/200], Batch [244/425], Loss: 2.5942\n",
            "Epoch [1/200], Batch [245/425], Loss: 2.9714\n",
            "Epoch [1/200], Batch [246/425], Loss: 2.7576\n",
            "Epoch [1/200], Batch [247/425], Loss: 2.9041\n",
            "Epoch [1/200], Batch [248/425], Loss: 3.0507\n",
            "Epoch [1/200], Batch [249/425], Loss: 2.8060\n",
            "Epoch [1/200], Batch [250/425], Loss: 2.6777\n",
            "Epoch [1/200], Batch [251/425], Loss: 2.7693\n",
            "Epoch [1/200], Batch [252/425], Loss: 2.9468\n",
            "Epoch [1/200], Batch [253/425], Loss: 2.7151\n",
            "Epoch [1/200], Batch [254/425], Loss: 2.5837\n",
            "Epoch [1/200], Batch [255/425], Loss: 3.1010\n",
            "Epoch [1/200], Batch [256/425], Loss: 2.8097\n",
            "Epoch [1/200], Batch [257/425], Loss: 2.5965\n",
            "Epoch [1/200], Batch [258/425], Loss: 2.6335\n",
            "Epoch [1/200], Batch [259/425], Loss: 2.4124\n",
            "Epoch [1/200], Batch [260/425], Loss: 2.8078\n",
            "Epoch [1/200], Batch [261/425], Loss: 2.9122\n",
            "Epoch [1/200], Batch [262/425], Loss: 2.6307\n",
            "Epoch [1/200], Batch [263/425], Loss: 2.7447\n",
            "Epoch [1/200], Batch [264/425], Loss: 2.9107\n",
            "Epoch [1/200], Batch [265/425], Loss: 2.8002\n",
            "Epoch [1/200], Batch [266/425], Loss: 2.4987\n",
            "Epoch [1/200], Batch [267/425], Loss: 2.6377\n",
            "Epoch [1/200], Batch [268/425], Loss: 2.6678\n",
            "Epoch [1/200], Batch [269/425], Loss: 2.9349\n",
            "Epoch [1/200], Batch [270/425], Loss: 2.6993\n",
            "Epoch [1/200], Batch [271/425], Loss: 2.8938\n",
            "Epoch [1/200], Batch [272/425], Loss: 2.7306\n",
            "Epoch [1/200], Batch [273/425], Loss: 2.7555\n",
            "Epoch [1/200], Batch [274/425], Loss: 2.7494\n",
            "Epoch [1/200], Batch [275/425], Loss: 2.7184\n",
            "Epoch [1/200], Batch [276/425], Loss: 2.7302\n",
            "Epoch [1/200], Batch [277/425], Loss: 2.6706\n",
            "Epoch [1/200], Batch [278/425], Loss: 2.6965\n",
            "Epoch [1/200], Batch [279/425], Loss: 2.6875\n",
            "Epoch [1/200], Batch [280/425], Loss: 2.7222\n",
            "Epoch [1/200], Batch [281/425], Loss: 2.7892\n",
            "Epoch [1/200], Batch [282/425], Loss: 2.9101\n",
            "Epoch [1/200], Batch [283/425], Loss: 2.8646\n",
            "Epoch [1/200], Batch [284/425], Loss: 2.6176\n",
            "Epoch [1/200], Batch [285/425], Loss: 2.7596\n",
            "Epoch [1/200], Batch [286/425], Loss: 2.7735\n",
            "Epoch [1/200], Batch [287/425], Loss: 2.8026\n",
            "Epoch [1/200], Batch [288/425], Loss: 2.8002\n",
            "Epoch [1/200], Batch [289/425], Loss: 2.6976\n",
            "Epoch [1/200], Batch [290/425], Loss: 2.8616\n",
            "Epoch [1/200], Batch [291/425], Loss: 2.8336\n",
            "Epoch [1/200], Batch [292/425], Loss: 2.7306\n",
            "Epoch [1/200], Batch [293/425], Loss: 2.6689\n",
            "Epoch [1/200], Batch [294/425], Loss: 2.7858\n",
            "Epoch [1/200], Batch [295/425], Loss: 2.7503\n",
            "Epoch [1/200], Batch [296/425], Loss: 2.7657\n",
            "Epoch [1/200], Batch [297/425], Loss: 2.6474\n",
            "Epoch [1/200], Batch [298/425], Loss: 2.6857\n",
            "Epoch [1/200], Batch [299/425], Loss: 2.8426\n",
            "Epoch [1/200], Batch [300/425], Loss: 2.6738\n",
            "Epoch [1/200], Batch [301/425], Loss: 2.8784\n",
            "Epoch [1/200], Batch [302/425], Loss: 2.5339\n",
            "Epoch [1/200], Batch [303/425], Loss: 3.1100\n",
            "Epoch [1/200], Batch [304/425], Loss: 3.0797\n",
            "Epoch [1/200], Batch [305/425], Loss: 2.7812\n",
            "Epoch [1/200], Batch [306/425], Loss: 2.7066\n",
            "Epoch [1/200], Batch [307/425], Loss: 2.5198\n",
            "Epoch [1/200], Batch [308/425], Loss: 2.6432\n",
            "Epoch [1/200], Batch [309/425], Loss: 2.5973\n",
            "Epoch [1/200], Batch [310/425], Loss: 2.7840\n",
            "Epoch [1/200], Batch [311/425], Loss: 2.7450\n",
            "Epoch [1/200], Batch [312/425], Loss: 2.7830\n",
            "Epoch [1/200], Batch [313/425], Loss: 2.8169\n",
            "Epoch [1/200], Batch [314/425], Loss: 2.7175\n",
            "Epoch [1/200], Batch [315/425], Loss: 2.8635\n",
            "Epoch [1/200], Batch [316/425], Loss: 2.6929\n",
            "Epoch [1/200], Batch [317/425], Loss: 2.8795\n",
            "Epoch [1/200], Batch [318/425], Loss: 2.7942\n",
            "Epoch [1/200], Batch [319/425], Loss: 2.6839\n",
            "Epoch [1/200], Batch [320/425], Loss: 2.8404\n",
            "Epoch [1/200], Batch [321/425], Loss: 2.7984\n",
            "Epoch [1/200], Batch [322/425], Loss: 2.8221\n",
            "Epoch [1/200], Batch [323/425], Loss: 2.6698\n",
            "Epoch [1/200], Batch [324/425], Loss: 2.6453\n",
            "Epoch [1/200], Batch [325/425], Loss: 2.6900\n",
            "Epoch [1/200], Batch [326/425], Loss: 3.0499\n",
            "Epoch [1/200], Batch [327/425], Loss: 2.7717\n",
            "Epoch [1/200], Batch [328/425], Loss: 2.7307\n",
            "Epoch [1/200], Batch [329/425], Loss: 2.7844\n",
            "Epoch [1/200], Batch [330/425], Loss: 2.8658\n",
            "Epoch [1/200], Batch [331/425], Loss: 2.7598\n",
            "Epoch [1/200], Batch [332/425], Loss: 2.6845\n",
            "Epoch [1/200], Batch [333/425], Loss: 2.8857\n",
            "Epoch [1/200], Batch [334/425], Loss: 2.8624\n",
            "Epoch [1/200], Batch [335/425], Loss: 2.5221\n",
            "Epoch [1/200], Batch [336/425], Loss: 2.6446\n",
            "Epoch [1/200], Batch [337/425], Loss: 2.9557\n",
            "Epoch [1/200], Batch [338/425], Loss: 2.6797\n",
            "Epoch [1/200], Batch [339/425], Loss: 2.9898\n",
            "Epoch [1/200], Batch [340/425], Loss: 2.6126\n",
            "Epoch [1/200], Batch [341/425], Loss: 2.8292\n",
            "Epoch [1/200], Batch [342/425], Loss: 2.5764\n",
            "Epoch [1/200], Batch [343/425], Loss: 2.8172\n",
            "Epoch [1/200], Batch [344/425], Loss: 2.6851\n",
            "Epoch [1/200], Batch [345/425], Loss: 2.7396\n",
            "Epoch [1/200], Batch [346/425], Loss: 2.7665\n",
            "Epoch [1/200], Batch [347/425], Loss: 2.9126\n",
            "Epoch [1/200], Batch [348/425], Loss: 2.7113\n",
            "Epoch [1/200], Batch [349/425], Loss: 2.8592\n",
            "Epoch [1/200], Batch [350/425], Loss: 2.6003\n",
            "Epoch [1/200], Batch [351/425], Loss: 2.6597\n",
            "Epoch [1/200], Batch [352/425], Loss: 2.9108\n",
            "Epoch [1/200], Batch [353/425], Loss: 2.8339\n",
            "Epoch [1/200], Batch [354/425], Loss: 2.5388\n",
            "Epoch [1/200], Batch [355/425], Loss: 2.8518\n",
            "Epoch [1/200], Batch [356/425], Loss: 2.8301\n",
            "Epoch [1/200], Batch [357/425], Loss: 2.5902\n",
            "Epoch [1/200], Batch [358/425], Loss: 2.6813\n",
            "Epoch [1/200], Batch [359/425], Loss: 2.7027\n",
            "Epoch [1/200], Batch [360/425], Loss: 3.0135\n",
            "Epoch [1/200], Batch [361/425], Loss: 2.7227\n",
            "Epoch [1/200], Batch [362/425], Loss: 2.5853\n",
            "Epoch [1/200], Batch [363/425], Loss: 2.8548\n",
            "Epoch [1/200], Batch [364/425], Loss: 2.7872\n",
            "Epoch [1/200], Batch [365/425], Loss: 2.6717\n",
            "Epoch [1/200], Batch [366/425], Loss: 2.7042\n",
            "Epoch [1/200], Batch [367/425], Loss: 2.8330\n",
            "Epoch [1/200], Batch [368/425], Loss: 2.6672\n",
            "Epoch [1/200], Batch [369/425], Loss: 2.7237\n",
            "Epoch [1/200], Batch [370/425], Loss: 2.6410\n",
            "Epoch [1/200], Batch [371/425], Loss: 2.7419\n",
            "Epoch [1/200], Batch [372/425], Loss: 2.7407\n",
            "Epoch [1/200], Batch [373/425], Loss: 2.8024\n",
            "Epoch [1/200], Batch [374/425], Loss: 2.7858\n",
            "Epoch [1/200], Batch [375/425], Loss: 2.8187\n",
            "Epoch [1/200], Batch [376/425], Loss: 2.8188\n",
            "Epoch [1/200], Batch [377/425], Loss: 2.5844\n",
            "Epoch [1/200], Batch [378/425], Loss: 2.8502\n",
            "Epoch [1/200], Batch [379/425], Loss: 2.7766\n",
            "Epoch [1/200], Batch [380/425], Loss: 2.8802\n",
            "Epoch [1/200], Batch [381/425], Loss: 2.7322\n",
            "Epoch [1/200], Batch [382/425], Loss: 2.6652\n",
            "Epoch [1/200], Batch [383/425], Loss: 2.7685\n",
            "Epoch [1/200], Batch [384/425], Loss: 2.7927\n",
            "Epoch [1/200], Batch [385/425], Loss: 2.6500\n",
            "Epoch [1/200], Batch [386/425], Loss: 2.7152\n",
            "Epoch [1/200], Batch [387/425], Loss: 2.4435\n",
            "Epoch [1/200], Batch [388/425], Loss: 2.6053\n",
            "Epoch [1/200], Batch [389/425], Loss: 2.6460\n",
            "Epoch [1/200], Batch [390/425], Loss: 2.7790\n",
            "Epoch [1/200], Batch [391/425], Loss: 2.6017\n",
            "Epoch [1/200], Batch [392/425], Loss: 2.8575\n",
            "Epoch [1/200], Batch [393/425], Loss: 2.6550\n",
            "Epoch [1/200], Batch [394/425], Loss: 2.7091\n",
            "Epoch [1/200], Batch [395/425], Loss: 2.5597\n",
            "Epoch [1/200], Batch [396/425], Loss: 2.9099\n",
            "Epoch [1/200], Batch [397/425], Loss: 2.8168\n",
            "Epoch [1/200], Batch [398/425], Loss: 2.9384\n",
            "Epoch [1/200], Batch [399/425], Loss: 2.4792\n",
            "Epoch [1/200], Batch [400/425], Loss: 2.9304\n",
            "Epoch [1/200], Batch [401/425], Loss: 2.9393\n",
            "Epoch [1/200], Batch [402/425], Loss: 2.8759\n",
            "Epoch [1/200], Batch [403/425], Loss: 2.5812\n",
            "Epoch [1/200], Batch [404/425], Loss: 2.8870\n",
            "Epoch [1/200], Batch [405/425], Loss: 2.9041\n",
            "Epoch [1/200], Batch [406/425], Loss: 2.6879\n",
            "Epoch [1/200], Batch [407/425], Loss: 2.8889\n",
            "Epoch [1/200], Batch [408/425], Loss: 2.8785\n",
            "Epoch [1/200], Batch [409/425], Loss: 2.4676\n",
            "Epoch [1/200], Batch [410/425], Loss: 2.8095\n",
            "Epoch [1/200], Batch [411/425], Loss: 2.9013\n",
            "Epoch [1/200], Batch [412/425], Loss: 2.7412\n",
            "Epoch [1/200], Batch [413/425], Loss: 2.5241\n",
            "Epoch [1/200], Batch [414/425], Loss: 2.7435\n",
            "Epoch [1/200], Batch [415/425], Loss: 2.7127\n",
            "Epoch [1/200], Batch [416/425], Loss: 2.8557\n",
            "Epoch [1/200], Batch [417/425], Loss: 2.7122\n",
            "Epoch [1/200], Batch [418/425], Loss: 2.8049\n",
            "Epoch [1/200], Batch [419/425], Loss: 2.7684\n",
            "Epoch [1/200], Batch [420/425], Loss: 2.7354\n",
            "Epoch [1/200], Batch [421/425], Loss: 2.6642\n",
            "Epoch [1/200], Batch [422/425], Loss: 2.7805\n",
            "Epoch [1/200], Batch [423/425], Loss: 2.6858\n",
            "Epoch [1/200], Batch [424/425], Loss: 2.7694\n",
            "Epoch [1/200], Batch [425/425], Loss: 2.9067\n",
            "Epoch [2/200], Batch [1/425], Loss: 2.8425\n",
            "Epoch [2/200], Batch [2/425], Loss: 2.6052\n",
            "Epoch [2/200], Batch [3/425], Loss: 2.6330\n",
            "Epoch [2/200], Batch [4/425], Loss: 2.6034\n",
            "Epoch [2/200], Batch [5/425], Loss: 2.8368\n",
            "Epoch [2/200], Batch [6/425], Loss: 2.8014\n",
            "Epoch [2/200], Batch [7/425], Loss: 2.5224\n",
            "Epoch [2/200], Batch [8/425], Loss: 2.6853\n",
            "Epoch [2/200], Batch [9/425], Loss: 3.0072\n",
            "Epoch [2/200], Batch [10/425], Loss: 2.8000\n",
            "Epoch [2/200], Batch [11/425], Loss: 2.7110\n",
            "Epoch [2/200], Batch [12/425], Loss: 2.7738\n",
            "Epoch [2/200], Batch [13/425], Loss: 2.7711\n",
            "Epoch [2/200], Batch [14/425], Loss: 2.7859\n",
            "Epoch [2/200], Batch [15/425], Loss: 2.7477\n",
            "Epoch [2/200], Batch [16/425], Loss: 2.6037\n",
            "Epoch [2/200], Batch [17/425], Loss: 2.7456\n",
            "Epoch [2/200], Batch [18/425], Loss: 2.8620\n",
            "Epoch [2/200], Batch [19/425], Loss: 2.8297\n",
            "Epoch [2/200], Batch [20/425], Loss: 2.6854\n",
            "Epoch [2/200], Batch [21/425], Loss: 2.6209\n",
            "Epoch [2/200], Batch [22/425], Loss: 2.5998\n",
            "Epoch [2/200], Batch [23/425], Loss: 2.6958\n",
            "Epoch [2/200], Batch [24/425], Loss: 2.6908\n",
            "Epoch [2/200], Batch [25/425], Loss: 2.8056\n",
            "Epoch [2/200], Batch [26/425], Loss: 2.7677\n",
            "Epoch [2/200], Batch [27/425], Loss: 2.7434\n",
            "Epoch [2/200], Batch [28/425], Loss: 2.7252\n",
            "Epoch [2/200], Batch [29/425], Loss: 2.8752\n",
            "Epoch [2/200], Batch [30/425], Loss: 2.7574\n",
            "Epoch [2/200], Batch [31/425], Loss: 2.5830\n",
            "Epoch [2/200], Batch [32/425], Loss: 2.7431\n",
            "Epoch [2/200], Batch [33/425], Loss: 2.7277\n",
            "Epoch [2/200], Batch [34/425], Loss: 2.6425\n",
            "Epoch [2/200], Batch [35/425], Loss: 2.8713\n",
            "Epoch [2/200], Batch [36/425], Loss: 2.4787\n",
            "Epoch [2/200], Batch [37/425], Loss: 2.8567\n",
            "Epoch [2/200], Batch [38/425], Loss: 2.5718\n",
            "Epoch [2/200], Batch [39/425], Loss: 2.7448\n",
            "Epoch [2/200], Batch [40/425], Loss: 2.8196\n",
            "Epoch [2/200], Batch [41/425], Loss: 2.6950\n",
            "Epoch [2/200], Batch [42/425], Loss: 2.8959\n",
            "Epoch [2/200], Batch [43/425], Loss: 2.3199\n",
            "Epoch [2/200], Batch [44/425], Loss: 2.8382\n",
            "Epoch [2/200], Batch [45/425], Loss: 2.6844\n",
            "Epoch [2/200], Batch [46/425], Loss: 2.6561\n",
            "Epoch [2/200], Batch [47/425], Loss: 2.8028\n",
            "Epoch [2/200], Batch [48/425], Loss: 2.7482\n",
            "Epoch [2/200], Batch [49/425], Loss: 2.8427\n",
            "Epoch [2/200], Batch [50/425], Loss: 2.9774\n",
            "Epoch [2/200], Batch [51/425], Loss: 2.7359\n",
            "Epoch [2/200], Batch [52/425], Loss: 2.5403\n",
            "Epoch [2/200], Batch [53/425], Loss: 2.7848\n",
            "Epoch [2/200], Batch [54/425], Loss: 2.8401\n",
            "Epoch [2/200], Batch [55/425], Loss: 2.6110\n",
            "Epoch [2/200], Batch [56/425], Loss: 2.7923\n",
            "Epoch [2/200], Batch [57/425], Loss: 2.7366\n",
            "Epoch [2/200], Batch [58/425], Loss: 2.8098\n",
            "Epoch [2/200], Batch [59/425], Loss: 2.7472\n",
            "Epoch [2/200], Batch [60/425], Loss: 2.8529\n",
            "Epoch [2/200], Batch [61/425], Loss: 2.4716\n",
            "Epoch [2/200], Batch [62/425], Loss: 2.8000\n",
            "Epoch [2/200], Batch [63/425], Loss: 2.8932\n",
            "Epoch [2/200], Batch [64/425], Loss: 2.4468\n",
            "Epoch [2/200], Batch [65/425], Loss: 2.7117\n",
            "Epoch [2/200], Batch [66/425], Loss: 2.5560\n",
            "Epoch [2/200], Batch [67/425], Loss: 2.8264\n",
            "Epoch [2/200], Batch [68/425], Loss: 2.6907\n",
            "Epoch [2/200], Batch [69/425], Loss: 2.7284\n",
            "Epoch [2/200], Batch [70/425], Loss: 2.7874\n",
            "Epoch [2/200], Batch [71/425], Loss: 2.6006\n",
            "Epoch [2/200], Batch [72/425], Loss: 2.8158\n",
            "Epoch [2/200], Batch [73/425], Loss: 2.6862\n",
            "Epoch [2/200], Batch [74/425], Loss: 2.8009\n",
            "Epoch [2/200], Batch [75/425], Loss: 2.6385\n",
            "Epoch [2/200], Batch [76/425], Loss: 2.5429\n",
            "Epoch [2/200], Batch [77/425], Loss: 2.6975\n",
            "Epoch [2/200], Batch [78/425], Loss: 2.5459\n",
            "Epoch [2/200], Batch [79/425], Loss: 2.6766\n",
            "Epoch [2/200], Batch [80/425], Loss: 2.6344\n",
            "Epoch [2/200], Batch [81/425], Loss: 2.7986\n",
            "Epoch [2/200], Batch [82/425], Loss: 2.6779\n",
            "Epoch [2/200], Batch [83/425], Loss: 2.7005\n",
            "Epoch [2/200], Batch [84/425], Loss: 2.6171\n",
            "Epoch [2/200], Batch [85/425], Loss: 2.6663\n",
            "Epoch [2/200], Batch [86/425], Loss: 2.6394\n",
            "Epoch [2/200], Batch [87/425], Loss: 2.6592\n",
            "Epoch [2/200], Batch [88/425], Loss: 2.9186\n",
            "Epoch [2/200], Batch [89/425], Loss: 2.8845\n",
            "Epoch [2/200], Batch [90/425], Loss: 2.7224\n",
            "Epoch [2/200], Batch [91/425], Loss: 2.7569\n",
            "Epoch [2/200], Batch [92/425], Loss: 2.5617\n",
            "Epoch [2/200], Batch [93/425], Loss: 2.5248\n",
            "Epoch [2/200], Batch [94/425], Loss: 2.7622\n",
            "Epoch [2/200], Batch [95/425], Loss: 2.6003\n",
            "Epoch [2/200], Batch [96/425], Loss: 2.8257\n",
            "Epoch [2/200], Batch [97/425], Loss: 2.7781\n",
            "Epoch [2/200], Batch [98/425], Loss: 2.7954\n",
            "Epoch [2/200], Batch [99/425], Loss: 2.7946\n",
            "Epoch [2/200], Batch [100/425], Loss: 2.6908\n",
            "Epoch [2/200], Batch [101/425], Loss: 2.7174\n",
            "Epoch [2/200], Batch [102/425], Loss: 2.7954\n",
            "Epoch [2/200], Batch [103/425], Loss: 2.5926\n",
            "Epoch [2/200], Batch [104/425], Loss: 2.7145\n",
            "Epoch [2/200], Batch [105/425], Loss: 2.7735\n",
            "Epoch [2/200], Batch [106/425], Loss: 2.7480\n",
            "Epoch [2/200], Batch [107/425], Loss: 2.6783\n",
            "Epoch [2/200], Batch [108/425], Loss: 2.9873\n",
            "Epoch [2/200], Batch [109/425], Loss: 2.6963\n",
            "Epoch [2/200], Batch [110/425], Loss: 2.5779\n",
            "Epoch [2/200], Batch [111/425], Loss: 2.6753\n",
            "Epoch [2/200], Batch [112/425], Loss: 2.5298\n",
            "Epoch [2/200], Batch [113/425], Loss: 2.5398\n",
            "Epoch [2/200], Batch [114/425], Loss: 2.7033\n",
            "Epoch [2/200], Batch [115/425], Loss: 2.8123\n",
            "Epoch [2/200], Batch [116/425], Loss: 2.9402\n",
            "Epoch [2/200], Batch [117/425], Loss: 2.7462\n",
            "Epoch [2/200], Batch [118/425], Loss: 2.6655\n",
            "Epoch [2/200], Batch [119/425], Loss: 2.6790\n",
            "Epoch [2/200], Batch [120/425], Loss: 2.7779\n",
            "Epoch [2/200], Batch [121/425], Loss: 2.6530\n",
            "Epoch [2/200], Batch [122/425], Loss: 2.6269\n",
            "Epoch [2/200], Batch [123/425], Loss: 2.5963\n",
            "Epoch [2/200], Batch [124/425], Loss: 2.6413\n",
            "Epoch [2/200], Batch [125/425], Loss: 2.5318\n",
            "Epoch [2/200], Batch [126/425], Loss: 2.8352\n",
            "Epoch [2/200], Batch [127/425], Loss: 2.6933\n",
            "Epoch [2/200], Batch [128/425], Loss: 2.6188\n",
            "Epoch [2/200], Batch [129/425], Loss: 2.7274\n",
            "Epoch [2/200], Batch [130/425], Loss: 2.4552\n",
            "Epoch [2/200], Batch [131/425], Loss: 2.6123\n",
            "Epoch [2/200], Batch [132/425], Loss: 2.7603\n",
            "Epoch [2/200], Batch [133/425], Loss: 2.6635\n",
            "Epoch [2/200], Batch [134/425], Loss: 2.8078\n",
            "Epoch [2/200], Batch [135/425], Loss: 2.7003\n",
            "Epoch [2/200], Batch [136/425], Loss: 2.6594\n",
            "Epoch [2/200], Batch [137/425], Loss: 2.6669\n",
            "Epoch [2/200], Batch [138/425], Loss: 2.6940\n",
            "Epoch [2/200], Batch [139/425], Loss: 2.6504\n",
            "Epoch [2/200], Batch [140/425], Loss: 2.5480\n",
            "Epoch [2/200], Batch [141/425], Loss: 2.7102\n",
            "Epoch [2/200], Batch [142/425], Loss: 2.6414\n",
            "Epoch [2/200], Batch [143/425], Loss: 2.7568\n",
            "Epoch [2/200], Batch [144/425], Loss: 2.4907\n",
            "Epoch [2/200], Batch [145/425], Loss: 2.8009\n",
            "Epoch [2/200], Batch [146/425], Loss: 2.5903\n",
            "Epoch [2/200], Batch [147/425], Loss: 2.7709\n",
            "Epoch [2/200], Batch [148/425], Loss: 2.6786\n",
            "Epoch [2/200], Batch [149/425], Loss: 2.7380\n",
            "Epoch [2/200], Batch [150/425], Loss: 2.5384\n",
            "Epoch [2/200], Batch [151/425], Loss: 2.7319\n",
            "Epoch [2/200], Batch [152/425], Loss: 2.7719\n",
            "Epoch [2/200], Batch [153/425], Loss: 2.7819\n",
            "Epoch [2/200], Batch [154/425], Loss: 2.8725\n",
            "Epoch [2/200], Batch [155/425], Loss: 2.7247\n",
            "Epoch [2/200], Batch [156/425], Loss: 2.5553\n",
            "Epoch [2/200], Batch [157/425], Loss: 2.9215\n",
            "Epoch [2/200], Batch [158/425], Loss: 2.9014\n",
            "Epoch [2/200], Batch [159/425], Loss: 2.6574\n",
            "Epoch [2/200], Batch [160/425], Loss: 2.6505\n",
            "Epoch [2/200], Batch [161/425], Loss: 2.6272\n",
            "Epoch [2/200], Batch [162/425], Loss: 2.7094\n",
            "Epoch [2/200], Batch [163/425], Loss: 2.6848\n",
            "Epoch [2/200], Batch [164/425], Loss: 2.6556\n",
            "Epoch [2/200], Batch [165/425], Loss: 2.6336\n",
            "Epoch [2/200], Batch [166/425], Loss: 2.6864\n",
            "Epoch [2/200], Batch [167/425], Loss: 2.8879\n",
            "Epoch [2/200], Batch [168/425], Loss: 2.7994\n",
            "Epoch [2/200], Batch [169/425], Loss: 2.6076\n",
            "Epoch [2/200], Batch [170/425], Loss: 2.6786\n",
            "Epoch [2/200], Batch [171/425], Loss: 2.6269\n",
            "Epoch [2/200], Batch [172/425], Loss: 2.6717\n",
            "Epoch [2/200], Batch [173/425], Loss: 2.7153\n",
            "Epoch [2/200], Batch [174/425], Loss: 2.7142\n",
            "Epoch [2/200], Batch [175/425], Loss: 2.7821\n",
            "Epoch [2/200], Batch [176/425], Loss: 2.6758\n",
            "Epoch [2/200], Batch [177/425], Loss: 2.8517\n",
            "Epoch [2/200], Batch [178/425], Loss: 2.6705\n",
            "Epoch [2/200], Batch [179/425], Loss: 2.7230\n",
            "Epoch [2/200], Batch [180/425], Loss: 2.7752\n",
            "Epoch [2/200], Batch [181/425], Loss: 2.5848\n",
            "Epoch [2/200], Batch [182/425], Loss: 2.8416\n",
            "Epoch [2/200], Batch [183/425], Loss: 2.8330\n",
            "Epoch [2/200], Batch [184/425], Loss: 2.9059\n",
            "Epoch [2/200], Batch [185/425], Loss: 2.7639\n",
            "Epoch [2/200], Batch [186/425], Loss: 2.7461\n",
            "Epoch [2/200], Batch [187/425], Loss: 2.6388\n",
            "Epoch [2/200], Batch [188/425], Loss: 2.6315\n",
            "Epoch [2/200], Batch [189/425], Loss: 2.6619\n",
            "Epoch [2/200], Batch [190/425], Loss: 2.4132\n",
            "Epoch [2/200], Batch [191/425], Loss: 2.6496\n",
            "Epoch [2/200], Batch [192/425], Loss: 2.5414\n",
            "Epoch [2/200], Batch [193/425], Loss: 2.5205\n",
            "Epoch [2/200], Batch [194/425], Loss: 2.6017\n",
            "Epoch [2/200], Batch [195/425], Loss: 2.5650\n",
            "Epoch [2/200], Batch [196/425], Loss: 2.5338\n",
            "Epoch [2/200], Batch [197/425], Loss: 2.7283\n",
            "Epoch [2/200], Batch [198/425], Loss: 2.8476\n",
            "Epoch [2/200], Batch [199/425], Loss: 2.6187\n",
            "Epoch [2/200], Batch [200/425], Loss: 2.7494\n",
            "Epoch [2/200], Batch [201/425], Loss: 2.8008\n",
            "Epoch [2/200], Batch [202/425], Loss: 2.6350\n",
            "Epoch [2/200], Batch [203/425], Loss: 2.5837\n",
            "Epoch [2/200], Batch [204/425], Loss: 2.8714\n",
            "Epoch [2/200], Batch [205/425], Loss: 2.6922\n",
            "Epoch [2/200], Batch [206/425], Loss: 2.5099\n",
            "Epoch [2/200], Batch [207/425], Loss: 2.7786\n",
            "Epoch [2/200], Batch [208/425], Loss: 2.6709\n",
            "Epoch [2/200], Batch [209/425], Loss: 2.6643\n",
            "Epoch [2/200], Batch [210/425], Loss: 2.5656\n",
            "Epoch [2/200], Batch [211/425], Loss: 2.6084\n",
            "Epoch [2/200], Batch [212/425], Loss: 2.5042\n",
            "Epoch [2/200], Batch [213/425], Loss: 2.6918\n",
            "Epoch [2/200], Batch [214/425], Loss: 2.6836\n",
            "Epoch [2/200], Batch [215/425], Loss: 2.5797\n",
            "Epoch [2/200], Batch [216/425], Loss: 2.6786\n",
            "Epoch [2/200], Batch [217/425], Loss: 2.6569\n",
            "Epoch [2/200], Batch [218/425], Loss: 2.6883\n",
            "Epoch [2/200], Batch [219/425], Loss: 2.6985\n",
            "Epoch [2/200], Batch [220/425], Loss: 2.9385\n",
            "Epoch [2/200], Batch [221/425], Loss: 2.7304\n",
            "Epoch [2/200], Batch [222/425], Loss: 2.6765\n",
            "Epoch [2/200], Batch [223/425], Loss: 2.5346\n",
            "Epoch [2/200], Batch [224/425], Loss: 2.4648\n",
            "Epoch [2/200], Batch [225/425], Loss: 2.6304\n",
            "Epoch [2/200], Batch [226/425], Loss: 2.8606\n",
            "Epoch [2/200], Batch [227/425], Loss: 2.6071\n",
            "Epoch [2/200], Batch [228/425], Loss: 2.8233\n",
            "Epoch [2/200], Batch [229/425], Loss: 2.6810\n",
            "Epoch [2/200], Batch [230/425], Loss: 2.7351\n",
            "Epoch [2/200], Batch [231/425], Loss: 2.6468\n",
            "Epoch [2/200], Batch [232/425], Loss: 2.5821\n",
            "Epoch [2/200], Batch [233/425], Loss: 2.6678\n",
            "Epoch [2/200], Batch [234/425], Loss: 2.6710\n",
            "Epoch [2/200], Batch [235/425], Loss: 2.7577\n",
            "Epoch [2/200], Batch [236/425], Loss: 2.7588\n",
            "Epoch [2/200], Batch [237/425], Loss: 2.6744\n",
            "Epoch [2/200], Batch [238/425], Loss: 2.8571\n",
            "Epoch [2/200], Batch [239/425], Loss: 2.8380\n",
            "Epoch [2/200], Batch [240/425], Loss: 2.6310\n",
            "Epoch [2/200], Batch [241/425], Loss: 2.6583\n",
            "Epoch [2/200], Batch [242/425], Loss: 2.6758\n",
            "Epoch [2/200], Batch [243/425], Loss: 2.5854\n",
            "Epoch [2/200], Batch [244/425], Loss: 2.6468\n",
            "Epoch [2/200], Batch [245/425], Loss: 2.6307\n",
            "Epoch [2/200], Batch [246/425], Loss: 2.4076\n",
            "Epoch [2/200], Batch [247/425], Loss: 2.5100\n",
            "Epoch [2/200], Batch [248/425], Loss: 2.3164\n",
            "Epoch [2/200], Batch [249/425], Loss: 2.8050\n",
            "Epoch [2/200], Batch [250/425], Loss: 2.8863\n",
            "Epoch [2/200], Batch [251/425], Loss: 2.8573\n",
            "Epoch [2/200], Batch [252/425], Loss: 2.6852\n",
            "Epoch [2/200], Batch [253/425], Loss: 2.5784\n",
            "Epoch [2/200], Batch [254/425], Loss: 2.4802\n",
            "Epoch [2/200], Batch [255/425], Loss: 2.6224\n",
            "Epoch [2/200], Batch [256/425], Loss: 2.6031\n",
            "Epoch [2/200], Batch [257/425], Loss: 2.7306\n",
            "Epoch [2/200], Batch [258/425], Loss: 2.6501\n",
            "Epoch [2/200], Batch [259/425], Loss: 2.4531\n",
            "Epoch [2/200], Batch [260/425], Loss: 2.5463\n",
            "Epoch [2/200], Batch [261/425], Loss: 2.5297\n",
            "Epoch [2/200], Batch [262/425], Loss: 2.6325\n",
            "Epoch [2/200], Batch [263/425], Loss: 2.4811\n",
            "Epoch [2/200], Batch [264/425], Loss: 2.5945\n",
            "Epoch [2/200], Batch [265/425], Loss: 2.4730\n",
            "Epoch [2/200], Batch [266/425], Loss: 2.6103\n",
            "Epoch [2/200], Batch [267/425], Loss: 2.7380\n",
            "Epoch [2/200], Batch [268/425], Loss: 2.6785\n",
            "Epoch [2/200], Batch [269/425], Loss: 2.7012\n",
            "Epoch [2/200], Batch [270/425], Loss: 2.4344\n",
            "Epoch [2/200], Batch [271/425], Loss: 2.7658\n",
            "Epoch [2/200], Batch [272/425], Loss: 2.7652\n",
            "Epoch [2/200], Batch [273/425], Loss: 2.5862\n",
            "Epoch [2/200], Batch [274/425], Loss: 2.7212\n",
            "Epoch [2/200], Batch [275/425], Loss: 2.8004\n",
            "Epoch [2/200], Batch [276/425], Loss: 2.5740\n",
            "Epoch [2/200], Batch [277/425], Loss: 2.6822\n",
            "Epoch [2/200], Batch [278/425], Loss: 2.6420\n",
            "Epoch [2/200], Batch [279/425], Loss: 2.7009\n",
            "Epoch [2/200], Batch [280/425], Loss: 2.5654\n",
            "Epoch [2/200], Batch [281/425], Loss: 2.6875\n",
            "Epoch [2/200], Batch [282/425], Loss: 2.6099\n",
            "Epoch [2/200], Batch [283/425], Loss: 2.6443\n",
            "Epoch [2/200], Batch [284/425], Loss: 2.5715\n",
            "Epoch [2/200], Batch [285/425], Loss: 2.6993\n",
            "Epoch [2/200], Batch [286/425], Loss: 2.6778\n",
            "Epoch [2/200], Batch [287/425], Loss: 2.4855\n",
            "Epoch [2/200], Batch [288/425], Loss: 2.4902\n",
            "Epoch [2/200], Batch [289/425], Loss: 2.7920\n",
            "Epoch [2/200], Batch [290/425], Loss: 2.5782\n",
            "Epoch [2/200], Batch [291/425], Loss: 2.4387\n",
            "Epoch [2/200], Batch [292/425], Loss: 2.6179\n",
            "Epoch [2/200], Batch [293/425], Loss: 2.6526\n",
            "Epoch [2/200], Batch [294/425], Loss: 2.6886\n",
            "Epoch [2/200], Batch [295/425], Loss: 2.7454\n",
            "Epoch [2/200], Batch [296/425], Loss: 2.7044\n",
            "Epoch [2/200], Batch [297/425], Loss: 2.9642\n",
            "Epoch [2/200], Batch [298/425], Loss: 2.8910\n",
            "Epoch [2/200], Batch [299/425], Loss: 2.4301\n",
            "Epoch [2/200], Batch [300/425], Loss: 2.6415\n",
            "Epoch [2/200], Batch [301/425], Loss: 2.6178\n",
            "Epoch [2/200], Batch [302/425], Loss: 2.5879\n",
            "Epoch [2/200], Batch [303/425], Loss: 2.6749\n",
            "Epoch [2/200], Batch [304/425], Loss: 2.5829\n",
            "Epoch [2/200], Batch [305/425], Loss: 2.7326\n",
            "Epoch [2/200], Batch [306/425], Loss: 2.8685\n",
            "Epoch [2/200], Batch [307/425], Loss: 2.6387\n",
            "Epoch [2/200], Batch [308/425], Loss: 2.5469\n",
            "Epoch [2/200], Batch [309/425], Loss: 2.8021\n",
            "Epoch [2/200], Batch [310/425], Loss: 2.7768\n",
            "Epoch [2/200], Batch [311/425], Loss: 2.6619\n",
            "Epoch [2/200], Batch [312/425], Loss: 2.7132\n",
            "Epoch [2/200], Batch [313/425], Loss: 2.7510\n",
            "Epoch [2/200], Batch [314/425], Loss: 2.6829\n",
            "Epoch [2/200], Batch [315/425], Loss: 2.4807\n",
            "Epoch [2/200], Batch [316/425], Loss: 2.7375\n",
            "Epoch [2/200], Batch [317/425], Loss: 2.5236\n",
            "Epoch [2/200], Batch [318/425], Loss: 2.5295\n",
            "Epoch [2/200], Batch [319/425], Loss: 2.6442\n",
            "Epoch [2/200], Batch [320/425], Loss: 2.6054\n",
            "Epoch [2/200], Batch [321/425], Loss: 2.5003\n",
            "Epoch [2/200], Batch [322/425], Loss: 2.4188\n",
            "Epoch [2/200], Batch [323/425], Loss: 2.5809\n",
            "Epoch [2/200], Batch [324/425], Loss: 2.6151\n",
            "Epoch [2/200], Batch [325/425], Loss: 2.6998\n",
            "Epoch [2/200], Batch [326/425], Loss: 2.5883\n",
            "Epoch [2/200], Batch [327/425], Loss: 2.5117\n",
            "Epoch [2/200], Batch [328/425], Loss: 2.4704\n",
            "Epoch [2/200], Batch [329/425], Loss: 2.6533\n",
            "Epoch [2/200], Batch [330/425], Loss: 2.7673\n",
            "Epoch [2/200], Batch [331/425], Loss: 2.9275\n",
            "Epoch [2/200], Batch [332/425], Loss: 2.8047\n",
            "Epoch [2/200], Batch [333/425], Loss: 2.7781\n",
            "Epoch [2/200], Batch [334/425], Loss: 2.7614\n",
            "Epoch [2/200], Batch [335/425], Loss: 2.6162\n",
            "Epoch [2/200], Batch [336/425], Loss: 2.5569\n",
            "Epoch [2/200], Batch [337/425], Loss: 2.7046\n",
            "Epoch [2/200], Batch [338/425], Loss: 2.7333\n",
            "Epoch [2/200], Batch [339/425], Loss: 2.5287\n",
            "Epoch [2/200], Batch [340/425], Loss: 2.8283\n",
            "Epoch [2/200], Batch [341/425], Loss: 2.6164\n",
            "Epoch [2/200], Batch [342/425], Loss: 2.6560\n",
            "Epoch [2/200], Batch [343/425], Loss: 2.7410\n",
            "Epoch [2/200], Batch [344/425], Loss: 2.5298\n",
            "Epoch [2/200], Batch [345/425], Loss: 2.8013\n",
            "Epoch [2/200], Batch [346/425], Loss: 2.6294\n",
            "Epoch [2/200], Batch [347/425], Loss: 2.6779\n",
            "Epoch [2/200], Batch [348/425], Loss: 2.7903\n",
            "Epoch [2/200], Batch [349/425], Loss: 2.6513\n",
            "Epoch [2/200], Batch [350/425], Loss: 2.6436\n",
            "Epoch [2/200], Batch [351/425], Loss: 2.6644\n",
            "Epoch [2/200], Batch [352/425], Loss: 2.6802\n",
            "Epoch [2/200], Batch [353/425], Loss: 2.7119\n",
            "Epoch [2/200], Batch [354/425], Loss: 2.5646\n",
            "Epoch [2/200], Batch [355/425], Loss: 2.6037\n",
            "Epoch [2/200], Batch [356/425], Loss: 2.6254\n",
            "Epoch [2/200], Batch [357/425], Loss: 2.7283\n",
            "Epoch [2/200], Batch [358/425], Loss: 2.5521\n",
            "Epoch [2/200], Batch [359/425], Loss: 2.6615\n",
            "Epoch [2/200], Batch [360/425], Loss: 2.5811\n",
            "Epoch [2/200], Batch [361/425], Loss: 2.5043\n",
            "Epoch [2/200], Batch [362/425], Loss: 2.9749\n",
            "Epoch [2/200], Batch [363/425], Loss: 2.7200\n",
            "Epoch [2/200], Batch [364/425], Loss: 2.5593\n",
            "Epoch [2/200], Batch [365/425], Loss: 2.6924\n",
            "Epoch [2/200], Batch [366/425], Loss: 2.5277\n",
            "Epoch [2/200], Batch [367/425], Loss: 2.6746\n",
            "Epoch [2/200], Batch [368/425], Loss: 2.6938\n",
            "Epoch [2/200], Batch [369/425], Loss: 2.7064\n",
            "Epoch [2/200], Batch [370/425], Loss: 2.6939\n",
            "Epoch [2/200], Batch [371/425], Loss: 2.5839\n",
            "Epoch [2/200], Batch [372/425], Loss: 2.6415\n",
            "Epoch [2/200], Batch [373/425], Loss: 2.6733\n",
            "Epoch [2/200], Batch [374/425], Loss: 2.6595\n",
            "Epoch [2/200], Batch [375/425], Loss: 2.8117\n",
            "Epoch [2/200], Batch [376/425], Loss: 2.5708\n",
            "Epoch [2/200], Batch [377/425], Loss: 2.5848\n",
            "Epoch [2/200], Batch [378/425], Loss: 2.5831\n",
            "Epoch [2/200], Batch [379/425], Loss: 2.5711\n",
            "Epoch [2/200], Batch [380/425], Loss: 2.4573\n",
            "Epoch [2/200], Batch [381/425], Loss: 2.7559\n",
            "Epoch [2/200], Batch [382/425], Loss: 2.6317\n",
            "Epoch [2/200], Batch [383/425], Loss: 2.6404\n",
            "Epoch [2/200], Batch [384/425], Loss: 2.8108\n",
            "Epoch [2/200], Batch [385/425], Loss: 2.7085\n",
            "Epoch [2/200], Batch [386/425], Loss: 2.6988\n",
            "Epoch [2/200], Batch [387/425], Loss: 2.4929\n",
            "Epoch [2/200], Batch [388/425], Loss: 2.4872\n",
            "Epoch [2/200], Batch [389/425], Loss: 2.7306\n",
            "Epoch [2/200], Batch [390/425], Loss: 2.4534\n",
            "Epoch [2/200], Batch [391/425], Loss: 2.6889\n",
            "Epoch [2/200], Batch [392/425], Loss: 2.5741\n",
            "Epoch [2/200], Batch [393/425], Loss: 2.7881\n",
            "Epoch [2/200], Batch [394/425], Loss: 2.7594\n",
            "Epoch [2/200], Batch [395/425], Loss: 2.7019\n",
            "Epoch [2/200], Batch [396/425], Loss: 2.4698\n",
            "Epoch [2/200], Batch [397/425], Loss: 2.7591\n",
            "Epoch [2/200], Batch [398/425], Loss: 2.8346\n",
            "Epoch [2/200], Batch [399/425], Loss: 2.5015\n",
            "Epoch [2/200], Batch [400/425], Loss: 2.6104\n",
            "Epoch [2/200], Batch [401/425], Loss: 2.8279\n",
            "Epoch [2/200], Batch [402/425], Loss: 2.4300\n",
            "Epoch [2/200], Batch [403/425], Loss: 2.2201\n",
            "Epoch [2/200], Batch [404/425], Loss: 2.4827\n",
            "Epoch [2/200], Batch [405/425], Loss: 2.8108\n",
            "Epoch [2/200], Batch [406/425], Loss: 2.5706\n",
            "Epoch [2/200], Batch [407/425], Loss: 2.6834\n",
            "Epoch [2/200], Batch [408/425], Loss: 2.7010\n",
            "Epoch [2/200], Batch [409/425], Loss: 2.4807\n",
            "Epoch [2/200], Batch [410/425], Loss: 2.5792\n",
            "Epoch [2/200], Batch [411/425], Loss: 2.5765\n",
            "Epoch [2/200], Batch [412/425], Loss: 2.7262\n",
            "Epoch [2/200], Batch [413/425], Loss: 2.7685\n",
            "Epoch [2/200], Batch [414/425], Loss: 2.8462\n",
            "Epoch [2/200], Batch [415/425], Loss: 2.7295\n",
            "Epoch [2/200], Batch [416/425], Loss: 2.6723\n",
            "Epoch [2/200], Batch [417/425], Loss: 2.5712\n",
            "Epoch [2/200], Batch [418/425], Loss: 2.6706\n",
            "Epoch [2/200], Batch [419/425], Loss: 2.7000\n",
            "Epoch [2/200], Batch [420/425], Loss: 2.5676\n",
            "Epoch [2/200], Batch [421/425], Loss: 2.6015\n",
            "Epoch [2/200], Batch [422/425], Loss: 2.4620\n",
            "Epoch [2/200], Batch [423/425], Loss: 2.5321\n",
            "Epoch [2/200], Batch [424/425], Loss: 2.3137\n",
            "Epoch [2/200], Batch [425/425], Loss: 2.6813\n",
            "Epoch [3/200], Batch [1/425], Loss: 2.5718\n",
            "Epoch [3/200], Batch [2/425], Loss: 2.6269\n",
            "Epoch [3/200], Batch [3/425], Loss: 2.8061\n",
            "Epoch [3/200], Batch [4/425], Loss: 2.6343\n",
            "Epoch [3/200], Batch [5/425], Loss: 2.6257\n",
            "Epoch [3/200], Batch [6/425], Loss: 2.6007\n",
            "Epoch [3/200], Batch [7/425], Loss: 2.6035\n",
            "Epoch [3/200], Batch [8/425], Loss: 2.5886\n",
            "Epoch [3/200], Batch [9/425], Loss: 2.5499\n",
            "Epoch [3/200], Batch [10/425], Loss: 2.8437\n",
            "Epoch [3/200], Batch [11/425], Loss: 2.6202\n",
            "Epoch [3/200], Batch [12/425], Loss: 2.6800\n",
            "Epoch [3/200], Batch [13/425], Loss: 2.7098\n",
            "Epoch [3/200], Batch [14/425], Loss: 2.7419\n",
            "Epoch [3/200], Batch [15/425], Loss: 2.8706\n",
            "Epoch [3/200], Batch [16/425], Loss: 2.7631\n",
            "Epoch [3/200], Batch [17/425], Loss: 2.7942\n",
            "Epoch [3/200], Batch [18/425], Loss: 2.6111\n",
            "Epoch [3/200], Batch [19/425], Loss: 2.7861\n",
            "Epoch [3/200], Batch [20/425], Loss: 2.4514\n",
            "Epoch [3/200], Batch [21/425], Loss: 2.6245\n",
            "Epoch [3/200], Batch [22/425], Loss: 2.7116\n",
            "Epoch [3/200], Batch [23/425], Loss: 2.7426\n",
            "Epoch [3/200], Batch [24/425], Loss: 2.7738\n",
            "Epoch [3/200], Batch [25/425], Loss: 2.5534\n",
            "Epoch [3/200], Batch [26/425], Loss: 2.5194\n",
            "Epoch [3/200], Batch [27/425], Loss: 2.8379\n",
            "Epoch [3/200], Batch [28/425], Loss: 2.5663\n",
            "Epoch [3/200], Batch [29/425], Loss: 2.6726\n",
            "Epoch [3/200], Batch [30/425], Loss: 2.7736\n",
            "Epoch [3/200], Batch [31/425], Loss: 2.5887\n",
            "Epoch [3/200], Batch [32/425], Loss: 2.6760\n",
            "Epoch [3/200], Batch [33/425], Loss: 2.6613\n",
            "Epoch [3/200], Batch [34/425], Loss: 2.8400\n",
            "Epoch [3/200], Batch [35/425], Loss: 2.6662\n",
            "Epoch [3/200], Batch [36/425], Loss: 2.6248\n",
            "Epoch [3/200], Batch [37/425], Loss: 2.7403\n",
            "Epoch [3/200], Batch [38/425], Loss: 2.6109\n",
            "Epoch [3/200], Batch [39/425], Loss: 2.7237\n",
            "Epoch [3/200], Batch [40/425], Loss: 2.7287\n",
            "Epoch [3/200], Batch [41/425], Loss: 2.5535\n",
            "Epoch [3/200], Batch [42/425], Loss: 2.6070\n",
            "Epoch [3/200], Batch [43/425], Loss: 2.6783\n",
            "Epoch [3/200], Batch [44/425], Loss: 2.7118\n",
            "Epoch [3/200], Batch [45/425], Loss: 2.4822\n",
            "Epoch [3/200], Batch [46/425], Loss: 2.4836\n",
            "Epoch [3/200], Batch [47/425], Loss: 2.5171\n",
            "Epoch [3/200], Batch [48/425], Loss: 2.4478\n",
            "Epoch [3/200], Batch [49/425], Loss: 2.5877\n",
            "Epoch [3/200], Batch [50/425], Loss: 2.4938\n",
            "Epoch [3/200], Batch [51/425], Loss: 2.6722\n",
            "Epoch [3/200], Batch [52/425], Loss: 2.6050\n",
            "Epoch [3/200], Batch [53/425], Loss: 2.6570\n",
            "Epoch [3/200], Batch [54/425], Loss: 2.6354\n",
            "Epoch [3/200], Batch [55/425], Loss: 2.6570\n",
            "Epoch [3/200], Batch [56/425], Loss: 2.7315\n",
            "Epoch [3/200], Batch [57/425], Loss: 2.6377\n",
            "Epoch [3/200], Batch [58/425], Loss: 2.5191\n",
            "Epoch [3/200], Batch [59/425], Loss: 2.7669\n",
            "Epoch [3/200], Batch [60/425], Loss: 2.5683\n",
            "Epoch [3/200], Batch [61/425], Loss: 2.6666\n",
            "Epoch [3/200], Batch [62/425], Loss: 2.5524\n",
            "Epoch [3/200], Batch [63/425], Loss: 2.6249\n",
            "Epoch [3/200], Batch [64/425], Loss: 2.4245\n",
            "Epoch [3/200], Batch [65/425], Loss: 2.6386\n",
            "Epoch [3/200], Batch [66/425], Loss: 2.4084\n",
            "Epoch [3/200], Batch [67/425], Loss: 2.7076\n",
            "Epoch [3/200], Batch [68/425], Loss: 2.7349\n",
            "Epoch [3/200], Batch [69/425], Loss: 2.5985\n",
            "Epoch [3/200], Batch [70/425], Loss: 2.6590\n",
            "Epoch [3/200], Batch [71/425], Loss: 2.6754\n",
            "Epoch [3/200], Batch [72/425], Loss: 2.4326\n",
            "Epoch [3/200], Batch [73/425], Loss: 2.7562\n",
            "Epoch [3/200], Batch [74/425], Loss: 2.6276\n",
            "Epoch [3/200], Batch [75/425], Loss: 2.7674\n",
            "Epoch [3/200], Batch [76/425], Loss: 2.7119\n",
            "Epoch [3/200], Batch [77/425], Loss: 2.7547\n",
            "Epoch [3/200], Batch [78/425], Loss: 2.5838\n",
            "Epoch [3/200], Batch [79/425], Loss: 2.6441\n",
            "Epoch [3/200], Batch [80/425], Loss: 2.5872\n",
            "Epoch [3/200], Batch [81/425], Loss: 2.5931\n",
            "Epoch [3/200], Batch [82/425], Loss: 2.6161\n",
            "Epoch [3/200], Batch [83/425], Loss: 2.7833\n",
            "Epoch [3/200], Batch [84/425], Loss: 2.5490\n",
            "Epoch [3/200], Batch [85/425], Loss: 2.6400\n",
            "Epoch [3/200], Batch [86/425], Loss: 2.6556\n",
            "Epoch [3/200], Batch [87/425], Loss: 2.7392\n",
            "Epoch [3/200], Batch [88/425], Loss: 2.6970\n",
            "Epoch [3/200], Batch [89/425], Loss: 2.5719\n",
            "Epoch [3/200], Batch [90/425], Loss: 2.5296\n",
            "Epoch [3/200], Batch [91/425], Loss: 2.5683\n",
            "Epoch [3/200], Batch [92/425], Loss: 2.7063\n",
            "Epoch [3/200], Batch [93/425], Loss: 2.6069\n",
            "Epoch [3/200], Batch [94/425], Loss: 2.4950\n",
            "Epoch [3/200], Batch [95/425], Loss: 2.6658\n",
            "Epoch [3/200], Batch [96/425], Loss: 2.3585\n",
            "Epoch [3/200], Batch [97/425], Loss: 2.5165\n",
            "Epoch [3/200], Batch [98/425], Loss: 2.6861\n",
            "Epoch [3/200], Batch [99/425], Loss: 2.5586\n",
            "Epoch [3/200], Batch [100/425], Loss: 2.7641\n",
            "Epoch [3/200], Batch [101/425], Loss: 2.5937\n",
            "Epoch [3/200], Batch [102/425], Loss: 2.5243\n",
            "Epoch [3/200], Batch [103/425], Loss: 2.6641\n",
            "Epoch [3/200], Batch [104/425], Loss: 2.3907\n",
            "Epoch [3/200], Batch [105/425], Loss: 3.0858\n",
            "Epoch [3/200], Batch [106/425], Loss: 2.5589\n",
            "Epoch [3/200], Batch [107/425], Loss: 2.6896\n",
            "Epoch [3/200], Batch [108/425], Loss: 2.5158\n",
            "Epoch [3/200], Batch [109/425], Loss: 2.4476\n",
            "Epoch [3/200], Batch [110/425], Loss: 2.4568\n",
            "Epoch [3/200], Batch [111/425], Loss: 2.3868\n",
            "Epoch [3/200], Batch [112/425], Loss: 2.7419\n",
            "Epoch [3/200], Batch [113/425], Loss: 2.6143\n",
            "Epoch [3/200], Batch [114/425], Loss: 2.5509\n",
            "Epoch [3/200], Batch [115/425], Loss: 2.7597\n",
            "Epoch [3/200], Batch [116/425], Loss: 2.5299\n",
            "Epoch [3/200], Batch [117/425], Loss: 2.6689\n",
            "Epoch [3/200], Batch [118/425], Loss: 2.6977\n",
            "Epoch [3/200], Batch [119/425], Loss: 2.7670\n",
            "Epoch [3/200], Batch [120/425], Loss: 2.5217\n",
            "Epoch [3/200], Batch [121/425], Loss: 2.6171\n",
            "Epoch [3/200], Batch [122/425], Loss: 2.6150\n",
            "Epoch [3/200], Batch [123/425], Loss: 2.4130\n",
            "Epoch [3/200], Batch [124/425], Loss: 2.7459\n",
            "Epoch [3/200], Batch [125/425], Loss: 2.6222\n",
            "Epoch [3/200], Batch [126/425], Loss: 2.7850\n",
            "Epoch [3/200], Batch [127/425], Loss: 2.5018\n",
            "Epoch [3/200], Batch [128/425], Loss: 2.5721\n",
            "Epoch [3/200], Batch [129/425], Loss: 2.5946\n",
            "Epoch [3/200], Batch [130/425], Loss: 2.5211\n",
            "Epoch [3/200], Batch [131/425], Loss: 2.5480\n",
            "Epoch [3/200], Batch [132/425], Loss: 2.6949\n",
            "Epoch [3/200], Batch [133/425], Loss: 2.6515\n",
            "Epoch [3/200], Batch [134/425], Loss: 2.9567\n",
            "Epoch [3/200], Batch [135/425], Loss: 2.6618\n",
            "Epoch [3/200], Batch [136/425], Loss: 2.4547\n",
            "Epoch [3/200], Batch [137/425], Loss: 2.7833\n",
            "Epoch [3/200], Batch [138/425], Loss: 2.6523\n",
            "Epoch [3/200], Batch [139/425], Loss: 2.4751\n",
            "Epoch [3/200], Batch [140/425], Loss: 2.8009\n",
            "Epoch [3/200], Batch [141/425], Loss: 2.6630\n",
            "Epoch [3/200], Batch [142/425], Loss: 2.5736\n",
            "Epoch [3/200], Batch [143/425], Loss: 2.4481\n",
            "Epoch [3/200], Batch [144/425], Loss: 2.5183\n",
            "Epoch [3/200], Batch [145/425], Loss: 2.6059\n",
            "Epoch [3/200], Batch [146/425], Loss: 2.6676\n",
            "Epoch [3/200], Batch [147/425], Loss: 2.5327\n",
            "Epoch [3/200], Batch [148/425], Loss: 2.8056\n",
            "Epoch [3/200], Batch [149/425], Loss: 2.7656\n",
            "Epoch [3/200], Batch [150/425], Loss: 2.4977\n",
            "Epoch [3/200], Batch [151/425], Loss: 2.4869\n",
            "Epoch [3/200], Batch [152/425], Loss: 2.7003\n",
            "Epoch [3/200], Batch [153/425], Loss: 2.3871\n",
            "Epoch [3/200], Batch [154/425], Loss: 2.5752\n",
            "Epoch [3/200], Batch [155/425], Loss: 2.6438\n",
            "Epoch [3/200], Batch [156/425], Loss: 2.7559\n",
            "Epoch [3/200], Batch [157/425], Loss: 2.6559\n",
            "Epoch [3/200], Batch [158/425], Loss: 2.3662\n",
            "Epoch [3/200], Batch [159/425], Loss: 2.7121\n",
            "Epoch [3/200], Batch [160/425], Loss: 2.6383\n",
            "Epoch [3/200], Batch [161/425], Loss: 2.6328\n",
            "Epoch [3/200], Batch [162/425], Loss: 2.6430\n",
            "Epoch [3/200], Batch [163/425], Loss: 2.7269\n",
            "Epoch [3/200], Batch [164/425], Loss: 2.4138\n",
            "Epoch [3/200], Batch [165/425], Loss: 2.6232\n",
            "Epoch [3/200], Batch [166/425], Loss: 2.6280\n",
            "Epoch [3/200], Batch [167/425], Loss: 2.5028\n",
            "Epoch [3/200], Batch [168/425], Loss: 2.5879\n",
            "Epoch [3/200], Batch [169/425], Loss: 2.7496\n",
            "Epoch [3/200], Batch [170/425], Loss: 2.6701\n",
            "Epoch [3/200], Batch [171/425], Loss: 2.7391\n",
            "Epoch [3/200], Batch [172/425], Loss: 2.5917\n",
            "Epoch [3/200], Batch [173/425], Loss: 2.7335\n",
            "Epoch [3/200], Batch [174/425], Loss: 2.4974\n",
            "Epoch [3/200], Batch [175/425], Loss: 2.7025\n",
            "Epoch [3/200], Batch [176/425], Loss: 2.3147\n",
            "Epoch [3/200], Batch [177/425], Loss: 2.5667\n",
            "Epoch [3/200], Batch [178/425], Loss: 2.6529\n",
            "Epoch [3/200], Batch [179/425], Loss: 2.5195\n",
            "Epoch [3/200], Batch [180/425], Loss: 2.7102\n",
            "Epoch [3/200], Batch [181/425], Loss: 2.5334\n",
            "Epoch [3/200], Batch [182/425], Loss: 2.5922\n",
            "Epoch [3/200], Batch [183/425], Loss: 2.4842\n",
            "Epoch [3/200], Batch [184/425], Loss: 2.5268\n",
            "Epoch [3/200], Batch [185/425], Loss: 2.6678\n",
            "Epoch [3/200], Batch [186/425], Loss: 2.5506\n",
            "Epoch [3/200], Batch [187/425], Loss: 2.7573\n",
            "Epoch [3/200], Batch [188/425], Loss: 2.5842\n",
            "Epoch [3/200], Batch [189/425], Loss: 2.6154\n",
            "Epoch [3/200], Batch [190/425], Loss: 2.7394\n",
            "Epoch [3/200], Batch [191/425], Loss: 2.6691\n",
            "Epoch [3/200], Batch [192/425], Loss: 2.6180\n",
            "Epoch [3/200], Batch [193/425], Loss: 2.6004\n",
            "Epoch [3/200], Batch [194/425], Loss: 2.4204\n",
            "Epoch [3/200], Batch [195/425], Loss: 2.4800\n",
            "Epoch [3/200], Batch [196/425], Loss: 2.5878\n",
            "Epoch [3/200], Batch [197/425], Loss: 2.5874\n",
            "Epoch [3/200], Batch [198/425], Loss: 2.7263\n",
            "Epoch [3/200], Batch [199/425], Loss: 2.7281\n",
            "Epoch [3/200], Batch [200/425], Loss: 2.4996\n",
            "Epoch [3/200], Batch [201/425], Loss: 2.4366\n",
            "Epoch [3/200], Batch [202/425], Loss: 2.5734\n",
            "Epoch [3/200], Batch [203/425], Loss: 2.5655\n",
            "Epoch [3/200], Batch [204/425], Loss: 2.5761\n",
            "Epoch [3/200], Batch [205/425], Loss: 2.5837\n",
            "Epoch [3/200], Batch [206/425], Loss: 2.8332\n",
            "Epoch [3/200], Batch [207/425], Loss: 2.7114\n",
            "Epoch [3/200], Batch [208/425], Loss: 2.5830\n",
            "Epoch [3/200], Batch [209/425], Loss: 2.8461\n",
            "Epoch [3/200], Batch [210/425], Loss: 2.7673\n",
            "Epoch [3/200], Batch [211/425], Loss: 2.6500\n",
            "Epoch [3/200], Batch [212/425], Loss: 2.6813\n",
            "Epoch [3/200], Batch [213/425], Loss: 2.6507\n",
            "Epoch [3/200], Batch [214/425], Loss: 2.6023\n",
            "Epoch [3/200], Batch [215/425], Loss: 2.3941\n",
            "Epoch [3/200], Batch [216/425], Loss: 2.6475\n",
            "Epoch [3/200], Batch [217/425], Loss: 2.4525\n",
            "Epoch [3/200], Batch [218/425], Loss: 2.6734\n",
            "Epoch [3/200], Batch [219/425], Loss: 2.3868\n",
            "Epoch [3/200], Batch [220/425], Loss: 2.6709\n",
            "Epoch [3/200], Batch [221/425], Loss: 2.5720\n",
            "Epoch [3/200], Batch [222/425], Loss: 2.5363\n",
            "Epoch [3/200], Batch [223/425], Loss: 2.4907\n",
            "Epoch [3/200], Batch [224/425], Loss: 2.4549\n",
            "Epoch [3/200], Batch [225/425], Loss: 2.7213\n",
            "Epoch [3/200], Batch [226/425], Loss: 2.6234\n",
            "Epoch [3/200], Batch [227/425], Loss: 2.7966\n",
            "Epoch [3/200], Batch [228/425], Loss: 2.5971\n",
            "Epoch [3/200], Batch [229/425], Loss: 2.8100\n",
            "Epoch [3/200], Batch [230/425], Loss: 2.6600\n",
            "Epoch [3/200], Batch [231/425], Loss: 2.6741\n",
            "Epoch [3/200], Batch [232/425], Loss: 2.5720\n",
            "Epoch [3/200], Batch [233/425], Loss: 2.5875\n",
            "Epoch [3/200], Batch [234/425], Loss: 2.6731\n",
            "Epoch [3/200], Batch [235/425], Loss: 2.6412\n",
            "Epoch [3/200], Batch [236/425], Loss: 2.6276\n",
            "Epoch [3/200], Batch [237/425], Loss: 2.6730\n",
            "Epoch [3/200], Batch [238/425], Loss: 2.6688\n",
            "Epoch [3/200], Batch [239/425], Loss: 2.7131\n",
            "Epoch [3/200], Batch [240/425], Loss: 2.4481\n",
            "Epoch [3/200], Batch [241/425], Loss: 2.5335\n",
            "Epoch [3/200], Batch [242/425], Loss: 2.6700\n",
            "Epoch [3/200], Batch [243/425], Loss: 2.6596\n",
            "Epoch [3/200], Batch [244/425], Loss: 2.9730\n",
            "Epoch [3/200], Batch [245/425], Loss: 2.7206\n",
            "Epoch [3/200], Batch [246/425], Loss: 2.5824\n",
            "Epoch [3/200], Batch [247/425], Loss: 2.4379\n",
            "Epoch [3/200], Batch [248/425], Loss: 2.5987\n",
            "Epoch [3/200], Batch [249/425], Loss: 2.5173\n",
            "Epoch [3/200], Batch [250/425], Loss: 2.5619\n",
            "Epoch [3/200], Batch [251/425], Loss: 2.7257\n",
            "Epoch [3/200], Batch [252/425], Loss: 2.6668\n",
            "Epoch [3/200], Batch [253/425], Loss: 2.6370\n",
            "Epoch [3/200], Batch [254/425], Loss: 2.5466\n",
            "Epoch [3/200], Batch [255/425], Loss: 2.9023\n",
            "Epoch [3/200], Batch [256/425], Loss: 2.5429\n",
            "Epoch [3/200], Batch [257/425], Loss: 2.5743\n",
            "Epoch [3/200], Batch [258/425], Loss: 2.4837\n",
            "Epoch [3/200], Batch [259/425], Loss: 2.5078\n",
            "Epoch [3/200], Batch [260/425], Loss: 2.5669\n",
            "Epoch [3/200], Batch [261/425], Loss: 2.6471\n",
            "Epoch [3/200], Batch [262/425], Loss: 2.6457\n",
            "Epoch [3/200], Batch [263/425], Loss: 2.8591\n",
            "Epoch [3/200], Batch [264/425], Loss: 2.6314\n",
            "Epoch [3/200], Batch [265/425], Loss: 2.7108\n",
            "Epoch [3/200], Batch [266/425], Loss: 2.5139\n",
            "Epoch [3/200], Batch [267/425], Loss: 2.7548\n",
            "Epoch [3/200], Batch [268/425], Loss: 2.6952\n",
            "Epoch [3/200], Batch [269/425], Loss: 2.5882\n",
            "Epoch [3/200], Batch [270/425], Loss: 2.3932\n",
            "Epoch [3/200], Batch [271/425], Loss: 2.7242\n",
            "Epoch [3/200], Batch [272/425], Loss: 2.9192\n",
            "Epoch [3/200], Batch [273/425], Loss: 2.5423\n",
            "Epoch [3/200], Batch [274/425], Loss: 2.6486\n",
            "Epoch [3/200], Batch [275/425], Loss: 2.4789\n",
            "Epoch [3/200], Batch [276/425], Loss: 2.5863\n",
            "Epoch [3/200], Batch [277/425], Loss: 2.7816\n",
            "Epoch [3/200], Batch [278/425], Loss: 2.6948\n",
            "Epoch [3/200], Batch [279/425], Loss: 2.4326\n",
            "Epoch [3/200], Batch [280/425], Loss: 2.4926\n",
            "Epoch [3/200], Batch [281/425], Loss: 2.5810\n",
            "Epoch [3/200], Batch [282/425], Loss: 2.6626\n",
            "Epoch [3/200], Batch [283/425], Loss: 2.5222\n",
            "Epoch [3/200], Batch [284/425], Loss: 2.6404\n",
            "Epoch [3/200], Batch [285/425], Loss: 2.5931\n",
            "Epoch [3/200], Batch [286/425], Loss: 2.6407\n",
            "Epoch [3/200], Batch [287/425], Loss: 2.4391\n",
            "Epoch [3/200], Batch [288/425], Loss: 2.6391\n",
            "Epoch [3/200], Batch [289/425], Loss: 2.4992\n",
            "Epoch [3/200], Batch [290/425], Loss: 2.6756\n",
            "Epoch [3/200], Batch [291/425], Loss: 2.6100\n",
            "Epoch [3/200], Batch [292/425], Loss: 2.5959\n",
            "Epoch [3/200], Batch [293/425], Loss: 2.6180\n",
            "Epoch [3/200], Batch [294/425], Loss: 2.6292\n",
            "Epoch [3/200], Batch [295/425], Loss: 2.6726\n",
            "Epoch [3/200], Batch [296/425], Loss: 2.4998\n",
            "Epoch [3/200], Batch [297/425], Loss: 2.6201\n",
            "Epoch [3/200], Batch [298/425], Loss: 2.5887\n",
            "Epoch [3/200], Batch [299/425], Loss: 2.3657\n",
            "Epoch [3/200], Batch [300/425], Loss: 2.5609\n",
            "Epoch [3/200], Batch [301/425], Loss: 2.5123\n",
            "Epoch [3/200], Batch [302/425], Loss: 2.6089\n",
            "Epoch [3/200], Batch [303/425], Loss: 2.8229\n",
            "Epoch [3/200], Batch [304/425], Loss: 2.7493\n",
            "Epoch [3/200], Batch [305/425], Loss: 2.6237\n",
            "Epoch [3/200], Batch [306/425], Loss: 2.7601\n",
            "Epoch [3/200], Batch [307/425], Loss: 2.5167\n",
            "Epoch [3/200], Batch [308/425], Loss: 2.6169\n",
            "Epoch [3/200], Batch [309/425], Loss: 2.6554\n",
            "Epoch [3/200], Batch [310/425], Loss: 2.5696\n",
            "Epoch [3/200], Batch [311/425], Loss: 2.6748\n",
            "Epoch [3/200], Batch [312/425], Loss: 2.6035\n",
            "Epoch [3/200], Batch [313/425], Loss: 2.6945\n",
            "Epoch [3/200], Batch [314/425], Loss: 2.6908\n",
            "Epoch [3/200], Batch [315/425], Loss: 2.7212\n",
            "Epoch [3/200], Batch [316/425], Loss: 2.5427\n",
            "Epoch [3/200], Batch [317/425], Loss: 2.8004\n",
            "Epoch [3/200], Batch [318/425], Loss: 2.8067\n",
            "Epoch [3/200], Batch [319/425], Loss: 2.5245\n",
            "Epoch [3/200], Batch [320/425], Loss: 2.5970\n",
            "Epoch [3/200], Batch [321/425], Loss: 2.5155\n",
            "Epoch [3/200], Batch [322/425], Loss: 2.5459\n",
            "Epoch [3/200], Batch [323/425], Loss: 2.6189\n",
            "Epoch [3/200], Batch [324/425], Loss: 2.6678\n",
            "Epoch [3/200], Batch [325/425], Loss: 2.6356\n",
            "Epoch [3/200], Batch [326/425], Loss: 2.4245\n",
            "Epoch [3/200], Batch [327/425], Loss: 2.7021\n",
            "Epoch [3/200], Batch [328/425], Loss: 2.7254\n",
            "Epoch [3/200], Batch [329/425], Loss: 2.5788\n",
            "Epoch [3/200], Batch [330/425], Loss: 2.8510\n",
            "Epoch [3/200], Batch [331/425], Loss: 2.6937\n",
            "Epoch [3/200], Batch [332/425], Loss: 2.6000\n",
            "Epoch [3/200], Batch [333/425], Loss: 2.6492\n",
            "Epoch [3/200], Batch [334/425], Loss: 2.4898\n",
            "Epoch [3/200], Batch [335/425], Loss: 2.7202\n",
            "Epoch [3/200], Batch [336/425], Loss: 2.6608\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-86cc29e0a967>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create the data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1.00) #loss 2.3866\n",
        "\n",
        "# Initialize the best validation loss and best_model_wts before the training loop\n",
        "best_val_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "# Instantiate the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize the lowest_loss and best_model_wts before the training loop\n",
        "lowest_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (hsi_batch, lidar_batch, label_batch) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        hsi_batch = hsi_batch.to(device)\n",
        "        lidar_batch = lidar_batch.to(device)\n",
        "        label_batch = label_batch.to(device)  # Reshape labels\n",
        "        # print('hsi_batch shape:', hsi_batch.shape)\n",
        "        # print('lidar_batch:', lidar_batch.shape)\n",
        "        # print('label_batch shape:', label_batch.shape)\n",
        "\n",
        "        # Forward pass\n",
        "        output,attn_scores  = model(lidar_batch, hsi_batch)\n",
        "        #print('output shape:', output.shape)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.transpose(1, 2), label_batch.squeeze(2))\n",
        "\n",
        "        # Print loss every 10 batches\n",
        "        #if (batch_idx + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "        # If the loss is lower than the current lowest, save the model's state\n",
        "        if loss.item() < lowest_loss:\n",
        "            lowest_loss = loss.item()\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "# Print finishing training\n",
        "print('Finishing training')\n",
        "\n",
        "# Save the best model weights\n",
        "#torch.save(best_model_wts, 'best_model_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8G-EKMlepL2t",
      "metadata": {
        "id": "8G-EKMlepL2t"
      },
      "outputs": [],
      "source": [
        "# Save the best model state\n",
        "torch.save(best_model_wts, 'best_model_aug_p9_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jOc5jb92pL6V",
      "metadata": {
        "id": "jOc5jb92pL6V"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "best_model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "\n",
        "# Load the weights\n",
        "best_model.load_state_dict(torch.load('best_model_aug_p9_weights.pth'))\n",
        "\n",
        "# Move the model to the GPU\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "# Now your model is ready for making predictions on GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oSKh2t4gJcGU",
      "metadata": {
        "id": "oSKh2t4gJcGU"
      },
      "source": [
        "# 5.0 Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IrRolVFaKZy_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrRolVFaKZy_",
        "outputId": "70e747b7-8ed2-40ba-809e-7b51d4d90bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "hsi_patch_embedding.pos_embedding \t torch.Size([1, 145, 128])\n",
            "hsi_patch_embedding.cls_token \t torch.Size([1, 1, 128])\n",
            "hsi_patch_embedding.proj.weight \t torch.Size([128, 81, 1, 1])\n",
            "hsi_patch_embedding.proj.bias \t torch.Size([128])\n",
            "lidar_patch_embedding.pos_embedding \t torch.Size([1, 2, 128])\n",
            "lidar_patch_embedding.cls_token \t torch.Size([1, 1, 128])\n",
            "lidar_patch_embedding.proj.weight \t torch.Size([128, 81, 1, 1])\n",
            "lidar_patch_embedding.proj.bias \t torch.Size([128])\n",
            "cross_attention.to_q.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_k.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_v.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_out.weight \t torch.Size([144, 512])\n",
            "cross_attention.to_out.bias \t torch.Size([144])\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "model.load_state_dict(torch.load('best_model_aug_p9_weights.pth'))\n",
        "model.eval()  # set the model to evaluation mode\n",
        "\n",
        "# Move the model to the GPU\n",
        "#\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2OAS0mPPKi9O",
      "metadata": {
        "id": "2OAS0mPPKi9O",
        "outputId": "e5c91c62-e2a9-4816-c7c1-7f96583cd437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "hsi_patch_embedding.pos_embedding \t tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
            "hsi_patch_embedding.cls_token \t tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "hsi_patch_embedding.proj.weight \t tensor([[[[-0.0568]],\n",
            "\n",
            "         [[-0.0421]],\n",
            "\n",
            "         [[-0.0649]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0843]],\n",
            "\n",
            "         [[-0.0656]],\n",
            "\n",
            "         [[-0.0100]]],\n",
            "\n",
            "\n",
            "        [[[-0.0122]],\n",
            "\n",
            "         [[ 0.0594]],\n",
            "\n",
            "         [[ 0.0505]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0736]],\n",
            "\n",
            "         [[ 0.0266]],\n",
            "\n",
            "         [[-0.0439]]],\n",
            "\n",
            "\n",
            "        [[[-0.0500]],\n",
            "\n",
            "         [[ 0.1098]],\n",
            "\n",
            "         [[ 0.0473]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1002]],\n",
            "\n",
            "         [[-0.0617]],\n",
            "\n",
            "         [[ 0.0610]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0525]],\n",
            "\n",
            "         [[ 0.0665]],\n",
            "\n",
            "         [[ 0.0531]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0179]],\n",
            "\n",
            "         [[-0.0581]],\n",
            "\n",
            "         [[ 0.0015]]],\n",
            "\n",
            "\n",
            "        [[[-0.0236]],\n",
            "\n",
            "         [[ 0.0450]],\n",
            "\n",
            "         [[-0.0263]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0980]],\n",
            "\n",
            "         [[ 0.0297]],\n",
            "\n",
            "         [[-0.0098]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0349]],\n",
            "\n",
            "         [[-0.0974]],\n",
            "\n",
            "         [[-0.0669]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0468]],\n",
            "\n",
            "         [[-0.0816]],\n",
            "\n",
            "         [[ 0.0448]]]])\n",
            "hsi_patch_embedding.proj.bias \t tensor([ 0.0578, -0.0567, -0.0208, -0.0335,  0.0964,  0.0907,  0.0612, -0.0217,\n",
            "         0.0388, -0.0880,  0.0414,  0.0867, -0.0960,  0.0908, -0.0273, -0.0187,\n",
            "         0.0432,  0.0865,  0.1002,  0.1038,  0.0364, -0.0926,  0.0759,  0.0859,\n",
            "         0.0655,  0.0939, -0.1030,  0.0400, -0.0748,  0.0567, -0.0621,  0.0786,\n",
            "         0.0813,  0.0462,  0.0489,  0.0671, -0.0662,  0.0770,  0.0546,  0.0947,\n",
            "         0.0862,  0.0640,  0.0045, -0.0909, -0.0123,  0.0974, -0.0995,  0.0231,\n",
            "         0.0167, -0.0600,  0.0179,  0.0078, -0.1043,  0.0725, -0.1026, -0.0115,\n",
            "         0.0490, -0.0986,  0.0955, -0.1066, -0.0926,  0.0967, -0.0147,  0.1098,\n",
            "         0.0798,  0.0162, -0.0082, -0.0691,  0.1105,  0.0329,  0.0112, -0.0735,\n",
            "        -0.1105, -0.0816,  0.0317, -0.0103,  0.1096, -0.0331, -0.0818, -0.0840,\n",
            "        -0.0298, -0.0575, -0.0732, -0.0626, -0.0351,  0.0930, -0.0991, -0.0434,\n",
            "         0.0588, -0.0729, -0.0363,  0.0843,  0.0704, -0.0437, -0.0365, -0.0905,\n",
            "         0.0894, -0.0672, -0.0478, -0.0612, -0.0941, -0.0108, -0.0791,  0.0304,\n",
            "         0.0585,  0.0233, -0.0789,  0.0193, -0.0779,  0.0473,  0.1018, -0.0647,\n",
            "         0.1072,  0.0601,  0.0276,  0.0427, -0.0112, -0.0540,  0.0685, -0.0812,\n",
            "         0.0166, -0.0357,  0.0807, -0.0039, -0.0078, -0.0510,  0.0380, -0.0964])\n",
            "lidar_patch_embedding.pos_embedding \t tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000],\n",
            "         [-0.8685, -0.2719, -0.5695,  0.7991,  0.7531,  0.3246, -0.9258,\n",
            "           0.3592, -0.3152, -0.1212, -0.4992,  0.6038, -0.7297,  0.4666,\n",
            "          -0.3419,  0.5776, -0.2707, -0.0729, -0.6892,  0.9240,  0.1795,\n",
            "          -0.7194,  0.3865,  0.0324, -0.0698,  0.3224,  0.3622,  0.6300,\n",
            "           0.6119, -0.2775, -0.5578, -0.6359, -0.7634, -0.8143, -0.7066,\n",
            "           0.6457,  0.5429,  0.0410,  0.4065, -0.6867, -0.5911, -0.4176,\n",
            "          -0.5359, -0.3177, -0.5372,  0.0091,  0.2461, -0.6297, -0.0123,\n",
            "          -0.1071,  0.5624,  0.4817, -0.0939, -0.5585, -1.0290,  0.8852,\n",
            "          -0.7417, -0.3733,  0.2689,  0.6270, -0.0287, -0.5370, -0.2358,\n",
            "          -0.6557,  0.2581, -0.0378, -0.1532, -0.5822,  0.2405, -0.7553,\n",
            "          -0.6591, -0.4746, -0.7700,  0.4080, -0.3327, -0.0196, -0.4035,\n",
            "           0.4527,  0.9492, -0.4993,  0.6760,  0.4684,  0.0820,  0.4969,\n",
            "           0.4682, -0.5273,  0.1320, -0.1449, -0.5595, -0.4723, -0.7127,\n",
            "          -0.2155, -0.1859, -0.0395, -0.4639,  0.5069, -0.4199,  0.4189,\n",
            "           0.4147,  0.1605,  0.7265, -0.4622, -0.6288, -0.2816,  0.6149,\n",
            "          -0.9056,  0.9675,  0.3237,  0.0850, -1.0042, -0.1145,  0.9037,\n",
            "           0.7861,  0.3593, -0.3606, -0.2744,  0.6485,  0.4742,  0.4407,\n",
            "           0.3530,  0.2714, -0.3183, -0.4165, -0.4149,  0.1262, -0.8781,\n",
            "          -0.6628,  0.1069]]])\n",
            "lidar_patch_embedding.cls_token \t tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "lidar_patch_embedding.proj.weight \t tensor([[[[-0.0441]],\n",
            "\n",
            "         [[-0.0967]],\n",
            "\n",
            "         [[ 0.0308]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0554]],\n",
            "\n",
            "         [[-0.0051]],\n",
            "\n",
            "         [[ 0.0033]]],\n",
            "\n",
            "\n",
            "        [[[-0.0763]],\n",
            "\n",
            "         [[ 0.0349]],\n",
            "\n",
            "         [[ 0.0939]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0101]],\n",
            "\n",
            "         [[ 0.0836]],\n",
            "\n",
            "         [[-0.0259]]],\n",
            "\n",
            "\n",
            "        [[[-0.0961]],\n",
            "\n",
            "         [[-0.0122]],\n",
            "\n",
            "         [[-0.0035]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0422]],\n",
            "\n",
            "         [[ 0.0143]],\n",
            "\n",
            "         [[-0.0821]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0317]],\n",
            "\n",
            "         [[ 0.0391]],\n",
            "\n",
            "         [[-0.0720]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0594]],\n",
            "\n",
            "         [[ 0.0162]],\n",
            "\n",
            "         [[-0.1070]]],\n",
            "\n",
            "\n",
            "        [[[-0.0967]],\n",
            "\n",
            "         [[-0.0409]],\n",
            "\n",
            "         [[ 0.0212]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0356]],\n",
            "\n",
            "         [[-0.0839]],\n",
            "\n",
            "         [[ 0.1184]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0765]],\n",
            "\n",
            "         [[-0.0755]],\n",
            "\n",
            "         [[-0.0429]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0766]],\n",
            "\n",
            "         [[-0.1032]],\n",
            "\n",
            "         [[ 0.0376]]]])\n",
            "lidar_patch_embedding.proj.bias \t tensor([-0.8611, -0.2062, -0.4980,  0.7808,  0.7933,  0.4223, -0.9454,  0.2771,\n",
            "        -0.2971, -0.2147, -0.5100,  0.6559, -0.7242,  0.4593, -0.4401,  0.6266,\n",
            "        -0.2777, -0.0625, -0.6691,  1.0171,  0.2044, -0.6841,  0.3453,  0.1293,\n",
            "        -0.0810,  0.2641,  0.4729,  0.7403,  0.5951, -0.1913, -0.6390, -0.6460,\n",
            "        -0.7037, -0.7963, -0.7146,  0.7405,  0.5199, -0.0050,  0.4802, -0.7666,\n",
            "        -0.5623, -0.4737, -0.5556, -0.2884, -0.6110, -0.0695,  0.2679, -0.6877,\n",
            "         0.0476, -0.0362,  0.4948,  0.5357, -0.1146, -0.5459, -1.0012,  0.9579,\n",
            "        -0.6558, -0.4745,  0.2394,  0.7317, -0.1124, -0.4558, -0.3184, -0.5691,\n",
            "         0.2954,  0.0201, -0.0518, -0.6330,  0.2637, -0.7452, -0.6752, -0.5785,\n",
            "        -0.8061,  0.3820, -0.4371, -0.0579, -0.3306,  0.4152,  0.8630, -0.4165,\n",
            "         0.5867,  0.5290,  0.1360,  0.5694,  0.5162, -0.4852,  0.1610, -0.0998,\n",
            "        -0.6626, -0.4352, -0.7256, -0.1510, -0.2206, -0.0218, -0.5431,  0.5501,\n",
            "        -0.4750,  0.3556,  0.3079,  0.1851,  0.7633, -0.4800, -0.6195, -0.2265,\n",
            "         0.6077, -0.8326,  1.0017,  0.2475,  0.0189, -0.9874, -0.0740,  0.9667,\n",
            "         0.6858,  0.2758, -0.4672, -0.2111,  0.7536,  0.5521,  0.3927,  0.3191,\n",
            "         0.1799, -0.3065, -0.4244, -0.4025,  0.2339, -0.8349, -0.5708,  0.0017])\n",
            "cross_attention.to_q.weight \t tensor([[-0.0447,  0.0465, -0.0207,  ..., -0.0549, -0.0284,  0.0144],\n",
            "        [ 0.0010, -0.0343,  0.0422,  ..., -0.0409,  0.0023,  0.0339],\n",
            "        [-0.0183, -0.0081,  0.0295,  ..., -0.0104,  0.0693,  0.0652],\n",
            "        ...,\n",
            "        [ 0.0266,  0.0044, -0.0120,  ..., -0.0210,  0.0538, -0.0409],\n",
            "        [ 0.0716,  0.0135, -0.0296,  ...,  0.0841, -0.0575, -0.0018],\n",
            "        [ 0.0873,  0.0778,  0.0130,  ...,  0.0753, -0.0036, -0.0741]])\n",
            "cross_attention.to_k.weight \t tensor([[-0.0690, -0.0388,  0.0030,  ...,  0.0173,  0.0855, -0.0802],\n",
            "        [ 0.0491,  0.0684,  0.0012,  ...,  0.0065, -0.0448,  0.0719],\n",
            "        [-0.0691, -0.0459, -0.0407,  ...,  0.0710,  0.0047, -0.0142],\n",
            "        ...,\n",
            "        [-0.0536, -0.0328, -0.0203,  ...,  0.0816, -0.0360,  0.0721],\n",
            "        [ 0.0626, -0.0583, -0.0238,  ...,  0.0012,  0.0432, -0.0540],\n",
            "        [-0.0731, -0.0288, -0.0469,  ..., -0.0824, -0.0641, -0.0720]])\n",
            "cross_attention.to_v.weight \t tensor([[-0.2326, -0.0259, -0.0534,  ..., -0.1162, -0.0529,  0.0124],\n",
            "        [-0.0650, -0.0046, -0.0424,  ...,  0.0592, -0.0012, -0.0015],\n",
            "        [ 0.0582, -0.0118,  0.0259,  ...,  0.0461,  0.0238,  0.0004],\n",
            "        ...,\n",
            "        [-0.0783, -0.2061,  0.0578,  ..., -0.0857,  0.0659, -0.1988],\n",
            "        [-0.0372, -0.2488,  0.0233,  ..., -0.2161, -0.0566, -0.2557],\n",
            "        [ 0.0632, -0.1328, -0.0493,  ..., -0.0607,  0.0256, -0.1519]])\n",
            "cross_attention.to_out.weight \t tensor([[-0.0121, -0.0015, -0.0411,  ..., -0.0157, -0.0151, -0.0359],\n",
            "        [ 0.0345, -0.0013,  0.0227,  ..., -0.0415, -0.0122,  0.0139],\n",
            "        [-0.0331,  0.0117,  0.0393,  ...,  0.0406, -0.0361,  0.0106],\n",
            "        ...,\n",
            "        [ 0.0035,  0.0306, -0.0389,  ..., -0.0091, -0.0030, -0.0139],\n",
            "        [ 0.0407, -0.0214,  0.0296,  ..., -0.0398,  0.0242,  0.0417],\n",
            "        [ 0.0376, -0.0184, -0.0342,  ..., -0.0200, -0.0107, -0.0043]])\n",
            "cross_attention.to_out.bias \t tensor([ 0.0183, -0.0169, -0.0053,  0.0333,  0.0087,  0.0320,  0.0301,  0.0184,\n",
            "         0.0223,  0.0376, -0.0406, -0.0014, -0.0269,  0.0237, -0.0067, -0.0273,\n",
            "        -0.0285, -0.0318,  0.0186, -0.0065, -0.0314,  0.0322, -0.0198, -0.0425,\n",
            "        -0.0031,  0.0171,  0.0201,  0.0311, -0.0079,  0.0078, -0.0129,  0.0016,\n",
            "         0.0291,  0.0372, -0.0379,  0.0055, -0.0052, -0.0187, -0.0148,  0.0399,\n",
            "        -0.0271, -0.0274,  0.0360,  0.0252,  0.0127,  0.0138, -0.0383,  0.0428,\n",
            "         0.0388,  0.0073, -0.0441, -0.0256,  0.0220,  0.0144, -0.0023,  0.0150,\n",
            "        -0.0041,  0.0184,  0.0066,  0.0104, -0.0200,  0.0058,  0.0235, -0.0236,\n",
            "        -0.0201, -0.0048, -0.0139, -0.0165,  0.0186,  0.0434, -0.0029,  0.0166,\n",
            "        -0.0147, -0.0418, -0.0369,  0.0084,  0.0277,  0.0236, -0.0215, -0.0271,\n",
            "        -0.0140,  0.0339, -0.0002, -0.0219, -0.0128, -0.0058,  0.0011,  0.0217,\n",
            "        -0.0017, -0.0342, -0.0373,  0.0391, -0.0215,  0.0089, -0.0199, -0.0427,\n",
            "         0.0204,  0.0289, -0.0070,  0.0160,  0.0278,  0.0277, -0.0069,  0.0118,\n",
            "         0.0347, -0.0392,  0.0196,  0.0170, -0.0271,  0.0039,  0.0255,  0.0142,\n",
            "         0.0202,  0.0351, -0.0296, -0.0286, -0.0073,  0.0331,  0.0087,  0.0243,\n",
            "         0.0388,  0.0276, -0.0042, -0.0002,  0.0062, -0.0441,  0.0378,  0.0414,\n",
            "        -0.0017, -0.0255, -0.0069, -0.0374, -0.0273, -0.0035, -0.0288, -0.0199,\n",
            "        -0.0402, -0.0071,  0.0224, -0.0183, -0.0381, -0.0344,  0.0440,  0.0022])\n"
          ]
        }
      ],
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S2xqXWkOndVP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2xqXWkOndVP",
        "outputId": "38e22e7c-f8c1-4d24-b365-a85e00ec04df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-b31b03d9f721>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).repeat(hsi_patch.shape[1], 1).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_predictions shape: (3399, 144, 512)\n",
            "all_attn_scores shape: (3399, 144, 144)\n"
          ]
        }
      ],
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "best_model.eval()\n",
        "\n",
        "# Initialize a list to hold all the predictions and attention scores\n",
        "all_predictions = []\n",
        "all_attn_scores = []\n",
        "\n",
        "# Loop over the validation set\n",
        "for hsi_batch, lidar_batch, _ in val_loader:  # we don't need the labels for predictions\n",
        "    # Move the batch to the desired device\n",
        "    hsi_batch = hsi_batch.to(device)\n",
        "    lidar_batch = lidar_batch.to(device)\n",
        "\n",
        "    # Pass the batch through the model\n",
        "    with torch.no_grad():\n",
        "        output, attn_scores = best_model(lidar_batch, hsi_batch)\n",
        "\n",
        "        # Add the predictions and attention scores to our lists\n",
        "        all_predictions.append(output.cpu().numpy())\n",
        "        all_attn_scores.append(attn_scores.cpu().numpy())\n",
        "\n",
        "# Concatenate all predictions and attention scores into a single numpy array\n",
        "all_predictions = np.concatenate(all_predictions)\n",
        "all_attn_scores = np.concatenate(all_attn_scores)\n",
        "\n",
        "# Now you can use the predictions and attention scores as needed\n",
        "print('all_predictions shape:', all_predictions.shape )\n",
        "print('all_attn_scores shape:', all_attn_scores.shape )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7o9FUM2ILtVK",
      "metadata": {
        "id": "7o9FUM2ILtVK"
      },
      "source": [
        "# Band Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oqc6nZNfXKEN",
      "metadata": {
        "id": "oqc6nZNfXKEN"
      },
      "source": [
        "Calculate the variance of the feature vectors for each band across all samples and then find the bands with the maximum variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4EH-ULRdlVVW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EH-ULRdlVVW",
        "outputId": "6322e3eb-1fb3-405b-c217-b052c6381039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_attention: (144,)\n",
            "sorted_band_indices : (144,)\n",
            "sorted_attention_values : (144,)\n",
            "Top 50 bands based on the mean attention score:\n",
            "Band index: 41, Mean attention score: 32.04867935180664\n",
            "Band index: 87, Mean attention score: 30.620180130004883\n",
            "Band index: 75, Mean attention score: 30.55144500732422\n",
            "Band index: 90, Mean attention score: 29.045427322387695\n",
            "Band index: 54, Mean attention score: 27.903484344482422\n",
            "Band index: 20, Mean attention score: 25.40079689025879\n",
            "Band index: 119, Mean attention score: 23.460657119750977\n",
            "Band index: 58, Mean attention score: 21.71855354309082\n",
            "Band index: 138, Mean attention score: 21.27195167541504\n",
            "Band index: 1, Mean attention score: 20.67475128173828\n",
            "Band index: 33, Mean attention score: 20.39455223083496\n",
            "Band index: 126, Mean attention score: 20.15049171447754\n",
            "Band index: 129, Mean attention score: 19.294536590576172\n",
            "Band index: 38, Mean attention score: 18.89678955078125\n",
            "Band index: 82, Mean attention score: 18.88149642944336\n",
            "Band index: 84, Mean attention score: 18.76651954650879\n",
            "Band index: 107, Mean attention score: 18.316844940185547\n",
            "Band index: 133, Mean attention score: 17.888628005981445\n",
            "Band index: 14, Mean attention score: 15.898938179016113\n",
            "Band index: 88, Mean attention score: 15.206921577453613\n",
            "Band index: 59, Mean attention score: 14.525398254394531\n",
            "Band index: 68, Mean attention score: 13.895918846130371\n",
            "Band index: 125, Mean attention score: 13.888677597045898\n",
            "Band index: 15, Mean attention score: 13.856305122375488\n",
            "Band index: 17, Mean attention score: 13.71356201171875\n",
            "Band index: 57, Mean attention score: 13.006380081176758\n",
            "Band index: 110, Mean attention score: 12.903614044189453\n",
            "Band index: 24, Mean attention score: 12.838214874267578\n",
            "Band index: 74, Mean attention score: 12.60300064086914\n",
            "Band index: 141, Mean attention score: 12.162283897399902\n",
            "Band index: 96, Mean attention score: 11.32258129119873\n",
            "Band index: 46, Mean attention score: 11.19692611694336\n",
            "Band index: 95, Mean attention score: 10.514333724975586\n",
            "Band index: 11, Mean attention score: 10.357483863830566\n",
            "Band index: 128, Mean attention score: 9.969893455505371\n",
            "Band index: 27, Mean attention score: 9.741143226623535\n",
            "Band index: 108, Mean attention score: 9.534699440002441\n",
            "Band index: 28, Mean attention score: 8.793224334716797\n",
            "Band index: 140, Mean attention score: 8.692217826843262\n",
            "Band index: 32, Mean attention score: 8.31902027130127\n",
            "Band index: 65, Mean attention score: 8.08210563659668\n",
            "Band index: 13, Mean attention score: 7.85645055770874\n",
            "Band index: 10, Mean attention score: 7.69718074798584\n",
            "Band index: 25, Mean attention score: 7.62586784362793\n",
            "Band index: 111, Mean attention score: 7.457036018371582\n",
            "Band index: 30, Mean attention score: 7.413517475128174\n",
            "Band index: 93, Mean attention score: 7.053833961486816\n",
            "Band index: 77, Mean attention score: 6.987701416015625\n",
            "Band index: 39, Mean attention score: 6.3541765213012695\n",
            "Band index: 106, Mean attention score: 6.209275722503662\n",
            "\n",
            "List of top 50 band indices:\n",
            "[41, 87, 75, 90, 54, 20, 119, 58, 138, 1, 33, 126, 129, 38, 82, 84, 107, 133, 14, 88, 59, 68, 125, 15, 17, 57, 110, 24, 74, 141, 96, 46, 95, 11, 128, 27, 108, 28, 140, 32, 65, 13, 10, 25, 111, 30, 93, 77, 39, 106]\n"
          ]
        }
      ],
      "source": [
        "# First, take the mean across all samples and heads. This will result in a single 144-dim vector.\n",
        "mean_attention = np.mean(all_attn_scores, axis=(0, 1))\n",
        "print('mean_attention:',mean_attention.shape)\n",
        "\n",
        "# Then, sort the bands by attention. This will give you the band indices in descending order of attention.\n",
        "sorted_band_indices = np.argsort(mean_attention)[::-1]\n",
        "print('sorted_band_indices :',sorted_band_indices.shape)\n",
        "\n",
        "# If you want to see the attention values as well, you can sort the mean_attention array in the same order.\n",
        "sorted_attention_values = mean_attention[sorted_band_indices]\n",
        "print('sorted_attention_values :',sorted_attention_values.shape)\n",
        "\n",
        "N = 50 #op N bands\n",
        "top_N_bands = sorted_band_indices[:N]\n",
        "top_N_attention_values = sorted_attention_values[:N]\n",
        "\n",
        "# Print the top N band indices with their attention scores\n",
        "print(f\"Top {N} bands based on the mean attention score:\")\n",
        "for band_index, attention_value in zip(top_N_bands, top_N_attention_values):\n",
        "    print(f\"Band index: {band_index}, Mean attention score: {attention_value}\")\n",
        "\n",
        "# Print the list of top N band indices\n",
        "print(\"\\nList of top {} band indices:\".format(N))\n",
        "print(top_N_bands.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bands_aug=[41, 87, 75, 90, 54, 20, 119, 58, 138, 1, 33, 126, 129, 38, 82, 84, 107, 133, 14, 88, 59, 68, 125, 15, 17, 57, 110, 24, 74, 141, 96, 46, 95, 11, 128, 27, 108, 28, 140, 32, 65, 13, 10, 25, 111, 30, 93, 77, 39, 106]"
      ],
      "metadata": {
        "id": "mAyJmeWExuVu"
      },
      "id": "mAyJmeWExuVu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "998fda90",
      "metadata": {
        "id": "998fda90"
      },
      "outputs": [],
      "source": [
        "#Patch_sixe=1\n",
        "bands_p1=[73, 100, 80, 101, 26, 40, 78, 121, 136, 107, 50, 122, 63, 113, 12, 141, 8, 139, 81, 127, 17, 116, 19, 125, 33, 128, 97, 36, 79, 142, 6, 134, 108, 55, 89, 75, 14, 46, 5, 72, 10, 35, 135, 96, 2, 130, 64, 129, 18, 37]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1E-TAAxz8X7e",
      "metadata": {
        "id": "1E-TAAxz8X7e"
      },
      "outputs": [],
      "source": [
        "band_50=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101]\n",
        "band_45=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21]\n",
        "band_40=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56]\n",
        "band_35=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58]\n",
        "band_30=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85]\n",
        "band_25=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81]\n",
        "band_20=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48]\n",
        "band_15=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75]\n",
        "band_10=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139]\n",
        "band_5=[115, 127, 63, 113, 54]\n",
        "band_80=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72, 131, 70, 49, 71, 65, 66, 45, 110, 93, 22, 16, 14, 123, 107, 100]\n",
        "band_75=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72, 131, 70, 49, 71, 65, 66, 45, 110, 93, 22]\n",
        "band_70=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72, 131, 70, 49, 71, 65]\n",
        "band_65=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72]\n",
        "band_60=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101]\n",
        "band_55=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K_eC9BnwQtwr",
      "metadata": {
        "id": "K_eC9BnwQtwr"
      },
      "outputs": [],
      "source": [
        "bands_p3=[54, 49, 21, 142, 83, 20, 16, 15, 109, 112, 132, 100, 135, 113, 42, 23, 93, 130, 9, 104, 5, 76, 91, 14, 44, 26, 129, 99, 120, 77, 137, 29, 111, 46, 70, 38, 101, 75, 84, 48, 102, 60, 27, 126, 56, 61, 97, 78, 6, 36, 2, 87, 64, 95, 143, 68, 34, 69, 71, 79, 131, 28, 82, 96, 66, 58, 12, 106, 35, 40, 114, 17, 116, 57, 105, 88, 33, 107, 37, 136, 1, 51, 85, 62, 0, 8, 141, 32, 65, 92, 121, 89, 50, 138, 4, 22, 103, 39, 90, 127, 59, 98, 139, 53, 63, 119, 73, 43, 52, 118, 117, 110, 24, 125, 11, 123, 19, 86, 7, 74, 115, 67, 55, 80, 3, 41, 140, 31, 128, 124, 45, 122, 72, 10, 13, 108, 25, 133, 47, 81, 134, 30, 18, 94]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8TdumuTVo6PE",
      "metadata": {
        "id": "8TdumuTVo6PE"
      },
      "outputs": [],
      "source": [
        "# patch_size=5\n",
        "bands_p5=[115, 127, 63, 113, 54, 12, 60, 62, 4, 139, 50, 90, 51, 112, 75, 42, 11, 97, 106, 48, 1, 35, 109, 76, 81, 53, 124, 80, 137, 85, 125, 37, 105, 135, 58, 126, 7, 40, 15, 56, 19, 18, 3, 38, 21, 61, 138, 95, 118, 101, 24, 73, 32, 141, 72, 131, 70, 49, 71, 65, 66, 45, 110, 93, 22, 16, 14, 123, 107, 100, 26, 6, 103, 79, 29, 92, 69, 5, 111, 128, 74, 31, 130, 102, 64, 9, 134, 67, 44, 34, 91, 89, 104, 121, 77, 86, 68, 59, 84, 114, 83, 119, 120, 43, 88, 55, 17, 133, 13, 116, 10, 98, 25, 39, 87, 23, 78, 27, 82, 96, 20, 108, 136, 140, 8, 28, 0, 132, 99, 94, 52, 47, 2, 142, 33, 57, 122, 143, 46, 30, 36, 117, 129, 41]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yNBFu0zVo6Tg",
      "metadata": {
        "id": "yNBFu0zVo6Tg"
      },
      "outputs": [],
      "source": [
        "#Patch_size=7\n",
        "band_p7=[130, 3, 85, 23, 40, 82, 119, 127, 95, 131, 108, 47, 36, 63, 67, 114, 141, 99, 59, 10, 58, 84, 5, 73, 9, 50, 0, 35, 7, 20, 89, 129, 24, 57, 126, 93, 134, 55, 14, 103, 140, 38, 139, 21, 135, 48, 74, 115, 66, 136, 100, 52, 65, 18, 45, 56, 76, 111, 61, 107, 113, 68, 8, 17, 16, 44, 91, 81, 51, 27, 138, 53, 118, 109, 143, 1, 122, 120, 11, 70, 32, 69, 125, 62, 90, 33, 28, 37, 102, 34, 6, 12, 80, 77, 79, 49, 105, 72, 124, 83, 19, 54, 15, 117, 31, 43, 39, 106, 96, 98, 116, 112, 133, 71, 92, 86, 128, 78, 101, 26, 94, 87, 64, 22, 13, 30, 42, 25, 75, 123, 29, 41, 142, 121, 132, 137, 4, 110, 2, 104, 97, 60, 46, 88]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8hEGaJzMeZEa",
      "metadata": {
        "id": "8hEGaJzMeZEa"
      },
      "outputs": [],
      "source": [
        "#Patch_size=9\n",
        "bands_p9=[42, 131, 71, 17, 56, 114, 47, 53, 124, 52, 139, 135, 50, 16, 48, 8, 11, 73, 99, 2, 63, 28, 88, 116, 32, 14, 133, 57, 102, 137, 15, 79, 132, 105, 120, 93, 80, 43, 81, 138, 142, 31, 97, 130, 34, 26, 110, 27, 134, 69, 39, 78, 37, 106, 77, 25, 29, 89, 95, 65, 141, 129, 18, 62, 91, 121, 58, 127, 118, 1, 19, 36, 30, 20, 128, 44, 104, 24, 107, 117, 0, 84, 21, 112, 4, 68, 125, 5, 103, 45, 33, 70, 111, 109, 23, 94, 136, 64, 12, 143, 85, 72, 122, 119, 83, 82, 38, 108, 66, 100, 55, 35, 40, 49, 101, 51, 3, 59, 9, 54, 41, 140, 86, 7, 10, 75, 46, 22, 98, 87, 13, 92, 90, 76, 61, 60, 115, 113, 123, 96, 126, 67, 6, 74]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "egCeY235eZmw",
      "metadata": {
        "id": "egCeY235eZmw"
      },
      "outputs": [],
      "source": [
        "#Patch_size=11\n",
        "bands_p11=[78, 84, 41, 49, 131, 5, 59, 118, 79, 136, 40, 86, 69, 104, 88, 100, 45, 26, 7, 81, 3, 99, 92, 132, 22, 119, 9, 127, 56, 109, 102, 19, 44, 116, 4, 107, 85, 53, 17, 113, 24, 65, 133, 54, 101, 141, 106, 13, 143, 63, 114, 108, 89, 27, 8, 57, 137, 58, 111, 47, 25, 42, 68, 48, 98, 38, 130, 36, 73, 82, 6, 95, 77, 115, 15, 71, 60, 21, 11, 110, 122, 117, 1, 123, 134, 52, 126, 0, 125, 55, 97, 43, 76, 120, 39, 18, 142, 74, 16, 67, 46, 2, 83, 37, 61, 135, 112, 31, 12, 10, 91, 129, 94, 80, 72, 64, 103, 34, 23, 90, 66, 121, 70, 29, 140, 14, 96, 138, 62, 32, 30, 139, 35, 93, 50, 87, 33, 51, 128, 75, 124, 105, 20, 28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0DQopcxFea4K",
      "metadata": {
        "id": "0DQopcxFea4K"
      },
      "outputs": [],
      "source": [
        "#Patch_size=13\n",
        "bands_p13=[143, 99, 109, 13, 16, 33, 94, 97, 53, 68, 89, 77, 93, 47, 87, 19, 2, 136, 57, 104, 26, 132, 38, 11, 76, 40, 140, 111, 119, 96, 102, 108, 90, 131, 113, 49, 62, 58, 56, 35, 128, 18, 14, 51, 110, 66, 24, 117, 43, 71, 137, 81, 74, 141, 120, 135, 60, 72, 103, 9, 21, 8, 3, 107, 123, 6, 37, 129, 138, 65, 44, 116, 78, 22, 10, 39, 73, 23, 121, 28, 86, 88, 139, 75, 31, 46, 27, 92, 115, 25, 114, 30, 91, 20, 48, 98, 50, 59, 17, 41, 79, 12, 101, 133, 105, 29, 69, 63, 1, 142, 15, 54, 126, 42, 7, 124, 85, 106, 125, 4, 0, 82, 80, 36, 5, 34, 67, 118, 70, 55, 64, 84, 52, 112, 61, 83, 32, 122, 95, 45, 127, 100, 130, 134]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MY9fXMGNebFE",
      "metadata": {
        "id": "MY9fXMGNebFE"
      },
      "outputs": [],
      "source": [
        "#Patch_size=15\n",
        "bands_p15=[138, 102, 136, 112, 130, 135, 85, 75, 97, 27, 71, 11, 38, 51, 29, 16, 47, 93, 39, 73, 9, 59, 31, 20, 30, 70, 5, 14, 65, 23, 61, 86, 119, 108, 56, 101, 33, 105, 116, 120, 43, 91, 74, 88, 52, 82, 4, 79, 19, 55, 21, 60, 36, 78, 45, 44, 28, 22, 37, 53, 7, 140, 90, 32, 100, 96, 48, 92, 141, 104, 68, 80, 98, 34, 94, 114, 58, 62, 35, 26, 106, 83, 41, 110, 127, 109, 72, 76, 13, 64, 103, 132, 99, 69, 54, 122, 12, 129, 89, 126, 63, 50, 49, 125, 124, 10, 66, 113, 2, 84, 121, 77, 18, 67, 17, 133, 8, 46, 134, 143, 115, 81, 42, 142, 118, 131, 25, 15, 57, 123, 139, 137, 6, 24, 3, 117, 1, 40, 95, 111, 87, 107, 128, 0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}