{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14525fd7"
      },
      "source": [
        "# 0.0 Import Libraries"
      ],
      "id": "14525fd7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuX4sZpytah9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9db313a-f35b-4184-e7a7-a552e416495e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/212.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/212.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/212.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mat73\n",
            "  Downloading mat73-0.60-py3-none-any.whl (19 kB)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.22.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from mat73) (3.8.0)\n",
            "Installing collected packages: spectral, einops, mat73\n",
            "Successfully installed einops-0.6.1 mat73-0.60 spectral-0.23.1\n"
          ]
        }
      ],
      "source": [
        "pip install spectral mat73  einops"
      ],
      "id": "vuX4sZpytah9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12826627"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "\n",
        "from einops import rearrange\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import io\n",
        "import torch.utils.data\n",
        "import scipy.io as sio\n",
        "import mat73\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "12826627"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import scipy.io\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential,  Model,load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
        "from keras.layers import Conv2D, MaxPooling2D, MaxPooling1D, MaxPool1D, GaussianNoise, GlobalMaxPooling1D, Conv1D, GlobalAveragePooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import KFold\n",
        "import math, time"
      ],
      "metadata": {
        "id": "trRzyDnfF96d"
      },
      "id": "trRzyDnfF96d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dRG3WratmCN"
      },
      "source": [
        "# 1.0 Upload Data"
      ],
      "id": "6dRG3WratmCN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vpwq4Yi-tgjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e2cc4d-caa8-4f9c-dc13-1a4b58ae24d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Vpwq4Yi-tgjs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMin2GFJtorP",
        "outputId": "298170ff-82e3-4560-c8c3-e87ae76552d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model_weights.pth\t\t\t   muufl_gulfport_campus_w_lidar_1.mat\n",
            "lidar_model11_iter1.h5\t\t\t   muufl_scene_labels_screenshot.png\n",
            "muufl_best_model_weights.pth\t\t   MUUFL_TruthForSubImage.mat\n",
            "muufl_gulfport_campus_1_hsi_220_label.mat  README.md\n",
            "muufl_gulfport_campus_3.mat\t\t   tgt_lab_spectra.mat\n",
            "muufl_gulfport_campus_4.mat\t\t   train_test_split_lidar11_iter1.npz\n"
          ]
        }
      ],
      "source": [
        "! ls '/content/drive/MyDrive/A02_RemoteSensingData/MUUFLGUlfportDataCollection/'"
      ],
      "id": "vMin2GFJtorP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "690eba9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d42bda-6a7d-42d1-bab2-cdf7e4c93c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 1: Tree\n",
            "Training Samples: 150\n",
            "Test Samples: 23246\n",
            "Total Samples: 23396\n",
            "\n",
            "Class 2: Mostly grass\n",
            "Training Samples: 150\n",
            "Test Samples: 4270\n",
            "Total Samples: 4420\n",
            "\n",
            "Class 3: Mixed ground surface\n",
            "Training Samples: 150\n",
            "Test Samples: 6882\n",
            "Total Samples: 7032\n",
            "\n",
            "Class 4: Dirt and sand\n",
            "Training Samples: 150\n",
            "Test Samples: 1826\n",
            "Total Samples: 1976\n",
            "\n",
            "Class 5: Road\n",
            "Training Samples: 150\n",
            "Test Samples: 6687\n",
            "Total Samples: 6837\n",
            "\n",
            "Class 6: Water\n",
            "Training Samples: 150\n",
            "Test Samples: 466\n",
            "Total Samples: 616\n",
            "\n",
            "Class 7: Building shadow\n",
            "Training Samples: 150\n",
            "Test Samples: 2233\n",
            "Total Samples: 2383\n",
            "\n",
            "Class 8: Building\n",
            "Training Samples: 150\n",
            "Test Samples: 6240\n",
            "Total Samples: 6390\n",
            "\n",
            "Class 9: Sidewalk\n",
            "Training Samples: 150\n",
            "Test Samples: 1385\n",
            "Total Samples: 1535\n",
            "\n",
            "Class 10: Yellow curb\n",
            "Training Samples: 150\n",
            "Test Samples: 183\n",
            "Total Samples: 333\n",
            "\n",
            "Class 11: Cloth panels\n",
            "Training Samples: 150\n",
            "Test Samples: 269\n",
            "Total Samples: 419\n",
            "\n",
            "{1: {'class_name': 'Tree', 'training': 150, 'test': 23246, 'samples': 23396}, 2: {'class_name': 'Mostly grass', 'training': 150, 'test': 4270, 'samples': 4420}, 3: {'class_name': 'Mixed ground surface', 'training': 150, 'test': 6882, 'samples': 7032}, 4: {'class_name': 'Dirt and sand', 'training': 150, 'test': 1826, 'samples': 1976}, 5: {'class_name': 'Road', 'training': 150, 'test': 6687, 'samples': 6837}, 6: {'class_name': 'Water', 'training': 150, 'test': 466, 'samples': 616}, 7: {'class_name': 'Building shadow', 'training': 150, 'test': 2233, 'samples': 2383}, 8: {'class_name': 'Building', 'training': 150, 'test': 6240, 'samples': 6390}, 9: {'class_name': 'Sidewalk', 'training': 150, 'test': 1385, 'samples': 1535}, 10: {'class_name': 'Yellow curb', 'training': 150, 'test': 183, 'samples': 333}, 11: {'class_name': 'Cloth panels', 'training': 150, 'test': 269, 'samples': 419}}\n"
          ]
        }
      ],
      "source": [
        "# 2.1 Define the class information\n",
        "class_info = [\n",
        "    (1, \"Tree\", 150, 23246, 23396),\n",
        "    (2, \"Mostly grass\", 150, 4270,4420),\n",
        "    (3, \"Mixed ground surface\", 150, 6882, 7032),\n",
        "    (4, \"Dirt and sand\", 150, 1826, 1976),\n",
        "    (5, \"Road\", 150, 6687, 6837),\n",
        "    (6, \"Water\", 150, 466, 616),\n",
        "    (7, \"Building shadow\", 150, 2233, 2383),\n",
        "    (8, \"Building\", 150, 6240, 6390),\n",
        "    (9, \"Sidewalk\", 150, 1385, 1535),\n",
        "    (10, \"Yellow curb\", 150, 183, 333),\n",
        "    (11, \"Cloth panels\", 150, 269, 419)\n",
        "\n",
        "]\n",
        "\n",
        "class_dict = {class_number: {\"class_name\": class_name, \"training\": training, \"test\": test, \"samples\": samples} for class_number, class_name, training, test, samples in class_info}\n",
        "\n",
        "for class_number, class_info in class_dict.items():\n",
        "    print(f\"Class {class_number}: {class_info['class_name']}\")\n",
        "    print(f\"Training Samples: {class_info['training']}\")\n",
        "    print(f\"Test Samples: {class_info['test']}\")\n",
        "    print(f\"Total Samples: {class_info['samples']}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "print(class_dict)"
      ],
      "id": "690eba9c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NMT_vf3t3Bg"
      },
      "outputs": [],
      "source": [
        "# path\n",
        "path ='/content/drive/MyDrive/A02_RemoteSensingData/MUUFLGUlfportDataCollection/'\n",
        "hsi_data_campus3=scipy.io.loadmat( path + \"muufl_gulfport_campus_3.mat\")\n",
        "hsi_data_campus4=scipy.io.loadmat( path + \"muufl_gulfport_campus_4.mat\")\n",
        "lidar_data= scipy.io.loadmat(path + \"muufl_gulfport_campus_1_hsi_220_label.mat\")\n",
        "gt_data=scipy.io.loadmat(path + \"MUUFL_TruthForSubImage.mat\")"
      ],
      "id": "4NMT_vf3t3Bg"
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "# Load the MATLAB file\n",
        "mat = scipy.io.loadmat(path + \"muufl_gulfport_campus_1_hsi_220_label.mat\")\n",
        "\n",
        "# Access the hyperspectral data\n",
        "hsi_data = mat['hsi']\n",
        "\n",
        "# Depending on the structure of the .mat file, you might have to access nested structures\n",
        "# For example, if the data is nested within a structure or cell array in MATLAB, you might need to do something like this:\n",
        "hsi_data = mat['hsi'][0,0]['Data']\n",
        "print('hsi_data .shape:',hsi_data .shape)\n",
        "\n",
        "# Access Ground truuth\n",
        "\n",
        "hsi = ((mat['hsi'])[0])[0]\n",
        "# RGB Image\n",
        "rgbIm = hsi[-1]\n",
        "\n",
        "# Ground truth\n",
        "truth = ((hsi[-2])[0])[-1]\n",
        "truth = truth[-1]\n",
        "print('truth.shape:', truth.shape)\n",
        "\n",
        "# Access LiDAR\n",
        "lidar = ((((hsi[-4])[0])[0])[0])[0]\n",
        "\n",
        "# x, y, z. z contains Height and Intensity\n",
        "x, y, z, info = lidar[0], lidar[1], lidar[2], lidar[3]\n",
        "\n",
        "print('x.shape:', x.shape)\n",
        "print('y.shape:', y.shape)\n",
        "print('z.shape:', z.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIzsxjGjQT4k",
        "outputId": "cce91e3e-1132-4be4-f5cc-afdfca9461df"
      },
      "id": "bIzsxjGjQT4k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_data .shape: (325, 220, 64)\n",
            "truth.shape: (325, 220)\n",
            "x.shape: (1, 220)\n",
            "y.shape: (1, 325)\n",
            "z.shape: (325, 220, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hsi_data=hsi_data\n",
        "lidar_data=z\n",
        "gt_data=truth\n",
        "print('hsi_data shap:', hsi_data.shape)\n",
        "print('lidar_data shap:', lidar_data.shape)\n",
        "print('gt_data shap:', gt_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF3sd4Pb4VRs",
        "outputId": "b2301ca4-cd01-45b9-dc60-1c68a2fe414c"
      },
      "id": "uF3sd4Pb4VRs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_data shap: (325, 220, 64)\n",
            "lidar_data shap: (325, 220, 2)\n",
            "gt_data shap: (325, 220)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create meshgrid\n",
        "# Height and Intesity of LiDAR\n",
        "height = z[:,:,0]\n",
        "intensity = z[:,:,1]\n",
        "# Shape of Height and Intensity is same\n",
        "row, col = intensity.shape\n",
        "x = np.arange(0, col, 1)\n",
        "y = np.arange(0, row, 1)\n",
        "xx, yy = np.meshgrid(x, y)\n",
        "\n",
        "# 3D visualization of Intensity\n",
        "fig = plt.figure(figsize=(4,4))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "I = ax.scatter(xx, yy, intensity, c=intensity, s= 3, cmap= 'coolwarm')\n",
        "fig.colorbar(I, ax= ax)\n",
        "plt.show()\n",
        "\n",
        "# 3D visualization of Height\n",
        "fig = plt.figure(figsize=(4,4))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "I = ax.scatter(xx, yy, height, c=height, s= 3, cmap= 'coolwarm')\n",
        "fig.colorbar(I, ax= ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "U6_bc48Raw1g",
        "outputId": "cd59f243-f8f2-40c8-a63d-0099a233680b"
      },
      "id": "U6_bc48Raw1g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 Data Preprocessing"
      ],
      "metadata": {
        "id": "6NHUyxT7-15r"
      },
      "id": "6NHUyxT7-15r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Find the NxN neighbourhood around each pixel"
      ],
      "metadata": {
        "id": "-KzlUX0_HLrs"
      },
      "id": "-KzlUX0_HLrs"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Samples Extraction\n",
        "\n",
        "# Create a mask with all class labels\n",
        "mask = np.copy(gt_data)\n",
        "\n",
        "# Set the background class to 0\n",
        "mask[mask == 0] = 0\n",
        "\n",
        "# Define patch size and stride\n",
        "patch_size = 9\n",
        "stride = 1\n",
        "\n",
        "# Create an empty list to store patches and labels\n",
        "hsi_samples = []\n",
        "lidar_samples = []\n",
        "labels = []\n",
        "\n",
        "# Initialize a dictionary to store class count\n",
        "class_count = {i: 0 for i in class_dict.keys()}\n",
        "\n",
        "# Function to check if all classes have the required number of samples\n",
        "def all_classes_completed(class_count, class_dict):\n",
        "    return all(class_count[class_num] == class_dict[class_num][\"samples\"] for class_num in class_dict.keys())\n",
        "\n",
        "while not all_classes_completed(class_count, class_dict):\n",
        "    # Loop through the ground truth data\n",
        "    for label in class_dict.keys():\n",
        "        # Get the coordinates of the ground truth pixels\n",
        "        coords = np.argwhere((gt_data == label) & (mask > 0))\n",
        "\n",
        "        # Shuffle the coordinates to randomize the patch extraction\n",
        "        np.random.shuffle(coords)\n",
        "\n",
        "        for coord in coords:\n",
        "            i, j = coord\n",
        "            # Calculate the patch indices\n",
        "            i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
        "            j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
        "\n",
        "            # Check if the indices are within the bounds of the HSI data\n",
        "            if i_start >= 0 and i_end <= hsi_data.shape[0] and j_start >= 0 and j_end <= hsi_data.shape[1]:\n",
        "                # Extract the patch\n",
        "                hsi_patch = hsi_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # Extract the LiDAR patch\n",
        "                lidar_patch = lidar_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # If the class count is less than the required samples\n",
        "                if class_count[label] < class_dict[label][\"samples\"]:\n",
        "                    # Append the patch and its label to the list\n",
        "                    hsi_samples.append(hsi_patch)\n",
        "                    lidar_samples.append(lidar_patch)\n",
        "                    labels.append(label)\n",
        "                    class_count[label] += 1\n",
        "\n",
        "                    # If all classes have the required number of samples, exit the loop\n",
        "                    if all_classes_completed(class_count, class_dict):\n",
        "                        break\n",
        "\n",
        "# Convert the list of patches and labels into arrays\n",
        "hsi_samples = np.array(hsi_samples)\n",
        "lidar_samples = np.array(lidar_samples)\n",
        "labels = np.array(labels)\n",
        "print('hsi_samples shape:', hsi_samples.shape)\n",
        "print('lidar_samples shape:', lidar_samples.shape)\n",
        "print('labels shape:', labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iM1KQNJ4LLd",
        "outputId": "c8fc39d9-44fc-432b-f05a-979cf7c401f2"
      },
      "id": "0iM1KQNJ4LLd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_samples shape: (55337, 9, 9, 64)\n",
            "lidar_samples shape: (55337, 9, 9, 2)\n",
            "labels shape: (55337,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Split the Split the data into train-test, save in npz file and load the file"
      ],
      "metadata": {
        "id": "GEfGyz3vH5XN"
      },
      "id": "GEfGyz3vH5XN"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize empty lists to store the train and test samples\n",
        "hsi_train = []\n",
        "hsi_test = []\n",
        "lidar_train = []\n",
        "lidar_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "# Split the data for each class\n",
        "for class_number, class_info in class_dict.items():\n",
        "    # Get the indices of the samples for this class\n",
        "    class_indices = np.where(labels == class_number)[0]\n",
        "\n",
        "    # Shuffle the class indices\n",
        "    np.random.shuffle(class_indices)\n",
        "\n",
        "    # Split the class indices into training and test indices\n",
        "    train_indices = class_indices[:class_info[\"training\"]]\n",
        "    test_indices = class_indices[class_info[\"training\"]:class_info[\"samples\"]]\n",
        "\n",
        "    # Add the selected training samples to the train lists\n",
        "    hsi_train.append(hsi_samples[train_indices])\n",
        "    lidar_train.append(lidar_samples[train_indices])\n",
        "    y_train.append(labels[train_indices])\n",
        "\n",
        "    # Add the remaining samples to the test lists\n",
        "    hsi_test.append(hsi_samples[test_indices])\n",
        "    lidar_test.append(lidar_samples[test_indices])\n",
        "    y_test.append(labels[test_indices])\n",
        "\n",
        "# Concatenate the train and test lists to create the train and test arrays\n",
        "hsi_train = np.concatenate(hsi_train)\n",
        "hsi_test = np.concatenate(hsi_test)\n",
        "lidar_train = np.concatenate(lidar_train)\n",
        "lidar_test = np.concatenate(lidar_test)\n",
        "y_train = np.concatenate(y_train)\n",
        "y_test = np.concatenate(y_test)\n",
        "\n",
        "# Print the shapes of the train and test arrays\n",
        "print('hsi_train shape:', hsi_train.shape)\n",
        "print('hsi_test shape:', hsi_test.shape)\n",
        "print('lidar_train shape:', lidar_train.shape)\n",
        "print('lidar_test shape:', lidar_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJCY-sJY7Gs6",
        "outputId": "73a1f77f-6e42-4855-a4f5-79efcb07dd5b"
      },
      "id": "CJCY-sJY7Gs6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_train shape: (1650, 9, 9, 64)\n",
            "hsi_test shape: (53687, 9, 9, 64)\n",
            "lidar_train shape: (1650, 9, 9, 2)\n",
            "lidar_test shape: (53687, 9, 9, 2)\n",
            "y_train shape: (1650,)\n",
            "y_test shape: (53687,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Normalize train and test data"
      ],
      "metadata": {
        "id": "CU_671yhINxu"
      },
      "id": "CU_671yhINxu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize channel 1(Height) of the train data\n",
        "ch1 = hsi_train[:, :, :, 0]\n",
        "pmin = np.amin(ch1)\n",
        "pmax = np.amax(ch1)\n",
        "ch1 = (ch1-pmin) / (pmax- pmin)\n",
        "\n",
        "# Normalize channel 2(Intensity) of the train data\n",
        "ch2 = hsi_train[:, :, :, 1]\n",
        "pmin1 = np.amin(ch2)\n",
        "pmax1 = np.amax(ch2)\n",
        "ch2 = (ch2-pmin1) / (pmax1- pmin1)\n",
        "\n",
        "hsi_train[:,:,:,0] = ch1\n",
        "hsi_train[:,:,:,1] = ch2\n",
        "\n",
        "# Normalize channel 1(Height) the test data\n",
        "ch3 = hsi_test[:, :, :, 0]\n",
        "ch3 = (ch3-pmin) / (pmax- pmin)\n",
        "\n",
        "# Normalize channel 2(Intensity) of the test data\n",
        "ch4 = hsi_test[:, :, :, 1]\n",
        "ch4 = (ch4-pmin1) / (pmax1- pmin1)\n",
        "\n",
        "hsi_test[:,:,:,0] = ch3\n",
        "hsi_test[:,:,:,1] = ch4\n",
        "\n",
        "print('ch1 shape:',ch1.shape)\n",
        "print('ch2 shape:',ch2.shape)\n",
        "print('ch3 shape:',ch3.shape)\n",
        "print('ch4 shape:',ch4.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgn0AeJ3H3kI",
        "outputId": "64ea89f4-33fe-4743-fc84-e54e594fd85a"
      },
      "id": "Hgn0AeJ3H3kI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ch1 shape: (1650, 9, 9)\n",
            "ch2 shape: (1650, 9, 9)\n",
            "ch3 shape: (53687, 9, 9)\n",
            "ch4 shape: (53687, 9, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 One-hot encode labels"
      ],
      "metadata": {
        "id": "zKZs8IxpIn9m"
      },
      "id": "zKZs8IxpIn9m"
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding of labels\n",
        "# Substract 1 from labels\n",
        "y_train_adj = y_train - 1\n",
        "y_test_adj = y_test - 1\n",
        "# y_train_adj = y_train\n",
        "# y_test_adj = y_test\n",
        "\n",
        "# One hot encoding of labels\n",
        "y_train = to_categorical(y_train_adj, num_classes = 11, dtype =\"int32\")\n",
        "y_test = to_categorical(y_test_adj, num_classes = 11, dtype =\"int32\")\n",
        "\n",
        "print('y_train.shape:',y_train.shape)\n",
        "print('y_test.shape:',y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKoYCN1xIU-t",
        "outputId": "4d165b1a-889b-42f6-abde-2a89f02e7c56"
      },
      "id": "VKoYCN1xIU-t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train.shape: (1650, 11)\n",
            "y_test.shape: (53687, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.0 Model Building"
      ],
      "metadata": {
        "id": "J7Ko9uYgIf2D"
      },
      "id": "J7Ko9uYgIf2D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Configuration"
      ],
      "metadata": {
        "id": "u1rQcEYmVP_W"
      },
      "id": "u1rQcEYmVP_W"
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Configuration\n",
        "class Config:\n",
        "    def __init__(self,in_channels,num_patches,kernel_size,patch_size,emb_size, dim,depth,heads,dim_head,mlp_dim,num_classes,dropout,pos_emb_size,class_emb_size,stride, ):\n",
        "        self.in_channels = in_channels\n",
        "        self.num_patches = num_patches\n",
        "        self.kernel_size = kernel_size\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "        self.pos_emb_size = pos_emb_size\n",
        "        self.class_emb_size = class_emb_size\n",
        "        self.stride = stride\n"
      ],
      "metadata": {
        "id": "35qAdvCxIPJ8"
      },
      "id": "35qAdvCxIPJ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 EmbeddingPatches"
      ],
      "metadata": {
        "id": "U5T_YmiQespq"
      },
      "id": "U5T_YmiQespq"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(\n",
        "            config.in_channels,\n",
        "            config.emb_size,\n",
        "            kernel_size=config.patch_size,\n",
        "            stride=config.stride,\n",
        "        )\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, config.num_patches + 1, config.emb_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.emb_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n",
        "        x = x + self.pos_embedding[:, :x.size(1)]\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "H0MaMR75xcKE"
      },
      "id": "H0MaMR75xcKE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Optianl Adding bandoutput"
      ],
      "metadata": {
        "id": "7DyoAhpwF1WO"
      },
      "id": "7DyoAhpwF1WO"
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, lidar_config, hsi_config):\n",
        "        super(CrossAttention, self).__init__()\n",
        "\n",
        "        # Define module parameters\n",
        "        self.dim_head = lidar_config.dim_head\n",
        "        self.num_patches=hsi_config.num_patches\n",
        "        self.num_classes=hsi_config.num_classes\n",
        "        self.num_heads = lidar_config.heads\n",
        "        self.sqrt_dim_head = math.sqrt(self.dim_head)\n",
        "\n",
        "        # Define linear layers for transforming Q, K, and V\n",
        "        self.to_q = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "        self.to_k = nn.Linear(hsi_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "        self.to_v = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "\n",
        "        self.to_out = nn.Linear( self.num_heads* self.dim_head,self.num_patches )  # added\n",
        "\n",
        "        # Final classification layer\n",
        "        self.class_layer = nn.Linear(self.num_patches, self.num_classes)\n",
        "\n",
        "    def forward(self, lidar, hsi):\n",
        "        B, N_lidar, _ = lidar.size()\n",
        "        _, N_hsi, _ = hsi.size()\n",
        "\n",
        "        outputs = []\n",
        "        attn_scores = []  # List to store attention scores\n",
        "\n",
        "       # Iterate over lidar and hsi patches\n",
        "        for i in range(2, N_lidar):\n",
        "        #for i in range(N_lidar):\n",
        "\n",
        "            lidar_patch = lidar[:, i].unsqueeze(1)  # Add a dimension for number of patches\n",
        "            for j in range(1, N_hsi):\n",
        "            #for j in range(N_hsi):\n",
        "\n",
        "                hsi_patch = hsi[:, j].unsqueeze(1)  # Add a dimension for number of patches\n",
        "                Q = self.to_q(lidar_patch)\n",
        "                K = self.to_k(hsi_patch)\n",
        "                V = self.to_v(lidar_patch)\n",
        "\n",
        "                Q = Q / self.sqrt_dim_head\n",
        "                attn_weights = F.softmax(Q @ K.transpose(-2, -1), dim=-1)\n",
        "\n",
        "                attn_output = attn_weights @ V\n",
        "                attn_score = self.to_out(attn_output)  # added\n",
        "                outputs.append(attn_output)\n",
        "                attn_scores.append(attn_score)  # Store the attention scores\n",
        "\n",
        "\n",
        "\n",
        "         # Concatenate all the outputs\n",
        "        output = torch.cat(outputs, dim=1)\n",
        "        attn_scores = torch.cat(attn_scores, dim=1)  # Concatenate all the attention scores\n",
        "\n",
        "        #Pass the output through the classification layer\n",
        "        # Compute the mean over the sequence dimension\n",
        "        #out_mean = torch.mean(output, dim=1)\n",
        "        # Pass the output through the classification layer\n",
        "        #out_class = self.class_layer(out_mean)\n",
        "\n",
        "        # Compute max_attn_indices here, after you compute the attention scores\n",
        "        #max_attn_indices = torch.argmax(attn_scores, dim=-1)\n",
        "\n",
        "        #return output, out_class, attn_scores #max_attn_indices  # Return both output and attention scores\n",
        "        return output, attn_scores #max_attn_indices  # Return both output and attention scores\n"
      ],
      "metadata": {
        "id": "7ZwPxVLX4uwk"
      },
      "id": "7ZwPxVLX4uwk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CrossAttention(nn.Module):\n",
        "#     def __init__(self, lidar_config, hsi_config):\n",
        "#         super(CrossAttention, self).__init__()\n",
        "\n",
        "#         self.dim_head = lidar_config.dim_head\n",
        "#         self.num_patches = hsi_config.num_patches\n",
        "#         self.num_heads = lidar_config.heads\n",
        "#         self.sqrt_dim_head = math.sqrt(self.dim_head)\n",
        "\n",
        "#         self.to_q = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "#         self.to_k = nn.Linear(hsi_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "#         self.to_v = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "\n",
        "#         self.to_out = nn.Linear(self.num_heads * self.dim_head, self.num_patches)\n",
        "\n",
        "#         self.aggregator = nn.Linear(self.dim_head * self.num_heads, lidar_config.dim)\n",
        "\n",
        "\n",
        "#     def forward(self, lidar, hsi):\n",
        "#         B, N_lidar, _ = lidar.size()\n",
        "#         _, N_hsi, _ = hsi.size()\n",
        "\n",
        "#         outputs = torch.zeros((B, N_hsi, self.num_heads * self.dim_head), device=hsi.device)\n",
        "#         attn_scores = torch.zeros((B, N_hsi), device=hsi.device)\n",
        "\n",
        "#         for j in range(1, N_hsi):\n",
        "#           hsi_patch = hsi[:, j].unsqueeze(1)\n",
        "#           lidar_attn_outputs = []\n",
        "#           lidar_attn_scores = []\n",
        "\n",
        "#           for i in range(0, N_lidar):\n",
        "#             lidar_patch = lidar[:, i].unsqueeze(1)\n",
        "\n",
        "#             Q = self.to_q(lidar_patch)\n",
        "#             K = self.to_k(hsi_patch)\n",
        "#             V = self.to_v(lidar_patch)\n",
        "\n",
        "#             Q = Q / self.sqrt_dim_head\n",
        "#             attn_weights = F.softmax(Q @ K.transpose(-2, -1), dim=-1)\n",
        "\n",
        "#             attn_output = attn_weights @ V\n",
        "#             attn_score = self.to_out(attn_output)\n",
        "\n",
        "#             lidar_attn_outputs.append(attn_output)\n",
        "#             lidar_attn_scores.append(attn_score)\n",
        "\n",
        "#           # Aggregate the attention outputs and scores for all LiDAR patches for the current HSI patch\n",
        "#           aggregated_attn_output = self.aggregator(torch.stack(lidar_attn_outputs).sum(dim=0))\n",
        "#           aggregated_attn_score = torch.stack(lidar_attn_scores).sum(dim=0)\n",
        "\n",
        "#           outputs[:, j] = aggregated_attn_output.squeeze()\n",
        "#           attn_scores[:, j] = aggregated_attn_score.squeeze()\n",
        "#         return outputs, attn_scores\n"
      ],
      "metadata": {
        "id": "98V0yz6JScKg"
      },
      "id": "98V0yz6JScKg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttentionModel(nn.Module):\n",
        "    def __init__(self, hsi_config, lidar_config):\n",
        "        super().__init__()\n",
        "        self.hsi_patch_embedding = PatchEmbedding(hsi_config)\n",
        "        self.lidar_patch_embedding = PatchEmbedding(lidar_config)\n",
        "        self.cross_attention = CrossAttention(lidar_config, hsi_config)\n",
        "\n",
        "    def forward(self, lidar_data, hsi_data):\n",
        "        # Apply PatchEmbedding\n",
        "        lidar_emb = self.lidar_patch_embedding(lidar_data)\n",
        "        hsi_emb = self.hsi_patch_embedding(hsi_data)\n",
        "\n",
        "        # Apply CrossAttention\n",
        "        #output, out_class, attn_scores = self.cross_attention(lidar_emb, hsi_emb)\n",
        "        output,  attn_scores = self.cross_attention(lidar_emb, hsi_emb)\n",
        "\n",
        "        return output, attn_scores  # Return both output and attention scores\n"
      ],
      "metadata": {
        "id": "at2xTwGv91ng"
      },
      "id": "at2xTwGv91ng",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1.1 Parameters Setting\n",
        "# Hsi configuration\n",
        "hsi_config = Config(\n",
        "    in_channels=81,  # Each sample covers 144 bands\n",
        "    num_patches=64,  # 25*1*144 bands are grouped into 3600 groups\n",
        "    kernel_size=1,  # Adjusted to match new patch size\n",
        "    patch_size=9,  # Adjusted to match new patch size (5*5, 144/24=6)\n",
        "    emb_size=128,  # Embedding size, this can be changed\n",
        "    dim=128,  # Dimension of the transformer, this can be changed\n",
        "    depth=6,  # Number of transformer layers, this can be changed\n",
        "    heads=8,  # Number of attention heads, this can be changed\n",
        "    dim_head=64,  # Dimension of each attention head, this can be changed\n",
        "    mlp_dim=256,  # Dimension of the MLP layer, this can be changed\n",
        "    num_classes=11,  # Number of classes, this can be changed\n",
        "    dropout=0.4,  # Dropout rate, this can be changed\n",
        "    pos_emb_size=128,  # Position embedding size, this can be changed\n",
        "    class_emb_size=128,  # Class embedding size, this can be changed\n",
        "    stride=1  # Stride for the convolution, this can be changed\n",
        ")\n",
        "\n",
        "\n",
        "# Lidara configuration\n",
        "lidar_config = Config(\n",
        "    in_channels=81,  # lidar group has 1 channels\n",
        "    num_patches=2,  # 1 band for Lidar\n",
        "    kernel_size=1,  # Adjusted to match new patch size\n",
        "    patch_size=9, # Adjusted to match new patch size\n",
        "    emb_size=128,  # Embedding size, this can be changed\n",
        "    dim=128,  # Dimension of the transformer, this can be changed\n",
        "    depth=6,  # Number of transformer layers, this can be changed\n",
        "    heads=8,  # Number of attention heads, this can be changed\n",
        "    dim_head=64,  # Dimension of each attention head, this can be changed\n",
        "    mlp_dim=256,  # Dimension of the MLP layer, this can be changed\n",
        "    num_classes=11,  # Number of classes, this can be changed\n",
        "    dropout=0.4,  # Dropout rate, this can be changed\n",
        "    pos_emb_size=128,  # Position embedding size, this can be changed\n",
        "    class_emb_size=128,  # Class embedding size, this can be changed\n",
        "    stride=1  # Stride for the convolution, this can be changed\n",
        ")\n"
      ],
      "metadata": {
        "id": "b8oC00YgYsSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "7e48022f-a3c5-4d41-ec40-2b3ce01c3d09"
      },
      "id": "b8oC00YgYsSS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-43e471b7f8d1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#3.1.1 Parameters Setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Hsi configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m hsi_config = Config(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Each sample covers 144 bands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 25*1*144 bands are grouped into 3600 groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch = hsi_train  # Shape: (1650, 5, 5, 64)\n",
        "lidar_batch = lidar_train  # Shape: (1650, 5, 5, 2)\n",
        "print('hsi_batch sahpe before transpose:', hsi_batch.shape)\n",
        "print('lidar_batch sahpe before transpose:', lidar_batch.shape)\n",
        "# Transpose the data to have the channel dimension at the correct place\n",
        "hsi_batch = hsi_batch.transpose(0, 3, 1, 2)  # New shape: (1650, 64, 5, 5)\n",
        "lidar_batch = lidar_batch.transpose(0, 3, 1, 2)  # New shape: (2832, 2, 5, 5)\n",
        "print('hsi_batch sahpe after transpose:', hsi_batch.shape)\n",
        "print('lidar_batch shape after transpose:', lidar_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_WjbDjBeODf",
        "outputId": "1af8aaae-fcc2-4101-fa2e-0bcb2ce1c682"
      },
      "id": "b_WjbDjBeODf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch sahpe before transpose: (1650, 9, 9, 64)\n",
            "lidar_batch sahpe before transpose: (1650, 9, 9, 2)\n",
            "hsi_batch sahpe after transpose: (1650, 64, 9, 9)\n",
            "lidar_batch shape after transpose: (1650, 2, 9, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "one_hsi_batch = hsi_train[:1]  # Shape: (1, 5, 5, 64)\n",
        "one_lidar_batch = lidar_train[:1]  # Shape: (1, 5, 5, 2)\n",
        "print('one_hsi_batch sahpe before transpose:', one_hsi_batch.shape)\n",
        "print('one_lidar_batch sahpe before transpose:', one_lidar_batch.shape)\n",
        "# Transpose the data to have the channel dimension at the correct place\n",
        "one_hsi_batch = one_hsi_batch.transpose(0, 3, 1, 2)  # New shape: (1, 64, 5, 5)\n",
        "one_lidar_batch = one_lidar_batch.transpose(0, 3, 1, 2)  # New shape: (1, 2, 5, 5)\n",
        "print('hsi_batch sahpe after transpose:', one_hsi_batch.shape)\n",
        "print('lidar_batch shape after transpose:', one_lidar_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWEESEK3a-hX",
        "outputId": "26285b18-75fc-4dcd-d12b-cb75443335f0"
      },
      "id": "GWEESEK3a-hX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one_hsi_batch sahpe before transpose: (1, 9, 9, 64)\n",
            "one_lidar_batch sahpe before transpose: (1, 9, 9, 2)\n",
            "hsi_batch sahpe after transpose: (1, 64, 9, 9)\n",
            "lidar_batch shape after transpose: (1, 2, 9, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "one_hsi_batch = hsi_train[:1]  # Shape: (1, 5, 5, 64)\n",
        "one_lidar_batch = lidar_train[:1]  # Shape: (1, 5, 5, 2)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "one_hsi_batch_flat = torch.from_numpy(one_hsi_batch.astype(np.float32).reshape(1, hsi_config.in_channels, hsi_config.num_patches, 1)).to(device)\n",
        "one_lidar_batch_flat = torch.from_numpy(one_lidar_batch.astype(np.float32).reshape(1, lidar_config.in_channels, lidar_config.num_patches, 1)).to(device)\n",
        "\n",
        "# Initialize the patch embedding module\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "one_hsi_batch_embedded = hsi_patch_embedding(one_hsi_batch_flat).to(device)\n",
        "one_lidar_batch_embedded = lidar_patch_embedding(one_lidar_batch_flat).to(device)\n",
        "\n",
        "print('one_hsi_batch_embedded shape:', one_hsi_batch_embedded.shape)\n",
        "print('one_lidar_batch_embedded shape:', one_lidar_batch_embedded.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjXtH2S9Q81f",
        "outputId": "4f4a1b7b-d8af-45c5-e629-cb0c6737325d"
      },
      "id": "qjXtH2S9Q81f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one_hsi_batch_embedded shape: torch.Size([1, 65, 128])\n",
            "one_lidar_batch_embedded shape: torch.Size([1, 3, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intialisation CrossAttention Class"
      ],
      "metadata": {
        "id": "gpa3oU5ndrhn"
      },
      "id": "gpa3oU5ndrhn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch_6 = hsi_train[:6]  # Shape: (6, 5, 5, 64)\n",
        "lidar_batch_6 = lidar_train[:6]  # Shape: (6, 5, 5, 2)\n",
        "\n",
        "print(\"hsi_batch_6  shape:\", hsi_batch_6 .shape)\n",
        "print(\"lidar_batch_6 shape:\", lidar_batch_6.shape)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "hsi_batch_6_flat = torch.from_numpy(hsi_batch_6.astype(np.float32).reshape(6, hsi_config.in_channels, hsi_config.num_patches, 1))\n",
        "lidar_batch_6_flat = torch.from_numpy(lidar_batch_6.astype(np.float32).reshape(6, lidar_config.in_channels, lidar_config.num_patches, 1))\n",
        "\n",
        "print(\"hsi_batch_6_flat  shape:\", hsi_batch_6_flat .shape)\n",
        "print(\"lidar_batch_6_flat shape:\", lidar_batch_6_flat.shape)\n",
        "\n",
        "hsi_batch_6_flat = hsi_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_6_flat = lidar_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "hsi_batch_6_embedded  = hsi_patch_embedding(hsi_batch_6_flat)\n",
        "lidar_batch_6_embedded = lidar_patch_embedding(lidar_batch_6_flat)\n",
        "\n",
        "device = torch.device(\"cuda:0\")  # Define the device (GPU)\n",
        "\n",
        "# Move the tensors to the desired device\n",
        "hsi_batch_6_embedded = hsi_batch_6_embedded.to(device)\n",
        "lidar_batch_6_embedded = lidar_batch_6_embedded.to(device)\n",
        "print(\"hsi_batch_6_embedded  shape:\", hsi_batch_6_embedded .shape)\n",
        "print(\"lidar_batch_6_embedded shape:\", lidar_batch_6_embedded.shape)\n",
        "\n",
        "# Define the dimension of the model and the number of heads\n",
        "d_model = hsi_config.emb_size  # the output dimension of PatchEmbedding\n",
        "num_heads = hsi_config.heads  # the number of attention heads in the transformer\n",
        "\n",
        "# Initialize CrossAttention module\n",
        "cross_attention = CrossAttention(lidar_config, hsi_config).to(device)\n",
        "\n",
        "# Apply the cross attention\n",
        "output,attn_scores = cross_attention(lidar_batch_6_embedded, hsi_batch_6_embedded)\n",
        "\n",
        "\n",
        "print(\"Cross attention output shape:\", attn_scores.shape)\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYlhrA99ncUq",
        "outputId": "2cda2482-cbeb-4914-80fe-b5479fc0d9a5"
      },
      "id": "XYlhrA99ncUq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch_6  shape: (6, 9, 9, 64)\n",
            "lidar_batch_6 shape: (6, 9, 9, 2)\n",
            "hsi_batch_6_flat  shape: torch.Size([6, 81, 64, 1])\n",
            "lidar_batch_6_flat shape: torch.Size([6, 81, 2, 1])\n",
            "hsi_batch_6_embedded  shape: torch.Size([6, 65, 128])\n",
            "lidar_batch_6_embedded shape: torch.Size([6, 3, 128])\n",
            "Cross attention output shape: torch.Size([6, 64, 64])\n",
            "Output shape: torch.Size([6, 64, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJTi3xzCYzd1",
        "outputId": "9538b093-0cfc-4c98-c4ab-3c20597ff22f"
      },
      "id": "nJTi3xzCYzd1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossAttentionModel(\n",
              "  (hsi_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(81, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (lidar_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(81, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (cross_attention): CrossAttention(\n",
              "    (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
              "    (class_layer): Linear(in_features=64, out_features=11, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1  Training DataLoader for Cross Attention Module"
      ],
      "metadata": {
        "id": "yem48RAvJSLH"
      },
      "id": "yem48RAvJSLH"
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperspectralDataset(Dataset):\n",
        "    def __init__(self, hsi_samples, lidar_samples, labels):\n",
        "        self.hsi_samples = hsi_samples\n",
        "        self.lidar_samples = lidar_samples\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hsi_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hsi_patch = self.hsi_samples[idx].float().to(device)\n",
        "        lidar_patch = self.lidar_samples[idx].float().to(device)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert label to a tensor and reshape to match the model's output shape\n",
        "        label = torch.tensor(label).repeat(hsi_patch.shape[1], 1).to(device)\n",
        "\n",
        "        return hsi_patch, lidar_patch, label\n"
      ],
      "metadata": {
        "id": "1Sw5JbJ23X3n"
      },
      "id": "1Sw5JbJ23X3n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize empty lists to store the train and test samples\n",
        "hsi_train = []\n",
        "hsi_test = []\n",
        "lidar_train = []\n",
        "lidar_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "# Split the data for each class\n",
        "for class_number, class_info in class_dict.items():\n",
        "    # Get the indices of the samples for this class\n",
        "    class_indices = np.where(labels == class_number)[0]\n",
        "\n",
        "    # Shuffle the class indices\n",
        "    np.random.shuffle(class_indices)\n",
        "\n",
        "    # Split the class indices into training and test indices\n",
        "    train_indices = class_indices[:class_info[\"training\"]]\n",
        "    test_indices = class_indices[class_info[\"training\"]:class_info[\"samples\"]]\n",
        "\n",
        "    # Add the selected training samples to the train lists\n",
        "    hsi_train.append(hsi_samples[train_indices])\n",
        "    lidar_train.append(lidar_samples[train_indices])\n",
        "    y_train.append(labels[train_indices])\n",
        "\n",
        "    # Add the remaining samples to the test lists\n",
        "    hsi_test.append(hsi_samples[test_indices])\n",
        "    lidar_test.append(lidar_samples[test_indices])\n",
        "    y_test.append(labels[test_indices])\n",
        "\n",
        "# Concatenate the train and test lists to create the train and test arrays\n",
        "hsi_train = np.concatenate(hsi_train)\n",
        "hsi_test = np.concatenate(hsi_test)\n",
        "lidar_train = np.concatenate(lidar_train)\n",
        "lidar_test = np.concatenate(lidar_test)\n",
        "y_train = np.concatenate(y_train)\n",
        "y_test = np.concatenate(y_test)\n",
        "\n",
        "# Print the shapes of the train and test arrays\n",
        "print('hsi_train shape:', hsi_train.shape)\n",
        "print('hsi_test shape:', hsi_test.shape)\n",
        "print('lidar_train shape:', lidar_train.shape)\n",
        "print('lidar_test shape:', lidar_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDmqRDUoBwmF",
        "outputId": "bf2507ea-491c-4379-c22f-1eb07708cf33"
      },
      "id": "lDmqRDUoBwmF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_train shape: (1650, 9, 9, 64)\n",
            "hsi_test shape: (53687, 9, 9, 64)\n",
            "lidar_train shape: (1650, 9, 9, 2)\n",
            "lidar_test shape: (53687, 9, 9, 2)\n",
            "y_train shape: (1650,)\n",
            "y_test shape: (53687,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# # One hot encoding of labels\n",
        "# # Substract 1 from labels\n",
        "# y_train_adj = augmented_training_labels - 1\n",
        "# y_test_adj = y_test - 1\n",
        "\n",
        "# # One hot encoding of labels\n",
        "# y_train = to_categorical(y_train_adj, num_classes = 11, dtype =\"int32\")\n",
        "# y_test = to_categorical(y_test_adj, num_classes = 11, dtype =\"int32\")\n",
        "\n",
        "# print('y_train.shape:',y_train.shape)\n",
        "# print('y_test.shape:',y_test.shape)\n",
        "\n",
        "# labels_tensor = torch.tensor(y_train)\n",
        "# #labels_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "# print('labels_tensor:',labels_tensor.shape)\n",
        "\n",
        "# # Assuming labels_tensor is one-hot encoded or contains probabilities\n",
        "# labels_tensor = torch.argmax(labels_tensor, dim=1)\n",
        "# y_train=labels_tensor\n",
        "# print('y_train shape:',y_train.shape)\n",
        "\n",
        "# y_test_tensor= torch.tensor(y_test)\n",
        "# y_test_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "# print('y_test_tensor shape:',y_test_tensor.shape)\n",
        "\n",
        "# # Assuming labels_tensor is one-hot encoded or contains probabilities\n",
        "# y_test_tensor = torch.argmax(y_test_tensor, dim=1)\n",
        "# print('y_test_tensor shape:',y_test_tensor.shape)\n",
        "\n",
        "# y_test= y_test_tensor\n",
        "# print('y_test.shape:',y_test.shape)"
      ],
      "metadata": {
        "id": "NekMispc6mj4"
      },
      "id": "NekMispc6mj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_tensor = torch.tensor(y_train)\n",
        "labels_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "print('labels_tensor:',labels_tensor.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-1Ahw-bTP9T",
        "outputId": "f4ff9566-79d8-4b2e-a47b-8b5191f28f5d"
      },
      "id": "V-1Ahw-bTP9T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels_tensor: torch.Size([1650])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = torch.unique(labels_tensor)\n",
        "print(\"Unique classes:\", unique_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-72dBlA1TbBj",
        "outputId": "ee6d239b-4dcf-4ca6-a9b5-0006f8f099f7"
      },
      "id": "-72dBlA1TbBj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labels_tensor = torch.tensor(labels)\n",
        "# labels_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "# print('labels_tensor:',labels_tensor.shape)\n",
        "# unique_classes = torch.unique(labels_tensor)\n",
        "# print(\"Unique classes:\", unique_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZXPipsHhKPo",
        "outputId": "fed5df3f-0393-4f3a-f695-d574b309cad8"
      },
      "id": "2ZXPipsHhKPo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels_tensor: torch.Size([55337])\n",
            "Unique classes: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VIS Data prepration\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Resahpe the data\n",
        "\n",
        "# Move the input data to the desired device\n",
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch = hsi_samples\n",
        "lidar_batch=lidar_samples\n",
        "label_batch=labels\n",
        "print(\"hsi_batch shape:\", hsi_batch.shape)\n",
        "print(\"lidar_batch shape:\", lidar_batch.shape)\n",
        "print(\"label_batch shape:\", label_batch.shape)\n",
        "\n",
        "## Reshape the data\n",
        "hsi_batch_flat = torch.from_numpy(hsi_batch.astype(np.float32).reshape(55337, hsi_config.in_channels, hsi_config.num_patches, 1))\n",
        "lidar_batch_flat = torch.from_numpy(lidar_batch.astype(np.float32).reshape(55337, lidar_config.in_channels, lidar_config.num_patches, 1))\n",
        "\n",
        "print(\"hsi_batch shape:\", hsi_batch_flat .shape)\n",
        "print(\"lidar_batch shape:\", lidar_batch_flat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEzZ7wfEfuyU",
        "outputId": "183269e9-39eb-4890-dab4-dea37d0800e3"
      },
      "id": "GEzZ7wfEfuyU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hsi_batch shape: (55337, 9, 9, 64)\n",
            "lidar_batch shape: (55337, 9, 9, 2)\n",
            "label_batch shape: (55337,)\n",
            "hsi_batch shape: torch.Size([55337, 81, 64, 1])\n",
            "lidar_batch shape: torch.Size([55337, 81, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hsi_batch_flat = hsi_batch_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_flat = lidar_batch_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "# Split into train and test\n",
        "hsi_samples_train, hsi_samples_test, lidar_samples_train, lidar_samples_test, labels_train, labels_test = train_test_split(\n",
        "    hsi_batch_flat, lidar_batch_flat, label_batch, test_size=0.5, random_state=42)\n",
        "print('labels_train shape:',labels_train.shape)\n",
        "\n",
        "# Split train into train and validation\n",
        "hsi_samples_train, hsi_samples_val, lidar_samples_train, lidar_samples_val, labels_train, labels_val = train_test_split(\n",
        "    hsi_samples_train, lidar_samples_train, labels_train, test_size=0.60, random_state=42)\n",
        "#print('labels_train2 shape:',labels_train.shape)\n",
        "\n",
        "# Now you have training, validation, and test data.\n",
        "# Create Datasets\n",
        "train_dataset = HyperspectralDataset(hsi_samples_train, lidar_samples_train, labels_train)\n",
        "val_dataset = HyperspectralDataset(hsi_samples_val, lidar_samples_val, labels_val)\n",
        "test_dataset = HyperspectralDataset(hsi_samples_test, lidar_samples_test, labels_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "CO7xDsCfcjDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd87bf0-1964-48ae-bb96-99f9cce1ca69"
      },
      "id": "CO7xDsCfcjDe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels_train shape: (27668,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Training CorssAttentionMode ltraining"
      ],
      "metadata": {
        "id": "_MBeu13kDLc6"
      },
      "id": "_MBeu13kDLc6"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create the data loader\n",
        "#train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1.00) #loss 2.3866\n",
        "\n",
        "# Initialize the best validation loss and best_model_wts before the training loop\n",
        "best_val_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "# Instantiate the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize the lowest_loss and best_model_wts before the training loop\n",
        "lowest_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (hsi_batch, lidar_batch, label_batch) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        hsi_batch = hsi_batch.to(device)\n",
        "        lidar_batch = lidar_batch.to(device)\n",
        "        label_batch = label_batch.to(device)  # Reshape labels\n",
        "        # print('hsi_batch shape:', hsi_batch.shape)\n",
        "        # print('lidar_batch:', lidar_batch.shape)\n",
        "        # print('label_batch shape:', label_batch.shape)\n",
        "\n",
        "        # Forward pass\n",
        "        output,attn_scores  = model(lidar_batch, hsi_batch)\n",
        "        #print('output shape:', output.shape)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.transpose(1, 2), label_batch.squeeze(2))\n",
        "\n",
        "        # Print loss every 10 batches\n",
        "        #if (batch_idx + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "        # If the loss is lower than the current lowest, save the model's state\n",
        "        if loss.item() < lowest_loss:\n",
        "            lowest_loss = loss.item()\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "# Print finishing training\n",
        "print('Finishing training')\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_wts, path+'muufl_p9_best_model_weights.pth')\n"
      ],
      "metadata": {
        "id": "w2D4wFzJ5ams",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca95c0b-41e2-47da-85ff-69847ed81d25"
      },
      "id": "w2D4wFzJ5ams",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [86/100], Batch [192/346], Loss: 1.3557\n",
            "Epoch [86/100], Batch [193/346], Loss: 0.9864\n",
            "Epoch [86/100], Batch [194/346], Loss: 1.2832\n",
            "Epoch [86/100], Batch [195/346], Loss: 2.1551\n",
            "Epoch [86/100], Batch [196/346], Loss: 2.1437\n",
            "Epoch [86/100], Batch [197/346], Loss: 1.7843\n",
            "Epoch [86/100], Batch [198/346], Loss: 1.6571\n",
            "Epoch [86/100], Batch [199/346], Loss: 1.4610\n",
            "Epoch [86/100], Batch [200/346], Loss: 1.5861\n",
            "Epoch [86/100], Batch [201/346], Loss: 1.7459\n",
            "Epoch [86/100], Batch [202/346], Loss: 1.9681\n",
            "Epoch [86/100], Batch [203/346], Loss: 1.7244\n",
            "Epoch [86/100], Batch [204/346], Loss: 1.5707\n",
            "Epoch [86/100], Batch [205/346], Loss: 1.3710\n",
            "Epoch [86/100], Batch [206/346], Loss: 1.8394\n",
            "Epoch [86/100], Batch [207/346], Loss: 2.0227\n",
            "Epoch [86/100], Batch [208/346], Loss: 2.2224\n",
            "Epoch [86/100], Batch [209/346], Loss: 1.3485\n",
            "Epoch [86/100], Batch [210/346], Loss: 1.7164\n",
            "Epoch [86/100], Batch [211/346], Loss: 1.0499\n",
            "Epoch [86/100], Batch [212/346], Loss: 1.4085\n",
            "Epoch [86/100], Batch [213/346], Loss: 1.7462\n",
            "Epoch [86/100], Batch [214/346], Loss: 1.8558\n",
            "Epoch [86/100], Batch [215/346], Loss: 1.5988\n",
            "Epoch [86/100], Batch [216/346], Loss: 1.7647\n",
            "Epoch [86/100], Batch [217/346], Loss: 1.5971\n",
            "Epoch [86/100], Batch [218/346], Loss: 1.8893\n",
            "Epoch [86/100], Batch [219/346], Loss: 1.6157\n",
            "Epoch [86/100], Batch [220/346], Loss: 1.6371\n",
            "Epoch [86/100], Batch [221/346], Loss: 1.4327\n",
            "Epoch [86/100], Batch [222/346], Loss: 1.6516\n",
            "Epoch [86/100], Batch [223/346], Loss: 1.3648\n",
            "Epoch [86/100], Batch [224/346], Loss: 1.9182\n",
            "Epoch [86/100], Batch [225/346], Loss: 2.0558\n",
            "Epoch [86/100], Batch [226/346], Loss: 1.5003\n",
            "Epoch [86/100], Batch [227/346], Loss: 1.6050\n",
            "Epoch [86/100], Batch [228/346], Loss: 1.5295\n",
            "Epoch [86/100], Batch [229/346], Loss: 1.8715\n",
            "Epoch [86/100], Batch [230/346], Loss: 1.9882\n",
            "Epoch [86/100], Batch [231/346], Loss: 1.0517\n",
            "Epoch [86/100], Batch [232/346], Loss: 1.7437\n",
            "Epoch [86/100], Batch [233/346], Loss: 1.6504\n",
            "Epoch [86/100], Batch [234/346], Loss: 1.5835\n",
            "Epoch [86/100], Batch [235/346], Loss: 1.5299\n",
            "Epoch [86/100], Batch [236/346], Loss: 1.5072\n",
            "Epoch [86/100], Batch [237/346], Loss: 1.5720\n",
            "Epoch [86/100], Batch [238/346], Loss: 1.6153\n",
            "Epoch [86/100], Batch [239/346], Loss: 1.7938\n",
            "Epoch [86/100], Batch [240/346], Loss: 1.5449\n",
            "Epoch [86/100], Batch [241/346], Loss: 1.1883\n",
            "Epoch [86/100], Batch [242/346], Loss: 1.3387\n",
            "Epoch [86/100], Batch [243/346], Loss: 2.1298\n",
            "Epoch [86/100], Batch [244/346], Loss: 1.5303\n",
            "Epoch [86/100], Batch [245/346], Loss: 1.2935\n",
            "Epoch [86/100], Batch [246/346], Loss: 1.4566\n",
            "Epoch [86/100], Batch [247/346], Loss: 1.1835\n",
            "Epoch [86/100], Batch [248/346], Loss: 1.4951\n",
            "Epoch [86/100], Batch [249/346], Loss: 2.1496\n",
            "Epoch [86/100], Batch [250/346], Loss: 1.3245\n",
            "Epoch [86/100], Batch [251/346], Loss: 1.3364\n",
            "Epoch [86/100], Batch [252/346], Loss: 1.7191\n",
            "Epoch [86/100], Batch [253/346], Loss: 1.3794\n",
            "Epoch [86/100], Batch [254/346], Loss: 1.2899\n",
            "Epoch [86/100], Batch [255/346], Loss: 1.7418\n",
            "Epoch [86/100], Batch [256/346], Loss: 1.5021\n",
            "Epoch [86/100], Batch [257/346], Loss: 1.6586\n",
            "Epoch [86/100], Batch [258/346], Loss: 1.8014\n",
            "Epoch [86/100], Batch [259/346], Loss: 2.1179\n",
            "Epoch [86/100], Batch [260/346], Loss: 0.9673\n",
            "Epoch [86/100], Batch [261/346], Loss: 1.3393\n",
            "Epoch [86/100], Batch [262/346], Loss: 1.9791\n",
            "Epoch [86/100], Batch [263/346], Loss: 1.8315\n",
            "Epoch [86/100], Batch [264/346], Loss: 1.5033\n",
            "Epoch [86/100], Batch [265/346], Loss: 1.3970\n",
            "Epoch [86/100], Batch [266/346], Loss: 1.2192\n",
            "Epoch [86/100], Batch [267/346], Loss: 1.6023\n",
            "Epoch [86/100], Batch [268/346], Loss: 1.9660\n",
            "Epoch [86/100], Batch [269/346], Loss: 2.0205\n",
            "Epoch [86/100], Batch [270/346], Loss: 1.7309\n",
            "Epoch [86/100], Batch [271/346], Loss: 1.5460\n",
            "Epoch [86/100], Batch [272/346], Loss: 1.8365\n",
            "Epoch [86/100], Batch [273/346], Loss: 1.3653\n",
            "Epoch [86/100], Batch [274/346], Loss: 1.4332\n",
            "Epoch [86/100], Batch [275/346], Loss: 1.8943\n",
            "Epoch [86/100], Batch [276/346], Loss: 1.3471\n",
            "Epoch [86/100], Batch [277/346], Loss: 1.5476\n",
            "Epoch [86/100], Batch [278/346], Loss: 1.9133\n",
            "Epoch [86/100], Batch [279/346], Loss: 1.8807\n",
            "Epoch [86/100], Batch [280/346], Loss: 1.7846\n",
            "Epoch [86/100], Batch [281/346], Loss: 1.7388\n",
            "Epoch [86/100], Batch [282/346], Loss: 1.5866\n",
            "Epoch [86/100], Batch [283/346], Loss: 1.5895\n",
            "Epoch [86/100], Batch [284/346], Loss: 1.9189\n",
            "Epoch [86/100], Batch [285/346], Loss: 1.5091\n",
            "Epoch [86/100], Batch [286/346], Loss: 1.4281\n",
            "Epoch [86/100], Batch [287/346], Loss: 1.4016\n",
            "Epoch [86/100], Batch [288/346], Loss: 1.3795\n",
            "Epoch [86/100], Batch [289/346], Loss: 1.5188\n",
            "Epoch [86/100], Batch [290/346], Loss: 1.5702\n",
            "Epoch [86/100], Batch [291/346], Loss: 1.3626\n",
            "Epoch [86/100], Batch [292/346], Loss: 1.5976\n",
            "Epoch [86/100], Batch [293/346], Loss: 1.3880\n",
            "Epoch [86/100], Batch [294/346], Loss: 1.2240\n",
            "Epoch [86/100], Batch [295/346], Loss: 1.4672\n",
            "Epoch [86/100], Batch [296/346], Loss: 1.5039\n",
            "Epoch [86/100], Batch [297/346], Loss: 1.3593\n",
            "Epoch [86/100], Batch [298/346], Loss: 1.4110\n",
            "Epoch [86/100], Batch [299/346], Loss: 1.7406\n",
            "Epoch [86/100], Batch [300/346], Loss: 1.6647\n",
            "Epoch [86/100], Batch [301/346], Loss: 1.1826\n",
            "Epoch [86/100], Batch [302/346], Loss: 1.3823\n",
            "Epoch [86/100], Batch [303/346], Loss: 1.9297\n",
            "Epoch [86/100], Batch [304/346], Loss: 1.5445\n",
            "Epoch [86/100], Batch [305/346], Loss: 1.5667\n",
            "Epoch [86/100], Batch [306/346], Loss: 1.5623\n",
            "Epoch [86/100], Batch [307/346], Loss: 1.5763\n",
            "Epoch [86/100], Batch [308/346], Loss: 1.4260\n",
            "Epoch [86/100], Batch [309/346], Loss: 1.7518\n",
            "Epoch [86/100], Batch [310/346], Loss: 1.1585\n",
            "Epoch [86/100], Batch [311/346], Loss: 1.3586\n",
            "Epoch [86/100], Batch [312/346], Loss: 1.3067\n",
            "Epoch [86/100], Batch [313/346], Loss: 1.3513\n",
            "Epoch [86/100], Batch [314/346], Loss: 1.5328\n",
            "Epoch [86/100], Batch [315/346], Loss: 1.7651\n",
            "Epoch [86/100], Batch [316/346], Loss: 1.2148\n",
            "Epoch [86/100], Batch [317/346], Loss: 1.4543\n",
            "Epoch [86/100], Batch [318/346], Loss: 1.5345\n",
            "Epoch [86/100], Batch [319/346], Loss: 1.6163\n",
            "Epoch [86/100], Batch [320/346], Loss: 1.1146\n",
            "Epoch [86/100], Batch [321/346], Loss: 2.1133\n",
            "Epoch [86/100], Batch [322/346], Loss: 1.5175\n",
            "Epoch [86/100], Batch [323/346], Loss: 1.6679\n",
            "Epoch [86/100], Batch [324/346], Loss: 1.9179\n",
            "Epoch [86/100], Batch [325/346], Loss: 1.6558\n",
            "Epoch [86/100], Batch [326/346], Loss: 1.5469\n",
            "Epoch [86/100], Batch [327/346], Loss: 1.4911\n",
            "Epoch [86/100], Batch [328/346], Loss: 1.2706\n",
            "Epoch [86/100], Batch [329/346], Loss: 1.4855\n",
            "Epoch [86/100], Batch [330/346], Loss: 1.4422\n",
            "Epoch [86/100], Batch [331/346], Loss: 1.3979\n",
            "Epoch [86/100], Batch [332/346], Loss: 1.6392\n",
            "Epoch [86/100], Batch [333/346], Loss: 1.7374\n",
            "Epoch [86/100], Batch [334/346], Loss: 1.4515\n",
            "Epoch [86/100], Batch [335/346], Loss: 1.6735\n",
            "Epoch [86/100], Batch [336/346], Loss: 1.8557\n",
            "Epoch [86/100], Batch [337/346], Loss: 1.3721\n",
            "Epoch [86/100], Batch [338/346], Loss: 1.4780\n",
            "Epoch [86/100], Batch [339/346], Loss: 2.0500\n",
            "Epoch [86/100], Batch [340/346], Loss: 1.5366\n",
            "Epoch [86/100], Batch [341/346], Loss: 1.4456\n",
            "Epoch [86/100], Batch [342/346], Loss: 1.6053\n",
            "Epoch [86/100], Batch [343/346], Loss: 1.6685\n",
            "Epoch [86/100], Batch [344/346], Loss: 1.7398\n",
            "Epoch [86/100], Batch [345/346], Loss: 1.6261\n",
            "Epoch [86/100], Batch [346/346], Loss: 2.2429\n",
            "Epoch [87/100], Batch [1/346], Loss: 1.5440\n",
            "Epoch [87/100], Batch [2/346], Loss: 1.4261\n",
            "Epoch [87/100], Batch [3/346], Loss: 1.8542\n",
            "Epoch [87/100], Batch [4/346], Loss: 1.9174\n",
            "Epoch [87/100], Batch [5/346], Loss: 1.5361\n",
            "Epoch [87/100], Batch [6/346], Loss: 1.0458\n",
            "Epoch [87/100], Batch [7/346], Loss: 1.9573\n",
            "Epoch [87/100], Batch [8/346], Loss: 1.5048\n",
            "Epoch [87/100], Batch [9/346], Loss: 1.8772\n",
            "Epoch [87/100], Batch [10/346], Loss: 1.4088\n",
            "Epoch [87/100], Batch [11/346], Loss: 1.4751\n",
            "Epoch [87/100], Batch [12/346], Loss: 1.5815\n",
            "Epoch [87/100], Batch [13/346], Loss: 1.4237\n",
            "Epoch [87/100], Batch [14/346], Loss: 1.2948\n",
            "Epoch [87/100], Batch [15/346], Loss: 1.5750\n",
            "Epoch [87/100], Batch [16/346], Loss: 1.5065\n",
            "Epoch [87/100], Batch [17/346], Loss: 1.3696\n",
            "Epoch [87/100], Batch [18/346], Loss: 1.5383\n",
            "Epoch [87/100], Batch [19/346], Loss: 1.3931\n",
            "Epoch [87/100], Batch [20/346], Loss: 1.5197\n",
            "Epoch [87/100], Batch [21/346], Loss: 1.3760\n",
            "Epoch [87/100], Batch [22/346], Loss: 1.3202\n",
            "Epoch [87/100], Batch [23/346], Loss: 1.5237\n",
            "Epoch [87/100], Batch [24/346], Loss: 1.5003\n",
            "Epoch [87/100], Batch [25/346], Loss: 1.5871\n",
            "Epoch [87/100], Batch [26/346], Loss: 1.2712\n",
            "Epoch [87/100], Batch [27/346], Loss: 1.2088\n",
            "Epoch [87/100], Batch [28/346], Loss: 1.3871\n",
            "Epoch [87/100], Batch [29/346], Loss: 1.6621\n",
            "Epoch [87/100], Batch [30/346], Loss: 1.6942\n",
            "Epoch [87/100], Batch [31/346], Loss: 1.5620\n",
            "Epoch [87/100], Batch [32/346], Loss: 1.8873\n",
            "Epoch [87/100], Batch [33/346], Loss: 1.5399\n",
            "Epoch [87/100], Batch [34/346], Loss: 1.7509\n",
            "Epoch [87/100], Batch [35/346], Loss: 1.5141\n",
            "Epoch [87/100], Batch [36/346], Loss: 1.2806\n",
            "Epoch [87/100], Batch [37/346], Loss: 1.5809\n",
            "Epoch [87/100], Batch [38/346], Loss: 1.4169\n",
            "Epoch [87/100], Batch [39/346], Loss: 1.5979\n",
            "Epoch [87/100], Batch [40/346], Loss: 1.3902\n",
            "Epoch [87/100], Batch [41/346], Loss: 1.4398\n",
            "Epoch [87/100], Batch [42/346], Loss: 1.6939\n",
            "Epoch [87/100], Batch [43/346], Loss: 1.5258\n",
            "Epoch [87/100], Batch [44/346], Loss: 1.3660\n",
            "Epoch [87/100], Batch [45/346], Loss: 1.3310\n",
            "Epoch [87/100], Batch [46/346], Loss: 1.7025\n",
            "Epoch [87/100], Batch [47/346], Loss: 1.4710\n",
            "Epoch [87/100], Batch [48/346], Loss: 1.5977\n",
            "Epoch [87/100], Batch [49/346], Loss: 1.7379\n",
            "Epoch [87/100], Batch [50/346], Loss: 1.6383\n",
            "Epoch [87/100], Batch [51/346], Loss: 1.7632\n",
            "Epoch [87/100], Batch [52/346], Loss: 1.6444\n",
            "Epoch [87/100], Batch [53/346], Loss: 1.7710\n",
            "Epoch [87/100], Batch [54/346], Loss: 1.2770\n",
            "Epoch [87/100], Batch [55/346], Loss: 1.7786\n",
            "Epoch [87/100], Batch [56/346], Loss: 1.3089\n",
            "Epoch [87/100], Batch [57/346], Loss: 1.4691\n",
            "Epoch [87/100], Batch [58/346], Loss: 1.8286\n",
            "Epoch [87/100], Batch [59/346], Loss: 1.3278\n",
            "Epoch [87/100], Batch [60/346], Loss: 1.4255\n",
            "Epoch [87/100], Batch [61/346], Loss: 1.3802\n",
            "Epoch [87/100], Batch [62/346], Loss: 1.8147\n",
            "Epoch [87/100], Batch [63/346], Loss: 1.6675\n",
            "Epoch [87/100], Batch [64/346], Loss: 1.5991\n",
            "Epoch [87/100], Batch [65/346], Loss: 1.1882\n",
            "Epoch [87/100], Batch [66/346], Loss: 1.5345\n",
            "Epoch [87/100], Batch [67/346], Loss: 1.2947\n",
            "Epoch [87/100], Batch [68/346], Loss: 1.2625\n",
            "Epoch [87/100], Batch [69/346], Loss: 1.2402\n",
            "Epoch [87/100], Batch [70/346], Loss: 2.0101\n",
            "Epoch [87/100], Batch [71/346], Loss: 1.4095\n",
            "Epoch [87/100], Batch [72/346], Loss: 1.4823\n",
            "Epoch [87/100], Batch [73/346], Loss: 1.8873\n",
            "Epoch [87/100], Batch [74/346], Loss: 1.5747\n",
            "Epoch [87/100], Batch [75/346], Loss: 1.9384\n",
            "Epoch [87/100], Batch [76/346], Loss: 1.4304\n",
            "Epoch [87/100], Batch [77/346], Loss: 1.4460\n",
            "Epoch [87/100], Batch [78/346], Loss: 1.7388\n",
            "Epoch [87/100], Batch [79/346], Loss: 1.1807\n",
            "Epoch [87/100], Batch [80/346], Loss: 1.6098\n",
            "Epoch [87/100], Batch [81/346], Loss: 1.4325\n",
            "Epoch [87/100], Batch [82/346], Loss: 1.1371\n",
            "Epoch [87/100], Batch [83/346], Loss: 1.3363\n",
            "Epoch [87/100], Batch [84/346], Loss: 1.4908\n",
            "Epoch [87/100], Batch [85/346], Loss: 1.6096\n",
            "Epoch [87/100], Batch [86/346], Loss: 1.2255\n",
            "Epoch [87/100], Batch [87/346], Loss: 1.6949\n",
            "Epoch [87/100], Batch [88/346], Loss: 1.3206\n",
            "Epoch [87/100], Batch [89/346], Loss: 1.5095\n",
            "Epoch [87/100], Batch [90/346], Loss: 1.4083\n",
            "Epoch [87/100], Batch [91/346], Loss: 1.4090\n",
            "Epoch [87/100], Batch [92/346], Loss: 1.6334\n",
            "Epoch [87/100], Batch [93/346], Loss: 1.5693\n",
            "Epoch [87/100], Batch [94/346], Loss: 1.1633\n",
            "Epoch [87/100], Batch [95/346], Loss: 1.4616\n",
            "Epoch [87/100], Batch [96/346], Loss: 1.4608\n",
            "Epoch [87/100], Batch [97/346], Loss: 1.4280\n",
            "Epoch [87/100], Batch [98/346], Loss: 1.5668\n",
            "Epoch [87/100], Batch [99/346], Loss: 1.3771\n",
            "Epoch [87/100], Batch [100/346], Loss: 1.5469\n",
            "Epoch [87/100], Batch [101/346], Loss: 1.3518\n",
            "Epoch [87/100], Batch [102/346], Loss: 1.4645\n",
            "Epoch [87/100], Batch [103/346], Loss: 1.8820\n",
            "Epoch [87/100], Batch [104/346], Loss: 1.4267\n",
            "Epoch [87/100], Batch [105/346], Loss: 1.2463\n",
            "Epoch [87/100], Batch [106/346], Loss: 1.4917\n",
            "Epoch [87/100], Batch [107/346], Loss: 1.6653\n",
            "Epoch [87/100], Batch [108/346], Loss: 1.8871\n",
            "Epoch [87/100], Batch [109/346], Loss: 1.6556\n",
            "Epoch [87/100], Batch [110/346], Loss: 1.8218\n",
            "Epoch [87/100], Batch [111/346], Loss: 1.6656\n",
            "Epoch [87/100], Batch [112/346], Loss: 1.5396\n",
            "Epoch [87/100], Batch [113/346], Loss: 1.7554\n",
            "Epoch [87/100], Batch [114/346], Loss: 1.0732\n",
            "Epoch [87/100], Batch [115/346], Loss: 1.7634\n",
            "Epoch [87/100], Batch [116/346], Loss: 1.9282\n",
            "Epoch [87/100], Batch [117/346], Loss: 1.4163\n",
            "Epoch [87/100], Batch [118/346], Loss: 1.5233\n",
            "Epoch [87/100], Batch [119/346], Loss: 1.3582\n",
            "Epoch [87/100], Batch [120/346], Loss: 1.6831\n",
            "Epoch [87/100], Batch [121/346], Loss: 1.7401\n",
            "Epoch [87/100], Batch [122/346], Loss: 1.3160\n",
            "Epoch [87/100], Batch [123/346], Loss: 1.5427\n",
            "Epoch [87/100], Batch [124/346], Loss: 1.2493\n",
            "Epoch [87/100], Batch [125/346], Loss: 1.8495\n",
            "Epoch [87/100], Batch [126/346], Loss: 1.7646\n",
            "Epoch [87/100], Batch [127/346], Loss: 2.0021\n",
            "Epoch [87/100], Batch [128/346], Loss: 1.6486\n",
            "Epoch [87/100], Batch [129/346], Loss: 1.5809\n",
            "Epoch [87/100], Batch [130/346], Loss: 1.6186\n",
            "Epoch [87/100], Batch [131/346], Loss: 1.6043\n",
            "Epoch [87/100], Batch [132/346], Loss: 2.4085\n",
            "Epoch [87/100], Batch [133/346], Loss: 1.3904\n",
            "Epoch [87/100], Batch [134/346], Loss: 1.2274\n",
            "Epoch [87/100], Batch [135/346], Loss: 1.8269\n",
            "Epoch [87/100], Batch [136/346], Loss: 1.4207\n",
            "Epoch [87/100], Batch [137/346], Loss: 1.6280\n",
            "Epoch [87/100], Batch [138/346], Loss: 1.5454\n",
            "Epoch [87/100], Batch [139/346], Loss: 1.4178\n",
            "Epoch [87/100], Batch [140/346], Loss: 1.3213\n",
            "Epoch [87/100], Batch [141/346], Loss: 1.2303\n",
            "Epoch [87/100], Batch [142/346], Loss: 1.1202\n",
            "Epoch [87/100], Batch [143/346], Loss: 1.7231\n",
            "Epoch [87/100], Batch [144/346], Loss: 2.0898\n",
            "Epoch [87/100], Batch [145/346], Loss: 1.5827\n",
            "Epoch [87/100], Batch [146/346], Loss: 1.6438\n",
            "Epoch [87/100], Batch [147/346], Loss: 1.4992\n",
            "Epoch [87/100], Batch [148/346], Loss: 1.2505\n",
            "Epoch [87/100], Batch [149/346], Loss: 1.6749\n",
            "Epoch [87/100], Batch [150/346], Loss: 1.6914\n",
            "Epoch [87/100], Batch [151/346], Loss: 1.5691\n",
            "Epoch [87/100], Batch [152/346], Loss: 1.9610\n",
            "Epoch [87/100], Batch [153/346], Loss: 1.8191\n",
            "Epoch [87/100], Batch [154/346], Loss: 1.5069\n",
            "Epoch [87/100], Batch [155/346], Loss: 1.4319\n",
            "Epoch [87/100], Batch [156/346], Loss: 1.2769\n",
            "Epoch [87/100], Batch [157/346], Loss: 2.4941\n",
            "Epoch [87/100], Batch [158/346], Loss: 1.7046\n",
            "Epoch [87/100], Batch [159/346], Loss: 1.4820\n",
            "Epoch [87/100], Batch [160/346], Loss: 0.9533\n",
            "Epoch [87/100], Batch [161/346], Loss: 2.2070\n",
            "Epoch [87/100], Batch [162/346], Loss: 1.6676\n",
            "Epoch [87/100], Batch [163/346], Loss: 1.2293\n",
            "Epoch [87/100], Batch [164/346], Loss: 1.3595\n",
            "Epoch [87/100], Batch [165/346], Loss: 1.5121\n",
            "Epoch [87/100], Batch [166/346], Loss: 1.5783\n",
            "Epoch [87/100], Batch [167/346], Loss: 1.5062\n",
            "Epoch [87/100], Batch [168/346], Loss: 1.2932\n",
            "Epoch [87/100], Batch [169/346], Loss: 2.1165\n",
            "Epoch [87/100], Batch [170/346], Loss: 1.9647\n",
            "Epoch [87/100], Batch [171/346], Loss: 1.4794\n",
            "Epoch [87/100], Batch [172/346], Loss: 1.3035\n",
            "Epoch [87/100], Batch [173/346], Loss: 1.6964\n",
            "Epoch [87/100], Batch [174/346], Loss: 1.2850\n",
            "Epoch [87/100], Batch [175/346], Loss: 1.7262\n",
            "Epoch [87/100], Batch [176/346], Loss: 1.1164\n",
            "Epoch [87/100], Batch [177/346], Loss: 1.6476\n",
            "Epoch [87/100], Batch [178/346], Loss: 1.0707\n",
            "Epoch [87/100], Batch [179/346], Loss: 1.6778\n",
            "Epoch [87/100], Batch [180/346], Loss: 1.5831\n",
            "Epoch [87/100], Batch [181/346], Loss: 1.7617\n",
            "Epoch [87/100], Batch [182/346], Loss: 1.7594\n",
            "Epoch [87/100], Batch [183/346], Loss: 1.4701\n",
            "Epoch [87/100], Batch [184/346], Loss: 1.5427\n",
            "Epoch [87/100], Batch [185/346], Loss: 1.3906\n",
            "Epoch [87/100], Batch [186/346], Loss: 1.4516\n",
            "Epoch [87/100], Batch [187/346], Loss: 1.7794\n",
            "Epoch [87/100], Batch [188/346], Loss: 1.2107\n",
            "Epoch [87/100], Batch [189/346], Loss: 1.3263\n",
            "Epoch [87/100], Batch [190/346], Loss: 1.7848\n",
            "Epoch [87/100], Batch [191/346], Loss: 1.5910\n",
            "Epoch [87/100], Batch [192/346], Loss: 1.4136\n",
            "Epoch [87/100], Batch [193/346], Loss: 1.4277\n",
            "Epoch [87/100], Batch [194/346], Loss: 1.4516\n",
            "Epoch [87/100], Batch [195/346], Loss: 1.5917\n",
            "Epoch [87/100], Batch [196/346], Loss: 1.3093\n",
            "Epoch [87/100], Batch [197/346], Loss: 2.1292\n",
            "Epoch [87/100], Batch [198/346], Loss: 1.5632\n",
            "Epoch [87/100], Batch [199/346], Loss: 1.3133\n",
            "Epoch [87/100], Batch [200/346], Loss: 1.5755\n",
            "Epoch [87/100], Batch [201/346], Loss: 1.9924\n",
            "Epoch [87/100], Batch [202/346], Loss: 1.6368\n",
            "Epoch [87/100], Batch [203/346], Loss: 1.5709\n",
            "Epoch [87/100], Batch [204/346], Loss: 1.6249\n",
            "Epoch [87/100], Batch [205/346], Loss: 1.5989\n",
            "Epoch [87/100], Batch [206/346], Loss: 1.5599\n",
            "Epoch [87/100], Batch [207/346], Loss: 1.6727\n",
            "Epoch [87/100], Batch [208/346], Loss: 1.5176\n",
            "Epoch [87/100], Batch [209/346], Loss: 1.7180\n",
            "Epoch [87/100], Batch [210/346], Loss: 2.4161\n",
            "Epoch [87/100], Batch [211/346], Loss: 1.6143\n",
            "Epoch [87/100], Batch [212/346], Loss: 2.0141\n",
            "Epoch [87/100], Batch [213/346], Loss: 1.4127\n",
            "Epoch [87/100], Batch [214/346], Loss: 1.3171\n",
            "Epoch [87/100], Batch [215/346], Loss: 1.9257\n",
            "Epoch [87/100], Batch [216/346], Loss: 1.5312\n",
            "Epoch [87/100], Batch [217/346], Loss: 1.5916\n",
            "Epoch [87/100], Batch [218/346], Loss: 1.9324\n",
            "Epoch [87/100], Batch [219/346], Loss: 1.8321\n",
            "Epoch [87/100], Batch [220/346], Loss: 1.5638\n",
            "Epoch [87/100], Batch [221/346], Loss: 1.2465\n",
            "Epoch [87/100], Batch [222/346], Loss: 1.5640\n",
            "Epoch [87/100], Batch [223/346], Loss: 1.1699\n",
            "Epoch [87/100], Batch [224/346], Loss: 1.6953\n",
            "Epoch [87/100], Batch [225/346], Loss: 1.7682\n",
            "Epoch [87/100], Batch [226/346], Loss: 1.2252\n",
            "Epoch [87/100], Batch [227/346], Loss: 1.6103\n",
            "Epoch [87/100], Batch [228/346], Loss: 1.2338\n",
            "Epoch [87/100], Batch [229/346], Loss: 1.4557\n",
            "Epoch [87/100], Batch [230/346], Loss: 1.4348\n",
            "Epoch [87/100], Batch [231/346], Loss: 1.5468\n",
            "Epoch [87/100], Batch [232/346], Loss: 1.3924\n",
            "Epoch [87/100], Batch [233/346], Loss: 1.6392\n",
            "Epoch [87/100], Batch [234/346], Loss: 1.4217\n",
            "Epoch [87/100], Batch [235/346], Loss: 1.4227\n",
            "Epoch [87/100], Batch [236/346], Loss: 1.8624\n",
            "Epoch [87/100], Batch [237/346], Loss: 1.4861\n",
            "Epoch [87/100], Batch [238/346], Loss: 1.5812\n",
            "Epoch [87/100], Batch [239/346], Loss: 1.6174\n",
            "Epoch [87/100], Batch [240/346], Loss: 1.4871\n",
            "Epoch [87/100], Batch [241/346], Loss: 1.3448\n",
            "Epoch [87/100], Batch [242/346], Loss: 1.2899\n",
            "Epoch [87/100], Batch [243/346], Loss: 1.6062\n",
            "Epoch [87/100], Batch [244/346], Loss: 1.1923\n",
            "Epoch [87/100], Batch [245/346], Loss: 1.2557\n",
            "Epoch [87/100], Batch [246/346], Loss: 1.4305\n",
            "Epoch [87/100], Batch [247/346], Loss: 1.8329\n",
            "Epoch [87/100], Batch [248/346], Loss: 1.4581\n",
            "Epoch [87/100], Batch [249/346], Loss: 1.2732\n",
            "Epoch [87/100], Batch [250/346], Loss: 1.3643\n",
            "Epoch [87/100], Batch [251/346], Loss: 1.2973\n",
            "Epoch [87/100], Batch [252/346], Loss: 1.3284\n",
            "Epoch [87/100], Batch [253/346], Loss: 1.8397\n",
            "Epoch [87/100], Batch [254/346], Loss: 1.6006\n",
            "Epoch [87/100], Batch [255/346], Loss: 1.8679\n",
            "Epoch [87/100], Batch [256/346], Loss: 1.3882\n",
            "Epoch [87/100], Batch [257/346], Loss: 1.3437\n",
            "Epoch [87/100], Batch [258/346], Loss: 1.7607\n",
            "Epoch [87/100], Batch [259/346], Loss: 1.4175\n",
            "Epoch [87/100], Batch [260/346], Loss: 1.6563\n",
            "Epoch [87/100], Batch [261/346], Loss: 1.1908\n",
            "Epoch [87/100], Batch [262/346], Loss: 1.4092\n",
            "Epoch [87/100], Batch [263/346], Loss: 1.3893\n",
            "Epoch [87/100], Batch [264/346], Loss: 1.6671\n",
            "Epoch [87/100], Batch [265/346], Loss: 1.5413\n",
            "Epoch [87/100], Batch [266/346], Loss: 2.0887\n",
            "Epoch [87/100], Batch [267/346], Loss: 1.4265\n",
            "Epoch [87/100], Batch [268/346], Loss: 1.3253\n",
            "Epoch [87/100], Batch [269/346], Loss: 1.4317\n",
            "Epoch [87/100], Batch [270/346], Loss: 1.5830\n",
            "Epoch [87/100], Batch [271/346], Loss: 1.8593\n",
            "Epoch [87/100], Batch [272/346], Loss: 1.4404\n",
            "Epoch [87/100], Batch [273/346], Loss: 2.0449\n",
            "Epoch [87/100], Batch [274/346], Loss: 1.2118\n",
            "Epoch [87/100], Batch [275/346], Loss: 1.9598\n",
            "Epoch [87/100], Batch [276/346], Loss: 1.6813\n",
            "Epoch [87/100], Batch [277/346], Loss: 1.6361\n",
            "Epoch [87/100], Batch [278/346], Loss: 1.2310\n",
            "Epoch [87/100], Batch [279/346], Loss: 1.4116\n",
            "Epoch [87/100], Batch [280/346], Loss: 1.4618\n",
            "Epoch [87/100], Batch [281/346], Loss: 1.8098\n",
            "Epoch [87/100], Batch [282/346], Loss: 1.3419\n",
            "Epoch [87/100], Batch [283/346], Loss: 1.5868\n",
            "Epoch [87/100], Batch [284/346], Loss: 1.5543\n",
            "Epoch [87/100], Batch [285/346], Loss: 1.5371\n",
            "Epoch [87/100], Batch [286/346], Loss: 1.5689\n",
            "Epoch [87/100], Batch [287/346], Loss: 1.6209\n",
            "Epoch [87/100], Batch [288/346], Loss: 1.7314\n",
            "Epoch [87/100], Batch [289/346], Loss: 1.4095\n",
            "Epoch [87/100], Batch [290/346], Loss: 1.4923\n",
            "Epoch [87/100], Batch [291/346], Loss: 1.3127\n",
            "Epoch [87/100], Batch [292/346], Loss: 1.5147\n",
            "Epoch [87/100], Batch [293/346], Loss: 1.5474\n",
            "Epoch [87/100], Batch [294/346], Loss: 1.3636\n",
            "Epoch [87/100], Batch [295/346], Loss: 1.4819\n",
            "Epoch [87/100], Batch [296/346], Loss: 1.2466\n",
            "Epoch [87/100], Batch [297/346], Loss: 1.7865\n",
            "Epoch [87/100], Batch [298/346], Loss: 1.6040\n",
            "Epoch [87/100], Batch [299/346], Loss: 1.3048\n",
            "Epoch [87/100], Batch [300/346], Loss: 1.2507\n",
            "Epoch [87/100], Batch [301/346], Loss: 1.6841\n",
            "Epoch [87/100], Batch [302/346], Loss: 1.3911\n",
            "Epoch [87/100], Batch [303/346], Loss: 1.7059\n",
            "Epoch [87/100], Batch [304/346], Loss: 1.7247\n",
            "Epoch [87/100], Batch [305/346], Loss: 1.5206\n",
            "Epoch [87/100], Batch [306/346], Loss: 1.2276\n",
            "Epoch [87/100], Batch [307/346], Loss: 1.3691\n",
            "Epoch [87/100], Batch [308/346], Loss: 1.6423\n",
            "Epoch [87/100], Batch [309/346], Loss: 1.3158\n",
            "Epoch [87/100], Batch [310/346], Loss: 1.4237\n",
            "Epoch [87/100], Batch [311/346], Loss: 1.1124\n",
            "Epoch [87/100], Batch [312/346], Loss: 1.8663\n",
            "Epoch [87/100], Batch [313/346], Loss: 1.4530\n",
            "Epoch [87/100], Batch [314/346], Loss: 1.6298\n",
            "Epoch [87/100], Batch [315/346], Loss: 1.5562\n",
            "Epoch [87/100], Batch [316/346], Loss: 1.4577\n",
            "Epoch [87/100], Batch [317/346], Loss: 1.3673\n",
            "Epoch [87/100], Batch [318/346], Loss: 1.1957\n",
            "Epoch [87/100], Batch [319/346], Loss: 1.5826\n",
            "Epoch [87/100], Batch [320/346], Loss: 1.6393\n",
            "Epoch [87/100], Batch [321/346], Loss: 2.0267\n",
            "Epoch [87/100], Batch [322/346], Loss: 1.5113\n",
            "Epoch [87/100], Batch [323/346], Loss: 1.4876\n",
            "Epoch [87/100], Batch [324/346], Loss: 1.3618\n",
            "Epoch [87/100], Batch [325/346], Loss: 2.0584\n",
            "Epoch [87/100], Batch [326/346], Loss: 1.5445\n",
            "Epoch [87/100], Batch [327/346], Loss: 1.9719\n",
            "Epoch [87/100], Batch [328/346], Loss: 1.9161\n",
            "Epoch [87/100], Batch [329/346], Loss: 1.2769\n",
            "Epoch [87/100], Batch [330/346], Loss: 1.6298\n",
            "Epoch [87/100], Batch [331/346], Loss: 1.8922\n",
            "Epoch [87/100], Batch [332/346], Loss: 1.1882\n",
            "Epoch [87/100], Batch [333/346], Loss: 1.6487\n",
            "Epoch [87/100], Batch [334/346], Loss: 1.4796\n",
            "Epoch [87/100], Batch [335/346], Loss: 1.7084\n",
            "Epoch [87/100], Batch [336/346], Loss: 1.3673\n",
            "Epoch [87/100], Batch [337/346], Loss: 1.5445\n",
            "Epoch [87/100], Batch [338/346], Loss: 1.5407\n",
            "Epoch [87/100], Batch [339/346], Loss: 1.8567\n",
            "Epoch [87/100], Batch [340/346], Loss: 1.5054\n",
            "Epoch [87/100], Batch [341/346], Loss: 1.2912\n",
            "Epoch [87/100], Batch [342/346], Loss: 1.3783\n",
            "Epoch [87/100], Batch [343/346], Loss: 1.1619\n",
            "Epoch [87/100], Batch [344/346], Loss: 1.5326\n",
            "Epoch [87/100], Batch [345/346], Loss: 1.3464\n",
            "Epoch [87/100], Batch [346/346], Loss: 1.5096\n",
            "Epoch [88/100], Batch [1/346], Loss: 1.2542\n",
            "Epoch [88/100], Batch [2/346], Loss: 1.1813\n",
            "Epoch [88/100], Batch [3/346], Loss: 1.6353\n",
            "Epoch [88/100], Batch [4/346], Loss: 1.3230\n",
            "Epoch [88/100], Batch [5/346], Loss: 1.4009\n",
            "Epoch [88/100], Batch [6/346], Loss: 1.5067\n",
            "Epoch [88/100], Batch [7/346], Loss: 1.4972\n",
            "Epoch [88/100], Batch [8/346], Loss: 1.2911\n",
            "Epoch [88/100], Batch [9/346], Loss: 1.2783\n",
            "Epoch [88/100], Batch [10/346], Loss: 1.4786\n",
            "Epoch [88/100], Batch [11/346], Loss: 1.3392\n",
            "Epoch [88/100], Batch [12/346], Loss: 1.3300\n",
            "Epoch [88/100], Batch [13/346], Loss: 1.4796\n",
            "Epoch [88/100], Batch [14/346], Loss: 1.3092\n",
            "Epoch [88/100], Batch [15/346], Loss: 1.6436\n",
            "Epoch [88/100], Batch [16/346], Loss: 1.4524\n",
            "Epoch [88/100], Batch [17/346], Loss: 1.5522\n",
            "Epoch [88/100], Batch [18/346], Loss: 1.9757\n",
            "Epoch [88/100], Batch [19/346], Loss: 1.4627\n",
            "Epoch [88/100], Batch [20/346], Loss: 1.5409\n",
            "Epoch [88/100], Batch [21/346], Loss: 1.7112\n",
            "Epoch [88/100], Batch [22/346], Loss: 1.8245\n",
            "Epoch [88/100], Batch [23/346], Loss: 1.7125\n",
            "Epoch [88/100], Batch [24/346], Loss: 1.4287\n",
            "Epoch [88/100], Batch [25/346], Loss: 1.4366\n",
            "Epoch [88/100], Batch [26/346], Loss: 1.1822\n",
            "Epoch [88/100], Batch [27/346], Loss: 1.3575\n",
            "Epoch [88/100], Batch [28/346], Loss: 1.5583\n",
            "Epoch [88/100], Batch [29/346], Loss: 1.5124\n",
            "Epoch [88/100], Batch [30/346], Loss: 1.0938\n",
            "Epoch [88/100], Batch [31/346], Loss: 1.1925\n",
            "Epoch [88/100], Batch [32/346], Loss: 1.4433\n",
            "Epoch [88/100], Batch [33/346], Loss: 1.3886\n",
            "Epoch [88/100], Batch [34/346], Loss: 1.5216\n",
            "Epoch [88/100], Batch [35/346], Loss: 1.7306\n",
            "Epoch [88/100], Batch [36/346], Loss: 1.4921\n",
            "Epoch [88/100], Batch [37/346], Loss: 1.6433\n",
            "Epoch [88/100], Batch [38/346], Loss: 1.4427\n",
            "Epoch [88/100], Batch [39/346], Loss: 1.3901\n",
            "Epoch [88/100], Batch [40/346], Loss: 1.7420\n",
            "Epoch [88/100], Batch [41/346], Loss: 1.2209\n",
            "Epoch [88/100], Batch [42/346], Loss: 1.9009\n",
            "Epoch [88/100], Batch [43/346], Loss: 1.5030\n",
            "Epoch [88/100], Batch [44/346], Loss: 1.8721\n",
            "Epoch [88/100], Batch [45/346], Loss: 1.5560\n",
            "Epoch [88/100], Batch [46/346], Loss: 1.6632\n",
            "Epoch [88/100], Batch [47/346], Loss: 2.0157\n",
            "Epoch [88/100], Batch [48/346], Loss: 1.2365\n",
            "Epoch [88/100], Batch [49/346], Loss: 1.5272\n",
            "Epoch [88/100], Batch [50/346], Loss: 1.2801\n",
            "Epoch [88/100], Batch [51/346], Loss: 1.6757\n",
            "Epoch [88/100], Batch [52/346], Loss: 1.3332\n",
            "Epoch [88/100], Batch [53/346], Loss: 1.6010\n",
            "Epoch [88/100], Batch [54/346], Loss: 1.3486\n",
            "Epoch [88/100], Batch [55/346], Loss: 1.4831\n",
            "Epoch [88/100], Batch [56/346], Loss: 1.9171\n",
            "Epoch [88/100], Batch [57/346], Loss: 1.2084\n",
            "Epoch [88/100], Batch [58/346], Loss: 1.5954\n",
            "Epoch [88/100], Batch [59/346], Loss: 1.3859\n",
            "Epoch [88/100], Batch [60/346], Loss: 1.7502\n",
            "Epoch [88/100], Batch [61/346], Loss: 1.3229\n",
            "Epoch [88/100], Batch [62/346], Loss: 1.6599\n",
            "Epoch [88/100], Batch [63/346], Loss: 1.4723\n",
            "Epoch [88/100], Batch [64/346], Loss: 1.7598\n",
            "Epoch [88/100], Batch [65/346], Loss: 1.6479\n",
            "Epoch [88/100], Batch [66/346], Loss: 1.2004\n",
            "Epoch [88/100], Batch [67/346], Loss: 1.3928\n",
            "Epoch [88/100], Batch [68/346], Loss: 0.9549\n",
            "Epoch [88/100], Batch [69/346], Loss: 1.3534\n",
            "Epoch [88/100], Batch [70/346], Loss: 1.7930\n",
            "Epoch [88/100], Batch [71/346], Loss: 1.8240\n",
            "Epoch [88/100], Batch [72/346], Loss: 1.3308\n",
            "Epoch [88/100], Batch [73/346], Loss: 1.3247\n",
            "Epoch [88/100], Batch [74/346], Loss: 1.4918\n",
            "Epoch [88/100], Batch [75/346], Loss: 1.2420\n",
            "Epoch [88/100], Batch [76/346], Loss: 1.5980\n",
            "Epoch [88/100], Batch [77/346], Loss: 1.4250\n",
            "Epoch [88/100], Batch [78/346], Loss: 1.8421\n",
            "Epoch [88/100], Batch [79/346], Loss: 1.6558\n",
            "Epoch [88/100], Batch [80/346], Loss: 1.5546\n",
            "Epoch [88/100], Batch [81/346], Loss: 1.7610\n",
            "Epoch [88/100], Batch [82/346], Loss: 1.5054\n",
            "Epoch [88/100], Batch [83/346], Loss: 1.6571\n",
            "Epoch [88/100], Batch [84/346], Loss: 1.4775\n",
            "Epoch [88/100], Batch [85/346], Loss: 1.7078\n",
            "Epoch [88/100], Batch [86/346], Loss: 1.3586\n",
            "Epoch [88/100], Batch [87/346], Loss: 1.4517\n",
            "Epoch [88/100], Batch [88/346], Loss: 1.5710\n",
            "Epoch [88/100], Batch [89/346], Loss: 1.4447\n",
            "Epoch [88/100], Batch [90/346], Loss: 1.3958\n",
            "Epoch [88/100], Batch [91/346], Loss: 2.0608\n",
            "Epoch [88/100], Batch [92/346], Loss: 1.5250\n",
            "Epoch [88/100], Batch [93/346], Loss: 1.3383\n",
            "Epoch [88/100], Batch [94/346], Loss: 1.4120\n",
            "Epoch [88/100], Batch [95/346], Loss: 1.1897\n",
            "Epoch [88/100], Batch [96/346], Loss: 1.4625\n",
            "Epoch [88/100], Batch [97/346], Loss: 1.4492\n",
            "Epoch [88/100], Batch [98/346], Loss: 1.6645\n",
            "Epoch [88/100], Batch [99/346], Loss: 1.3566\n",
            "Epoch [88/100], Batch [100/346], Loss: 1.4082\n",
            "Epoch [88/100], Batch [101/346], Loss: 1.4274\n",
            "Epoch [88/100], Batch [102/346], Loss: 1.5275\n",
            "Epoch [88/100], Batch [103/346], Loss: 1.4708\n",
            "Epoch [88/100], Batch [104/346], Loss: 1.1865\n",
            "Epoch [88/100], Batch [105/346], Loss: 1.4765\n",
            "Epoch [88/100], Batch [106/346], Loss: 1.5607\n",
            "Epoch [88/100], Batch [107/346], Loss: 1.3302\n",
            "Epoch [88/100], Batch [108/346], Loss: 1.4200\n",
            "Epoch [88/100], Batch [109/346], Loss: 1.3025\n",
            "Epoch [88/100], Batch [110/346], Loss: 1.3482\n",
            "Epoch [88/100], Batch [111/346], Loss: 1.5042\n",
            "Epoch [88/100], Batch [112/346], Loss: 1.6379\n",
            "Epoch [88/100], Batch [113/346], Loss: 1.3866\n",
            "Epoch [88/100], Batch [114/346], Loss: 1.2891\n",
            "Epoch [88/100], Batch [115/346], Loss: 1.5158\n",
            "Epoch [88/100], Batch [116/346], Loss: 1.3340\n",
            "Epoch [88/100], Batch [117/346], Loss: 1.5191\n",
            "Epoch [88/100], Batch [118/346], Loss: 1.7884\n",
            "Epoch [88/100], Batch [119/346], Loss: 1.6375\n",
            "Epoch [88/100], Batch [120/346], Loss: 1.3673\n",
            "Epoch [88/100], Batch [121/346], Loss: 1.3575\n",
            "Epoch [88/100], Batch [122/346], Loss: 1.4702\n",
            "Epoch [88/100], Batch [123/346], Loss: 1.1104\n",
            "Epoch [88/100], Batch [124/346], Loss: 1.8211\n",
            "Epoch [88/100], Batch [125/346], Loss: 1.2483\n",
            "Epoch [88/100], Batch [126/346], Loss: 1.7584\n",
            "Epoch [88/100], Batch [127/346], Loss: 1.6660\n",
            "Epoch [88/100], Batch [128/346], Loss: 1.8252\n",
            "Epoch [88/100], Batch [129/346], Loss: 1.6781\n",
            "Epoch [88/100], Batch [130/346], Loss: 1.8495\n",
            "Epoch [88/100], Batch [131/346], Loss: 1.8666\n",
            "Epoch [88/100], Batch [132/346], Loss: 1.5815\n",
            "Epoch [88/100], Batch [133/346], Loss: 1.6710\n",
            "Epoch [88/100], Batch [134/346], Loss: 1.4839\n",
            "Epoch [88/100], Batch [135/346], Loss: 1.2739\n",
            "Epoch [88/100], Batch [136/346], Loss: 1.8305\n",
            "Epoch [88/100], Batch [137/346], Loss: 1.7860\n",
            "Epoch [88/100], Batch [138/346], Loss: 1.5120\n",
            "Epoch [88/100], Batch [139/346], Loss: 1.8117\n",
            "Epoch [88/100], Batch [140/346], Loss: 1.6121\n",
            "Epoch [88/100], Batch [141/346], Loss: 1.6254\n",
            "Epoch [88/100], Batch [142/346], Loss: 1.5135\n",
            "Epoch [88/100], Batch [143/346], Loss: 1.7687\n",
            "Epoch [88/100], Batch [144/346], Loss: 1.7936\n",
            "Epoch [88/100], Batch [145/346], Loss: 1.7915\n",
            "Epoch [88/100], Batch [146/346], Loss: 1.6934\n",
            "Epoch [88/100], Batch [147/346], Loss: 1.7424\n",
            "Epoch [88/100], Batch [148/346], Loss: 1.5183\n",
            "Epoch [88/100], Batch [149/346], Loss: 1.3106\n",
            "Epoch [88/100], Batch [150/346], Loss: 1.5682\n",
            "Epoch [88/100], Batch [151/346], Loss: 1.4327\n",
            "Epoch [88/100], Batch [152/346], Loss: 1.3129\n",
            "Epoch [88/100], Batch [153/346], Loss: 1.4353\n",
            "Epoch [88/100], Batch [154/346], Loss: 1.7911\n",
            "Epoch [88/100], Batch [155/346], Loss: 1.6936\n",
            "Epoch [88/100], Batch [156/346], Loss: 1.4874\n",
            "Epoch [88/100], Batch [157/346], Loss: 1.7000\n",
            "Epoch [88/100], Batch [158/346], Loss: 1.6239\n",
            "Epoch [88/100], Batch [159/346], Loss: 1.2779\n",
            "Epoch [88/100], Batch [160/346], Loss: 1.5323\n",
            "Epoch [88/100], Batch [161/346], Loss: 1.4881\n",
            "Epoch [88/100], Batch [162/346], Loss: 2.0131\n",
            "Epoch [88/100], Batch [163/346], Loss: 1.5915\n",
            "Epoch [88/100], Batch [164/346], Loss: 1.5014\n",
            "Epoch [88/100], Batch [165/346], Loss: 1.6372\n",
            "Epoch [88/100], Batch [166/346], Loss: 1.9328\n",
            "Epoch [88/100], Batch [167/346], Loss: 1.3201\n",
            "Epoch [88/100], Batch [168/346], Loss: 1.2904\n",
            "Epoch [88/100], Batch [169/346], Loss: 1.4611\n",
            "Epoch [88/100], Batch [170/346], Loss: 1.4575\n",
            "Epoch [88/100], Batch [171/346], Loss: 1.9673\n",
            "Epoch [88/100], Batch [172/346], Loss: 1.8588\n",
            "Epoch [88/100], Batch [173/346], Loss: 1.5123\n",
            "Epoch [88/100], Batch [174/346], Loss: 1.5988\n",
            "Epoch [88/100], Batch [175/346], Loss: 1.3924\n",
            "Epoch [88/100], Batch [176/346], Loss: 1.3256\n",
            "Epoch [88/100], Batch [177/346], Loss: 1.2673\n",
            "Epoch [88/100], Batch [178/346], Loss: 1.4589\n",
            "Epoch [88/100], Batch [179/346], Loss: 1.7538\n",
            "Epoch [88/100], Batch [180/346], Loss: 1.8144\n",
            "Epoch [88/100], Batch [181/346], Loss: 1.5174\n",
            "Epoch [88/100], Batch [182/346], Loss: 1.5572\n",
            "Epoch [88/100], Batch [183/346], Loss: 1.3699\n",
            "Epoch [88/100], Batch [184/346], Loss: 1.2845\n",
            "Epoch [88/100], Batch [185/346], Loss: 1.2706\n",
            "Epoch [88/100], Batch [186/346], Loss: 1.6660\n",
            "Epoch [88/100], Batch [187/346], Loss: 1.7945\n",
            "Epoch [88/100], Batch [188/346], Loss: 1.4182\n",
            "Epoch [88/100], Batch [189/346], Loss: 1.1218\n",
            "Epoch [88/100], Batch [190/346], Loss: 1.4884\n",
            "Epoch [88/100], Batch [191/346], Loss: 1.5599\n",
            "Epoch [88/100], Batch [192/346], Loss: 1.8489\n",
            "Epoch [88/100], Batch [193/346], Loss: 1.3053\n",
            "Epoch [88/100], Batch [194/346], Loss: 1.3740\n",
            "Epoch [88/100], Batch [195/346], Loss: 1.2185\n",
            "Epoch [88/100], Batch [196/346], Loss: 1.6254\n",
            "Epoch [88/100], Batch [197/346], Loss: 1.4685\n",
            "Epoch [88/100], Batch [198/346], Loss: 1.5768\n",
            "Epoch [88/100], Batch [199/346], Loss: 1.3412\n",
            "Epoch [88/100], Batch [200/346], Loss: 2.1455\n",
            "Epoch [88/100], Batch [201/346], Loss: 1.6613\n",
            "Epoch [88/100], Batch [202/346], Loss: 1.7276\n",
            "Epoch [88/100], Batch [203/346], Loss: 1.1116\n",
            "Epoch [88/100], Batch [204/346], Loss: 1.1888\n",
            "Epoch [88/100], Batch [205/346], Loss: 1.4250\n",
            "Epoch [88/100], Batch [206/346], Loss: 1.8728\n",
            "Epoch [88/100], Batch [207/346], Loss: 1.6573\n",
            "Epoch [88/100], Batch [208/346], Loss: 1.4245\n",
            "Epoch [88/100], Batch [209/346], Loss: 1.5340\n",
            "Epoch [88/100], Batch [210/346], Loss: 1.6163\n",
            "Epoch [88/100], Batch [211/346], Loss: 1.2069\n",
            "Epoch [88/100], Batch [212/346], Loss: 1.7150\n",
            "Epoch [88/100], Batch [213/346], Loss: 1.3460\n",
            "Epoch [88/100], Batch [214/346], Loss: 1.7690\n",
            "Epoch [88/100], Batch [215/346], Loss: 1.9039\n",
            "Epoch [88/100], Batch [216/346], Loss: 1.6307\n",
            "Epoch [88/100], Batch [217/346], Loss: 1.5969\n",
            "Epoch [88/100], Batch [218/346], Loss: 1.2221\n",
            "Epoch [88/100], Batch [219/346], Loss: 1.2569\n",
            "Epoch [88/100], Batch [220/346], Loss: 1.4439\n",
            "Epoch [88/100], Batch [221/346], Loss: 2.3906\n",
            "Epoch [88/100], Batch [222/346], Loss: 2.0408\n",
            "Epoch [88/100], Batch [223/346], Loss: 2.3153\n",
            "Epoch [88/100], Batch [224/346], Loss: 1.7664\n",
            "Epoch [88/100], Batch [225/346], Loss: 1.7593\n",
            "Epoch [88/100], Batch [226/346], Loss: 2.1363\n",
            "Epoch [88/100], Batch [227/346], Loss: 1.4121\n",
            "Epoch [88/100], Batch [228/346], Loss: 1.5959\n",
            "Epoch [88/100], Batch [229/346], Loss: 1.8374\n",
            "Epoch [88/100], Batch [230/346], Loss: 1.9065\n",
            "Epoch [88/100], Batch [231/346], Loss: 1.5089\n",
            "Epoch [88/100], Batch [232/346], Loss: 1.9403\n",
            "Epoch [88/100], Batch [233/346], Loss: 1.4417\n",
            "Epoch [88/100], Batch [234/346], Loss: 1.0618\n",
            "Epoch [88/100], Batch [235/346], Loss: 1.5957\n",
            "Epoch [88/100], Batch [236/346], Loss: 1.6558\n",
            "Epoch [88/100], Batch [237/346], Loss: 1.5028\n",
            "Epoch [88/100], Batch [238/346], Loss: 1.2440\n",
            "Epoch [88/100], Batch [239/346], Loss: 1.6263\n",
            "Epoch [88/100], Batch [240/346], Loss: 2.3605\n",
            "Epoch [88/100], Batch [241/346], Loss: 1.6693\n",
            "Epoch [88/100], Batch [242/346], Loss: 1.9798\n",
            "Epoch [88/100], Batch [243/346], Loss: 1.6042\n",
            "Epoch [88/100], Batch [244/346], Loss: 1.4472\n",
            "Epoch [88/100], Batch [245/346], Loss: 1.5485\n",
            "Epoch [88/100], Batch [246/346], Loss: 1.7204\n",
            "Epoch [88/100], Batch [247/346], Loss: 1.6685\n",
            "Epoch [88/100], Batch [248/346], Loss: 1.6071\n",
            "Epoch [88/100], Batch [249/346], Loss: 1.4937\n",
            "Epoch [88/100], Batch [250/346], Loss: 1.5492\n",
            "Epoch [88/100], Batch [251/346], Loss: 2.1610\n",
            "Epoch [88/100], Batch [252/346], Loss: 1.2055\n",
            "Epoch [88/100], Batch [253/346], Loss: 1.3602\n",
            "Epoch [88/100], Batch [254/346], Loss: 1.8306\n",
            "Epoch [88/100], Batch [255/346], Loss: 1.9808\n",
            "Epoch [88/100], Batch [256/346], Loss: 1.3768\n",
            "Epoch [88/100], Batch [257/346], Loss: 1.3523\n",
            "Epoch [88/100], Batch [258/346], Loss: 1.4160\n",
            "Epoch [88/100], Batch [259/346], Loss: 1.5052\n",
            "Epoch [88/100], Batch [260/346], Loss: 1.7220\n",
            "Epoch [88/100], Batch [261/346], Loss: 1.5940\n",
            "Epoch [88/100], Batch [262/346], Loss: 1.3849\n",
            "Epoch [88/100], Batch [263/346], Loss: 1.2669\n",
            "Epoch [88/100], Batch [264/346], Loss: 1.4394\n",
            "Epoch [88/100], Batch [265/346], Loss: 1.3481\n",
            "Epoch [88/100], Batch [266/346], Loss: 1.9763\n",
            "Epoch [88/100], Batch [267/346], Loss: 1.6025\n",
            "Epoch [88/100], Batch [268/346], Loss: 1.4509\n",
            "Epoch [88/100], Batch [269/346], Loss: 1.5664\n",
            "Epoch [88/100], Batch [270/346], Loss: 1.4432\n",
            "Epoch [88/100], Batch [271/346], Loss: 1.6723\n",
            "Epoch [88/100], Batch [272/346], Loss: 1.6486\n",
            "Epoch [88/100], Batch [273/346], Loss: 1.3949\n",
            "Epoch [88/100], Batch [274/346], Loss: 1.6819\n",
            "Epoch [88/100], Batch [275/346], Loss: 1.8549\n",
            "Epoch [88/100], Batch [276/346], Loss: 1.4091\n",
            "Epoch [88/100], Batch [277/346], Loss: 1.9180\n",
            "Epoch [88/100], Batch [278/346], Loss: 1.6491\n",
            "Epoch [88/100], Batch [279/346], Loss: 1.7416\n",
            "Epoch [88/100], Batch [280/346], Loss: 1.6382\n",
            "Epoch [88/100], Batch [281/346], Loss: 1.3250\n",
            "Epoch [88/100], Batch [282/346], Loss: 1.3144\n",
            "Epoch [88/100], Batch [283/346], Loss: 1.4494\n",
            "Epoch [88/100], Batch [284/346], Loss: 1.1616\n",
            "Epoch [88/100], Batch [285/346], Loss: 1.6841\n",
            "Epoch [88/100], Batch [286/346], Loss: 1.2753\n",
            "Epoch [88/100], Batch [287/346], Loss: 2.0382\n",
            "Epoch [88/100], Batch [288/346], Loss: 1.3742\n",
            "Epoch [88/100], Batch [289/346], Loss: 1.5055\n",
            "Epoch [88/100], Batch [290/346], Loss: 1.4872\n",
            "Epoch [88/100], Batch [291/346], Loss: 1.5852\n",
            "Epoch [88/100], Batch [292/346], Loss: 1.5417\n",
            "Epoch [88/100], Batch [293/346], Loss: 1.4415\n",
            "Epoch [88/100], Batch [294/346], Loss: 1.5131\n",
            "Epoch [88/100], Batch [295/346], Loss: 1.4931\n",
            "Epoch [88/100], Batch [296/346], Loss: 1.7080\n",
            "Epoch [88/100], Batch [297/346], Loss: 1.2071\n",
            "Epoch [88/100], Batch [298/346], Loss: 1.6024\n",
            "Epoch [88/100], Batch [299/346], Loss: 1.3912\n",
            "Epoch [88/100], Batch [300/346], Loss: 1.3768\n",
            "Epoch [88/100], Batch [301/346], Loss: 1.1254\n",
            "Epoch [88/100], Batch [302/346], Loss: 2.0763\n",
            "Epoch [88/100], Batch [303/346], Loss: 1.5787\n",
            "Epoch [88/100], Batch [304/346], Loss: 1.7640\n",
            "Epoch [88/100], Batch [305/346], Loss: 1.6271\n",
            "Epoch [88/100], Batch [306/346], Loss: 1.5017\n",
            "Epoch [88/100], Batch [307/346], Loss: 1.3894\n",
            "Epoch [88/100], Batch [308/346], Loss: 1.5898\n",
            "Epoch [88/100], Batch [309/346], Loss: 1.4720\n",
            "Epoch [88/100], Batch [310/346], Loss: 1.5159\n",
            "Epoch [88/100], Batch [311/346], Loss: 1.7764\n",
            "Epoch [88/100], Batch [312/346], Loss: 1.7447\n",
            "Epoch [88/100], Batch [313/346], Loss: 1.3480\n",
            "Epoch [88/100], Batch [314/346], Loss: 1.5619\n",
            "Epoch [88/100], Batch [315/346], Loss: 1.8880\n",
            "Epoch [88/100], Batch [316/346], Loss: 2.1698\n",
            "Epoch [88/100], Batch [317/346], Loss: 1.6686\n",
            "Epoch [88/100], Batch [318/346], Loss: 1.8815\n",
            "Epoch [88/100], Batch [319/346], Loss: 1.3247\n",
            "Epoch [88/100], Batch [320/346], Loss: 1.9692\n",
            "Epoch [88/100], Batch [321/346], Loss: 1.5965\n",
            "Epoch [88/100], Batch [322/346], Loss: 1.6809\n",
            "Epoch [88/100], Batch [323/346], Loss: 1.6958\n",
            "Epoch [88/100], Batch [324/346], Loss: 1.6753\n",
            "Epoch [88/100], Batch [325/346], Loss: 1.4588\n",
            "Epoch [88/100], Batch [326/346], Loss: 1.6766\n",
            "Epoch [88/100], Batch [327/346], Loss: 1.5172\n",
            "Epoch [88/100], Batch [328/346], Loss: 1.2434\n",
            "Epoch [88/100], Batch [329/346], Loss: 1.7603\n",
            "Epoch [88/100], Batch [330/346], Loss: 1.6124\n",
            "Epoch [88/100], Batch [331/346], Loss: 1.3210\n",
            "Epoch [88/100], Batch [332/346], Loss: 1.4927\n",
            "Epoch [88/100], Batch [333/346], Loss: 1.4573\n",
            "Epoch [88/100], Batch [334/346], Loss: 1.4622\n",
            "Epoch [88/100], Batch [335/346], Loss: 1.7627\n",
            "Epoch [88/100], Batch [336/346], Loss: 1.3065\n",
            "Epoch [88/100], Batch [337/346], Loss: 2.0640\n",
            "Epoch [88/100], Batch [338/346], Loss: 1.5466\n",
            "Epoch [88/100], Batch [339/346], Loss: 1.3134\n",
            "Epoch [88/100], Batch [340/346], Loss: 1.4313\n",
            "Epoch [88/100], Batch [341/346], Loss: 1.5640\n",
            "Epoch [88/100], Batch [342/346], Loss: 1.4697\n",
            "Epoch [88/100], Batch [343/346], Loss: 1.1887\n",
            "Epoch [88/100], Batch [344/346], Loss: 1.2583\n",
            "Epoch [88/100], Batch [345/346], Loss: 1.3679\n",
            "Epoch [88/100], Batch [346/346], Loss: 1.7241\n",
            "Epoch [89/100], Batch [1/346], Loss: 1.1141\n",
            "Epoch [89/100], Batch [2/346], Loss: 1.9541\n",
            "Epoch [89/100], Batch [3/346], Loss: 1.6096\n",
            "Epoch [89/100], Batch [4/346], Loss: 1.6086\n",
            "Epoch [89/100], Batch [5/346], Loss: 1.3877\n",
            "Epoch [89/100], Batch [6/346], Loss: 1.2293\n",
            "Epoch [89/100], Batch [7/346], Loss: 1.5083\n",
            "Epoch [89/100], Batch [8/346], Loss: 1.6219\n",
            "Epoch [89/100], Batch [9/346], Loss: 1.8184\n",
            "Epoch [89/100], Batch [10/346], Loss: 1.4373\n",
            "Epoch [89/100], Batch [11/346], Loss: 1.5907\n",
            "Epoch [89/100], Batch [12/346], Loss: 1.7615\n",
            "Epoch [89/100], Batch [13/346], Loss: 1.3524\n",
            "Epoch [89/100], Batch [14/346], Loss: 1.7785\n",
            "Epoch [89/100], Batch [15/346], Loss: 1.7440\n",
            "Epoch [89/100], Batch [16/346], Loss: 1.6506\n",
            "Epoch [89/100], Batch [17/346], Loss: 1.4462\n",
            "Epoch [89/100], Batch [18/346], Loss: 1.1545\n",
            "Epoch [89/100], Batch [19/346], Loss: 1.9712\n",
            "Epoch [89/100], Batch [20/346], Loss: 1.4389\n",
            "Epoch [89/100], Batch [21/346], Loss: 1.4460\n",
            "Epoch [89/100], Batch [22/346], Loss: 1.3386\n",
            "Epoch [89/100], Batch [23/346], Loss: 1.3925\n",
            "Epoch [89/100], Batch [24/346], Loss: 1.5300\n",
            "Epoch [89/100], Batch [25/346], Loss: 1.5862\n",
            "Epoch [89/100], Batch [26/346], Loss: 1.5199\n",
            "Epoch [89/100], Batch [27/346], Loss: 1.0994\n",
            "Epoch [89/100], Batch [28/346], Loss: 1.7186\n",
            "Epoch [89/100], Batch [29/346], Loss: 1.4325\n",
            "Epoch [89/100], Batch [30/346], Loss: 1.7246\n",
            "Epoch [89/100], Batch [31/346], Loss: 1.7533\n",
            "Epoch [89/100], Batch [32/346], Loss: 1.4391\n",
            "Epoch [89/100], Batch [33/346], Loss: 1.2937\n",
            "Epoch [89/100], Batch [34/346], Loss: 1.7377\n",
            "Epoch [89/100], Batch [35/346], Loss: 1.3955\n",
            "Epoch [89/100], Batch [36/346], Loss: 1.7981\n",
            "Epoch [89/100], Batch [37/346], Loss: 1.5883\n",
            "Epoch [89/100], Batch [38/346], Loss: 1.1337\n",
            "Epoch [89/100], Batch [39/346], Loss: 1.4787\n",
            "Epoch [89/100], Batch [40/346], Loss: 1.4222\n",
            "Epoch [89/100], Batch [41/346], Loss: 1.2697\n",
            "Epoch [89/100], Batch [42/346], Loss: 1.3695\n",
            "Epoch [89/100], Batch [43/346], Loss: 1.3711\n",
            "Epoch [89/100], Batch [44/346], Loss: 1.8658\n",
            "Epoch [89/100], Batch [45/346], Loss: 1.1828\n",
            "Epoch [89/100], Batch [46/346], Loss: 1.4257\n",
            "Epoch [89/100], Batch [47/346], Loss: 1.3265\n",
            "Epoch [89/100], Batch [48/346], Loss: 1.2958\n",
            "Epoch [89/100], Batch [49/346], Loss: 1.3607\n",
            "Epoch [89/100], Batch [50/346], Loss: 1.4429\n",
            "Epoch [89/100], Batch [51/346], Loss: 1.3685\n",
            "Epoch [89/100], Batch [52/346], Loss: 1.2846\n",
            "Epoch [89/100], Batch [53/346], Loss: 1.6787\n",
            "Epoch [89/100], Batch [54/346], Loss: 1.3963\n",
            "Epoch [89/100], Batch [55/346], Loss: 1.5349\n",
            "Epoch [89/100], Batch [56/346], Loss: 1.2287\n",
            "Epoch [89/100], Batch [57/346], Loss: 1.4720\n",
            "Epoch [89/100], Batch [58/346], Loss: 1.2426\n",
            "Epoch [89/100], Batch [59/346], Loss: 1.4093\n",
            "Epoch [89/100], Batch [60/346], Loss: 1.2273\n",
            "Epoch [89/100], Batch [61/346], Loss: 1.5249\n",
            "Epoch [89/100], Batch [62/346], Loss: 2.0441\n",
            "Epoch [89/100], Batch [63/346], Loss: 1.4798\n",
            "Epoch [89/100], Batch [64/346], Loss: 1.6171\n",
            "Epoch [89/100], Batch [65/346], Loss: 1.8348\n",
            "Epoch [89/100], Batch [66/346], Loss: 1.7642\n",
            "Epoch [89/100], Batch [67/346], Loss: 1.9487\n",
            "Epoch [89/100], Batch [68/346], Loss: 1.6076\n",
            "Epoch [89/100], Batch [69/346], Loss: 1.7141\n",
            "Epoch [89/100], Batch [70/346], Loss: 1.6962\n",
            "Epoch [89/100], Batch [71/346], Loss: 1.6754\n",
            "Epoch [89/100], Batch [72/346], Loss: 1.3655\n",
            "Epoch [89/100], Batch [73/346], Loss: 1.3773\n",
            "Epoch [89/100], Batch [74/346], Loss: 1.4214\n",
            "Epoch [89/100], Batch [75/346], Loss: 1.5081\n",
            "Epoch [89/100], Batch [76/346], Loss: 1.4437\n",
            "Epoch [89/100], Batch [77/346], Loss: 1.5136\n",
            "Epoch [89/100], Batch [78/346], Loss: 1.7207\n",
            "Epoch [89/100], Batch [79/346], Loss: 1.8434\n",
            "Epoch [89/100], Batch [80/346], Loss: 1.4229\n",
            "Epoch [89/100], Batch [81/346], Loss: 1.6119\n",
            "Epoch [89/100], Batch [82/346], Loss: 1.4790\n",
            "Epoch [89/100], Batch [83/346], Loss: 1.4269\n",
            "Epoch [89/100], Batch [84/346], Loss: 1.4103\n",
            "Epoch [89/100], Batch [85/346], Loss: 1.4138\n",
            "Epoch [89/100], Batch [86/346], Loss: 1.8496\n",
            "Epoch [89/100], Batch [87/346], Loss: 1.3198\n",
            "Epoch [89/100], Batch [88/346], Loss: 1.4482\n",
            "Epoch [89/100], Batch [89/346], Loss: 1.4391\n",
            "Epoch [89/100], Batch [90/346], Loss: 2.0635\n",
            "Epoch [89/100], Batch [91/346], Loss: 1.4660\n",
            "Epoch [89/100], Batch [92/346], Loss: 1.6312\n",
            "Epoch [89/100], Batch [93/346], Loss: 1.8752\n",
            "Epoch [89/100], Batch [94/346], Loss: 1.1855\n",
            "Epoch [89/100], Batch [95/346], Loss: 1.4850\n",
            "Epoch [89/100], Batch [96/346], Loss: 1.6110\n",
            "Epoch [89/100], Batch [97/346], Loss: 1.4385\n",
            "Epoch [89/100], Batch [98/346], Loss: 1.6106\n",
            "Epoch [89/100], Batch [99/346], Loss: 1.6114\n",
            "Epoch [89/100], Batch [100/346], Loss: 1.6844\n",
            "Epoch [89/100], Batch [101/346], Loss: 1.2790\n",
            "Epoch [89/100], Batch [102/346], Loss: 1.5413\n",
            "Epoch [89/100], Batch [103/346], Loss: 1.5996\n",
            "Epoch [89/100], Batch [104/346], Loss: 1.6459\n",
            "Epoch [89/100], Batch [105/346], Loss: 1.7486\n",
            "Epoch [89/100], Batch [106/346], Loss: 1.6615\n",
            "Epoch [89/100], Batch [107/346], Loss: 1.6293\n",
            "Epoch [89/100], Batch [108/346], Loss: 1.6120\n",
            "Epoch [89/100], Batch [109/346], Loss: 1.8539\n",
            "Epoch [89/100], Batch [110/346], Loss: 1.5192\n",
            "Epoch [89/100], Batch [111/346], Loss: 1.5850\n",
            "Epoch [89/100], Batch [112/346], Loss: 1.4797\n",
            "Epoch [89/100], Batch [113/346], Loss: 1.2734\n",
            "Epoch [89/100], Batch [114/346], Loss: 2.0317\n",
            "Epoch [89/100], Batch [115/346], Loss: 1.5632\n",
            "Epoch [89/100], Batch [116/346], Loss: 1.3593\n",
            "Epoch [89/100], Batch [117/346], Loss: 1.3806\n",
            "Epoch [89/100], Batch [118/346], Loss: 1.6768\n",
            "Epoch [89/100], Batch [119/346], Loss: 1.5973\n",
            "Epoch [89/100], Batch [120/346], Loss: 1.7579\n",
            "Epoch [89/100], Batch [121/346], Loss: 1.3279\n",
            "Epoch [89/100], Batch [122/346], Loss: 1.3385\n",
            "Epoch [89/100], Batch [123/346], Loss: 1.4198\n",
            "Epoch [89/100], Batch [124/346], Loss: 1.3432\n",
            "Epoch [89/100], Batch [125/346], Loss: 1.4857\n",
            "Epoch [89/100], Batch [126/346], Loss: 1.2032\n",
            "Epoch [89/100], Batch [127/346], Loss: 1.2415\n",
            "Epoch [89/100], Batch [128/346], Loss: 1.4756\n",
            "Epoch [89/100], Batch [129/346], Loss: 1.4122\n",
            "Epoch [89/100], Batch [130/346], Loss: 1.6927\n",
            "Epoch [89/100], Batch [131/346], Loss: 1.2044\n",
            "Epoch [89/100], Batch [132/346], Loss: 1.8508\n",
            "Epoch [89/100], Batch [133/346], Loss: 1.4997\n",
            "Epoch [89/100], Batch [134/346], Loss: 1.6544\n",
            "Epoch [89/100], Batch [135/346], Loss: 1.7058\n",
            "Epoch [89/100], Batch [136/346], Loss: 1.6191\n",
            "Epoch [89/100], Batch [137/346], Loss: 1.6401\n",
            "Epoch [89/100], Batch [138/346], Loss: 1.6015\n",
            "Epoch [89/100], Batch [139/346], Loss: 1.7551\n",
            "Epoch [89/100], Batch [140/346], Loss: 1.3777\n",
            "Epoch [89/100], Batch [141/346], Loss: 1.8375\n",
            "Epoch [89/100], Batch [142/346], Loss: 1.2051\n",
            "Epoch [89/100], Batch [143/346], Loss: 1.3563\n",
            "Epoch [89/100], Batch [144/346], Loss: 1.0740\n",
            "Epoch [89/100], Batch [145/346], Loss: 1.2954\n",
            "Epoch [89/100], Batch [146/346], Loss: 1.4513\n",
            "Epoch [89/100], Batch [147/346], Loss: 1.7342\n",
            "Epoch [89/100], Batch [148/346], Loss: 1.7088\n",
            "Epoch [89/100], Batch [149/346], Loss: 1.4784\n",
            "Epoch [89/100], Batch [150/346], Loss: 1.5067\n",
            "Epoch [89/100], Batch [151/346], Loss: 1.4992\n",
            "Epoch [89/100], Batch [152/346], Loss: 1.7434\n",
            "Epoch [89/100], Batch [153/346], Loss: 1.5775\n",
            "Epoch [89/100], Batch [154/346], Loss: 1.2458\n",
            "Epoch [89/100], Batch [155/346], Loss: 1.4391\n",
            "Epoch [89/100], Batch [156/346], Loss: 1.5864\n",
            "Epoch [89/100], Batch [157/346], Loss: 1.5805\n",
            "Epoch [89/100], Batch [158/346], Loss: 1.4233\n",
            "Epoch [89/100], Batch [159/346], Loss: 1.4325\n",
            "Epoch [89/100], Batch [160/346], Loss: 1.4520\n",
            "Epoch [89/100], Batch [161/346], Loss: 1.4913\n",
            "Epoch [89/100], Batch [162/346], Loss: 1.1989\n",
            "Epoch [89/100], Batch [163/346], Loss: 1.2538\n",
            "Epoch [89/100], Batch [164/346], Loss: 1.4749\n",
            "Epoch [89/100], Batch [165/346], Loss: 2.4292\n",
            "Epoch [89/100], Batch [166/346], Loss: 1.2824\n",
            "Epoch [89/100], Batch [167/346], Loss: 1.3219\n",
            "Epoch [89/100], Batch [168/346], Loss: 1.3887\n",
            "Epoch [89/100], Batch [169/346], Loss: 1.9577\n",
            "Epoch [89/100], Batch [170/346], Loss: 1.4509\n",
            "Epoch [89/100], Batch [171/346], Loss: 1.6282\n",
            "Epoch [89/100], Batch [172/346], Loss: 1.5407\n",
            "Epoch [89/100], Batch [173/346], Loss: 1.5188\n",
            "Epoch [89/100], Batch [174/346], Loss: 1.5384\n",
            "Epoch [89/100], Batch [175/346], Loss: 1.7726\n",
            "Epoch [89/100], Batch [176/346], Loss: 1.2469\n",
            "Epoch [89/100], Batch [177/346], Loss: 1.6515\n",
            "Epoch [89/100], Batch [178/346], Loss: 1.7268\n",
            "Epoch [89/100], Batch [179/346], Loss: 1.2822\n",
            "Epoch [89/100], Batch [180/346], Loss: 1.2251\n",
            "Epoch [89/100], Batch [181/346], Loss: 2.0084\n",
            "Epoch [89/100], Batch [182/346], Loss: 1.5604\n",
            "Epoch [89/100], Batch [183/346], Loss: 2.3410\n",
            "Epoch [89/100], Batch [184/346], Loss: 1.0643\n",
            "Epoch [89/100], Batch [185/346], Loss: 1.3589\n",
            "Epoch [89/100], Batch [186/346], Loss: 1.5779\n",
            "Epoch [89/100], Batch [187/346], Loss: 1.6900\n",
            "Epoch [89/100], Batch [188/346], Loss: 1.7132\n",
            "Epoch [89/100], Batch [189/346], Loss: 1.5079\n",
            "Epoch [89/100], Batch [190/346], Loss: 1.9626\n",
            "Epoch [89/100], Batch [191/346], Loss: 1.6425\n",
            "Epoch [89/100], Batch [192/346], Loss: 1.7146\n",
            "Epoch [89/100], Batch [193/346], Loss: 1.9037\n",
            "Epoch [89/100], Batch [194/346], Loss: 1.3830\n",
            "Epoch [89/100], Batch [195/346], Loss: 1.3975\n",
            "Epoch [89/100], Batch [196/346], Loss: 1.0809\n",
            "Epoch [89/100], Batch [197/346], Loss: 1.4732\n",
            "Epoch [89/100], Batch [198/346], Loss: 1.7720\n",
            "Epoch [89/100], Batch [199/346], Loss: 1.6332\n",
            "Epoch [89/100], Batch [200/346], Loss: 1.6209\n",
            "Epoch [89/100], Batch [201/346], Loss: 1.5835\n",
            "Epoch [89/100], Batch [202/346], Loss: 1.5908\n",
            "Epoch [89/100], Batch [203/346], Loss: 1.6026\n",
            "Epoch [89/100], Batch [204/346], Loss: 1.2451\n",
            "Epoch [89/100], Batch [205/346], Loss: 1.6583\n",
            "Epoch [89/100], Batch [206/346], Loss: 1.5984\n",
            "Epoch [89/100], Batch [207/346], Loss: 1.2861\n",
            "Epoch [89/100], Batch [208/346], Loss: 1.9007\n",
            "Epoch [89/100], Batch [209/346], Loss: 1.2189\n",
            "Epoch [89/100], Batch [210/346], Loss: 1.3853\n",
            "Epoch [89/100], Batch [211/346], Loss: 1.8500\n",
            "Epoch [89/100], Batch [212/346], Loss: 1.4258\n",
            "Epoch [89/100], Batch [213/346], Loss: 1.3253\n",
            "Epoch [89/100], Batch [214/346], Loss: 1.2666\n",
            "Epoch [89/100], Batch [215/346], Loss: 1.7158\n",
            "Epoch [89/100], Batch [216/346], Loss: 1.3848\n",
            "Epoch [89/100], Batch [217/346], Loss: 1.2308\n",
            "Epoch [89/100], Batch [218/346], Loss: 1.7398\n",
            "Epoch [89/100], Batch [219/346], Loss: 1.4964\n",
            "Epoch [89/100], Batch [220/346], Loss: 1.2552\n",
            "Epoch [89/100], Batch [221/346], Loss: 1.5345\n",
            "Epoch [89/100], Batch [222/346], Loss: 1.3578\n",
            "Epoch [89/100], Batch [223/346], Loss: 1.5367\n",
            "Epoch [89/100], Batch [224/346], Loss: 1.6102\n",
            "Epoch [89/100], Batch [225/346], Loss: 1.7008\n",
            "Epoch [89/100], Batch [226/346], Loss: 1.6044\n",
            "Epoch [89/100], Batch [227/346], Loss: 1.4780\n",
            "Epoch [89/100], Batch [228/346], Loss: 1.9446\n",
            "Epoch [89/100], Batch [229/346], Loss: 1.4577\n",
            "Epoch [89/100], Batch [230/346], Loss: 1.4167\n",
            "Epoch [89/100], Batch [231/346], Loss: 1.6630\n",
            "Epoch [89/100], Batch [232/346], Loss: 1.4257\n",
            "Epoch [89/100], Batch [233/346], Loss: 1.5940\n",
            "Epoch [89/100], Batch [234/346], Loss: 2.0672\n",
            "Epoch [89/100], Batch [235/346], Loss: 1.5514\n",
            "Epoch [89/100], Batch [236/346], Loss: 1.3390\n",
            "Epoch [89/100], Batch [237/346], Loss: 1.4343\n",
            "Epoch [89/100], Batch [238/346], Loss: 1.2534\n",
            "Epoch [89/100], Batch [239/346], Loss: 1.9925\n",
            "Epoch [89/100], Batch [240/346], Loss: 1.7342\n",
            "Epoch [89/100], Batch [241/346], Loss: 1.6879\n",
            "Epoch [89/100], Batch [242/346], Loss: 1.9951\n",
            "Epoch [89/100], Batch [243/346], Loss: 1.6022\n",
            "Epoch [89/100], Batch [244/346], Loss: 1.9315\n",
            "Epoch [89/100], Batch [245/346], Loss: 1.7001\n",
            "Epoch [89/100], Batch [246/346], Loss: 1.4387\n",
            "Epoch [89/100], Batch [247/346], Loss: 1.5024\n",
            "Epoch [89/100], Batch [248/346], Loss: 1.5793\n",
            "Epoch [89/100], Batch [249/346], Loss: 1.4048\n",
            "Epoch [89/100], Batch [250/346], Loss: 1.5000\n",
            "Epoch [89/100], Batch [251/346], Loss: 1.7358\n",
            "Epoch [89/100], Batch [252/346], Loss: 1.4789\n",
            "Epoch [89/100], Batch [253/346], Loss: 1.3741\n",
            "Epoch [89/100], Batch [254/346], Loss: 1.6464\n",
            "Epoch [89/100], Batch [255/346], Loss: 1.5862\n",
            "Epoch [89/100], Batch [256/346], Loss: 1.6983\n",
            "Epoch [89/100], Batch [257/346], Loss: 1.3787\n",
            "Epoch [89/100], Batch [258/346], Loss: 1.9801\n",
            "Epoch [89/100], Batch [259/346], Loss: 1.5079\n",
            "Epoch [89/100], Batch [260/346], Loss: 1.2680\n",
            "Epoch [89/100], Batch [261/346], Loss: 1.4634\n",
            "Epoch [89/100], Batch [262/346], Loss: 1.2845\n",
            "Epoch [89/100], Batch [263/346], Loss: 1.6580\n",
            "Epoch [89/100], Batch [264/346], Loss: 1.6516\n",
            "Epoch [89/100], Batch [265/346], Loss: 1.5359\n",
            "Epoch [89/100], Batch [266/346], Loss: 1.2779\n",
            "Epoch [89/100], Batch [267/346], Loss: 1.5339\n",
            "Epoch [89/100], Batch [268/346], Loss: 1.3903\n",
            "Epoch [89/100], Batch [269/346], Loss: 1.5087\n",
            "Epoch [89/100], Batch [270/346], Loss: 1.5257\n",
            "Epoch [89/100], Batch [271/346], Loss: 1.4981\n",
            "Epoch [89/100], Batch [272/346], Loss: 1.4770\n",
            "Epoch [89/100], Batch [273/346], Loss: 1.5997\n",
            "Epoch [89/100], Batch [274/346], Loss: 1.8696\n",
            "Epoch [89/100], Batch [275/346], Loss: 1.6066\n",
            "Epoch [89/100], Batch [276/346], Loss: 1.4115\n",
            "Epoch [89/100], Batch [277/346], Loss: 1.1812\n",
            "Epoch [89/100], Batch [278/346], Loss: 1.8490\n",
            "Epoch [89/100], Batch [279/346], Loss: 1.5022\n",
            "Epoch [89/100], Batch [280/346], Loss: 1.6012\n",
            "Epoch [89/100], Batch [281/346], Loss: 1.6105\n",
            "Epoch [89/100], Batch [282/346], Loss: 1.5586\n",
            "Epoch [89/100], Batch [283/346], Loss: 1.8065\n",
            "Epoch [89/100], Batch [284/346], Loss: 1.2475\n",
            "Epoch [89/100], Batch [285/346], Loss: 1.6774\n",
            "Epoch [89/100], Batch [286/346], Loss: 1.5069\n",
            "Epoch [89/100], Batch [287/346], Loss: 1.6700\n",
            "Epoch [89/100], Batch [288/346], Loss: 1.2481\n",
            "Epoch [89/100], Batch [289/346], Loss: 1.6833\n",
            "Epoch [89/100], Batch [290/346], Loss: 1.3166\n",
            "Epoch [89/100], Batch [291/346], Loss: 1.2899\n",
            "Epoch [89/100], Batch [292/346], Loss: 1.9123\n",
            "Epoch [89/100], Batch [293/346], Loss: 1.1776\n",
            "Epoch [89/100], Batch [294/346], Loss: 1.8308\n",
            "Epoch [89/100], Batch [295/346], Loss: 1.1647\n",
            "Epoch [89/100], Batch [296/346], Loss: 1.6647\n",
            "Epoch [89/100], Batch [297/346], Loss: 1.5460\n",
            "Epoch [89/100], Batch [298/346], Loss: 1.1468\n",
            "Epoch [89/100], Batch [299/346], Loss: 1.3173\n",
            "Epoch [89/100], Batch [300/346], Loss: 1.3558\n",
            "Epoch [89/100], Batch [301/346], Loss: 1.5345\n",
            "Epoch [89/100], Batch [302/346], Loss: 1.3923\n",
            "Epoch [89/100], Batch [303/346], Loss: 1.4089\n",
            "Epoch [89/100], Batch [304/346], Loss: 1.5277\n",
            "Epoch [89/100], Batch [305/346], Loss: 1.3256\n",
            "Epoch [89/100], Batch [306/346], Loss: 1.8210\n",
            "Epoch [89/100], Batch [307/346], Loss: 1.7902\n",
            "Epoch [89/100], Batch [308/346], Loss: 1.6540\n",
            "Epoch [89/100], Batch [309/346], Loss: 1.4377\n",
            "Epoch [89/100], Batch [310/346], Loss: 1.4887\n",
            "Epoch [89/100], Batch [311/346], Loss: 1.4132\n",
            "Epoch [89/100], Batch [312/346], Loss: 1.1757\n",
            "Epoch [89/100], Batch [313/346], Loss: 1.3353\n",
            "Epoch [89/100], Batch [314/346], Loss: 1.2712\n",
            "Epoch [89/100], Batch [315/346], Loss: 1.4529\n",
            "Epoch [89/100], Batch [316/346], Loss: 1.9859\n",
            "Epoch [89/100], Batch [317/346], Loss: 1.5854\n",
            "Epoch [89/100], Batch [318/346], Loss: 1.7770\n",
            "Epoch [89/100], Batch [319/346], Loss: 1.9604\n",
            "Epoch [89/100], Batch [320/346], Loss: 1.4209\n",
            "Epoch [89/100], Batch [321/346], Loss: 1.1302\n",
            "Epoch [89/100], Batch [322/346], Loss: 1.5427\n",
            "Epoch [89/100], Batch [323/346], Loss: 1.3971\n",
            "Epoch [89/100], Batch [324/346], Loss: 1.3018\n",
            "Epoch [89/100], Batch [325/346], Loss: 1.3946\n",
            "Epoch [89/100], Batch [326/346], Loss: 2.0725\n",
            "Epoch [89/100], Batch [327/346], Loss: 1.2104\n",
            "Epoch [89/100], Batch [328/346], Loss: 1.0140\n",
            "Epoch [89/100], Batch [329/346], Loss: 1.6050\n",
            "Epoch [89/100], Batch [330/346], Loss: 1.0275\n",
            "Epoch [89/100], Batch [331/346], Loss: 1.6233\n",
            "Epoch [89/100], Batch [332/346], Loss: 1.1512\n",
            "Epoch [89/100], Batch [333/346], Loss: 1.5968\n",
            "Epoch [89/100], Batch [334/346], Loss: 1.4673\n",
            "Epoch [89/100], Batch [335/346], Loss: 1.8045\n",
            "Epoch [89/100], Batch [336/346], Loss: 1.1377\n",
            "Epoch [89/100], Batch [337/346], Loss: 1.4505\n",
            "Epoch [89/100], Batch [338/346], Loss: 1.5276\n",
            "Epoch [89/100], Batch [339/346], Loss: 1.8297\n",
            "Epoch [89/100], Batch [340/346], Loss: 1.4502\n",
            "Epoch [89/100], Batch [341/346], Loss: 1.2586\n",
            "Epoch [89/100], Batch [342/346], Loss: 1.6672\n",
            "Epoch [89/100], Batch [343/346], Loss: 1.2845\n",
            "Epoch [89/100], Batch [344/346], Loss: 1.5232\n",
            "Epoch [89/100], Batch [345/346], Loss: 1.9975\n",
            "Epoch [89/100], Batch [346/346], Loss: 1.5801\n",
            "Epoch [90/100], Batch [1/346], Loss: 1.6724\n",
            "Epoch [90/100], Batch [2/346], Loss: 1.4475\n",
            "Epoch [90/100], Batch [3/346], Loss: 1.3876\n",
            "Epoch [90/100], Batch [4/346], Loss: 1.3811\n",
            "Epoch [90/100], Batch [5/346], Loss: 1.4942\n",
            "Epoch [90/100], Batch [6/346], Loss: 1.2402\n",
            "Epoch [90/100], Batch [7/346], Loss: 1.1328\n",
            "Epoch [90/100], Batch [8/346], Loss: 1.9575\n",
            "Epoch [90/100], Batch [9/346], Loss: 1.5932\n",
            "Epoch [90/100], Batch [10/346], Loss: 1.5350\n",
            "Epoch [90/100], Batch [11/346], Loss: 1.3909\n",
            "Epoch [90/100], Batch [12/346], Loss: 1.1377\n",
            "Epoch [90/100], Batch [13/346], Loss: 1.4307\n",
            "Epoch [90/100], Batch [14/346], Loss: 1.3565\n",
            "Epoch [90/100], Batch [15/346], Loss: 1.4121\n",
            "Epoch [90/100], Batch [16/346], Loss: 1.4600\n",
            "Epoch [90/100], Batch [17/346], Loss: 1.6017\n",
            "Epoch [90/100], Batch [18/346], Loss: 1.7110\n",
            "Epoch [90/100], Batch [19/346], Loss: 1.3301\n",
            "Epoch [90/100], Batch [20/346], Loss: 1.8123\n",
            "Epoch [90/100], Batch [21/346], Loss: 2.1245\n",
            "Epoch [90/100], Batch [22/346], Loss: 1.5477\n",
            "Epoch [90/100], Batch [23/346], Loss: 1.0482\n",
            "Epoch [90/100], Batch [24/346], Loss: 1.6941\n",
            "Epoch [90/100], Batch [25/346], Loss: 1.4926\n",
            "Epoch [90/100], Batch [26/346], Loss: 1.6434\n",
            "Epoch [90/100], Batch [27/346], Loss: 1.2392\n",
            "Epoch [90/100], Batch [28/346], Loss: 1.3258\n",
            "Epoch [90/100], Batch [29/346], Loss: 1.0572\n",
            "Epoch [90/100], Batch [30/346], Loss: 1.8827\n",
            "Epoch [90/100], Batch [31/346], Loss: 1.4456\n",
            "Epoch [90/100], Batch [32/346], Loss: 1.2673\n",
            "Epoch [90/100], Batch [33/346], Loss: 1.7608\n",
            "Epoch [90/100], Batch [34/346], Loss: 1.4814\n",
            "Epoch [90/100], Batch [35/346], Loss: 1.1119\n",
            "Epoch [90/100], Batch [36/346], Loss: 1.4821\n",
            "Epoch [90/100], Batch [37/346], Loss: 1.6189\n",
            "Epoch [90/100], Batch [38/346], Loss: 1.5768\n",
            "Epoch [90/100], Batch [39/346], Loss: 1.5350\n",
            "Epoch [90/100], Batch [40/346], Loss: 1.2741\n",
            "Epoch [90/100], Batch [41/346], Loss: 1.5505\n",
            "Epoch [90/100], Batch [42/346], Loss: 1.5592\n",
            "Epoch [90/100], Batch [43/346], Loss: 1.4741\n",
            "Epoch [90/100], Batch [44/346], Loss: 1.3547\n",
            "Epoch [90/100], Batch [45/346], Loss: 1.3680\n",
            "Epoch [90/100], Batch [46/346], Loss: 1.2624\n",
            "Epoch [90/100], Batch [47/346], Loss: 1.7046\n",
            "Epoch [90/100], Batch [48/346], Loss: 1.6387\n",
            "Epoch [90/100], Batch [49/346], Loss: 1.4147\n",
            "Epoch [90/100], Batch [50/346], Loss: 1.4028\n",
            "Epoch [90/100], Batch [51/346], Loss: 1.2940\n",
            "Epoch [90/100], Batch [52/346], Loss: 1.6057\n",
            "Epoch [90/100], Batch [53/346], Loss: 1.2004\n",
            "Epoch [90/100], Batch [54/346], Loss: 1.1282\n",
            "Epoch [90/100], Batch [55/346], Loss: 1.4367\n",
            "Epoch [90/100], Batch [56/346], Loss: 1.0928\n",
            "Epoch [90/100], Batch [57/346], Loss: 1.4683\n",
            "Epoch [90/100], Batch [58/346], Loss: 1.5219\n",
            "Epoch [90/100], Batch [59/346], Loss: 1.7199\n",
            "Epoch [90/100], Batch [60/346], Loss: 1.7042\n",
            "Epoch [90/100], Batch [61/346], Loss: 1.4555\n",
            "Epoch [90/100], Batch [62/346], Loss: 1.3351\n",
            "Epoch [90/100], Batch [63/346], Loss: 1.5112\n",
            "Epoch [90/100], Batch [64/346], Loss: 1.5660\n",
            "Epoch [90/100], Batch [65/346], Loss: 1.1747\n",
            "Epoch [90/100], Batch [66/346], Loss: 1.7955\n",
            "Epoch [90/100], Batch [67/346], Loss: 1.6543\n",
            "Epoch [90/100], Batch [68/346], Loss: 1.5624\n",
            "Epoch [90/100], Batch [69/346], Loss: 1.9239\n",
            "Epoch [90/100], Batch [70/346], Loss: 1.5191\n",
            "Epoch [90/100], Batch [71/346], Loss: 1.4255\n",
            "Epoch [90/100], Batch [72/346], Loss: 1.7720\n",
            "Epoch [90/100], Batch [73/346], Loss: 1.6294\n",
            "Epoch [90/100], Batch [74/346], Loss: 1.2966\n",
            "Epoch [90/100], Batch [75/346], Loss: 1.6469\n",
            "Epoch [90/100], Batch [76/346], Loss: 1.7103\n",
            "Epoch [90/100], Batch [77/346], Loss: 1.6770\n",
            "Epoch [90/100], Batch [78/346], Loss: 1.2836\n",
            "Epoch [90/100], Batch [79/346], Loss: 1.3695\n",
            "Epoch [90/100], Batch [80/346], Loss: 1.5153\n",
            "Epoch [90/100], Batch [81/346], Loss: 1.9430\n",
            "Epoch [90/100], Batch [82/346], Loss: 1.2785\n",
            "Epoch [90/100], Batch [83/346], Loss: 1.5420\n",
            "Epoch [90/100], Batch [84/346], Loss: 1.2609\n",
            "Epoch [90/100], Batch [85/346], Loss: 1.3242\n",
            "Epoch [90/100], Batch [86/346], Loss: 1.5456\n",
            "Epoch [90/100], Batch [87/346], Loss: 1.2405\n",
            "Epoch [90/100], Batch [88/346], Loss: 1.4254\n",
            "Epoch [90/100], Batch [89/346], Loss: 1.4442\n",
            "Epoch [90/100], Batch [90/346], Loss: 1.3031\n",
            "Epoch [90/100], Batch [91/346], Loss: 1.5824\n",
            "Epoch [90/100], Batch [92/346], Loss: 1.5342\n",
            "Epoch [90/100], Batch [93/346], Loss: 1.2777\n",
            "Epoch [90/100], Batch [94/346], Loss: 1.4380\n",
            "Epoch [90/100], Batch [95/346], Loss: 1.3737\n",
            "Epoch [90/100], Batch [96/346], Loss: 1.4754\n",
            "Epoch [90/100], Batch [97/346], Loss: 1.1014\n",
            "Epoch [90/100], Batch [98/346], Loss: 1.2340\n",
            "Epoch [90/100], Batch [99/346], Loss: 1.7143\n",
            "Epoch [90/100], Batch [100/346], Loss: 1.3003\n",
            "Epoch [90/100], Batch [101/346], Loss: 1.5189\n",
            "Epoch [90/100], Batch [102/346], Loss: 1.7842\n",
            "Epoch [90/100], Batch [103/346], Loss: 1.3150\n",
            "Epoch [90/100], Batch [104/346], Loss: 1.3766\n",
            "Epoch [90/100], Batch [105/346], Loss: 1.6493\n",
            "Epoch [90/100], Batch [106/346], Loss: 1.9437\n",
            "Epoch [90/100], Batch [107/346], Loss: 1.2668\n",
            "Epoch [90/100], Batch [108/346], Loss: 1.8217\n",
            "Epoch [90/100], Batch [109/346], Loss: 1.3926\n",
            "Epoch [90/100], Batch [110/346], Loss: 1.6544\n",
            "Epoch [90/100], Batch [111/346], Loss: 1.4071\n",
            "Epoch [90/100], Batch [112/346], Loss: 1.5703\n",
            "Epoch [90/100], Batch [113/346], Loss: 1.4262\n",
            "Epoch [90/100], Batch [114/346], Loss: 1.4004\n",
            "Epoch [90/100], Batch [115/346], Loss: 1.5692\n",
            "Epoch [90/100], Batch [116/346], Loss: 1.5641\n",
            "Epoch [90/100], Batch [117/346], Loss: 1.4903\n",
            "Epoch [90/100], Batch [118/346], Loss: 1.6182\n",
            "Epoch [90/100], Batch [119/346], Loss: 1.4660\n",
            "Epoch [90/100], Batch [120/346], Loss: 1.4160\n",
            "Epoch [90/100], Batch [121/346], Loss: 1.2721\n",
            "Epoch [90/100], Batch [122/346], Loss: 1.4319\n",
            "Epoch [90/100], Batch [123/346], Loss: 1.9870\n",
            "Epoch [90/100], Batch [124/346], Loss: 1.6587\n",
            "Epoch [90/100], Batch [125/346], Loss: 1.6364\n",
            "Epoch [90/100], Batch [126/346], Loss: 1.4317\n",
            "Epoch [90/100], Batch [127/346], Loss: 1.4575\n",
            "Epoch [90/100], Batch [128/346], Loss: 1.4690\n",
            "Epoch [90/100], Batch [129/346], Loss: 1.4386\n",
            "Epoch [90/100], Batch [130/346], Loss: 1.8011\n",
            "Epoch [90/100], Batch [131/346], Loss: 1.2884\n",
            "Epoch [90/100], Batch [132/346], Loss: 1.4505\n",
            "Epoch [90/100], Batch [133/346], Loss: 1.3351\n",
            "Epoch [90/100], Batch [134/346], Loss: 1.5094\n",
            "Epoch [90/100], Batch [135/346], Loss: 1.4037\n",
            "Epoch [90/100], Batch [136/346], Loss: 1.7249\n",
            "Epoch [90/100], Batch [137/346], Loss: 1.3738\n",
            "Epoch [90/100], Batch [138/346], Loss: 1.7066\n",
            "Epoch [90/100], Batch [139/346], Loss: 1.8985\n",
            "Epoch [90/100], Batch [140/346], Loss: 1.4083\n",
            "Epoch [90/100], Batch [141/346], Loss: 1.5020\n",
            "Epoch [90/100], Batch [142/346], Loss: 1.4538\n",
            "Epoch [90/100], Batch [143/346], Loss: 1.3264\n",
            "Epoch [90/100], Batch [144/346], Loss: 1.7967\n",
            "Epoch [90/100], Batch [145/346], Loss: 1.7453\n",
            "Epoch [90/100], Batch [146/346], Loss: 1.6208\n",
            "Epoch [90/100], Batch [147/346], Loss: 1.2139\n",
            "Epoch [90/100], Batch [148/346], Loss: 1.3836\n",
            "Epoch [90/100], Batch [149/346], Loss: 1.6210\n",
            "Epoch [90/100], Batch [150/346], Loss: 1.8611\n",
            "Epoch [90/100], Batch [151/346], Loss: 1.5275\n",
            "Epoch [90/100], Batch [152/346], Loss: 1.3300\n",
            "Epoch [90/100], Batch [153/346], Loss: 1.2932\n",
            "Epoch [90/100], Batch [154/346], Loss: 1.4373\n",
            "Epoch [90/100], Batch [155/346], Loss: 1.5851\n",
            "Epoch [90/100], Batch [156/346], Loss: 1.6202\n",
            "Epoch [90/100], Batch [157/346], Loss: 1.6975\n",
            "Epoch [90/100], Batch [158/346], Loss: 1.5463\n",
            "Epoch [90/100], Batch [159/346], Loss: 1.3830\n",
            "Epoch [90/100], Batch [160/346], Loss: 1.6551\n",
            "Epoch [90/100], Batch [161/346], Loss: 1.0569\n",
            "Epoch [90/100], Batch [162/346], Loss: 1.3705\n",
            "Epoch [90/100], Batch [163/346], Loss: 1.3989\n",
            "Epoch [90/100], Batch [164/346], Loss: 1.6510\n",
            "Epoch [90/100], Batch [165/346], Loss: 1.1888\n",
            "Epoch [90/100], Batch [166/346], Loss: 1.4492\n",
            "Epoch [90/100], Batch [167/346], Loss: 1.6174\n",
            "Epoch [90/100], Batch [168/346], Loss: 1.5877\n",
            "Epoch [90/100], Batch [169/346], Loss: 1.4310\n",
            "Epoch [90/100], Batch [170/346], Loss: 1.4616\n",
            "Epoch [90/100], Batch [171/346], Loss: 1.3003\n",
            "Epoch [90/100], Batch [172/346], Loss: 1.3012\n",
            "Epoch [90/100], Batch [173/346], Loss: 1.3578\n",
            "Epoch [90/100], Batch [174/346], Loss: 1.4306\n",
            "Epoch [90/100], Batch [175/346], Loss: 1.2902\n",
            "Epoch [90/100], Batch [176/346], Loss: 1.4387\n",
            "Epoch [90/100], Batch [177/346], Loss: 1.9302\n",
            "Epoch [90/100], Batch [178/346], Loss: 1.7085\n",
            "Epoch [90/100], Batch [179/346], Loss: 1.4220\n",
            "Epoch [90/100], Batch [180/346], Loss: 1.1127\n",
            "Epoch [90/100], Batch [181/346], Loss: 1.3640\n",
            "Epoch [90/100], Batch [182/346], Loss: 1.3132\n",
            "Epoch [90/100], Batch [183/346], Loss: 2.0916\n",
            "Epoch [90/100], Batch [184/346], Loss: 1.2873\n",
            "Epoch [90/100], Batch [185/346], Loss: 1.5172\n",
            "Epoch [90/100], Batch [186/346], Loss: 1.3241\n",
            "Epoch [90/100], Batch [187/346], Loss: 1.4756\n",
            "Epoch [90/100], Batch [188/346], Loss: 1.9921\n",
            "Epoch [90/100], Batch [189/346], Loss: 1.4571\n",
            "Epoch [90/100], Batch [190/346], Loss: 1.4887\n",
            "Epoch [90/100], Batch [191/346], Loss: 1.8700\n",
            "Epoch [90/100], Batch [192/346], Loss: 1.5228\n",
            "Epoch [90/100], Batch [193/346], Loss: 1.0450\n",
            "Epoch [90/100], Batch [194/346], Loss: 1.2802\n",
            "Epoch [90/100], Batch [195/346], Loss: 1.7087\n",
            "Epoch [90/100], Batch [196/346], Loss: 1.5639\n",
            "Epoch [90/100], Batch [197/346], Loss: 1.4396\n",
            "Epoch [90/100], Batch [198/346], Loss: 2.3937\n",
            "Epoch [90/100], Batch [199/346], Loss: 1.4650\n",
            "Epoch [90/100], Batch [200/346], Loss: 1.3770\n",
            "Epoch [90/100], Batch [201/346], Loss: 1.6383\n",
            "Epoch [90/100], Batch [202/346], Loss: 1.3998\n",
            "Epoch [90/100], Batch [203/346], Loss: 1.2595\n",
            "Epoch [90/100], Batch [204/346], Loss: 1.7271\n",
            "Epoch [90/100], Batch [205/346], Loss: 1.5271\n",
            "Epoch [90/100], Batch [206/346], Loss: 1.2743\n",
            "Epoch [90/100], Batch [207/346], Loss: 1.6369\n",
            "Epoch [90/100], Batch [208/346], Loss: 1.3467\n",
            "Epoch [90/100], Batch [209/346], Loss: 1.5584\n",
            "Epoch [90/100], Batch [210/346], Loss: 1.6888\n",
            "Epoch [90/100], Batch [211/346], Loss: 1.8508\n",
            "Epoch [90/100], Batch [212/346], Loss: 1.4080\n",
            "Epoch [90/100], Batch [213/346], Loss: 1.4445\n",
            "Epoch [90/100], Batch [214/346], Loss: 1.5572\n",
            "Epoch [90/100], Batch [215/346], Loss: 1.6622\n",
            "Epoch [90/100], Batch [216/346], Loss: 1.5358\n",
            "Epoch [90/100], Batch [217/346], Loss: 1.8505\n",
            "Epoch [90/100], Batch [218/346], Loss: 1.7997\n",
            "Epoch [90/100], Batch [219/346], Loss: 1.4528\n",
            "Epoch [90/100], Batch [220/346], Loss: 1.2911\n",
            "Epoch [90/100], Batch [221/346], Loss: 1.5164\n",
            "Epoch [90/100], Batch [222/346], Loss: 1.3370\n",
            "Epoch [90/100], Batch [223/346], Loss: 1.4750\n",
            "Epoch [90/100], Batch [224/346], Loss: 2.4058\n",
            "Epoch [90/100], Batch [225/346], Loss: 2.0625\n",
            "Epoch [90/100], Batch [226/346], Loss: 2.2252\n",
            "Epoch [90/100], Batch [227/346], Loss: 1.6366\n",
            "Epoch [90/100], Batch [228/346], Loss: 1.6039\n",
            "Epoch [90/100], Batch [229/346], Loss: 1.5413\n",
            "Epoch [90/100], Batch [230/346], Loss: 1.5539\n",
            "Epoch [90/100], Batch [231/346], Loss: 2.1938\n",
            "Epoch [90/100], Batch [232/346], Loss: 1.6840\n",
            "Epoch [90/100], Batch [233/346], Loss: 1.7890\n",
            "Epoch [90/100], Batch [234/346], Loss: 1.5657\n",
            "Epoch [90/100], Batch [235/346], Loss: 1.6337\n",
            "Epoch [90/100], Batch [236/346], Loss: 1.4481\n",
            "Epoch [90/100], Batch [237/346], Loss: 1.8671\n",
            "Epoch [90/100], Batch [238/346], Loss: 1.8732\n",
            "Epoch [90/100], Batch [239/346], Loss: 1.4859\n",
            "Epoch [90/100], Batch [240/346], Loss: 1.3428\n",
            "Epoch [90/100], Batch [241/346], Loss: 1.7892\n",
            "Epoch [90/100], Batch [242/346], Loss: 1.3768\n",
            "Epoch [90/100], Batch [243/346], Loss: 1.8889\n",
            "Epoch [90/100], Batch [244/346], Loss: 1.8032\n",
            "Epoch [90/100], Batch [245/346], Loss: 1.2822\n",
            "Epoch [90/100], Batch [246/346], Loss: 1.4005\n",
            "Epoch [90/100], Batch [247/346], Loss: 1.5056\n",
            "Epoch [90/100], Batch [248/346], Loss: 1.5864\n",
            "Epoch [90/100], Batch [249/346], Loss: 1.2149\n",
            "Epoch [90/100], Batch [250/346], Loss: 1.6949\n",
            "Epoch [90/100], Batch [251/346], Loss: 1.3670\n",
            "Epoch [90/100], Batch [252/346], Loss: 1.2491\n",
            "Epoch [90/100], Batch [253/346], Loss: 1.3998\n",
            "Epoch [90/100], Batch [254/346], Loss: 1.5483\n",
            "Epoch [90/100], Batch [255/346], Loss: 1.5382\n",
            "Epoch [90/100], Batch [256/346], Loss: 1.5569\n",
            "Epoch [90/100], Batch [257/346], Loss: 1.5397\n",
            "Epoch [90/100], Batch [258/346], Loss: 1.6259\n",
            "Epoch [90/100], Batch [259/346], Loss: 1.6456\n",
            "Epoch [90/100], Batch [260/346], Loss: 1.3057\n",
            "Epoch [90/100], Batch [261/346], Loss: 1.2051\n",
            "Epoch [90/100], Batch [262/346], Loss: 1.6627\n",
            "Epoch [90/100], Batch [263/346], Loss: 1.8929\n",
            "Epoch [90/100], Batch [264/346], Loss: 1.3460\n",
            "Epoch [90/100], Batch [265/346], Loss: 1.4476\n",
            "Epoch [90/100], Batch [266/346], Loss: 1.3335\n",
            "Epoch [90/100], Batch [267/346], Loss: 1.4225\n",
            "Epoch [90/100], Batch [268/346], Loss: 1.4140\n",
            "Epoch [90/100], Batch [269/346], Loss: 1.3835\n",
            "Epoch [90/100], Batch [270/346], Loss: 1.5955\n",
            "Epoch [90/100], Batch [271/346], Loss: 1.1687\n",
            "Epoch [90/100], Batch [272/346], Loss: 1.5288\n",
            "Epoch [90/100], Batch [273/346], Loss: 1.5276\n",
            "Epoch [90/100], Batch [274/346], Loss: 1.4218\n",
            "Epoch [90/100], Batch [275/346], Loss: 1.1482\n",
            "Epoch [90/100], Batch [276/346], Loss: 1.7647\n",
            "Epoch [90/100], Batch [277/346], Loss: 1.5327\n",
            "Epoch [90/100], Batch [278/346], Loss: 1.2514\n",
            "Epoch [90/100], Batch [279/346], Loss: 1.1598\n",
            "Epoch [90/100], Batch [280/346], Loss: 1.3961\n",
            "Epoch [90/100], Batch [281/346], Loss: 1.2228\n",
            "Epoch [90/100], Batch [282/346], Loss: 1.5703\n",
            "Epoch [90/100], Batch [283/346], Loss: 1.9456\n",
            "Epoch [90/100], Batch [284/346], Loss: 1.7387\n",
            "Epoch [90/100], Batch [285/346], Loss: 1.1899\n",
            "Epoch [90/100], Batch [286/346], Loss: 1.6735\n",
            "Epoch [90/100], Batch [287/346], Loss: 1.3269\n",
            "Epoch [90/100], Batch [288/346], Loss: 1.9920\n",
            "Epoch [90/100], Batch [289/346], Loss: 1.4494\n",
            "Epoch [90/100], Batch [290/346], Loss: 1.6579\n",
            "Epoch [90/100], Batch [291/346], Loss: 1.4406\n",
            "Epoch [90/100], Batch [292/346], Loss: 1.0844\n",
            "Epoch [90/100], Batch [293/346], Loss: 1.6165\n",
            "Epoch [90/100], Batch [294/346], Loss: 1.3962\n",
            "Epoch [90/100], Batch [295/346], Loss: 1.6803\n",
            "Epoch [90/100], Batch [296/346], Loss: 1.3555\n",
            "Epoch [90/100], Batch [297/346], Loss: 1.9022\n",
            "Epoch [90/100], Batch [298/346], Loss: 1.6704\n",
            "Epoch [90/100], Batch [299/346], Loss: 1.5658\n",
            "Epoch [90/100], Batch [300/346], Loss: 1.4784\n",
            "Epoch [90/100], Batch [301/346], Loss: 2.0163\n",
            "Epoch [90/100], Batch [302/346], Loss: 1.8392\n",
            "Epoch [90/100], Batch [303/346], Loss: 1.4851\n",
            "Epoch [90/100], Batch [304/346], Loss: 1.9569\n",
            "Epoch [90/100], Batch [305/346], Loss: 1.7099\n",
            "Epoch [90/100], Batch [306/346], Loss: 1.2406\n",
            "Epoch [90/100], Batch [307/346], Loss: 1.5206\n",
            "Epoch [90/100], Batch [308/346], Loss: 1.7179\n",
            "Epoch [90/100], Batch [309/346], Loss: 2.0700\n",
            "Epoch [90/100], Batch [310/346], Loss: 1.5741\n",
            "Epoch [90/100], Batch [311/346], Loss: 1.6187\n",
            "Epoch [90/100], Batch [312/346], Loss: 1.9089\n",
            "Epoch [90/100], Batch [313/346], Loss: 2.3501\n",
            "Epoch [90/100], Batch [314/346], Loss: 1.6550\n",
            "Epoch [90/100], Batch [315/346], Loss: 1.5674\n",
            "Epoch [90/100], Batch [316/346], Loss: 1.6813\n",
            "Epoch [90/100], Batch [317/346], Loss: 1.5617\n",
            "Epoch [90/100], Batch [318/346], Loss: 1.8615\n",
            "Epoch [90/100], Batch [319/346], Loss: 1.6623\n",
            "Epoch [90/100], Batch [320/346], Loss: 1.5139\n",
            "Epoch [90/100], Batch [321/346], Loss: 1.3615\n",
            "Epoch [90/100], Batch [322/346], Loss: 1.6934\n",
            "Epoch [90/100], Batch [323/346], Loss: 1.7041\n",
            "Epoch [90/100], Batch [324/346], Loss: 1.3909\n",
            "Epoch [90/100], Batch [325/346], Loss: 1.6443\n",
            "Epoch [90/100], Batch [326/346], Loss: 1.5796\n",
            "Epoch [90/100], Batch [327/346], Loss: 1.9378\n",
            "Epoch [90/100], Batch [328/346], Loss: 1.4068\n",
            "Epoch [90/100], Batch [329/346], Loss: 1.3545\n",
            "Epoch [90/100], Batch [330/346], Loss: 1.5957\n",
            "Epoch [90/100], Batch [331/346], Loss: 1.1958\n",
            "Epoch [90/100], Batch [332/346], Loss: 1.4866\n",
            "Epoch [90/100], Batch [333/346], Loss: 1.4052\n",
            "Epoch [90/100], Batch [334/346], Loss: 1.3971\n",
            "Epoch [90/100], Batch [335/346], Loss: 1.5239\n",
            "Epoch [90/100], Batch [336/346], Loss: 1.7702\n",
            "Epoch [90/100], Batch [337/346], Loss: 1.4895\n",
            "Epoch [90/100], Batch [338/346], Loss: 1.5477\n",
            "Epoch [90/100], Batch [339/346], Loss: 1.5721\n",
            "Epoch [90/100], Batch [340/346], Loss: 1.9217\n",
            "Epoch [90/100], Batch [341/346], Loss: 2.0350\n",
            "Epoch [90/100], Batch [342/346], Loss: 1.3739\n",
            "Epoch [90/100], Batch [343/346], Loss: 1.5628\n",
            "Epoch [90/100], Batch [344/346], Loss: 2.2048\n",
            "Epoch [90/100], Batch [345/346], Loss: 1.7520\n",
            "Epoch [90/100], Batch [346/346], Loss: 1.8314\n",
            "Epoch [91/100], Batch [1/346], Loss: 1.8078\n",
            "Epoch [91/100], Batch [2/346], Loss: 1.8460\n",
            "Epoch [91/100], Batch [3/346], Loss: 1.2383\n",
            "Epoch [91/100], Batch [4/346], Loss: 1.6186\n",
            "Epoch [91/100], Batch [5/346], Loss: 1.5443\n",
            "Epoch [91/100], Batch [6/346], Loss: 1.6789\n",
            "Epoch [91/100], Batch [7/346], Loss: 1.4452\n",
            "Epoch [91/100], Batch [8/346], Loss: 1.8143\n",
            "Epoch [91/100], Batch [9/346], Loss: 1.0884\n",
            "Epoch [91/100], Batch [10/346], Loss: 1.4924\n",
            "Epoch [91/100], Batch [11/346], Loss: 1.9458\n",
            "Epoch [91/100], Batch [12/346], Loss: 1.7603\n",
            "Epoch [91/100], Batch [13/346], Loss: 1.9424\n",
            "Epoch [91/100], Batch [14/346], Loss: 1.6585\n",
            "Epoch [91/100], Batch [15/346], Loss: 1.7048\n",
            "Epoch [91/100], Batch [16/346], Loss: 2.0145\n",
            "Epoch [91/100], Batch [17/346], Loss: 1.4973\n",
            "Epoch [91/100], Batch [18/346], Loss: 1.4634\n",
            "Epoch [91/100], Batch [19/346], Loss: 1.6320\n",
            "Epoch [91/100], Batch [20/346], Loss: 1.3768\n",
            "Epoch [91/100], Batch [21/346], Loss: 1.7089\n",
            "Epoch [91/100], Batch [22/346], Loss: 1.2384\n",
            "Epoch [91/100], Batch [23/346], Loss: 1.7481\n",
            "Epoch [91/100], Batch [24/346], Loss: 1.8183\n",
            "Epoch [91/100], Batch [25/346], Loss: 1.3398\n",
            "Epoch [91/100], Batch [26/346], Loss: 1.3652\n",
            "Epoch [91/100], Batch [27/346], Loss: 1.4894\n",
            "Epoch [91/100], Batch [28/346], Loss: 1.6452\n",
            "Epoch [91/100], Batch [29/346], Loss: 1.6341\n",
            "Epoch [91/100], Batch [30/346], Loss: 1.7049\n",
            "Epoch [91/100], Batch [31/346], Loss: 1.2231\n",
            "Epoch [91/100], Batch [32/346], Loss: 1.5038\n",
            "Epoch [91/100], Batch [33/346], Loss: 1.2604\n",
            "Epoch [91/100], Batch [34/346], Loss: 1.4266\n",
            "Epoch [91/100], Batch [35/346], Loss: 1.6980\n",
            "Epoch [91/100], Batch [36/346], Loss: 1.6924\n",
            "Epoch [91/100], Batch [37/346], Loss: 1.5601\n",
            "Epoch [91/100], Batch [38/346], Loss: 1.4570\n",
            "Epoch [91/100], Batch [39/346], Loss: 1.6371\n",
            "Epoch [91/100], Batch [40/346], Loss: 1.1547\n",
            "Epoch [91/100], Batch [41/346], Loss: 1.5562\n",
            "Epoch [91/100], Batch [42/346], Loss: 1.4568\n",
            "Epoch [91/100], Batch [43/346], Loss: 1.2558\n",
            "Epoch [91/100], Batch [44/346], Loss: 1.8142\n",
            "Epoch [91/100], Batch [45/346], Loss: 1.4948\n",
            "Epoch [91/100], Batch [46/346], Loss: 1.3375\n",
            "Epoch [91/100], Batch [47/346], Loss: 1.5175\n",
            "Epoch [91/100], Batch [48/346], Loss: 1.4290\n",
            "Epoch [91/100], Batch [49/346], Loss: 1.7020\n",
            "Epoch [91/100], Batch [50/346], Loss: 1.5744\n",
            "Epoch [91/100], Batch [51/346], Loss: 1.4723\n",
            "Epoch [91/100], Batch [52/346], Loss: 1.5517\n",
            "Epoch [91/100], Batch [53/346], Loss: 1.9278\n",
            "Epoch [91/100], Batch [54/346], Loss: 1.9334\n",
            "Epoch [91/100], Batch [55/346], Loss: 1.4616\n",
            "Epoch [91/100], Batch [56/346], Loss: 1.5788\n",
            "Epoch [91/100], Batch [57/346], Loss: 1.6789\n",
            "Epoch [91/100], Batch [58/346], Loss: 1.4791\n",
            "Epoch [91/100], Batch [59/346], Loss: 1.5676\n",
            "Epoch [91/100], Batch [60/346], Loss: 1.1993\n",
            "Epoch [91/100], Batch [61/346], Loss: 1.4755\n",
            "Epoch [91/100], Batch [62/346], Loss: 1.5003\n",
            "Epoch [91/100], Batch [63/346], Loss: 1.7916\n",
            "Epoch [91/100], Batch [64/346], Loss: 1.6246\n",
            "Epoch [91/100], Batch [65/346], Loss: 1.3703\n",
            "Epoch [91/100], Batch [66/346], Loss: 1.5580\n",
            "Epoch [91/100], Batch [67/346], Loss: 1.2257\n",
            "Epoch [91/100], Batch [68/346], Loss: 1.9492\n",
            "Epoch [91/100], Batch [69/346], Loss: 2.3570\n",
            "Epoch [91/100], Batch [70/346], Loss: 1.4954\n",
            "Epoch [91/100], Batch [71/346], Loss: 1.4135\n",
            "Epoch [91/100], Batch [72/346], Loss: 1.4916\n",
            "Epoch [91/100], Batch [73/346], Loss: 1.7013\n",
            "Epoch [91/100], Batch [74/346], Loss: 1.7821\n",
            "Epoch [91/100], Batch [75/346], Loss: 1.5462\n",
            "Epoch [91/100], Batch [76/346], Loss: 1.5823\n",
            "Epoch [91/100], Batch [77/346], Loss: 1.9655\n",
            "Epoch [91/100], Batch [78/346], Loss: 1.3161\n",
            "Epoch [91/100], Batch [79/346], Loss: 2.0685\n",
            "Epoch [91/100], Batch [80/346], Loss: 1.2629\n",
            "Epoch [91/100], Batch [81/346], Loss: 1.6124\n",
            "Epoch [91/100], Batch [82/346], Loss: 1.2945\n",
            "Epoch [91/100], Batch [83/346], Loss: 2.3158\n",
            "Epoch [91/100], Batch [84/346], Loss: 1.6662\n",
            "Epoch [91/100], Batch [85/346], Loss: 1.3985\n",
            "Epoch [91/100], Batch [86/346], Loss: 1.8119\n",
            "Epoch [91/100], Batch [87/346], Loss: 1.6696\n",
            "Epoch [91/100], Batch [88/346], Loss: 1.3759\n",
            "Epoch [91/100], Batch [89/346], Loss: 1.4992\n",
            "Epoch [91/100], Batch [90/346], Loss: 1.4440\n",
            "Epoch [91/100], Batch [91/346], Loss: 1.2246\n",
            "Epoch [91/100], Batch [92/346], Loss: 1.4794\n",
            "Epoch [91/100], Batch [93/346], Loss: 1.7273\n",
            "Epoch [91/100], Batch [94/346], Loss: 1.8297\n",
            "Epoch [91/100], Batch [95/346], Loss: 1.8800\n",
            "Epoch [91/100], Batch [96/346], Loss: 1.6067\n",
            "Epoch [91/100], Batch [97/346], Loss: 1.3004\n",
            "Epoch [91/100], Batch [98/346], Loss: 1.6205\n",
            "Epoch [91/100], Batch [99/346], Loss: 1.6997\n",
            "Epoch [91/100], Batch [100/346], Loss: 2.0465\n",
            "Epoch [91/100], Batch [101/346], Loss: 1.5640\n",
            "Epoch [91/100], Batch [102/346], Loss: 1.4254\n",
            "Epoch [91/100], Batch [103/346], Loss: 1.9116\n",
            "Epoch [91/100], Batch [104/346], Loss: 1.6268\n",
            "Epoch [91/100], Batch [105/346], Loss: 1.4789\n",
            "Epoch [91/100], Batch [106/346], Loss: 1.8291\n",
            "Epoch [91/100], Batch [107/346], Loss: 1.6053\n",
            "Epoch [91/100], Batch [108/346], Loss: 1.0911\n",
            "Epoch [91/100], Batch [109/346], Loss: 1.5793\n",
            "Epoch [91/100], Batch [110/346], Loss: 1.5433\n",
            "Epoch [91/100], Batch [111/346], Loss: 1.7690\n",
            "Epoch [91/100], Batch [112/346], Loss: 1.4080\n",
            "Epoch [91/100], Batch [113/346], Loss: 1.5354\n",
            "Epoch [91/100], Batch [114/346], Loss: 1.6788\n",
            "Epoch [91/100], Batch [115/346], Loss: 1.4868\n",
            "Epoch [91/100], Batch [116/346], Loss: 1.7004\n",
            "Epoch [91/100], Batch [117/346], Loss: 1.1439\n",
            "Epoch [91/100], Batch [118/346], Loss: 1.6942\n",
            "Epoch [91/100], Batch [119/346], Loss: 1.6901\n",
            "Epoch [91/100], Batch [120/346], Loss: 1.4492\n",
            "Epoch [91/100], Batch [121/346], Loss: 1.5712\n",
            "Epoch [91/100], Batch [122/346], Loss: 1.5415\n",
            "Epoch [91/100], Batch [123/346], Loss: 1.5852\n",
            "Epoch [91/100], Batch [124/346], Loss: 1.6026\n",
            "Epoch [91/100], Batch [125/346], Loss: 1.2800\n",
            "Epoch [91/100], Batch [126/346], Loss: 1.2821\n",
            "Epoch [91/100], Batch [127/346], Loss: 1.3850\n",
            "Epoch [91/100], Batch [128/346], Loss: 1.4787\n",
            "Epoch [91/100], Batch [129/346], Loss: 1.3733\n",
            "Epoch [91/100], Batch [130/346], Loss: 1.1870\n",
            "Epoch [91/100], Batch [131/346], Loss: 1.5382\n",
            "Epoch [91/100], Batch [132/346], Loss: 1.6204\n",
            "Epoch [91/100], Batch [133/346], Loss: 1.4014\n",
            "Epoch [91/100], Batch [134/346], Loss: 1.5462\n",
            "Epoch [91/100], Batch [135/346], Loss: 1.7193\n",
            "Epoch [91/100], Batch [136/346], Loss: 1.7800\n",
            "Epoch [91/100], Batch [137/346], Loss: 1.4407\n",
            "Epoch [91/100], Batch [138/346], Loss: 1.4556\n",
            "Epoch [91/100], Batch [139/346], Loss: 1.3127\n",
            "Epoch [91/100], Batch [140/346], Loss: 1.8327\n",
            "Epoch [91/100], Batch [141/346], Loss: 1.8902\n",
            "Epoch [91/100], Batch [142/346], Loss: 1.6217\n",
            "Epoch [91/100], Batch [143/346], Loss: 1.4753\n",
            "Epoch [91/100], Batch [144/346], Loss: 1.8540\n",
            "Epoch [91/100], Batch [145/346], Loss: 1.2385\n",
            "Epoch [91/100], Batch [146/346], Loss: 1.4695\n",
            "Epoch [91/100], Batch [147/346], Loss: 1.6170\n",
            "Epoch [91/100], Batch [148/346], Loss: 1.7984\n",
            "Epoch [91/100], Batch [149/346], Loss: 1.5344\n",
            "Epoch [91/100], Batch [150/346], Loss: 1.8540\n",
            "Epoch [91/100], Batch [151/346], Loss: 1.5315\n",
            "Epoch [91/100], Batch [152/346], Loss: 1.6015\n",
            "Epoch [91/100], Batch [153/346], Loss: 1.5866\n",
            "Epoch [91/100], Batch [154/346], Loss: 1.6626\n",
            "Epoch [91/100], Batch [155/346], Loss: 1.4799\n",
            "Epoch [91/100], Batch [156/346], Loss: 1.1110\n",
            "Epoch [91/100], Batch [157/346], Loss: 1.0621\n",
            "Epoch [91/100], Batch [158/346], Loss: 1.7226\n",
            "Epoch [91/100], Batch [159/346], Loss: 1.1894\n",
            "Epoch [91/100], Batch [160/346], Loss: 1.4349\n",
            "Epoch [91/100], Batch [161/346], Loss: 1.1390\n",
            "Epoch [91/100], Batch [162/346], Loss: 1.1200\n",
            "Epoch [91/100], Batch [163/346], Loss: 1.6154\n",
            "Epoch [91/100], Batch [164/346], Loss: 1.7425\n",
            "Epoch [91/100], Batch [165/346], Loss: 1.7969\n",
            "Epoch [91/100], Batch [166/346], Loss: 2.0468\n",
            "Epoch [91/100], Batch [167/346], Loss: 1.6137\n",
            "Epoch [91/100], Batch [168/346], Loss: 1.6244\n",
            "Epoch [91/100], Batch [169/346], Loss: 1.3232\n",
            "Epoch [91/100], Batch [170/346], Loss: 1.4018\n",
            "Epoch [91/100], Batch [171/346], Loss: 1.2807\n",
            "Epoch [91/100], Batch [172/346], Loss: 1.3991\n",
            "Epoch [91/100], Batch [173/346], Loss: 1.4739\n",
            "Epoch [91/100], Batch [174/346], Loss: 1.5151\n",
            "Epoch [91/100], Batch [175/346], Loss: 1.2332\n",
            "Epoch [91/100], Batch [176/346], Loss: 1.5274\n",
            "Epoch [91/100], Batch [177/346], Loss: 1.3163\n",
            "Epoch [91/100], Batch [178/346], Loss: 1.3711\n",
            "Epoch [91/100], Batch [179/346], Loss: 1.5674\n",
            "Epoch [91/100], Batch [180/346], Loss: 1.5246\n",
            "Epoch [91/100], Batch [181/346], Loss: 1.2629\n",
            "Epoch [91/100], Batch [182/346], Loss: 2.3321\n",
            "Epoch [91/100], Batch [183/346], Loss: 1.8699\n",
            "Epoch [91/100], Batch [184/346], Loss: 1.5543\n",
            "Epoch [91/100], Batch [185/346], Loss: 1.7734\n",
            "Epoch [91/100], Batch [186/346], Loss: 1.6561\n",
            "Epoch [91/100], Batch [187/346], Loss: 1.6055\n",
            "Epoch [91/100], Batch [188/346], Loss: 1.3745\n",
            "Epoch [91/100], Batch [189/346], Loss: 1.5005\n",
            "Epoch [91/100], Batch [190/346], Loss: 1.7507\n",
            "Epoch [91/100], Batch [191/346], Loss: 1.5374\n",
            "Epoch [91/100], Batch [192/346], Loss: 1.4016\n",
            "Epoch [91/100], Batch [193/346], Loss: 1.3939\n",
            "Epoch [91/100], Batch [194/346], Loss: 1.5421\n",
            "Epoch [91/100], Batch [195/346], Loss: 1.7850\n",
            "Epoch [91/100], Batch [196/346], Loss: 1.3824\n",
            "Epoch [91/100], Batch [197/346], Loss: 1.4935\n",
            "Epoch [91/100], Batch [198/346], Loss: 1.6016\n",
            "Epoch [91/100], Batch [199/346], Loss: 1.5894\n",
            "Epoch [91/100], Batch [200/346], Loss: 1.3734\n",
            "Epoch [91/100], Batch [201/346], Loss: 1.2489\n",
            "Epoch [91/100], Batch [202/346], Loss: 1.9438\n",
            "Epoch [91/100], Batch [203/346], Loss: 1.5675\n",
            "Epoch [91/100], Batch [204/346], Loss: 1.5555\n",
            "Epoch [91/100], Batch [205/346], Loss: 1.8650\n",
            "Epoch [91/100], Batch [206/346], Loss: 1.8901\n",
            "Epoch [91/100], Batch [207/346], Loss: 1.4466\n",
            "Epoch [91/100], Batch [208/346], Loss: 1.7268\n",
            "Epoch [91/100], Batch [209/346], Loss: 1.4930\n",
            "Epoch [91/100], Batch [210/346], Loss: 1.3853\n",
            "Epoch [91/100], Batch [211/346], Loss: 1.4085\n",
            "Epoch [91/100], Batch [212/346], Loss: 1.3303\n",
            "Epoch [91/100], Batch [213/346], Loss: 1.4866\n",
            "Epoch [91/100], Batch [214/346], Loss: 1.1565\n",
            "Epoch [91/100], Batch [215/346], Loss: 1.2988\n",
            "Epoch [91/100], Batch [216/346], Loss: 1.1775\n",
            "Epoch [91/100], Batch [217/346], Loss: 1.2692\n",
            "Epoch [91/100], Batch [218/346], Loss: 1.3668\n",
            "Epoch [91/100], Batch [219/346], Loss: 2.0173\n",
            "Epoch [91/100], Batch [220/346], Loss: 1.5267\n",
            "Epoch [91/100], Batch [221/346], Loss: 1.7061\n",
            "Epoch [91/100], Batch [222/346], Loss: 1.4263\n",
            "Epoch [91/100], Batch [223/346], Loss: 1.7059\n",
            "Epoch [91/100], Batch [224/346], Loss: 1.3908\n",
            "Epoch [91/100], Batch [225/346], Loss: 1.6954\n",
            "Epoch [91/100], Batch [226/346], Loss: 1.7689\n",
            "Epoch [91/100], Batch [227/346], Loss: 1.4941\n",
            "Epoch [91/100], Batch [228/346], Loss: 1.7372\n",
            "Epoch [91/100], Batch [229/346], Loss: 1.9499\n",
            "Epoch [91/100], Batch [230/346], Loss: 1.5640\n",
            "Epoch [91/100], Batch [231/346], Loss: 1.2862\n",
            "Epoch [91/100], Batch [232/346], Loss: 1.6519\n",
            "Epoch [91/100], Batch [233/346], Loss: 1.4063\n",
            "Epoch [91/100], Batch [234/346], Loss: 1.2258\n",
            "Epoch [91/100], Batch [235/346], Loss: 1.0504\n",
            "Epoch [91/100], Batch [236/346], Loss: 1.5576\n",
            "Epoch [91/100], Batch [237/346], Loss: 1.9583\n",
            "Epoch [91/100], Batch [238/346], Loss: 1.4501\n",
            "Epoch [91/100], Batch [239/346], Loss: 1.3838\n",
            "Epoch [91/100], Batch [240/346], Loss: 1.5888\n",
            "Epoch [91/100], Batch [241/346], Loss: 1.4891\n",
            "Epoch [91/100], Batch [242/346], Loss: 1.4325\n",
            "Epoch [91/100], Batch [243/346], Loss: 1.6838\n",
            "Epoch [91/100], Batch [244/346], Loss: 1.8938\n",
            "Epoch [91/100], Batch [245/346], Loss: 1.6987\n",
            "Epoch [91/100], Batch [246/346], Loss: 1.1628\n",
            "Epoch [91/100], Batch [247/346], Loss: 1.4275\n",
            "Epoch [91/100], Batch [248/346], Loss: 1.2162\n",
            "Epoch [91/100], Batch [249/346], Loss: 1.3725\n",
            "Epoch [91/100], Batch [250/346], Loss: 2.0705\n",
            "Epoch [91/100], Batch [251/346], Loss: 1.8222\n",
            "Epoch [91/100], Batch [252/346], Loss: 1.5683\n",
            "Epoch [91/100], Batch [253/346], Loss: 1.1291\n",
            "Epoch [91/100], Batch [254/346], Loss: 1.3505\n",
            "Epoch [91/100], Batch [255/346], Loss: 1.2329\n",
            "Epoch [91/100], Batch [256/346], Loss: 1.4663\n",
            "Epoch [91/100], Batch [257/346], Loss: 1.4884\n",
            "Epoch [91/100], Batch [258/346], Loss: 1.6482\n",
            "Epoch [91/100], Batch [259/346], Loss: 1.8689\n",
            "Epoch [91/100], Batch [260/346], Loss: 1.7066\n",
            "Epoch [91/100], Batch [261/346], Loss: 1.6656\n",
            "Epoch [91/100], Batch [262/346], Loss: 1.5655\n",
            "Epoch [91/100], Batch [263/346], Loss: 1.6445\n",
            "Epoch [91/100], Batch [264/346], Loss: 1.7265\n",
            "Epoch [91/100], Batch [265/346], Loss: 1.3966\n",
            "Epoch [91/100], Batch [266/346], Loss: 1.7217\n",
            "Epoch [91/100], Batch [267/346], Loss: 1.6109\n",
            "Epoch [91/100], Batch [268/346], Loss: 1.5664\n",
            "Epoch [91/100], Batch [269/346], Loss: 1.5316\n",
            "Epoch [91/100], Batch [270/346], Loss: 1.1604\n",
            "Epoch [91/100], Batch [271/346], Loss: 1.5778\n",
            "Epoch [91/100], Batch [272/346], Loss: 1.2259\n",
            "Epoch [91/100], Batch [273/346], Loss: 1.5564\n",
            "Epoch [91/100], Batch [274/346], Loss: 1.1257\n",
            "Epoch [91/100], Batch [275/346], Loss: 1.6536\n",
            "Epoch [91/100], Batch [276/346], Loss: 1.5684\n",
            "Epoch [91/100], Batch [277/346], Loss: 1.2484\n",
            "Epoch [91/100], Batch [278/346], Loss: 1.7806\n",
            "Epoch [91/100], Batch [279/346], Loss: 1.3773\n",
            "Epoch [91/100], Batch [280/346], Loss: 1.5882\n",
            "Epoch [91/100], Batch [281/346], Loss: 1.4815\n",
            "Epoch [91/100], Batch [282/346], Loss: 1.4266\n",
            "Epoch [91/100], Batch [283/346], Loss: 1.7373\n",
            "Epoch [91/100], Batch [284/346], Loss: 1.5297\n",
            "Epoch [91/100], Batch [285/346], Loss: 1.6203\n",
            "Epoch [91/100], Batch [286/346], Loss: 1.6820\n",
            "Epoch [91/100], Batch [287/346], Loss: 1.7402\n",
            "Epoch [91/100], Batch [288/346], Loss: 1.4032\n",
            "Epoch [91/100], Batch [289/346], Loss: 1.6186\n",
            "Epoch [91/100], Batch [290/346], Loss: 1.4746\n",
            "Epoch [91/100], Batch [291/346], Loss: 1.3114\n",
            "Epoch [91/100], Batch [292/346], Loss: 1.5076\n",
            "Epoch [91/100], Batch [293/346], Loss: 1.2788\n",
            "Epoch [91/100], Batch [294/346], Loss: 1.3236\n",
            "Epoch [91/100], Batch [295/346], Loss: 1.5152\n",
            "Epoch [91/100], Batch [296/346], Loss: 1.2609\n",
            "Epoch [91/100], Batch [297/346], Loss: 1.5915\n",
            "Epoch [91/100], Batch [298/346], Loss: 2.0319\n",
            "Epoch [91/100], Batch [299/346], Loss: 1.5381\n",
            "Epoch [91/100], Batch [300/346], Loss: 1.2984\n",
            "Epoch [91/100], Batch [301/346], Loss: 1.3497\n",
            "Epoch [91/100], Batch [302/346], Loss: 1.5782\n",
            "Epoch [91/100], Batch [303/346], Loss: 1.5338\n",
            "Epoch [91/100], Batch [304/346], Loss: 1.6633\n",
            "Epoch [91/100], Batch [305/346], Loss: 1.5202\n",
            "Epoch [91/100], Batch [306/346], Loss: 1.6009\n",
            "Epoch [91/100], Batch [307/346], Loss: 1.5942\n",
            "Epoch [91/100], Batch [308/346], Loss: 1.4440\n",
            "Epoch [91/100], Batch [309/346], Loss: 1.6097\n",
            "Epoch [91/100], Batch [310/346], Loss: 1.5968\n",
            "Epoch [91/100], Batch [311/346], Loss: 1.5878\n",
            "Epoch [91/100], Batch [312/346], Loss: 1.4182\n",
            "Epoch [91/100], Batch [313/346], Loss: 1.5143\n",
            "Epoch [91/100], Batch [314/346], Loss: 1.4709\n",
            "Epoch [91/100], Batch [315/346], Loss: 1.3884\n",
            "Epoch [91/100], Batch [316/346], Loss: 1.0777\n",
            "Epoch [91/100], Batch [317/346], Loss: 1.4040\n",
            "Epoch [91/100], Batch [318/346], Loss: 1.4418\n",
            "Epoch [91/100], Batch [319/346], Loss: 1.3465\n",
            "Epoch [91/100], Batch [320/346], Loss: 1.6638\n",
            "Epoch [91/100], Batch [321/346], Loss: 1.9325\n",
            "Epoch [91/100], Batch [322/346], Loss: 1.1380\n",
            "Epoch [91/100], Batch [323/346], Loss: 1.4012\n",
            "Epoch [91/100], Batch [324/346], Loss: 1.5015\n",
            "Epoch [91/100], Batch [325/346], Loss: 1.4221\n",
            "Epoch [91/100], Batch [326/346], Loss: 1.8859\n",
            "Epoch [91/100], Batch [327/346], Loss: 1.3124\n",
            "Epoch [91/100], Batch [328/346], Loss: 1.1892\n",
            "Epoch [91/100], Batch [329/346], Loss: 1.7092\n",
            "Epoch [91/100], Batch [330/346], Loss: 1.6949\n",
            "Epoch [91/100], Batch [331/346], Loss: 1.9771\n",
            "Epoch [91/100], Batch [332/346], Loss: 1.2586\n",
            "Epoch [91/100], Batch [333/346], Loss: 1.3848\n",
            "Epoch [91/100], Batch [334/346], Loss: 2.2233\n",
            "Epoch [91/100], Batch [335/346], Loss: 1.4237\n",
            "Epoch [91/100], Batch [336/346], Loss: 1.5865\n",
            "Epoch [91/100], Batch [337/346], Loss: 1.5421\n",
            "Epoch [91/100], Batch [338/346], Loss: 1.3324\n",
            "Epoch [91/100], Batch [339/346], Loss: 1.3118\n",
            "Epoch [91/100], Batch [340/346], Loss: 1.8651\n",
            "Epoch [91/100], Batch [341/346], Loss: 1.6587\n",
            "Epoch [91/100], Batch [342/346], Loss: 1.2958\n",
            "Epoch [91/100], Batch [343/346], Loss: 1.5244\n",
            "Epoch [91/100], Batch [344/346], Loss: 1.4821\n",
            "Epoch [91/100], Batch [345/346], Loss: 1.5474\n",
            "Epoch [91/100], Batch [346/346], Loss: 1.9578\n",
            "Epoch [92/100], Batch [1/346], Loss: 1.6279\n",
            "Epoch [92/100], Batch [2/346], Loss: 2.1279\n",
            "Epoch [92/100], Batch [3/346], Loss: 1.4946\n",
            "Epoch [92/100], Batch [4/346], Loss: 1.4639\n",
            "Epoch [92/100], Batch [5/346], Loss: 1.3685\n",
            "Epoch [92/100], Batch [6/346], Loss: 1.4718\n",
            "Epoch [92/100], Batch [7/346], Loss: 1.4680\n",
            "Epoch [92/100], Batch [8/346], Loss: 1.6948\n",
            "Epoch [92/100], Batch [9/346], Loss: 1.4478\n",
            "Epoch [92/100], Batch [10/346], Loss: 1.7004\n",
            "Epoch [92/100], Batch [11/346], Loss: 1.4794\n",
            "Epoch [92/100], Batch [12/346], Loss: 1.4012\n",
            "Epoch [92/100], Batch [13/346], Loss: 1.2613\n",
            "Epoch [92/100], Batch [14/346], Loss: 1.4959\n",
            "Epoch [92/100], Batch [15/346], Loss: 1.4232\n",
            "Epoch [92/100], Batch [16/346], Loss: 1.4651\n",
            "Epoch [92/100], Batch [17/346], Loss: 1.3357\n",
            "Epoch [92/100], Batch [18/346], Loss: 1.4603\n",
            "Epoch [92/100], Batch [19/346], Loss: 1.4033\n",
            "Epoch [92/100], Batch [20/346], Loss: 1.4902\n",
            "Epoch [92/100], Batch [21/346], Loss: 1.5042\n",
            "Epoch [92/100], Batch [22/346], Loss: 1.2943\n",
            "Epoch [92/100], Batch [23/346], Loss: 1.6378\n",
            "Epoch [92/100], Batch [24/346], Loss: 1.3013\n",
            "Epoch [92/100], Batch [25/346], Loss: 1.4318\n",
            "Epoch [92/100], Batch [26/346], Loss: 1.5176\n",
            "Epoch [92/100], Batch [27/346], Loss: 1.2724\n",
            "Epoch [92/100], Batch [28/346], Loss: 1.4936\n",
            "Epoch [92/100], Batch [29/346], Loss: 1.4807\n",
            "Epoch [92/100], Batch [30/346], Loss: 1.6985\n",
            "Epoch [92/100], Batch [31/346], Loss: 1.0754\n",
            "Epoch [92/100], Batch [32/346], Loss: 1.3636\n",
            "Epoch [92/100], Batch [33/346], Loss: 2.0159\n",
            "Epoch [92/100], Batch [34/346], Loss: 1.4301\n",
            "Epoch [92/100], Batch [35/346], Loss: 1.8154\n",
            "Epoch [92/100], Batch [36/346], Loss: 1.7604\n",
            "Epoch [92/100], Batch [37/346], Loss: 1.9152\n",
            "Epoch [92/100], Batch [38/346], Loss: 1.5252\n",
            "Epoch [92/100], Batch [39/346], Loss: 1.6090\n",
            "Epoch [92/100], Batch [40/346], Loss: 1.5095\n",
            "Epoch [92/100], Batch [41/346], Loss: 1.5138\n",
            "Epoch [92/100], Batch [42/346], Loss: 1.1023\n",
            "Epoch [92/100], Batch [43/346], Loss: 1.4330\n",
            "Epoch [92/100], Batch [44/346], Loss: 1.9857\n",
            "Epoch [92/100], Batch [45/346], Loss: 1.5105\n",
            "Epoch [92/100], Batch [46/346], Loss: 1.5091\n",
            "Epoch [92/100], Batch [47/346], Loss: 1.1537\n",
            "Epoch [92/100], Batch [48/346], Loss: 1.4108\n",
            "Epoch [92/100], Batch [49/346], Loss: 1.7884\n",
            "Epoch [92/100], Batch [50/346], Loss: 1.5206\n",
            "Epoch [92/100], Batch [51/346], Loss: 1.6615\n",
            "Epoch [92/100], Batch [52/346], Loss: 1.8174\n",
            "Epoch [92/100], Batch [53/346], Loss: 1.7042\n",
            "Epoch [92/100], Batch [54/346], Loss: 1.2462\n",
            "Epoch [92/100], Batch [55/346], Loss: 1.7565\n",
            "Epoch [92/100], Batch [56/346], Loss: 1.7801\n",
            "Epoch [92/100], Batch [57/346], Loss: 2.1775\n",
            "Epoch [92/100], Batch [58/346], Loss: 1.5510\n",
            "Epoch [92/100], Batch [59/346], Loss: 1.5995\n",
            "Epoch [92/100], Batch [60/346], Loss: 1.5664\n",
            "Epoch [92/100], Batch [61/346], Loss: 1.3958\n",
            "Epoch [92/100], Batch [62/346], Loss: 1.1656\n",
            "Epoch [92/100], Batch [63/346], Loss: 1.4535\n",
            "Epoch [92/100], Batch [64/346], Loss: 1.5959\n",
            "Epoch [92/100], Batch [65/346], Loss: 1.7220\n",
            "Epoch [92/100], Batch [66/346], Loss: 1.2879\n",
            "Epoch [92/100], Batch [67/346], Loss: 2.0599\n",
            "Epoch [92/100], Batch [68/346], Loss: 1.2487\n",
            "Epoch [92/100], Batch [69/346], Loss: 1.5704\n",
            "Epoch [92/100], Batch [70/346], Loss: 1.8836\n",
            "Epoch [92/100], Batch [71/346], Loss: 1.9445\n",
            "Epoch [92/100], Batch [72/346], Loss: 1.4174\n",
            "Epoch [92/100], Batch [73/346], Loss: 1.5434\n",
            "Epoch [92/100], Batch [74/346], Loss: 1.8734\n",
            "Epoch [92/100], Batch [75/346], Loss: 1.7060\n",
            "Epoch [92/100], Batch [76/346], Loss: 1.4848\n",
            "Epoch [92/100], Batch [77/346], Loss: 1.3169\n",
            "Epoch [92/100], Batch [78/346], Loss: 1.4347\n",
            "Epoch [92/100], Batch [79/346], Loss: 1.4534\n",
            "Epoch [92/100], Batch [80/346], Loss: 1.8968\n",
            "Epoch [92/100], Batch [81/346], Loss: 1.4424\n",
            "Epoch [92/100], Batch [82/346], Loss: 1.4207\n",
            "Epoch [92/100], Batch [83/346], Loss: 1.7699\n",
            "Epoch [92/100], Batch [84/346], Loss: 1.1167\n",
            "Epoch [92/100], Batch [85/346], Loss: 1.5881\n",
            "Epoch [92/100], Batch [86/346], Loss: 1.2060\n",
            "Epoch [92/100], Batch [87/346], Loss: 1.5890\n",
            "Epoch [92/100], Batch [88/346], Loss: 2.0793\n",
            "Epoch [92/100], Batch [89/346], Loss: 1.4824\n",
            "Epoch [92/100], Batch [90/346], Loss: 1.6525\n",
            "Epoch [92/100], Batch [91/346], Loss: 1.5996\n",
            "Epoch [92/100], Batch [92/346], Loss: 1.5716\n",
            "Epoch [92/100], Batch [93/346], Loss: 1.7584\n",
            "Epoch [92/100], Batch [94/346], Loss: 1.4911\n",
            "Epoch [92/100], Batch [95/346], Loss: 1.5748\n",
            "Epoch [92/100], Batch [96/346], Loss: 1.4581\n",
            "Epoch [92/100], Batch [97/346], Loss: 1.4333\n",
            "Epoch [92/100], Batch [98/346], Loss: 1.4151\n",
            "Epoch [92/100], Batch [99/346], Loss: 1.4183\n",
            "Epoch [92/100], Batch [100/346], Loss: 1.3800\n",
            "Epoch [92/100], Batch [101/346], Loss: 1.2765\n",
            "Epoch [92/100], Batch [102/346], Loss: 1.3460\n",
            "Epoch [92/100], Batch [103/346], Loss: 1.3990\n",
            "Epoch [92/100], Batch [104/346], Loss: 1.5323\n",
            "Epoch [92/100], Batch [105/346], Loss: 1.3237\n",
            "Epoch [92/100], Batch [106/346], Loss: 1.1720\n",
            "Epoch [92/100], Batch [107/346], Loss: 1.7184\n",
            "Epoch [92/100], Batch [108/346], Loss: 1.2362\n",
            "Epoch [92/100], Batch [109/346], Loss: 1.3852\n",
            "Epoch [92/100], Batch [110/346], Loss: 1.4927\n",
            "Epoch [92/100], Batch [111/346], Loss: 1.5114\n",
            "Epoch [92/100], Batch [112/346], Loss: 1.7486\n",
            "Epoch [92/100], Batch [113/346], Loss: 1.5167\n",
            "Epoch [92/100], Batch [114/346], Loss: 1.4276\n",
            "Epoch [92/100], Batch [115/346], Loss: 1.4220\n",
            "Epoch [92/100], Batch [116/346], Loss: 1.4852\n",
            "Epoch [92/100], Batch [117/346], Loss: 1.2267\n",
            "Epoch [92/100], Batch [118/346], Loss: 1.4280\n",
            "Epoch [92/100], Batch [119/346], Loss: 1.8277\n",
            "Epoch [92/100], Batch [120/346], Loss: 1.3149\n",
            "Epoch [92/100], Batch [121/346], Loss: 1.6594\n",
            "Epoch [92/100], Batch [122/346], Loss: 1.0226\n",
            "Epoch [92/100], Batch [123/346], Loss: 1.1895\n",
            "Epoch [92/100], Batch [124/346], Loss: 1.2453\n",
            "Epoch [92/100], Batch [125/346], Loss: 1.7873\n",
            "Epoch [92/100], Batch [126/346], Loss: 1.4823\n",
            "Epoch [92/100], Batch [127/346], Loss: 1.3248\n",
            "Epoch [92/100], Batch [128/346], Loss: 1.4443\n",
            "Epoch [92/100], Batch [129/346], Loss: 1.7047\n",
            "Epoch [92/100], Batch [130/346], Loss: 1.4015\n",
            "Epoch [92/100], Batch [131/346], Loss: 1.0761\n",
            "Epoch [92/100], Batch [132/346], Loss: 1.8460\n",
            "Epoch [92/100], Batch [133/346], Loss: 1.7804\n",
            "Epoch [92/100], Batch [134/346], Loss: 1.4578\n",
            "Epoch [92/100], Batch [135/346], Loss: 1.4924\n",
            "Epoch [92/100], Batch [136/346], Loss: 1.5903\n",
            "Epoch [92/100], Batch [137/346], Loss: 1.6451\n",
            "Epoch [92/100], Batch [138/346], Loss: 1.7786\n",
            "Epoch [92/100], Batch [139/346], Loss: 1.5979\n",
            "Epoch [92/100], Batch [140/346], Loss: 1.4567\n",
            "Epoch [92/100], Batch [141/346], Loss: 1.6297\n",
            "Epoch [92/100], Batch [142/346], Loss: 1.3098\n",
            "Epoch [92/100], Batch [143/346], Loss: 1.6210\n",
            "Epoch [92/100], Batch [144/346], Loss: 1.3906\n",
            "Epoch [92/100], Batch [145/346], Loss: 1.4136\n",
            "Epoch [92/100], Batch [146/346], Loss: 2.0678\n",
            "Epoch [92/100], Batch [147/346], Loss: 1.4013\n",
            "Epoch [92/100], Batch [148/346], Loss: 1.6138\n",
            "Epoch [92/100], Batch [149/346], Loss: 1.3481\n",
            "Epoch [92/100], Batch [150/346], Loss: 1.5954\n",
            "Epoch [92/100], Batch [151/346], Loss: 1.6588\n",
            "Epoch [92/100], Batch [152/346], Loss: 1.6324\n",
            "Epoch [92/100], Batch [153/346], Loss: 1.2257\n",
            "Epoch [92/100], Batch [154/346], Loss: 1.6489\n",
            "Epoch [92/100], Batch [155/346], Loss: 1.2786\n",
            "Epoch [92/100], Batch [156/346], Loss: 1.9148\n",
            "Epoch [92/100], Batch [157/346], Loss: 1.6598\n",
            "Epoch [92/100], Batch [158/346], Loss: 1.5404\n",
            "Epoch [92/100], Batch [159/346], Loss: 1.5864\n",
            "Epoch [92/100], Batch [160/346], Loss: 1.5786\n",
            "Epoch [92/100], Batch [161/346], Loss: 1.5042\n",
            "Epoch [92/100], Batch [162/346], Loss: 1.6233\n",
            "Epoch [92/100], Batch [163/346], Loss: 2.1198\n",
            "Epoch [92/100], Batch [164/346], Loss: 1.5970\n",
            "Epoch [92/100], Batch [165/346], Loss: 1.4496\n",
            "Epoch [92/100], Batch [166/346], Loss: 1.4269\n",
            "Epoch [92/100], Batch [167/346], Loss: 1.4593\n",
            "Epoch [92/100], Batch [168/346], Loss: 1.5970\n",
            "Epoch [92/100], Batch [169/346], Loss: 1.8608\n",
            "Epoch [92/100], Batch [170/346], Loss: 2.1951\n",
            "Epoch [92/100], Batch [171/346], Loss: 1.4646\n",
            "Epoch [92/100], Batch [172/346], Loss: 1.5798\n",
            "Epoch [92/100], Batch [173/346], Loss: 1.2944\n",
            "Epoch [92/100], Batch [174/346], Loss: 1.7698\n",
            "Epoch [92/100], Batch [175/346], Loss: 2.0012\n",
            "Epoch [92/100], Batch [176/346], Loss: 1.5428\n",
            "Epoch [92/100], Batch [177/346], Loss: 1.9774\n",
            "Epoch [92/100], Batch [178/346], Loss: 1.3050\n",
            "Epoch [92/100], Batch [179/346], Loss: 1.5992\n",
            "Epoch [92/100], Batch [180/346], Loss: 1.4853\n",
            "Epoch [92/100], Batch [181/346], Loss: 1.0118\n",
            "Epoch [92/100], Batch [182/346], Loss: 1.5751\n",
            "Epoch [92/100], Batch [183/346], Loss: 1.5846\n",
            "Epoch [92/100], Batch [184/346], Loss: 1.6515\n",
            "Epoch [92/100], Batch [185/346], Loss: 1.1494\n",
            "Epoch [92/100], Batch [186/346], Loss: 1.5896\n",
            "Epoch [92/100], Batch [187/346], Loss: 1.7006\n",
            "Epoch [92/100], Batch [188/346], Loss: 1.6156\n",
            "Epoch [92/100], Batch [189/346], Loss: 1.3322\n",
            "Epoch [92/100], Batch [190/346], Loss: 1.4482\n",
            "Epoch [92/100], Batch [191/346], Loss: 1.3016\n",
            "Epoch [92/100], Batch [192/346], Loss: 1.8760\n",
            "Epoch [92/100], Batch [193/346], Loss: 1.5634\n",
            "Epoch [92/100], Batch [194/346], Loss: 1.3481\n",
            "Epoch [92/100], Batch [195/346], Loss: 2.0474\n",
            "Epoch [92/100], Batch [196/346], Loss: 1.9791\n",
            "Epoch [92/100], Batch [197/346], Loss: 1.3831\n",
            "Epoch [92/100], Batch [198/346], Loss: 1.5479\n",
            "Epoch [92/100], Batch [199/346], Loss: 1.4073\n",
            "Epoch [92/100], Batch [200/346], Loss: 1.7897\n",
            "Epoch [92/100], Batch [201/346], Loss: 1.3511\n",
            "Epoch [92/100], Batch [202/346], Loss: 1.9411\n",
            "Epoch [92/100], Batch [203/346], Loss: 1.7652\n",
            "Epoch [92/100], Batch [204/346], Loss: 1.6239\n",
            "Epoch [92/100], Batch [205/346], Loss: 1.7892\n",
            "Epoch [92/100], Batch [206/346], Loss: 1.3226\n",
            "Epoch [92/100], Batch [207/346], Loss: 1.3317\n",
            "Epoch [92/100], Batch [208/346], Loss: 1.9292\n",
            "Epoch [92/100], Batch [209/346], Loss: 1.4471\n",
            "Epoch [92/100], Batch [210/346], Loss: 1.2303\n",
            "Epoch [92/100], Batch [211/346], Loss: 1.5334\n",
            "Epoch [92/100], Batch [212/346], Loss: 1.5737\n",
            "Epoch [92/100], Batch [213/346], Loss: 1.3931\n",
            "Epoch [92/100], Batch [214/346], Loss: 1.6583\n",
            "Epoch [92/100], Batch [215/346], Loss: 1.1673\n",
            "Epoch [92/100], Batch [216/346], Loss: 1.4632\n",
            "Epoch [92/100], Batch [217/346], Loss: 1.3184\n",
            "Epoch [92/100], Batch [218/346], Loss: 1.5679\n",
            "Epoch [92/100], Batch [219/346], Loss: 1.3247\n",
            "Epoch [92/100], Batch [220/346], Loss: 1.5709\n",
            "Epoch [92/100], Batch [221/346], Loss: 1.2990\n",
            "Epoch [92/100], Batch [222/346], Loss: 1.4653\n",
            "Epoch [92/100], Batch [223/346], Loss: 1.6181\n",
            "Epoch [92/100], Batch [224/346], Loss: 1.6605\n",
            "Epoch [92/100], Batch [225/346], Loss: 1.1614\n",
            "Epoch [92/100], Batch [226/346], Loss: 1.4785\n",
            "Epoch [92/100], Batch [227/346], Loss: 1.1961\n",
            "Epoch [92/100], Batch [228/346], Loss: 2.1595\n",
            "Epoch [92/100], Batch [229/346], Loss: 1.9627\n",
            "Epoch [92/100], Batch [230/346], Loss: 1.2994\n",
            "Epoch [92/100], Batch [231/346], Loss: 1.9152\n",
            "Epoch [92/100], Batch [232/346], Loss: 1.8901\n",
            "Epoch [92/100], Batch [233/346], Loss: 1.6346\n",
            "Epoch [92/100], Batch [234/346], Loss: 1.8342\n",
            "Epoch [92/100], Batch [235/346], Loss: 1.4401\n",
            "Epoch [92/100], Batch [236/346], Loss: 1.9459\n",
            "Epoch [92/100], Batch [237/346], Loss: 1.9485\n",
            "Epoch [92/100], Batch [238/346], Loss: 2.7257\n",
            "Epoch [92/100], Batch [239/346], Loss: 1.7509\n",
            "Epoch [92/100], Batch [240/346], Loss: 2.0676\n",
            "Epoch [92/100], Batch [241/346], Loss: 1.7913\n",
            "Epoch [92/100], Batch [242/346], Loss: 2.0440\n",
            "Epoch [92/100], Batch [243/346], Loss: 1.8078\n",
            "Epoch [92/100], Batch [244/346], Loss: 1.7575\n",
            "Epoch [92/100], Batch [245/346], Loss: 1.6660\n",
            "Epoch [92/100], Batch [246/346], Loss: 1.9040\n",
            "Epoch [92/100], Batch [247/346], Loss: 1.4263\n",
            "Epoch [92/100], Batch [248/346], Loss: 1.8224\n",
            "Epoch [92/100], Batch [249/346], Loss: 1.6910\n",
            "Epoch [92/100], Batch [250/346], Loss: 1.7205\n",
            "Epoch [92/100], Batch [251/346], Loss: 1.7477\n",
            "Epoch [92/100], Batch [252/346], Loss: 1.2308\n",
            "Epoch [92/100], Batch [253/346], Loss: 1.6291\n",
            "Epoch [92/100], Batch [254/346], Loss: 1.1941\n",
            "Epoch [92/100], Batch [255/346], Loss: 1.4001\n",
            "Epoch [92/100], Batch [256/346], Loss: 2.0933\n",
            "Epoch [92/100], Batch [257/346], Loss: 1.2982\n",
            "Epoch [92/100], Batch [258/346], Loss: 1.5388\n",
            "Epoch [92/100], Batch [259/346], Loss: 1.3514\n",
            "Epoch [92/100], Batch [260/346], Loss: 1.5514\n",
            "Epoch [92/100], Batch [261/346], Loss: 1.3450\n",
            "Epoch [92/100], Batch [262/346], Loss: 1.4703\n",
            "Epoch [92/100], Batch [263/346], Loss: 1.3269\n",
            "Epoch [92/100], Batch [264/346], Loss: 1.6532\n",
            "Epoch [92/100], Batch [265/346], Loss: 1.2312\n",
            "Epoch [92/100], Batch [266/346], Loss: 1.5230\n",
            "Epoch [92/100], Batch [267/346], Loss: 1.7193\n",
            "Epoch [92/100], Batch [268/346], Loss: 1.5148\n",
            "Epoch [92/100], Batch [269/346], Loss: 2.0258\n",
            "Epoch [92/100], Batch [270/346], Loss: 1.5605\n",
            "Epoch [92/100], Batch [271/346], Loss: 1.7324\n",
            "Epoch [92/100], Batch [272/346], Loss: 1.4283\n",
            "Epoch [92/100], Batch [273/346], Loss: 1.6110\n",
            "Epoch [92/100], Batch [274/346], Loss: 1.4434\n",
            "Epoch [92/100], Batch [275/346], Loss: 1.4605\n",
            "Epoch [92/100], Batch [276/346], Loss: 1.6468\n",
            "Epoch [92/100], Batch [277/346], Loss: 1.5699\n",
            "Epoch [92/100], Batch [278/346], Loss: 1.2826\n",
            "Epoch [92/100], Batch [279/346], Loss: 1.4517\n",
            "Epoch [92/100], Batch [280/346], Loss: 1.0458\n",
            "Epoch [92/100], Batch [281/346], Loss: 2.8037\n",
            "Epoch [92/100], Batch [282/346], Loss: 2.0881\n",
            "Epoch [92/100], Batch [283/346], Loss: 1.1336\n",
            "Epoch [92/100], Batch [284/346], Loss: 1.3393\n",
            "Epoch [92/100], Batch [285/346], Loss: 1.6526\n",
            "Epoch [92/100], Batch [286/346], Loss: 1.6647\n",
            "Epoch [92/100], Batch [287/346], Loss: 1.3626\n",
            "Epoch [92/100], Batch [288/346], Loss: 1.2574\n",
            "Epoch [92/100], Batch [289/346], Loss: 2.0408\n",
            "Epoch [92/100], Batch [290/346], Loss: 1.9453\n",
            "Epoch [92/100], Batch [291/346], Loss: 1.3892\n",
            "Epoch [92/100], Batch [292/346], Loss: 1.5067\n",
            "Epoch [92/100], Batch [293/346], Loss: 1.9866\n",
            "Epoch [92/100], Batch [294/346], Loss: 1.5857\n",
            "Epoch [92/100], Batch [295/346], Loss: 1.4631\n",
            "Epoch [92/100], Batch [296/346], Loss: 1.3650\n",
            "Epoch [92/100], Batch [297/346], Loss: 1.7232\n",
            "Epoch [92/100], Batch [298/346], Loss: 1.8483\n",
            "Epoch [92/100], Batch [299/346], Loss: 1.4835\n",
            "Epoch [92/100], Batch [300/346], Loss: 0.9446\n",
            "Epoch [92/100], Batch [301/346], Loss: 1.2505\n",
            "Epoch [92/100], Batch [302/346], Loss: 1.2674\n",
            "Epoch [92/100], Batch [303/346], Loss: 1.7966\n",
            "Epoch [92/100], Batch [304/346], Loss: 1.7521\n",
            "Epoch [92/100], Batch [305/346], Loss: 1.2601\n",
            "Epoch [92/100], Batch [306/346], Loss: 1.5746\n",
            "Epoch [92/100], Batch [307/346], Loss: 1.5687\n",
            "Epoch [92/100], Batch [308/346], Loss: 1.5380\n",
            "Epoch [92/100], Batch [309/346], Loss: 1.3520\n",
            "Epoch [92/100], Batch [310/346], Loss: 1.7082\n",
            "Epoch [92/100], Batch [311/346], Loss: 1.5443\n",
            "Epoch [92/100], Batch [312/346], Loss: 1.4369\n",
            "Epoch [92/100], Batch [313/346], Loss: 1.4969\n",
            "Epoch [92/100], Batch [314/346], Loss: 1.3567\n",
            "Epoch [92/100], Batch [315/346], Loss: 1.4431\n",
            "Epoch [92/100], Batch [316/346], Loss: 1.2162\n",
            "Epoch [92/100], Batch [317/346], Loss: 1.1883\n",
            "Epoch [92/100], Batch [318/346], Loss: 1.3280\n",
            "Epoch [92/100], Batch [319/346], Loss: 1.3207\n",
            "Epoch [92/100], Batch [320/346], Loss: 1.5873\n",
            "Epoch [92/100], Batch [321/346], Loss: 2.0214\n",
            "Epoch [92/100], Batch [322/346], Loss: 1.5444\n",
            "Epoch [92/100], Batch [323/346], Loss: 1.3414\n",
            "Epoch [92/100], Batch [324/346], Loss: 1.4436\n",
            "Epoch [92/100], Batch [325/346], Loss: 1.5872\n",
            "Epoch [92/100], Batch [326/346], Loss: 1.6515\n",
            "Epoch [92/100], Batch [327/346], Loss: 1.3264\n",
            "Epoch [92/100], Batch [328/346], Loss: 1.5296\n",
            "Epoch [92/100], Batch [329/346], Loss: 1.6592\n",
            "Epoch [92/100], Batch [330/346], Loss: 1.7732\n",
            "Epoch [92/100], Batch [331/346], Loss: 1.7496\n",
            "Epoch [92/100], Batch [332/346], Loss: 1.4235\n",
            "Epoch [92/100], Batch [333/346], Loss: 1.3691\n",
            "Epoch [92/100], Batch [334/346], Loss: 1.5901\n",
            "Epoch [92/100], Batch [335/346], Loss: 1.4824\n",
            "Epoch [92/100], Batch [336/346], Loss: 1.4131\n",
            "Epoch [92/100], Batch [337/346], Loss: 1.2557\n",
            "Epoch [92/100], Batch [338/346], Loss: 1.1895\n",
            "Epoch [92/100], Batch [339/346], Loss: 1.7526\n",
            "Epoch [92/100], Batch [340/346], Loss: 1.3405\n",
            "Epoch [92/100], Batch [341/346], Loss: 1.4276\n",
            "Epoch [92/100], Batch [342/346], Loss: 1.4553\n",
            "Epoch [92/100], Batch [343/346], Loss: 1.3033\n",
            "Epoch [92/100], Batch [344/346], Loss: 1.4927\n",
            "Epoch [92/100], Batch [345/346], Loss: 1.3393\n",
            "Epoch [92/100], Batch [346/346], Loss: 1.7280\n",
            "Epoch [93/100], Batch [1/346], Loss: 1.3823\n",
            "Epoch [93/100], Batch [2/346], Loss: 1.5886\n",
            "Epoch [93/100], Batch [3/346], Loss: 1.5502\n",
            "Epoch [93/100], Batch [4/346], Loss: 1.4847\n",
            "Epoch [93/100], Batch [5/346], Loss: 1.3250\n",
            "Epoch [93/100], Batch [6/346], Loss: 1.4787\n",
            "Epoch [93/100], Batch [7/346], Loss: 1.4915\n",
            "Epoch [93/100], Batch [8/346], Loss: 1.8725\n",
            "Epoch [93/100], Batch [9/346], Loss: 1.7676\n",
            "Epoch [93/100], Batch [10/346], Loss: 1.6048\n",
            "Epoch [93/100], Batch [11/346], Loss: 1.5671\n",
            "Epoch [93/100], Batch [12/346], Loss: 1.5412\n",
            "Epoch [93/100], Batch [13/346], Loss: 1.4859\n",
            "Epoch [93/100], Batch [14/346], Loss: 1.3174\n",
            "Epoch [93/100], Batch [15/346], Loss: 1.4458\n",
            "Epoch [93/100], Batch [16/346], Loss: 1.5618\n",
            "Epoch [93/100], Batch [17/346], Loss: 1.2984\n",
            "Epoch [93/100], Batch [18/346], Loss: 1.4047\n",
            "Epoch [93/100], Batch [19/346], Loss: 1.3689\n",
            "Epoch [93/100], Batch [20/346], Loss: 1.3950\n",
            "Epoch [93/100], Batch [21/346], Loss: 2.0063\n",
            "Epoch [93/100], Batch [22/346], Loss: 1.9953\n",
            "Epoch [93/100], Batch [23/346], Loss: 1.6786\n",
            "Epoch [93/100], Batch [24/346], Loss: 1.2326\n",
            "Epoch [93/100], Batch [25/346], Loss: 1.3380\n",
            "Epoch [93/100], Batch [26/346], Loss: 1.6720\n",
            "Epoch [93/100], Batch [27/346], Loss: 1.3129\n",
            "Epoch [93/100], Batch [28/346], Loss: 1.4458\n",
            "Epoch [93/100], Batch [29/346], Loss: 1.3732\n",
            "Epoch [93/100], Batch [30/346], Loss: 1.3700\n",
            "Epoch [93/100], Batch [31/346], Loss: 1.5478\n",
            "Epoch [93/100], Batch [32/346], Loss: 1.2326\n",
            "Epoch [93/100], Batch [33/346], Loss: 1.4477\n",
            "Epoch [93/100], Batch [34/346], Loss: 1.4290\n",
            "Epoch [93/100], Batch [35/346], Loss: 1.6720\n",
            "Epoch [93/100], Batch [36/346], Loss: 1.4429\n",
            "Epoch [93/100], Batch [37/346], Loss: 1.2720\n",
            "Epoch [93/100], Batch [38/346], Loss: 1.7792\n",
            "Epoch [93/100], Batch [39/346], Loss: 1.5143\n",
            "Epoch [93/100], Batch [40/346], Loss: 1.4111\n",
            "Epoch [93/100], Batch [41/346], Loss: 1.3043\n",
            "Epoch [93/100], Batch [42/346], Loss: 1.3029\n",
            "Epoch [93/100], Batch [43/346], Loss: 1.3193\n",
            "Epoch [93/100], Batch [44/346], Loss: 1.6316\n",
            "Epoch [93/100], Batch [45/346], Loss: 1.3315\n",
            "Epoch [93/100], Batch [46/346], Loss: 1.1654\n",
            "Epoch [93/100], Batch [47/346], Loss: 1.6563\n",
            "Epoch [93/100], Batch [48/346], Loss: 1.7929\n",
            "Epoch [93/100], Batch [49/346], Loss: 1.5779\n",
            "Epoch [93/100], Batch [50/346], Loss: 1.3375\n",
            "Epoch [93/100], Batch [51/346], Loss: 1.9548\n",
            "Epoch [93/100], Batch [52/346], Loss: 1.3396\n",
            "Epoch [93/100], Batch [53/346], Loss: 1.6424\n",
            "Epoch [93/100], Batch [54/346], Loss: 1.1267\n",
            "Epoch [93/100], Batch [55/346], Loss: 1.0715\n",
            "Epoch [93/100], Batch [56/346], Loss: 1.4368\n",
            "Epoch [93/100], Batch [57/346], Loss: 1.5291\n",
            "Epoch [93/100], Batch [58/346], Loss: 1.3839\n",
            "Epoch [93/100], Batch [59/346], Loss: 1.4894\n",
            "Epoch [93/100], Batch [60/346], Loss: 1.5403\n",
            "Epoch [93/100], Batch [61/346], Loss: 1.6967\n",
            "Epoch [93/100], Batch [62/346], Loss: 1.2097\n",
            "Epoch [93/100], Batch [63/346], Loss: 1.6045\n",
            "Epoch [93/100], Batch [64/346], Loss: 1.1833\n",
            "Epoch [93/100], Batch [65/346], Loss: 1.4254\n",
            "Epoch [93/100], Batch [66/346], Loss: 1.9766\n",
            "Epoch [93/100], Batch [67/346], Loss: 1.6144\n",
            "Epoch [93/100], Batch [68/346], Loss: 1.6090\n",
            "Epoch [93/100], Batch [69/346], Loss: 1.8963\n",
            "Epoch [93/100], Batch [70/346], Loss: 1.9084\n",
            "Epoch [93/100], Batch [71/346], Loss: 1.5444\n",
            "Epoch [93/100], Batch [72/346], Loss: 1.6151\n",
            "Epoch [93/100], Batch [73/346], Loss: 1.5155\n",
            "Epoch [93/100], Batch [74/346], Loss: 1.3910\n",
            "Epoch [93/100], Batch [75/346], Loss: 1.5846\n",
            "Epoch [93/100], Batch [76/346], Loss: 1.5636\n",
            "Epoch [93/100], Batch [77/346], Loss: 1.5094\n",
            "Epoch [93/100], Batch [78/346], Loss: 1.3844\n",
            "Epoch [93/100], Batch [79/346], Loss: 1.6873\n",
            "Epoch [93/100], Batch [80/346], Loss: 1.1935\n",
            "Epoch [93/100], Batch [81/346], Loss: 2.1519\n",
            "Epoch [93/100], Batch [82/346], Loss: 1.5050\n",
            "Epoch [93/100], Batch [83/346], Loss: 1.5653\n",
            "Epoch [93/100], Batch [84/346], Loss: 1.9337\n",
            "Epoch [93/100], Batch [85/346], Loss: 1.3547\n",
            "Epoch [93/100], Batch [86/346], Loss: 1.4679\n",
            "Epoch [93/100], Batch [87/346], Loss: 1.4526\n",
            "Epoch [93/100], Batch [88/346], Loss: 1.2054\n",
            "Epoch [93/100], Batch [89/346], Loss: 1.8200\n",
            "Epoch [93/100], Batch [90/346], Loss: 1.5518\n",
            "Epoch [93/100], Batch [91/346], Loss: 1.8512\n",
            "Epoch [93/100], Batch [92/346], Loss: 1.1773\n",
            "Epoch [93/100], Batch [93/346], Loss: 1.6528\n",
            "Epoch [93/100], Batch [94/346], Loss: 1.4932\n",
            "Epoch [93/100], Batch [95/346], Loss: 1.4643\n",
            "Epoch [93/100], Batch [96/346], Loss: 1.7455\n",
            "Epoch [93/100], Batch [97/346], Loss: 1.5984\n",
            "Epoch [93/100], Batch [98/346], Loss: 1.7854\n",
            "Epoch [93/100], Batch [99/346], Loss: 1.2052\n",
            "Epoch [93/100], Batch [100/346], Loss: 1.2846\n",
            "Epoch [93/100], Batch [101/346], Loss: 1.3893\n",
            "Epoch [93/100], Batch [102/346], Loss: 1.7020\n",
            "Epoch [93/100], Batch [103/346], Loss: 1.5290\n",
            "Epoch [93/100], Batch [104/346], Loss: 1.5061\n",
            "Epoch [93/100], Batch [105/346], Loss: 1.4530\n",
            "Epoch [93/100], Batch [106/346], Loss: 1.4548\n",
            "Epoch [93/100], Batch [107/346], Loss: 1.4061\n",
            "Epoch [93/100], Batch [108/346], Loss: 1.3393\n",
            "Epoch [93/100], Batch [109/346], Loss: 1.2298\n",
            "Epoch [93/100], Batch [110/346], Loss: 1.3404\n",
            "Epoch [93/100], Batch [111/346], Loss: 1.8136\n",
            "Epoch [93/100], Batch [112/346], Loss: 1.3763\n",
            "Epoch [93/100], Batch [113/346], Loss: 1.3492\n",
            "Epoch [93/100], Batch [114/346], Loss: 1.4484\n",
            "Epoch [93/100], Batch [115/346], Loss: 1.3897\n",
            "Epoch [93/100], Batch [116/346], Loss: 1.6095\n",
            "Epoch [93/100], Batch [117/346], Loss: 1.7068\n",
            "Epoch [93/100], Batch [118/346], Loss: 1.2906\n",
            "Epoch [93/100], Batch [119/346], Loss: 1.6252\n",
            "Epoch [93/100], Batch [120/346], Loss: 1.4129\n",
            "Epoch [93/100], Batch [121/346], Loss: 1.3847\n",
            "Epoch [93/100], Batch [122/346], Loss: 1.4798\n",
            "Epoch [93/100], Batch [123/346], Loss: 1.3995\n",
            "Epoch [93/100], Batch [124/346], Loss: 1.4798\n",
            "Epoch [93/100], Batch [125/346], Loss: 1.6885\n",
            "Epoch [93/100], Batch [126/346], Loss: 1.1439\n",
            "Epoch [93/100], Batch [127/346], Loss: 1.7774\n",
            "Epoch [93/100], Batch [128/346], Loss: 1.6835\n",
            "Epoch [93/100], Batch [129/346], Loss: 1.8225\n",
            "Epoch [93/100], Batch [130/346], Loss: 1.4484\n",
            "Epoch [93/100], Batch [131/346], Loss: 1.6530\n",
            "Epoch [93/100], Batch [132/346], Loss: 1.5138\n",
            "Epoch [93/100], Batch [133/346], Loss: 1.4352\n",
            "Epoch [93/100], Batch [134/346], Loss: 1.8368\n",
            "Epoch [93/100], Batch [135/346], Loss: 1.4309\n",
            "Epoch [93/100], Batch [136/346], Loss: 1.7969\n",
            "Epoch [93/100], Batch [137/346], Loss: 1.2782\n",
            "Epoch [93/100], Batch [138/346], Loss: 1.3357\n",
            "Epoch [93/100], Batch [139/346], Loss: 1.2529\n",
            "Epoch [93/100], Batch [140/346], Loss: 1.8125\n",
            "Epoch [93/100], Batch [141/346], Loss: 1.3180\n",
            "Epoch [93/100], Batch [142/346], Loss: 1.1505\n",
            "Epoch [93/100], Batch [143/346], Loss: 1.5944\n",
            "Epoch [93/100], Batch [144/346], Loss: 1.3784\n",
            "Epoch [93/100], Batch [145/346], Loss: 1.7383\n",
            "Epoch [93/100], Batch [146/346], Loss: 1.4930\n",
            "Epoch [93/100], Batch [147/346], Loss: 1.3645\n",
            "Epoch [93/100], Batch [148/346], Loss: 1.3027\n",
            "Epoch [93/100], Batch [149/346], Loss: 1.6622\n",
            "Epoch [93/100], Batch [150/346], Loss: 1.8335\n",
            "Epoch [93/100], Batch [151/346], Loss: 1.3085\n",
            "Epoch [93/100], Batch [152/346], Loss: 1.8451\n",
            "Epoch [93/100], Batch [153/346], Loss: 1.6170\n",
            "Epoch [93/100], Batch [154/346], Loss: 1.6318\n",
            "Epoch [93/100], Batch [155/346], Loss: 1.8429\n",
            "Epoch [93/100], Batch [156/346], Loss: 1.9010\n",
            "Epoch [93/100], Batch [157/346], Loss: 1.4726\n",
            "Epoch [93/100], Batch [158/346], Loss: 1.8904\n",
            "Epoch [93/100], Batch [159/346], Loss: 1.4841\n",
            "Epoch [93/100], Batch [160/346], Loss: 1.3645\n",
            "Epoch [93/100], Batch [161/346], Loss: 1.4815\n",
            "Epoch [93/100], Batch [162/346], Loss: 1.5637\n",
            "Epoch [93/100], Batch [163/346], Loss: 1.6038\n",
            "Epoch [93/100], Batch [164/346], Loss: 1.3495\n",
            "Epoch [93/100], Batch [165/346], Loss: 1.4810\n",
            "Epoch [93/100], Batch [166/346], Loss: 1.6126\n",
            "Epoch [93/100], Batch [167/346], Loss: 1.3282\n",
            "Epoch [93/100], Batch [168/346], Loss: 1.5731\n",
            "Epoch [93/100], Batch [169/346], Loss: 1.4207\n",
            "Epoch [93/100], Batch [170/346], Loss: 1.5121\n",
            "Epoch [93/100], Batch [171/346], Loss: 1.7674\n",
            "Epoch [93/100], Batch [172/346], Loss: 1.5727\n",
            "Epoch [93/100], Batch [173/346], Loss: 1.8543\n",
            "Epoch [93/100], Batch [174/346], Loss: 1.4060\n",
            "Epoch [93/100], Batch [175/346], Loss: 1.6123\n",
            "Epoch [93/100], Batch [176/346], Loss: 1.6385\n",
            "Epoch [93/100], Batch [177/346], Loss: 1.5500\n",
            "Epoch [93/100], Batch [178/346], Loss: 1.4669\n",
            "Epoch [93/100], Batch [179/346], Loss: 1.5773\n",
            "Epoch [93/100], Batch [180/346], Loss: 1.4669\n",
            "Epoch [93/100], Batch [181/346], Loss: 2.0500\n",
            "Epoch [93/100], Batch [182/346], Loss: 1.4990\n",
            "Epoch [93/100], Batch [183/346], Loss: 1.4651\n",
            "Epoch [93/100], Batch [184/346], Loss: 1.9371\n",
            "Epoch [93/100], Batch [185/346], Loss: 1.7381\n",
            "Epoch [93/100], Batch [186/346], Loss: 1.4982\n",
            "Epoch [93/100], Batch [187/346], Loss: 1.5489\n",
            "Epoch [93/100], Batch [188/346], Loss: 1.3177\n",
            "Epoch [93/100], Batch [189/346], Loss: 1.5639\n",
            "Epoch [93/100], Batch [190/346], Loss: 1.7349\n",
            "Epoch [93/100], Batch [191/346], Loss: 1.6362\n",
            "Epoch [93/100], Batch [192/346], Loss: 1.5749\n",
            "Epoch [93/100], Batch [193/346], Loss: 1.6321\n",
            "Epoch [93/100], Batch [194/346], Loss: 1.5253\n",
            "Epoch [93/100], Batch [195/346], Loss: 1.9562\n",
            "Epoch [93/100], Batch [196/346], Loss: 1.6019\n",
            "Epoch [93/100], Batch [197/346], Loss: 1.3496\n",
            "Epoch [93/100], Batch [198/346], Loss: 2.1816\n",
            "Epoch [93/100], Batch [199/346], Loss: 1.4983\n",
            "Epoch [93/100], Batch [200/346], Loss: 1.7404\n",
            "Epoch [93/100], Batch [201/346], Loss: 1.3877\n",
            "Epoch [93/100], Batch [202/346], Loss: 1.7174\n",
            "Epoch [93/100], Batch [203/346], Loss: 1.7313\n",
            "Epoch [93/100], Batch [204/346], Loss: 1.3793\n",
            "Epoch [93/100], Batch [205/346], Loss: 1.4342\n",
            "Epoch [93/100], Batch [206/346], Loss: 1.3492\n",
            "Epoch [93/100], Batch [207/346], Loss: 1.2308\n",
            "Epoch [93/100], Batch [208/346], Loss: 1.5743\n",
            "Epoch [93/100], Batch [209/346], Loss: 1.4606\n",
            "Epoch [93/100], Batch [210/346], Loss: 1.7171\n",
            "Epoch [93/100], Batch [211/346], Loss: 1.1832\n",
            "Epoch [93/100], Batch [212/346], Loss: 1.0178\n",
            "Epoch [93/100], Batch [213/346], Loss: 1.4920\n",
            "Epoch [93/100], Batch [214/346], Loss: 1.8758\n",
            "Epoch [93/100], Batch [215/346], Loss: 1.5126\n",
            "Epoch [93/100], Batch [216/346], Loss: 1.4561\n",
            "Epoch [93/100], Batch [217/346], Loss: 1.5899\n",
            "Epoch [93/100], Batch [218/346], Loss: 1.5752\n",
            "Epoch [93/100], Batch [219/346], Loss: 1.6007\n",
            "Epoch [93/100], Batch [220/346], Loss: 1.3920\n",
            "Epoch [93/100], Batch [221/346], Loss: 1.4835\n",
            "Epoch [93/100], Batch [222/346], Loss: 1.7288\n",
            "Epoch [93/100], Batch [223/346], Loss: 1.5058\n",
            "Epoch [93/100], Batch [224/346], Loss: 1.4992\n",
            "Epoch [93/100], Batch [225/346], Loss: 1.5670\n",
            "Epoch [93/100], Batch [226/346], Loss: 1.5004\n",
            "Epoch [93/100], Batch [227/346], Loss: 1.3154\n",
            "Epoch [93/100], Batch [228/346], Loss: 1.6166\n",
            "Epoch [93/100], Batch [229/346], Loss: 1.5899\n",
            "Epoch [93/100], Batch [230/346], Loss: 1.6311\n",
            "Epoch [93/100], Batch [231/346], Loss: 1.3902\n",
            "Epoch [93/100], Batch [232/346], Loss: 1.5889\n",
            "Epoch [93/100], Batch [233/346], Loss: 1.4354\n",
            "Epoch [93/100], Batch [234/346], Loss: 1.2646\n",
            "Epoch [93/100], Batch [235/346], Loss: 1.6245\n",
            "Epoch [93/100], Batch [236/346], Loss: 1.3429\n",
            "Epoch [93/100], Batch [237/346], Loss: 1.0209\n",
            "Epoch [93/100], Batch [238/346], Loss: 1.2578\n",
            "Epoch [93/100], Batch [239/346], Loss: 1.4786\n",
            "Epoch [93/100], Batch [240/346], Loss: 1.8455\n",
            "Epoch [93/100], Batch [241/346], Loss: 1.5412\n",
            "Epoch [93/100], Batch [242/346], Loss: 1.3522\n",
            "Epoch [93/100], Batch [243/346], Loss: 1.5241\n",
            "Epoch [93/100], Batch [244/346], Loss: 1.6589\n",
            "Epoch [93/100], Batch [245/346], Loss: 1.4330\n",
            "Epoch [93/100], Batch [246/346], Loss: 1.3965\n",
            "Epoch [93/100], Batch [247/346], Loss: 1.6007\n",
            "Epoch [93/100], Batch [248/346], Loss: 1.4988\n",
            "Epoch [93/100], Batch [249/346], Loss: 1.7271\n",
            "Epoch [93/100], Batch [250/346], Loss: 1.6390\n",
            "Epoch [93/100], Batch [251/346], Loss: 1.9159\n",
            "Epoch [93/100], Batch [252/346], Loss: 1.5275\n",
            "Epoch [93/100], Batch [253/346], Loss: 1.3954\n",
            "Epoch [93/100], Batch [254/346], Loss: 1.7103\n",
            "Epoch [93/100], Batch [255/346], Loss: 1.7023\n",
            "Epoch [93/100], Batch [256/346], Loss: 1.7169\n",
            "Epoch [93/100], Batch [257/346], Loss: 1.4202\n",
            "Epoch [93/100], Batch [258/346], Loss: 1.3882\n",
            "Epoch [93/100], Batch [259/346], Loss: 1.4209\n",
            "Epoch [93/100], Batch [260/346], Loss: 1.4849\n",
            "Epoch [93/100], Batch [261/346], Loss: 1.5995\n",
            "Epoch [93/100], Batch [262/346], Loss: 1.3846\n",
            "Epoch [93/100], Batch [263/346], Loss: 1.7431\n",
            "Epoch [93/100], Batch [264/346], Loss: 1.2811\n",
            "Epoch [93/100], Batch [265/346], Loss: 1.2700\n",
            "Epoch [93/100], Batch [266/346], Loss: 1.4895\n",
            "Epoch [93/100], Batch [267/346], Loss: 2.0043\n",
            "Epoch [93/100], Batch [268/346], Loss: 1.4688\n",
            "Epoch [93/100], Batch [269/346], Loss: 1.5475\n",
            "Epoch [93/100], Batch [270/346], Loss: 1.5176\n",
            "Epoch [93/100], Batch [271/346], Loss: 1.4914\n",
            "Epoch [93/100], Batch [272/346], Loss: 1.3919\n",
            "Epoch [93/100], Batch [273/346], Loss: 1.3597\n",
            "Epoch [93/100], Batch [274/346], Loss: 1.6255\n",
            "Epoch [93/100], Batch [275/346], Loss: 1.4082\n",
            "Epoch [93/100], Batch [276/346], Loss: 1.8474\n",
            "Epoch [93/100], Batch [277/346], Loss: 1.5844\n",
            "Epoch [93/100], Batch [278/346], Loss: 1.6934\n",
            "Epoch [93/100], Batch [279/346], Loss: 1.3270\n",
            "Epoch [93/100], Batch [280/346], Loss: 1.3747\n",
            "Epoch [93/100], Batch [281/346], Loss: 1.7979\n",
            "Epoch [93/100], Batch [282/346], Loss: 1.6773\n",
            "Epoch [93/100], Batch [283/346], Loss: 1.4154\n",
            "Epoch [93/100], Batch [284/346], Loss: 1.3051\n",
            "Epoch [93/100], Batch [285/346], Loss: 1.6300\n",
            "Epoch [93/100], Batch [286/346], Loss: 1.6007\n",
            "Epoch [93/100], Batch [287/346], Loss: 1.3566\n",
            "Epoch [93/100], Batch [288/346], Loss: 1.6554\n",
            "Epoch [93/100], Batch [289/346], Loss: 1.2638\n",
            "Epoch [93/100], Batch [290/346], Loss: 1.3251\n",
            "Epoch [93/100], Batch [291/346], Loss: 1.1071\n",
            "Epoch [93/100], Batch [292/346], Loss: 1.8427\n",
            "Epoch [93/100], Batch [293/346], Loss: 1.1988\n",
            "Epoch [93/100], Batch [294/346], Loss: 1.2061\n",
            "Epoch [93/100], Batch [295/346], Loss: 1.3799\n",
            "Epoch [93/100], Batch [296/346], Loss: 1.6333\n",
            "Epoch [93/100], Batch [297/346], Loss: 1.4870\n",
            "Epoch [93/100], Batch [298/346], Loss: 2.0407\n",
            "Epoch [93/100], Batch [299/346], Loss: 1.4757\n",
            "Epoch [93/100], Batch [300/346], Loss: 1.6263\n",
            "Epoch [93/100], Batch [301/346], Loss: 1.3159\n",
            "Epoch [93/100], Batch [302/346], Loss: 1.2260\n",
            "Epoch [93/100], Batch [303/346], Loss: 1.5617\n",
            "Epoch [93/100], Batch [304/346], Loss: 1.5342\n",
            "Epoch [93/100], Batch [305/346], Loss: 1.7301\n",
            "Epoch [93/100], Batch [306/346], Loss: 1.3232\n",
            "Epoch [93/100], Batch [307/346], Loss: 1.4410\n",
            "Epoch [93/100], Batch [308/346], Loss: 1.7963\n",
            "Epoch [93/100], Batch [309/346], Loss: 1.4349\n",
            "Epoch [93/100], Batch [310/346], Loss: 1.7892\n",
            "Epoch [93/100], Batch [311/346], Loss: 1.4272\n",
            "Epoch [93/100], Batch [312/346], Loss: 1.3921\n",
            "Epoch [93/100], Batch [313/346], Loss: 1.5150\n",
            "Epoch [93/100], Batch [314/346], Loss: 1.8913\n",
            "Epoch [93/100], Batch [315/346], Loss: 1.3288\n",
            "Epoch [93/100], Batch [316/346], Loss: 1.1142\n",
            "Epoch [93/100], Batch [317/346], Loss: 1.4202\n",
            "Epoch [93/100], Batch [318/346], Loss: 1.7640\n",
            "Epoch [93/100], Batch [319/346], Loss: 1.3581\n",
            "Epoch [93/100], Batch [320/346], Loss: 1.5581\n",
            "Epoch [93/100], Batch [321/346], Loss: 1.3607\n",
            "Epoch [93/100], Batch [322/346], Loss: 1.5805\n",
            "Epoch [93/100], Batch [323/346], Loss: 1.6479\n",
            "Epoch [93/100], Batch [324/346], Loss: 1.3742\n",
            "Epoch [93/100], Batch [325/346], Loss: 1.7159\n",
            "Epoch [93/100], Batch [326/346], Loss: 1.6545\n",
            "Epoch [93/100], Batch [327/346], Loss: 1.3737\n",
            "Epoch [93/100], Batch [328/346], Loss: 1.1325\n",
            "Epoch [93/100], Batch [329/346], Loss: 2.0407\n",
            "Epoch [93/100], Batch [330/346], Loss: 1.5354\n",
            "Epoch [93/100], Batch [331/346], Loss: 1.7937\n",
            "Epoch [93/100], Batch [332/346], Loss: 1.6721\n",
            "Epoch [93/100], Batch [333/346], Loss: 1.6033\n",
            "Epoch [93/100], Batch [334/346], Loss: 1.6266\n",
            "Epoch [93/100], Batch [335/346], Loss: 1.5479\n",
            "Epoch [93/100], Batch [336/346], Loss: 1.6658\n",
            "Epoch [93/100], Batch [337/346], Loss: 1.9435\n",
            "Epoch [93/100], Batch [338/346], Loss: 1.5430\n",
            "Epoch [93/100], Batch [339/346], Loss: 1.2436\n",
            "Epoch [93/100], Batch [340/346], Loss: 1.7621\n",
            "Epoch [93/100], Batch [341/346], Loss: 1.4666\n",
            "Epoch [93/100], Batch [342/346], Loss: 1.8025\n",
            "Epoch [93/100], Batch [343/346], Loss: 1.4311\n",
            "Epoch [93/100], Batch [344/346], Loss: 1.5803\n",
            "Epoch [93/100], Batch [345/346], Loss: 1.6879\n",
            "Epoch [93/100], Batch [346/346], Loss: 1.8105\n",
            "Epoch [94/100], Batch [1/346], Loss: 1.7747\n",
            "Epoch [94/100], Batch [2/346], Loss: 1.5327\n",
            "Epoch [94/100], Batch [3/346], Loss: 1.2826\n",
            "Epoch [94/100], Batch [4/346], Loss: 2.2264\n",
            "Epoch [94/100], Batch [5/346], Loss: 1.4953\n",
            "Epoch [94/100], Batch [6/346], Loss: 1.4736\n",
            "Epoch [94/100], Batch [7/346], Loss: 1.7083\n",
            "Epoch [94/100], Batch [8/346], Loss: 1.7216\n",
            "Epoch [94/100], Batch [9/346], Loss: 1.5002\n",
            "Epoch [94/100], Batch [10/346], Loss: 1.6710\n",
            "Epoch [94/100], Batch [11/346], Loss: 1.5738\n",
            "Epoch [94/100], Batch [12/346], Loss: 1.5431\n",
            "Epoch [94/100], Batch [13/346], Loss: 1.3206\n",
            "Epoch [94/100], Batch [14/346], Loss: 1.6520\n",
            "Epoch [94/100], Batch [15/346], Loss: 1.8198\n",
            "Epoch [94/100], Batch [16/346], Loss: 1.3376\n",
            "Epoch [94/100], Batch [17/346], Loss: 1.5858\n",
            "Epoch [94/100], Batch [18/346], Loss: 1.4885\n",
            "Epoch [94/100], Batch [19/346], Loss: 1.2476\n",
            "Epoch [94/100], Batch [20/346], Loss: 1.5812\n",
            "Epoch [94/100], Batch [21/346], Loss: 1.1851\n",
            "Epoch [94/100], Batch [22/346], Loss: 1.6906\n",
            "Epoch [94/100], Batch [23/346], Loss: 1.4943\n",
            "Epoch [94/100], Batch [24/346], Loss: 1.7249\n",
            "Epoch [94/100], Batch [25/346], Loss: 1.4918\n",
            "Epoch [94/100], Batch [26/346], Loss: 1.4122\n",
            "Epoch [94/100], Batch [27/346], Loss: 1.7886\n",
            "Epoch [94/100], Batch [28/346], Loss: 1.8594\n",
            "Epoch [94/100], Batch [29/346], Loss: 1.3320\n",
            "Epoch [94/100], Batch [30/346], Loss: 1.0907\n",
            "Epoch [94/100], Batch [31/346], Loss: 1.3989\n",
            "Epoch [94/100], Batch [32/346], Loss: 1.4961\n",
            "Epoch [94/100], Batch [33/346], Loss: 1.3866\n",
            "Epoch [94/100], Batch [34/346], Loss: 1.8659\n",
            "Epoch [94/100], Batch [35/346], Loss: 1.5600\n",
            "Epoch [94/100], Batch [36/346], Loss: 1.6936\n",
            "Epoch [94/100], Batch [37/346], Loss: 1.6235\n",
            "Epoch [94/100], Batch [38/346], Loss: 1.4609\n",
            "Epoch [94/100], Batch [39/346], Loss: 1.4177\n",
            "Epoch [94/100], Batch [40/346], Loss: 1.0955\n",
            "Epoch [94/100], Batch [41/346], Loss: 1.3738\n",
            "Epoch [94/100], Batch [42/346], Loss: 1.9301\n",
            "Epoch [94/100], Batch [43/346], Loss: 1.1769\n",
            "Epoch [94/100], Batch [44/346], Loss: 2.0154\n",
            "Epoch [94/100], Batch [45/346], Loss: 2.1175\n",
            "Epoch [94/100], Batch [46/346], Loss: 1.4361\n",
            "Epoch [94/100], Batch [47/346], Loss: 1.4583\n",
            "Epoch [94/100], Batch [48/346], Loss: 1.6481\n",
            "Epoch [94/100], Batch [49/346], Loss: 1.5819\n",
            "Epoch [94/100], Batch [50/346], Loss: 1.9183\n",
            "Epoch [94/100], Batch [51/346], Loss: 1.4624\n",
            "Epoch [94/100], Batch [52/346], Loss: 1.4830\n",
            "Epoch [94/100], Batch [53/346], Loss: 1.5969\n",
            "Epoch [94/100], Batch [54/346], Loss: 1.7282\n",
            "Epoch [94/100], Batch [55/346], Loss: 1.3107\n",
            "Epoch [94/100], Batch [56/346], Loss: 1.1077\n",
            "Epoch [94/100], Batch [57/346], Loss: 1.7262\n",
            "Epoch [94/100], Batch [58/346], Loss: 1.3230\n",
            "Epoch [94/100], Batch [59/346], Loss: 1.4053\n",
            "Epoch [94/100], Batch [60/346], Loss: 1.6042\n",
            "Epoch [94/100], Batch [61/346], Loss: 1.7069\n",
            "Epoch [94/100], Batch [62/346], Loss: 1.1694\n",
            "Epoch [94/100], Batch [63/346], Loss: 1.1973\n",
            "Epoch [94/100], Batch [64/346], Loss: 1.3406\n",
            "Epoch [94/100], Batch [65/346], Loss: 1.2393\n",
            "Epoch [94/100], Batch [66/346], Loss: 1.6921\n",
            "Epoch [94/100], Batch [67/346], Loss: 1.4258\n",
            "Epoch [94/100], Batch [68/346], Loss: 1.6458\n",
            "Epoch [94/100], Batch [69/346], Loss: 1.5946\n",
            "Epoch [94/100], Batch [70/346], Loss: 1.5119\n",
            "Epoch [94/100], Batch [71/346], Loss: 1.6694\n",
            "Epoch [94/100], Batch [72/346], Loss: 1.2735\n",
            "Epoch [94/100], Batch [73/346], Loss: 1.4014\n",
            "Epoch [94/100], Batch [74/346], Loss: 1.7565\n",
            "Epoch [94/100], Batch [75/346], Loss: 1.6134\n",
            "Epoch [94/100], Batch [76/346], Loss: 1.4246\n",
            "Epoch [94/100], Batch [77/346], Loss: 1.6473\n",
            "Epoch [94/100], Batch [78/346], Loss: 1.4713\n",
            "Epoch [94/100], Batch [79/346], Loss: 1.2940\n",
            "Epoch [94/100], Batch [80/346], Loss: 1.4464\n",
            "Epoch [94/100], Batch [81/346], Loss: 1.2062\n",
            "Epoch [94/100], Batch [82/346], Loss: 1.8192\n",
            "Epoch [94/100], Batch [83/346], Loss: 1.4298\n",
            "Epoch [94/100], Batch [84/346], Loss: 1.6777\n",
            "Epoch [94/100], Batch [85/346], Loss: 1.6771\n",
            "Epoch [94/100], Batch [86/346], Loss: 1.5704\n",
            "Epoch [94/100], Batch [87/346], Loss: 1.9816\n",
            "Epoch [94/100], Batch [88/346], Loss: 1.2913\n",
            "Epoch [94/100], Batch [89/346], Loss: 1.3020\n",
            "Epoch [94/100], Batch [90/346], Loss: 1.3306\n",
            "Epoch [94/100], Batch [91/346], Loss: 1.5574\n",
            "Epoch [94/100], Batch [92/346], Loss: 1.4090\n",
            "Epoch [94/100], Batch [93/346], Loss: 1.5892\n",
            "Epoch [94/100], Batch [94/346], Loss: 1.5275\n",
            "Epoch [94/100], Batch [95/346], Loss: 1.4757\n",
            "Epoch [94/100], Batch [96/346], Loss: 1.5260\n",
            "Epoch [94/100], Batch [97/346], Loss: 1.6669\n",
            "Epoch [94/100], Batch [98/346], Loss: 1.5720\n",
            "Epoch [94/100], Batch [99/346], Loss: 1.2756\n",
            "Epoch [94/100], Batch [100/346], Loss: 1.5279\n",
            "Epoch [94/100], Batch [101/346], Loss: 1.6841\n",
            "Epoch [94/100], Batch [102/346], Loss: 1.5481\n",
            "Epoch [94/100], Batch [103/346], Loss: 1.3919\n",
            "Epoch [94/100], Batch [104/346], Loss: 1.4141\n",
            "Epoch [94/100], Batch [105/346], Loss: 1.5129\n",
            "Epoch [94/100], Batch [106/346], Loss: 1.4593\n",
            "Epoch [94/100], Batch [107/346], Loss: 1.4780\n",
            "Epoch [94/100], Batch [108/346], Loss: 1.2824\n",
            "Epoch [94/100], Batch [109/346], Loss: 1.0140\n",
            "Epoch [94/100], Batch [110/346], Loss: 1.5129\n",
            "Epoch [94/100], Batch [111/346], Loss: 1.6142\n",
            "Epoch [94/100], Batch [112/346], Loss: 1.4646\n",
            "Epoch [94/100], Batch [113/346], Loss: 1.8815\n",
            "Epoch [94/100], Batch [114/346], Loss: 1.5013\n",
            "Epoch [94/100], Batch [115/346], Loss: 1.4907\n",
            "Epoch [94/100], Batch [116/346], Loss: 1.5867\n",
            "Epoch [94/100], Batch [117/346], Loss: 1.3204\n",
            "Epoch [94/100], Batch [118/346], Loss: 1.2867\n",
            "Epoch [94/100], Batch [119/346], Loss: 1.7755\n",
            "Epoch [94/100], Batch [120/346], Loss: 1.7928\n",
            "Epoch [94/100], Batch [121/346], Loss: 1.5635\n",
            "Epoch [94/100], Batch [122/346], Loss: 1.5315\n",
            "Epoch [94/100], Batch [123/346], Loss: 1.5087\n",
            "Epoch [94/100], Batch [124/346], Loss: 1.3119\n",
            "Epoch [94/100], Batch [125/346], Loss: 1.3662\n",
            "Epoch [94/100], Batch [126/346], Loss: 1.3952\n",
            "Epoch [94/100], Batch [127/346], Loss: 1.5625\n",
            "Epoch [94/100], Batch [128/346], Loss: 1.1885\n",
            "Epoch [94/100], Batch [129/346], Loss: 1.2291\n",
            "Epoch [94/100], Batch [130/346], Loss: 1.8791\n",
            "Epoch [94/100], Batch [131/346], Loss: 1.4081\n",
            "Epoch [94/100], Batch [132/346], Loss: 1.7742\n",
            "Epoch [94/100], Batch [133/346], Loss: 1.6885\n",
            "Epoch [94/100], Batch [134/346], Loss: 1.5032\n",
            "Epoch [94/100], Batch [135/346], Loss: 1.1150\n",
            "Epoch [94/100], Batch [136/346], Loss: 1.3364\n",
            "Epoch [94/100], Batch [137/346], Loss: 1.6643\n",
            "Epoch [94/100], Batch [138/346], Loss: 2.0195\n",
            "Epoch [94/100], Batch [139/346], Loss: 1.5727\n",
            "Epoch [94/100], Batch [140/346], Loss: 1.3808\n",
            "Epoch [94/100], Batch [141/346], Loss: 1.6110\n",
            "Epoch [94/100], Batch [142/346], Loss: 2.2474\n",
            "Epoch [94/100], Batch [143/346], Loss: 1.9355\n",
            "Epoch [94/100], Batch [144/346], Loss: 1.6926\n",
            "Epoch [94/100], Batch [145/346], Loss: 1.3757\n",
            "Epoch [94/100], Batch [146/346], Loss: 1.8221\n",
            "Epoch [94/100], Batch [147/346], Loss: 1.8846\n",
            "Epoch [94/100], Batch [148/346], Loss: 2.0903\n",
            "Epoch [94/100], Batch [149/346], Loss: 1.6917\n",
            "Epoch [94/100], Batch [150/346], Loss: 1.7726\n",
            "Epoch [94/100], Batch [151/346], Loss: 1.9002\n",
            "Epoch [94/100], Batch [152/346], Loss: 1.5582\n",
            "Epoch [94/100], Batch [153/346], Loss: 1.8176\n",
            "Epoch [94/100], Batch [154/346], Loss: 1.2552\n",
            "Epoch [94/100], Batch [155/346], Loss: 1.5511\n",
            "Epoch [94/100], Batch [156/346], Loss: 1.7562\n",
            "Epoch [94/100], Batch [157/346], Loss: 2.0334\n",
            "Epoch [94/100], Batch [158/346], Loss: 1.6725\n",
            "Epoch [94/100], Batch [159/346], Loss: 1.5689\n",
            "Epoch [94/100], Batch [160/346], Loss: 2.0043\n",
            "Epoch [94/100], Batch [161/346], Loss: 1.7871\n",
            "Epoch [94/100], Batch [162/346], Loss: 1.9521\n",
            "Epoch [94/100], Batch [163/346], Loss: 1.3319\n",
            "Epoch [94/100], Batch [164/346], Loss: 2.0720\n",
            "Epoch [94/100], Batch [165/346], Loss: 1.5436\n",
            "Epoch [94/100], Batch [166/346], Loss: 1.3059\n",
            "Epoch [94/100], Batch [167/346], Loss: 2.1055\n",
            "Epoch [94/100], Batch [168/346], Loss: 1.4706\n",
            "Epoch [94/100], Batch [169/346], Loss: 1.8048\n",
            "Epoch [94/100], Batch [170/346], Loss: 2.1738\n",
            "Epoch [94/100], Batch [171/346], Loss: 1.5802\n",
            "Epoch [94/100], Batch [172/346], Loss: 1.3548\n",
            "Epoch [94/100], Batch [173/346], Loss: 1.7394\n",
            "Epoch [94/100], Batch [174/346], Loss: 1.5606\n",
            "Epoch [94/100], Batch [175/346], Loss: 1.8242\n",
            "Epoch [94/100], Batch [176/346], Loss: 1.2715\n",
            "Epoch [94/100], Batch [177/346], Loss: 1.3616\n",
            "Epoch [94/100], Batch [178/346], Loss: 1.3786\n",
            "Epoch [94/100], Batch [179/346], Loss: 2.4179\n",
            "Epoch [94/100], Batch [180/346], Loss: 1.9375\n",
            "Epoch [94/100], Batch [181/346], Loss: 1.2111\n",
            "Epoch [94/100], Batch [182/346], Loss: 1.5724\n",
            "Epoch [94/100], Batch [183/346], Loss: 1.4507\n",
            "Epoch [94/100], Batch [184/346], Loss: 1.7788\n",
            "Epoch [94/100], Batch [185/346], Loss: 1.4272\n",
            "Epoch [94/100], Batch [186/346], Loss: 1.5607\n",
            "Epoch [94/100], Batch [187/346], Loss: 1.8404\n",
            "Epoch [94/100], Batch [188/346], Loss: 1.5378\n",
            "Epoch [94/100], Batch [189/346], Loss: 1.6820\n",
            "Epoch [94/100], Batch [190/346], Loss: 1.6496\n",
            "Epoch [94/100], Batch [191/346], Loss: 1.3028\n",
            "Epoch [94/100], Batch [192/346], Loss: 1.1678\n",
            "Epoch [94/100], Batch [193/346], Loss: 1.4354\n",
            "Epoch [94/100], Batch [194/346], Loss: 1.8731\n",
            "Epoch [94/100], Batch [195/346], Loss: 1.4374\n",
            "Epoch [94/100], Batch [196/346], Loss: 1.4815\n",
            "Epoch [94/100], Batch [197/346], Loss: 1.9308\n",
            "Epoch [94/100], Batch [198/346], Loss: 1.6475\n",
            "Epoch [94/100], Batch [199/346], Loss: 1.3608\n",
            "Epoch [94/100], Batch [200/346], Loss: 1.3409\n",
            "Epoch [94/100], Batch [201/346], Loss: 1.5255\n",
            "Epoch [94/100], Batch [202/346], Loss: 1.7708\n",
            "Epoch [94/100], Batch [203/346], Loss: 1.7460\n",
            "Epoch [94/100], Batch [204/346], Loss: 1.7313\n",
            "Epoch [94/100], Batch [205/346], Loss: 1.4849\n",
            "Epoch [94/100], Batch [206/346], Loss: 1.5938\n",
            "Epoch [94/100], Batch [207/346], Loss: 1.1749\n",
            "Epoch [94/100], Batch [208/346], Loss: 1.8566\n",
            "Epoch [94/100], Batch [209/346], Loss: 1.9870\n",
            "Epoch [94/100], Batch [210/346], Loss: 1.3745\n",
            "Epoch [94/100], Batch [211/346], Loss: 1.5419\n",
            "Epoch [94/100], Batch [212/346], Loss: 1.8108\n",
            "Epoch [94/100], Batch [213/346], Loss: 1.7336\n",
            "Epoch [94/100], Batch [214/346], Loss: 1.3290\n",
            "Epoch [94/100], Batch [215/346], Loss: 1.4782\n",
            "Epoch [94/100], Batch [216/346], Loss: 1.5581\n",
            "Epoch [94/100], Batch [217/346], Loss: 1.3413\n",
            "Epoch [94/100], Batch [218/346], Loss: 1.6863\n",
            "Epoch [94/100], Batch [219/346], Loss: 1.6930\n",
            "Epoch [94/100], Batch [220/346], Loss: 1.2943\n",
            "Epoch [94/100], Batch [221/346], Loss: 1.3890\n",
            "Epoch [94/100], Batch [222/346], Loss: 1.5819\n",
            "Epoch [94/100], Batch [223/346], Loss: 1.4199\n",
            "Epoch [94/100], Batch [224/346], Loss: 1.4561\n",
            "Epoch [94/100], Batch [225/346], Loss: 1.4113\n",
            "Epoch [94/100], Batch [226/346], Loss: 1.3228\n",
            "Epoch [94/100], Batch [227/346], Loss: 1.3490\n",
            "Epoch [94/100], Batch [228/346], Loss: 1.3420\n",
            "Epoch [94/100], Batch [229/346], Loss: 1.6675\n",
            "Epoch [94/100], Batch [230/346], Loss: 1.8101\n",
            "Epoch [94/100], Batch [231/346], Loss: 1.3490\n",
            "Epoch [94/100], Batch [232/346], Loss: 1.2753\n",
            "Epoch [94/100], Batch [233/346], Loss: 1.4080\n",
            "Epoch [94/100], Batch [234/346], Loss: 1.4790\n",
            "Epoch [94/100], Batch [235/346], Loss: 1.6239\n",
            "Epoch [94/100], Batch [236/346], Loss: 1.6020\n",
            "Epoch [94/100], Batch [237/346], Loss: 1.6750\n",
            "Epoch [94/100], Batch [238/346], Loss: 1.2799\n",
            "Epoch [94/100], Batch [239/346], Loss: 1.7655\n",
            "Epoch [94/100], Batch [240/346], Loss: 1.9914\n",
            "Epoch [94/100], Batch [241/346], Loss: 1.6438\n",
            "Epoch [94/100], Batch [242/346], Loss: 1.1960\n",
            "Epoch [94/100], Batch [243/346], Loss: 1.9610\n",
            "Epoch [94/100], Batch [244/346], Loss: 1.4637\n",
            "Epoch [94/100], Batch [245/346], Loss: 1.0752\n",
            "Epoch [94/100], Batch [246/346], Loss: 1.5030\n",
            "Epoch [94/100], Batch [247/346], Loss: 1.5523\n",
            "Epoch [94/100], Batch [248/346], Loss: 1.6000\n",
            "Epoch [94/100], Batch [249/346], Loss: 1.3810\n",
            "Epoch [94/100], Batch [250/346], Loss: 1.4856\n",
            "Epoch [94/100], Batch [251/346], Loss: 1.1574\n",
            "Epoch [94/100], Batch [252/346], Loss: 1.4697\n",
            "Epoch [94/100], Batch [253/346], Loss: 1.8987\n",
            "Epoch [94/100], Batch [254/346], Loss: 1.2722\n",
            "Epoch [94/100], Batch [255/346], Loss: 1.4342\n",
            "Epoch [94/100], Batch [256/346], Loss: 1.6278\n",
            "Epoch [94/100], Batch [257/346], Loss: 1.9075\n",
            "Epoch [94/100], Batch [258/346], Loss: 1.3411\n",
            "Epoch [94/100], Batch [259/346], Loss: 1.2408\n",
            "Epoch [94/100], Batch [260/346], Loss: 1.3475\n",
            "Epoch [94/100], Batch [261/346], Loss: 1.3882\n",
            "Epoch [94/100], Batch [262/346], Loss: 1.6679\n",
            "Epoch [94/100], Batch [263/346], Loss: 1.8905\n",
            "Epoch [94/100], Batch [264/346], Loss: 1.5067\n",
            "Epoch [94/100], Batch [265/346], Loss: 1.3873\n",
            "Epoch [94/100], Batch [266/346], Loss: 1.8240\n",
            "Epoch [94/100], Batch [267/346], Loss: 1.4982\n",
            "Epoch [94/100], Batch [268/346], Loss: 1.7318\n",
            "Epoch [94/100], Batch [269/346], Loss: 1.5490\n",
            "Epoch [94/100], Batch [270/346], Loss: 1.8203\n",
            "Epoch [94/100], Batch [271/346], Loss: 1.5294\n",
            "Epoch [94/100], Batch [272/346], Loss: 1.4629\n",
            "Epoch [94/100], Batch [273/346], Loss: 1.7364\n",
            "Epoch [94/100], Batch [274/346], Loss: 1.4683\n",
            "Epoch [94/100], Batch [275/346], Loss: 1.8937\n",
            "Epoch [94/100], Batch [276/346], Loss: 1.4646\n",
            "Epoch [94/100], Batch [277/346], Loss: 1.3473\n",
            "Epoch [94/100], Batch [278/346], Loss: 1.2953\n",
            "Epoch [94/100], Batch [279/346], Loss: 1.3142\n",
            "Epoch [94/100], Batch [280/346], Loss: 1.7859\n",
            "Epoch [94/100], Batch [281/346], Loss: 1.5040\n",
            "Epoch [94/100], Batch [282/346], Loss: 1.7375\n",
            "Epoch [94/100], Batch [283/346], Loss: 1.5463\n",
            "Epoch [94/100], Batch [284/346], Loss: 1.7286\n",
            "Epoch [94/100], Batch [285/346], Loss: 1.6130\n",
            "Epoch [94/100], Batch [286/346], Loss: 1.7372\n",
            "Epoch [94/100], Batch [287/346], Loss: 1.4626\n",
            "Epoch [94/100], Batch [288/346], Loss: 1.7028\n",
            "Epoch [94/100], Batch [289/346], Loss: 1.9060\n",
            "Epoch [94/100], Batch [290/346], Loss: 1.3353\n",
            "Epoch [94/100], Batch [291/346], Loss: 1.3626\n",
            "Epoch [94/100], Batch [292/346], Loss: 1.7645\n",
            "Epoch [94/100], Batch [293/346], Loss: 1.3847\n",
            "Epoch [94/100], Batch [294/346], Loss: 1.6055\n",
            "Epoch [94/100], Batch [295/346], Loss: 1.7476\n",
            "Epoch [94/100], Batch [296/346], Loss: 1.5241\n",
            "Epoch [94/100], Batch [297/346], Loss: 1.6410\n",
            "Epoch [94/100], Batch [298/346], Loss: 1.6689\n",
            "Epoch [94/100], Batch [299/346], Loss: 2.0238\n",
            "Epoch [94/100], Batch [300/346], Loss: 1.2996\n",
            "Epoch [94/100], Batch [301/346], Loss: 1.1409\n",
            "Epoch [94/100], Batch [302/346], Loss: 1.3557\n",
            "Epoch [94/100], Batch [303/346], Loss: 1.8080\n",
            "Epoch [94/100], Batch [304/346], Loss: 1.5728\n",
            "Epoch [94/100], Batch [305/346], Loss: 1.6307\n",
            "Epoch [94/100], Batch [306/346], Loss: 1.5780\n",
            "Epoch [94/100], Batch [307/346], Loss: 1.7719\n",
            "Epoch [94/100], Batch [308/346], Loss: 1.6640\n",
            "Epoch [94/100], Batch [309/346], Loss: 1.0903\n",
            "Epoch [94/100], Batch [310/346], Loss: 1.4603\n",
            "Epoch [94/100], Batch [311/346], Loss: 1.3869\n",
            "Epoch [94/100], Batch [312/346], Loss: 1.9564\n",
            "Epoch [94/100], Batch [313/346], Loss: 2.0049\n",
            "Epoch [94/100], Batch [314/346], Loss: 1.7015\n",
            "Epoch [94/100], Batch [315/346], Loss: 1.5025\n",
            "Epoch [94/100], Batch [316/346], Loss: 1.5927\n",
            "Epoch [94/100], Batch [317/346], Loss: 1.7418\n",
            "Epoch [94/100], Batch [318/346], Loss: 1.7148\n",
            "Epoch [94/100], Batch [319/346], Loss: 1.4264\n",
            "Epoch [94/100], Batch [320/346], Loss: 0.8697\n",
            "Epoch [94/100], Batch [321/346], Loss: 1.5901\n",
            "Epoch [94/100], Batch [322/346], Loss: 1.6284\n",
            "Epoch [94/100], Batch [323/346], Loss: 1.3464\n",
            "Epoch [94/100], Batch [324/346], Loss: 1.5622\n",
            "Epoch [94/100], Batch [325/346], Loss: 1.3427\n",
            "Epoch [94/100], Batch [326/346], Loss: 1.5562\n",
            "Epoch [94/100], Batch [327/346], Loss: 1.6873\n",
            "Epoch [94/100], Batch [328/346], Loss: 1.9309\n",
            "Epoch [94/100], Batch [329/346], Loss: 1.9427\n",
            "Epoch [94/100], Batch [330/346], Loss: 0.8498\n",
            "Epoch [94/100], Batch [331/346], Loss: 1.2248\n",
            "Epoch [94/100], Batch [332/346], Loss: 1.7429\n",
            "Epoch [94/100], Batch [333/346], Loss: 1.5979\n",
            "Epoch [94/100], Batch [334/346], Loss: 1.6393\n",
            "Epoch [94/100], Batch [335/346], Loss: 1.5320\n",
            "Epoch [94/100], Batch [336/346], Loss: 1.3304\n",
            "Epoch [94/100], Batch [337/346], Loss: 1.5958\n",
            "Epoch [94/100], Batch [338/346], Loss: 1.5732\n",
            "Epoch [94/100], Batch [339/346], Loss: 1.6535\n",
            "Epoch [94/100], Batch [340/346], Loss: 1.4686\n",
            "Epoch [94/100], Batch [341/346], Loss: 1.7345\n",
            "Epoch [94/100], Batch [342/346], Loss: 1.2237\n",
            "Epoch [94/100], Batch [343/346], Loss: 1.5571\n",
            "Epoch [94/100], Batch [344/346], Loss: 1.5221\n",
            "Epoch [94/100], Batch [345/346], Loss: 1.4053\n",
            "Epoch [94/100], Batch [346/346], Loss: 2.2227\n",
            "Epoch [95/100], Batch [1/346], Loss: 2.1446\n",
            "Epoch [95/100], Batch [2/346], Loss: 1.3840\n",
            "Epoch [95/100], Batch [3/346], Loss: 1.8169\n",
            "Epoch [95/100], Batch [4/346], Loss: 1.3758\n",
            "Epoch [95/100], Batch [5/346], Loss: 1.4636\n",
            "Epoch [95/100], Batch [6/346], Loss: 1.6982\n",
            "Epoch [95/100], Batch [7/346], Loss: 1.6469\n",
            "Epoch [95/100], Batch [8/346], Loss: 1.6006\n",
            "Epoch [95/100], Batch [9/346], Loss: 1.5290\n",
            "Epoch [95/100], Batch [10/346], Loss: 1.2816\n",
            "Epoch [95/100], Batch [11/346], Loss: 1.2972\n",
            "Epoch [95/100], Batch [12/346], Loss: 1.4688\n",
            "Epoch [95/100], Batch [13/346], Loss: 1.1113\n",
            "Epoch [95/100], Batch [14/346], Loss: 1.6849\n",
            "Epoch [95/100], Batch [15/346], Loss: 1.8416\n",
            "Epoch [95/100], Batch [16/346], Loss: 1.4989\n",
            "Epoch [95/100], Batch [17/346], Loss: 1.7384\n",
            "Epoch [95/100], Batch [18/346], Loss: 1.4358\n",
            "Epoch [95/100], Batch [19/346], Loss: 1.5875\n",
            "Epoch [95/100], Batch [20/346], Loss: 1.3499\n",
            "Epoch [95/100], Batch [21/346], Loss: 1.4333\n",
            "Epoch [95/100], Batch [22/346], Loss: 1.2873\n",
            "Epoch [95/100], Batch [23/346], Loss: 1.8038\n",
            "Epoch [95/100], Batch [24/346], Loss: 1.4552\n",
            "Epoch [95/100], Batch [25/346], Loss: 1.9901\n",
            "Epoch [95/100], Batch [26/346], Loss: 1.1899\n",
            "Epoch [95/100], Batch [27/346], Loss: 1.5748\n",
            "Epoch [95/100], Batch [28/346], Loss: 1.4279\n",
            "Epoch [95/100], Batch [29/346], Loss: 1.4214\n",
            "Epoch [95/100], Batch [30/346], Loss: 1.6709\n",
            "Epoch [95/100], Batch [31/346], Loss: 1.3936\n",
            "Epoch [95/100], Batch [32/346], Loss: 1.3866\n",
            "Epoch [95/100], Batch [33/346], Loss: 1.1748\n",
            "Epoch [95/100], Batch [34/346], Loss: 1.1986\n",
            "Epoch [95/100], Batch [35/346], Loss: 1.2835\n",
            "Epoch [95/100], Batch [36/346], Loss: 1.4197\n",
            "Epoch [95/100], Batch [37/346], Loss: 1.5096\n",
            "Epoch [95/100], Batch [38/346], Loss: 1.2851\n",
            "Epoch [95/100], Batch [39/346], Loss: 1.5357\n",
            "Epoch [95/100], Batch [40/346], Loss: 1.2242\n",
            "Epoch [95/100], Batch [41/346], Loss: 1.3413\n",
            "Epoch [95/100], Batch [42/346], Loss: 1.2908\n",
            "Epoch [95/100], Batch [43/346], Loss: 1.2544\n",
            "Epoch [95/100], Batch [44/346], Loss: 1.6642\n",
            "Epoch [95/100], Batch [45/346], Loss: 1.1834\n",
            "Epoch [95/100], Batch [46/346], Loss: 1.3261\n",
            "Epoch [95/100], Batch [47/346], Loss: 1.7143\n",
            "Epoch [95/100], Batch [48/346], Loss: 1.5279\n",
            "Epoch [95/100], Batch [49/346], Loss: 1.3595\n",
            "Epoch [95/100], Batch [50/346], Loss: 1.5469\n",
            "Epoch [95/100], Batch [51/346], Loss: 1.8334\n",
            "Epoch [95/100], Batch [52/346], Loss: 1.4843\n",
            "Epoch [95/100], Batch [53/346], Loss: 1.3942\n",
            "Epoch [95/100], Batch [54/346], Loss: 1.6008\n",
            "Epoch [95/100], Batch [55/346], Loss: 1.5736\n",
            "Epoch [95/100], Batch [56/346], Loss: 1.7367\n",
            "Epoch [95/100], Batch [57/346], Loss: 1.5845\n",
            "Epoch [95/100], Batch [58/346], Loss: 1.5720\n",
            "Epoch [95/100], Batch [59/346], Loss: 1.2542\n",
            "Epoch [95/100], Batch [60/346], Loss: 1.0955\n",
            "Epoch [95/100], Batch [61/346], Loss: 1.1525\n",
            "Epoch [95/100], Batch [62/346], Loss: 2.1105\n",
            "Epoch [95/100], Batch [63/346], Loss: 1.8782\n",
            "Epoch [95/100], Batch [64/346], Loss: 2.1126\n",
            "Epoch [95/100], Batch [65/346], Loss: 1.6331\n",
            "Epoch [95/100], Batch [66/346], Loss: 1.6183\n",
            "Epoch [95/100], Batch [67/346], Loss: 1.5522\n",
            "Epoch [95/100], Batch [68/346], Loss: 1.8304\n",
            "Epoch [95/100], Batch [69/346], Loss: 1.4942\n",
            "Epoch [95/100], Batch [70/346], Loss: 1.5817\n",
            "Epoch [95/100], Batch [71/346], Loss: 1.5006\n",
            "Epoch [95/100], Batch [72/346], Loss: 1.8158\n",
            "Epoch [95/100], Batch [73/346], Loss: 1.7840\n",
            "Epoch [95/100], Batch [74/346], Loss: 1.5253\n",
            "Epoch [95/100], Batch [75/346], Loss: 1.6573\n",
            "Epoch [95/100], Batch [76/346], Loss: 1.4367\n",
            "Epoch [95/100], Batch [77/346], Loss: 1.5580\n",
            "Epoch [95/100], Batch [78/346], Loss: 1.5642\n",
            "Epoch [95/100], Batch [79/346], Loss: 1.8461\n",
            "Epoch [95/100], Batch [80/346], Loss: 1.4768\n",
            "Epoch [95/100], Batch [81/346], Loss: 2.1046\n",
            "Epoch [95/100], Batch [82/346], Loss: 1.4954\n",
            "Epoch [95/100], Batch [83/346], Loss: 1.4425\n",
            "Epoch [95/100], Batch [84/346], Loss: 1.4986\n",
            "Epoch [95/100], Batch [85/346], Loss: 1.5859\n",
            "Epoch [95/100], Batch [86/346], Loss: 1.8455\n",
            "Epoch [95/100], Batch [87/346], Loss: 1.9500\n",
            "Epoch [95/100], Batch [88/346], Loss: 1.8078\n",
            "Epoch [95/100], Batch [89/346], Loss: 1.7112\n",
            "Epoch [95/100], Batch [90/346], Loss: 2.2203\n",
            "Epoch [95/100], Batch [91/346], Loss: 1.4294\n",
            "Epoch [95/100], Batch [92/346], Loss: 1.4469\n",
            "Epoch [95/100], Batch [93/346], Loss: 1.6227\n",
            "Epoch [95/100], Batch [94/346], Loss: 1.3829\n",
            "Epoch [95/100], Batch [95/346], Loss: 1.4926\n",
            "Epoch [95/100], Batch [96/346], Loss: 1.2400\n",
            "Epoch [95/100], Batch [97/346], Loss: 1.6732\n",
            "Epoch [95/100], Batch [98/346], Loss: 1.3584\n",
            "Epoch [95/100], Batch [99/346], Loss: 1.6903\n",
            "Epoch [95/100], Batch [100/346], Loss: 1.5905\n",
            "Epoch [95/100], Batch [101/346], Loss: 1.1984\n",
            "Epoch [95/100], Batch [102/346], Loss: 1.4138\n",
            "Epoch [95/100], Batch [103/346], Loss: 1.1846\n",
            "Epoch [95/100], Batch [104/346], Loss: 1.5636\n",
            "Epoch [95/100], Batch [105/346], Loss: 1.8517\n",
            "Epoch [95/100], Batch [106/346], Loss: 1.5164\n",
            "Epoch [95/100], Batch [107/346], Loss: 1.4696\n",
            "Epoch [95/100], Batch [108/346], Loss: 1.7878\n",
            "Epoch [95/100], Batch [109/346], Loss: 1.5305\n",
            "Epoch [95/100], Batch [110/346], Loss: 1.3836\n",
            "Epoch [95/100], Batch [111/346], Loss: 1.8122\n",
            "Epoch [95/100], Batch [112/346], Loss: 1.5408\n",
            "Epoch [95/100], Batch [113/346], Loss: 1.5577\n",
            "Epoch [95/100], Batch [114/346], Loss: 1.3007\n",
            "Epoch [95/100], Batch [115/346], Loss: 1.3634\n",
            "Epoch [95/100], Batch [116/346], Loss: 1.5818\n",
            "Epoch [95/100], Batch [117/346], Loss: 1.3207\n",
            "Epoch [95/100], Batch [118/346], Loss: 1.5908\n",
            "Epoch [95/100], Batch [119/346], Loss: 1.2618\n",
            "Epoch [95/100], Batch [120/346], Loss: 1.5979\n",
            "Epoch [95/100], Batch [121/346], Loss: 1.4064\n",
            "Epoch [95/100], Batch [122/346], Loss: 1.2406\n",
            "Epoch [95/100], Batch [123/346], Loss: 1.7167\n",
            "Epoch [95/100], Batch [124/346], Loss: 1.3032\n",
            "Epoch [95/100], Batch [125/346], Loss: 1.3153\n",
            "Epoch [95/100], Batch [126/346], Loss: 1.4569\n",
            "Epoch [95/100], Batch [127/346], Loss: 1.6939\n",
            "Epoch [95/100], Batch [128/346], Loss: 1.5243\n",
            "Epoch [95/100], Batch [129/346], Loss: 1.5244\n",
            "Epoch [95/100], Batch [130/346], Loss: 1.3764\n",
            "Epoch [95/100], Batch [131/346], Loss: 1.7090\n",
            "Epoch [95/100], Batch [132/346], Loss: 1.4259\n",
            "Epoch [95/100], Batch [133/346], Loss: 1.4663\n",
            "Epoch [95/100], Batch [134/346], Loss: 1.6890\n",
            "Epoch [95/100], Batch [135/346], Loss: 1.7747\n",
            "Epoch [95/100], Batch [136/346], Loss: 1.3960\n",
            "Epoch [95/100], Batch [137/346], Loss: 1.8730\n",
            "Epoch [95/100], Batch [138/346], Loss: 1.2694\n",
            "Epoch [95/100], Batch [139/346], Loss: 1.4828\n",
            "Epoch [95/100], Batch [140/346], Loss: 2.0066\n",
            "Epoch [95/100], Batch [141/346], Loss: 1.5488\n",
            "Epoch [95/100], Batch [142/346], Loss: 1.6430\n",
            "Epoch [95/100], Batch [143/346], Loss: 1.4587\n",
            "Epoch [95/100], Batch [144/346], Loss: 1.4341\n",
            "Epoch [95/100], Batch [145/346], Loss: 1.3793\n",
            "Epoch [95/100], Batch [146/346], Loss: 1.6574\n",
            "Epoch [95/100], Batch [147/346], Loss: 1.9971\n",
            "Epoch [95/100], Batch [148/346], Loss: 1.5215\n",
            "Epoch [95/100], Batch [149/346], Loss: 1.2700\n",
            "Epoch [95/100], Batch [150/346], Loss: 1.3815\n",
            "Epoch [95/100], Batch [151/346], Loss: 1.6543\n",
            "Epoch [95/100], Batch [152/346], Loss: 1.7416\n",
            "Epoch [95/100], Batch [153/346], Loss: 1.3824\n",
            "Epoch [95/100], Batch [154/346], Loss: 1.8215\n",
            "Epoch [95/100], Batch [155/346], Loss: 1.7932\n",
            "Epoch [95/100], Batch [156/346], Loss: 1.6976\n",
            "Epoch [95/100], Batch [157/346], Loss: 1.5171\n",
            "Epoch [95/100], Batch [158/346], Loss: 1.6353\n",
            "Epoch [95/100], Batch [159/346], Loss: 1.5362\n",
            "Epoch [95/100], Batch [160/346], Loss: 1.2988\n",
            "Epoch [95/100], Batch [161/346], Loss: 1.3215\n",
            "Epoch [95/100], Batch [162/346], Loss: 1.6787\n",
            "Epoch [95/100], Batch [163/346], Loss: 2.2092\n",
            "Epoch [95/100], Batch [164/346], Loss: 1.4078\n",
            "Epoch [95/100], Batch [165/346], Loss: 2.0257\n",
            "Epoch [95/100], Batch [166/346], Loss: 1.6222\n",
            "Epoch [95/100], Batch [167/346], Loss: 1.5524\n",
            "Epoch [95/100], Batch [168/346], Loss: 1.9157\n",
            "Epoch [95/100], Batch [169/346], Loss: 1.6643\n",
            "Epoch [95/100], Batch [170/346], Loss: 1.5431\n",
            "Epoch [95/100], Batch [171/346], Loss: 1.4160\n",
            "Epoch [95/100], Batch [172/346], Loss: 1.5198\n",
            "Epoch [95/100], Batch [173/346], Loss: 1.5278\n",
            "Epoch [95/100], Batch [174/346], Loss: 1.5180\n",
            "Epoch [95/100], Batch [175/346], Loss: 1.5185\n",
            "Epoch [95/100], Batch [176/346], Loss: 1.7183\n",
            "Epoch [95/100], Batch [177/346], Loss: 1.1920\n",
            "Epoch [95/100], Batch [178/346], Loss: 1.6847\n",
            "Epoch [95/100], Batch [179/346], Loss: 1.8122\n",
            "Epoch [95/100], Batch [180/346], Loss: 1.4617\n",
            "Epoch [95/100], Batch [181/346], Loss: 1.2318\n",
            "Epoch [95/100], Batch [182/346], Loss: 1.4633\n",
            "Epoch [95/100], Batch [183/346], Loss: 1.4686\n",
            "Epoch [95/100], Batch [184/346], Loss: 1.7911\n",
            "Epoch [95/100], Batch [185/346], Loss: 1.4827\n",
            "Epoch [95/100], Batch [186/346], Loss: 1.2865\n",
            "Epoch [95/100], Batch [187/346], Loss: 1.4232\n",
            "Epoch [95/100], Batch [188/346], Loss: 1.3859\n",
            "Epoch [95/100], Batch [189/346], Loss: 1.8488\n",
            "Epoch [95/100], Batch [190/346], Loss: 1.4309\n",
            "Epoch [95/100], Batch [191/346], Loss: 1.9057\n",
            "Epoch [95/100], Batch [192/346], Loss: 1.5170\n",
            "Epoch [95/100], Batch [193/346], Loss: 1.1188\n",
            "Epoch [95/100], Batch [194/346], Loss: 1.4126\n",
            "Epoch [95/100], Batch [195/346], Loss: 1.3876\n",
            "Epoch [95/100], Batch [196/346], Loss: 1.3863\n",
            "Epoch [95/100], Batch [197/346], Loss: 1.6546\n",
            "Epoch [95/100], Batch [198/346], Loss: 1.6526\n",
            "Epoch [95/100], Batch [199/346], Loss: 1.9921\n",
            "Epoch [95/100], Batch [200/346], Loss: 1.3212\n",
            "Epoch [95/100], Batch [201/346], Loss: 1.7846\n",
            "Epoch [95/100], Batch [202/346], Loss: 1.8634\n",
            "Epoch [95/100], Batch [203/346], Loss: 1.1364\n",
            "Epoch [95/100], Batch [204/346], Loss: 1.2893\n",
            "Epoch [95/100], Batch [205/346], Loss: 1.1628\n",
            "Epoch [95/100], Batch [206/346], Loss: 1.5026\n",
            "Epoch [95/100], Batch [207/346], Loss: 1.5206\n",
            "Epoch [95/100], Batch [208/346], Loss: 1.5505\n",
            "Epoch [95/100], Batch [209/346], Loss: 1.5363\n",
            "Epoch [95/100], Batch [210/346], Loss: 1.8282\n",
            "Epoch [95/100], Batch [211/346], Loss: 1.4886\n",
            "Epoch [95/100], Batch [212/346], Loss: 1.5552\n",
            "Epoch [95/100], Batch [213/346], Loss: 1.1733\n",
            "Epoch [95/100], Batch [214/346], Loss: 1.3164\n",
            "Epoch [95/100], Batch [215/346], Loss: 1.4491\n",
            "Epoch [95/100], Batch [216/346], Loss: 1.8397\n",
            "Epoch [95/100], Batch [217/346], Loss: 1.3692\n",
            "Epoch [95/100], Batch [218/346], Loss: 1.2827\n",
            "Epoch [95/100], Batch [219/346], Loss: 1.2797\n",
            "Epoch [95/100], Batch [220/346], Loss: 1.4796\n",
            "Epoch [95/100], Batch [221/346], Loss: 1.7659\n",
            "Epoch [95/100], Batch [222/346], Loss: 1.3792\n",
            "Epoch [95/100], Batch [223/346], Loss: 1.5010\n",
            "Epoch [95/100], Batch [224/346], Loss: 1.1154\n",
            "Epoch [95/100], Batch [225/346], Loss: 1.5824\n",
            "Epoch [95/100], Batch [226/346], Loss: 1.7226\n",
            "Epoch [95/100], Batch [227/346], Loss: 1.6528\n",
            "Epoch [95/100], Batch [228/346], Loss: 1.7564\n",
            "Epoch [95/100], Batch [229/346], Loss: 1.6508\n",
            "Epoch [95/100], Batch [230/346], Loss: 1.1896\n",
            "Epoch [95/100], Batch [231/346], Loss: 1.6829\n",
            "Epoch [95/100], Batch [232/346], Loss: 1.7283\n",
            "Epoch [95/100], Batch [233/346], Loss: 1.7357\n",
            "Epoch [95/100], Batch [234/346], Loss: 1.5097\n",
            "Epoch [95/100], Batch [235/346], Loss: 1.3143\n",
            "Epoch [95/100], Batch [236/346], Loss: 1.1922\n",
            "Epoch [95/100], Batch [237/346], Loss: 1.3033\n",
            "Epoch [95/100], Batch [238/346], Loss: 1.7110\n",
            "Epoch [95/100], Batch [239/346], Loss: 2.1623\n",
            "Epoch [95/100], Batch [240/346], Loss: 1.6327\n",
            "Epoch [95/100], Batch [241/346], Loss: 1.6198\n",
            "Epoch [95/100], Batch [242/346], Loss: 1.3210\n",
            "Epoch [95/100], Batch [243/346], Loss: 1.4243\n",
            "Epoch [95/100], Batch [244/346], Loss: 1.1374\n",
            "Epoch [95/100], Batch [245/346], Loss: 1.3635\n",
            "Epoch [95/100], Batch [246/346], Loss: 1.1935\n",
            "Epoch [95/100], Batch [247/346], Loss: 1.7455\n",
            "Epoch [95/100], Batch [248/346], Loss: 2.0961\n",
            "Epoch [95/100], Batch [249/346], Loss: 1.6752\n",
            "Epoch [95/100], Batch [250/346], Loss: 1.7369\n",
            "Epoch [95/100], Batch [251/346], Loss: 1.3301\n",
            "Epoch [95/100], Batch [252/346], Loss: 1.4792\n",
            "Epoch [95/100], Batch [253/346], Loss: 1.3721\n",
            "Epoch [95/100], Batch [254/346], Loss: 1.4054\n",
            "Epoch [95/100], Batch [255/346], Loss: 1.7411\n",
            "Epoch [95/100], Batch [256/346], Loss: 1.3248\n",
            "Epoch [95/100], Batch [257/346], Loss: 1.4685\n",
            "Epoch [95/100], Batch [258/346], Loss: 1.6104\n",
            "Epoch [95/100], Batch [259/346], Loss: 1.2536\n",
            "Epoch [95/100], Batch [260/346], Loss: 1.8614\n",
            "Epoch [95/100], Batch [261/346], Loss: 1.9903\n",
            "Epoch [95/100], Batch [262/346], Loss: 1.3822\n",
            "Epoch [95/100], Batch [263/346], Loss: 1.7686\n",
            "Epoch [95/100], Batch [264/346], Loss: 1.7744\n",
            "Epoch [95/100], Batch [265/346], Loss: 1.6550\n",
            "Epoch [95/100], Batch [266/346], Loss: 1.8516\n",
            "Epoch [95/100], Batch [267/346], Loss: 1.5222\n",
            "Epoch [95/100], Batch [268/346], Loss: 1.4926\n",
            "Epoch [95/100], Batch [269/346], Loss: 1.2600\n",
            "Epoch [95/100], Batch [270/346], Loss: 1.3444\n",
            "Epoch [95/100], Batch [271/346], Loss: 1.4776\n",
            "Epoch [95/100], Batch [272/346], Loss: 1.6737\n",
            "Epoch [95/100], Batch [273/346], Loss: 1.1482\n",
            "Epoch [95/100], Batch [274/346], Loss: 1.6138\n",
            "Epoch [95/100], Batch [275/346], Loss: 1.4029\n",
            "Epoch [95/100], Batch [276/346], Loss: 1.2139\n",
            "Epoch [95/100], Batch [277/346], Loss: 1.5006\n",
            "Epoch [95/100], Batch [278/346], Loss: 1.4674\n",
            "Epoch [95/100], Batch [279/346], Loss: 1.3969\n",
            "Epoch [95/100], Batch [280/346], Loss: 1.3466\n",
            "Epoch [95/100], Batch [281/346], Loss: 1.4752\n",
            "Epoch [95/100], Batch [282/346], Loss: 1.3942\n",
            "Epoch [95/100], Batch [283/346], Loss: 1.4137\n",
            "Epoch [95/100], Batch [284/346], Loss: 1.6771\n",
            "Epoch [95/100], Batch [285/346], Loss: 1.4880\n",
            "Epoch [95/100], Batch [286/346], Loss: 1.5591\n",
            "Epoch [95/100], Batch [287/346], Loss: 1.3730\n",
            "Epoch [95/100], Batch [288/346], Loss: 1.7250\n",
            "Epoch [95/100], Batch [289/346], Loss: 1.2316\n",
            "Epoch [95/100], Batch [290/346], Loss: 1.4692\n",
            "Epoch [95/100], Batch [291/346], Loss: 1.2244\n",
            "Epoch [95/100], Batch [292/346], Loss: 1.4887\n",
            "Epoch [95/100], Batch [293/346], Loss: 2.1168\n",
            "Epoch [95/100], Batch [294/346], Loss: 1.4556\n",
            "Epoch [95/100], Batch [295/346], Loss: 1.5099\n",
            "Epoch [95/100], Batch [296/346], Loss: 1.6120\n",
            "Epoch [95/100], Batch [297/346], Loss: 1.4087\n",
            "Epoch [95/100], Batch [298/346], Loss: 1.5792\n",
            "Epoch [95/100], Batch [299/346], Loss: 1.2484\n",
            "Epoch [95/100], Batch [300/346], Loss: 1.4976\n",
            "Epoch [95/100], Batch [301/346], Loss: 1.8203\n",
            "Epoch [95/100], Batch [302/346], Loss: 1.6999\n",
            "Epoch [95/100], Batch [303/346], Loss: 2.0978\n",
            "Epoch [95/100], Batch [304/346], Loss: 1.9074\n",
            "Epoch [95/100], Batch [305/346], Loss: 1.5876\n",
            "Epoch [95/100], Batch [306/346], Loss: 1.3899\n",
            "Epoch [95/100], Batch [307/346], Loss: 1.5150\n",
            "Epoch [95/100], Batch [308/346], Loss: 1.7284\n",
            "Epoch [95/100], Batch [309/346], Loss: 1.6809\n",
            "Epoch [95/100], Batch [310/346], Loss: 1.5179\n",
            "Epoch [95/100], Batch [311/346], Loss: 1.4401\n",
            "Epoch [95/100], Batch [312/346], Loss: 1.3973\n",
            "Epoch [95/100], Batch [313/346], Loss: 1.2331\n",
            "Epoch [95/100], Batch [314/346], Loss: 1.4724\n",
            "Epoch [95/100], Batch [315/346], Loss: 1.4488\n",
            "Epoch [95/100], Batch [316/346], Loss: 1.9797\n",
            "Epoch [95/100], Batch [317/346], Loss: 1.3284\n",
            "Epoch [95/100], Batch [318/346], Loss: 1.4673\n",
            "Epoch [95/100], Batch [319/346], Loss: 1.7159\n",
            "Epoch [95/100], Batch [320/346], Loss: 1.5013\n",
            "Epoch [95/100], Batch [321/346], Loss: 1.7311\n",
            "Epoch [95/100], Batch [322/346], Loss: 1.3296\n",
            "Epoch [95/100], Batch [323/346], Loss: 1.4403\n",
            "Epoch [95/100], Batch [324/346], Loss: 1.3030\n",
            "Epoch [95/100], Batch [325/346], Loss: 1.2522\n",
            "Epoch [95/100], Batch [326/346], Loss: 1.7469\n",
            "Epoch [95/100], Batch [327/346], Loss: 1.3021\n",
            "Epoch [95/100], Batch [328/346], Loss: 1.7191\n",
            "Epoch [95/100], Batch [329/346], Loss: 1.5007\n",
            "Epoch [95/100], Batch [330/346], Loss: 1.6610\n",
            "Epoch [95/100], Batch [331/346], Loss: 1.2822\n",
            "Epoch [95/100], Batch [332/346], Loss: 1.7566\n",
            "Epoch [95/100], Batch [333/346], Loss: 1.6041\n",
            "Epoch [95/100], Batch [334/346], Loss: 1.4450\n",
            "Epoch [95/100], Batch [335/346], Loss: 1.4955\n",
            "Epoch [95/100], Batch [336/346], Loss: 1.2988\n",
            "Epoch [95/100], Batch [337/346], Loss: 1.1474\n",
            "Epoch [95/100], Batch [338/346], Loss: 1.5314\n",
            "Epoch [95/100], Batch [339/346], Loss: 1.3530\n",
            "Epoch [95/100], Batch [340/346], Loss: 1.4878\n",
            "Epoch [95/100], Batch [341/346], Loss: 1.3784\n",
            "Epoch [95/100], Batch [342/346], Loss: 1.7499\n",
            "Epoch [95/100], Batch [343/346], Loss: 1.4421\n",
            "Epoch [95/100], Batch [344/346], Loss: 1.7332\n",
            "Epoch [95/100], Batch [345/346], Loss: 1.4869\n",
            "Epoch [95/100], Batch [346/346], Loss: 1.7708\n",
            "Epoch [96/100], Batch [1/346], Loss: 1.2453\n",
            "Epoch [96/100], Batch [2/346], Loss: 1.5481\n",
            "Epoch [96/100], Batch [3/346], Loss: 1.7782\n",
            "Epoch [96/100], Batch [4/346], Loss: 1.4871\n",
            "Epoch [96/100], Batch [5/346], Loss: 1.3186\n",
            "Epoch [96/100], Batch [6/346], Loss: 1.4318\n",
            "Epoch [96/100], Batch [7/346], Loss: 1.2843\n",
            "Epoch [96/100], Batch [8/346], Loss: 1.3471\n",
            "Epoch [96/100], Batch [9/346], Loss: 1.5846\n",
            "Epoch [96/100], Batch [10/346], Loss: 1.5930\n",
            "Epoch [96/100], Batch [11/346], Loss: 1.6195\n",
            "Epoch [96/100], Batch [12/346], Loss: 1.7143\n",
            "Epoch [96/100], Batch [13/346], Loss: 1.1522\n",
            "Epoch [96/100], Batch [14/346], Loss: 1.4259\n",
            "Epoch [96/100], Batch [15/346], Loss: 1.9823\n",
            "Epoch [96/100], Batch [16/346], Loss: 1.5910\n",
            "Epoch [96/100], Batch [17/346], Loss: 1.5495\n",
            "Epoch [96/100], Batch [18/346], Loss: 1.3504\n",
            "Epoch [96/100], Batch [19/346], Loss: 1.6642\n",
            "Epoch [96/100], Batch [20/346], Loss: 1.4969\n",
            "Epoch [96/100], Batch [21/346], Loss: 1.5421\n",
            "Epoch [96/100], Batch [22/346], Loss: 1.4412\n",
            "Epoch [96/100], Batch [23/346], Loss: 1.5376\n",
            "Epoch [96/100], Batch [24/346], Loss: 1.9719\n",
            "Epoch [96/100], Batch [25/346], Loss: 1.2033\n",
            "Epoch [96/100], Batch [26/346], Loss: 1.9754\n",
            "Epoch [96/100], Batch [27/346], Loss: 1.5691\n",
            "Epoch [96/100], Batch [28/346], Loss: 1.5405\n",
            "Epoch [96/100], Batch [29/346], Loss: 2.1727\n",
            "Epoch [96/100], Batch [30/346], Loss: 1.7902\n",
            "Epoch [96/100], Batch [31/346], Loss: 1.7654\n",
            "Epoch [96/100], Batch [32/346], Loss: 1.4442\n",
            "Epoch [96/100], Batch [33/346], Loss: 2.0947\n",
            "Epoch [96/100], Batch [34/346], Loss: 1.8573\n",
            "Epoch [96/100], Batch [35/346], Loss: 1.7682\n",
            "Epoch [96/100], Batch [36/346], Loss: 1.3409\n",
            "Epoch [96/100], Batch [37/346], Loss: 1.2799\n",
            "Epoch [96/100], Batch [38/346], Loss: 1.5901\n",
            "Epoch [96/100], Batch [39/346], Loss: 1.6454\n",
            "Epoch [96/100], Batch [40/346], Loss: 1.2688\n",
            "Epoch [96/100], Batch [41/346], Loss: 1.2042\n",
            "Epoch [96/100], Batch [42/346], Loss: 1.2467\n",
            "Epoch [96/100], Batch [43/346], Loss: 1.4794\n",
            "Epoch [96/100], Batch [44/346], Loss: 1.3783\n",
            "Epoch [96/100], Batch [45/346], Loss: 1.9316\n",
            "Epoch [96/100], Batch [46/346], Loss: 1.2032\n",
            "Epoch [96/100], Batch [47/346], Loss: 1.4502\n",
            "Epoch [96/100], Batch [48/346], Loss: 1.7500\n",
            "Epoch [96/100], Batch [49/346], Loss: 1.7467\n",
            "Epoch [96/100], Batch [50/346], Loss: 1.3157\n",
            "Epoch [96/100], Batch [51/346], Loss: 1.6204\n",
            "Epoch [96/100], Batch [52/346], Loss: 1.5662\n",
            "Epoch [96/100], Batch [53/346], Loss: 1.2775\n",
            "Epoch [96/100], Batch [54/346], Loss: 1.4989\n",
            "Epoch [96/100], Batch [55/346], Loss: 1.3532\n",
            "Epoch [96/100], Batch [56/346], Loss: 1.3863\n",
            "Epoch [96/100], Batch [57/346], Loss: 1.7453\n",
            "Epoch [96/100], Batch [58/346], Loss: 1.6916\n",
            "Epoch [96/100], Batch [59/346], Loss: 1.2341\n",
            "Epoch [96/100], Batch [60/346], Loss: 1.6022\n",
            "Epoch [96/100], Batch [61/346], Loss: 1.4018\n",
            "Epoch [96/100], Batch [62/346], Loss: 1.5884\n",
            "Epoch [96/100], Batch [63/346], Loss: 1.6030\n",
            "Epoch [96/100], Batch [64/346], Loss: 1.3038\n",
            "Epoch [96/100], Batch [65/346], Loss: 1.4237\n",
            "Epoch [96/100], Batch [66/346], Loss: 1.1258\n",
            "Epoch [96/100], Batch [67/346], Loss: 1.5346\n",
            "Epoch [96/100], Batch [68/346], Loss: 1.4952\n",
            "Epoch [96/100], Batch [69/346], Loss: 1.8833\n",
            "Epoch [96/100], Batch [70/346], Loss: 1.5104\n",
            "Epoch [96/100], Batch [71/346], Loss: 1.9088\n",
            "Epoch [96/100], Batch [72/346], Loss: 1.6169\n",
            "Epoch [96/100], Batch [73/346], Loss: 1.4913\n",
            "Epoch [96/100], Batch [74/346], Loss: 1.5474\n",
            "Epoch [96/100], Batch [75/346], Loss: 1.5879\n",
            "Epoch [96/100], Batch [76/346], Loss: 1.2277\n",
            "Epoch [96/100], Batch [77/346], Loss: 1.2439\n",
            "Epoch [96/100], Batch [78/346], Loss: 1.7800\n",
            "Epoch [96/100], Batch [79/346], Loss: 2.1041\n",
            "Epoch [96/100], Batch [80/346], Loss: 1.7212\n",
            "Epoch [96/100], Batch [81/346], Loss: 1.3522\n",
            "Epoch [96/100], Batch [82/346], Loss: 1.8822\n",
            "Epoch [96/100], Batch [83/346], Loss: 1.6077\n",
            "Epoch [96/100], Batch [84/346], Loss: 1.3619\n",
            "Epoch [96/100], Batch [85/346], Loss: 1.9908\n",
            "Epoch [96/100], Batch [86/346], Loss: 1.5870\n",
            "Epoch [96/100], Batch [87/346], Loss: 1.3365\n",
            "Epoch [96/100], Batch [88/346], Loss: 1.6894\n",
            "Epoch [96/100], Batch [89/346], Loss: 1.3086\n",
            "Epoch [96/100], Batch [90/346], Loss: 1.3271\n",
            "Epoch [96/100], Batch [91/346], Loss: 1.5608\n",
            "Epoch [96/100], Batch [92/346], Loss: 1.4675\n",
            "Epoch [96/100], Batch [93/346], Loss: 1.6162\n",
            "Epoch [96/100], Batch [94/346], Loss: 1.2117\n",
            "Epoch [96/100], Batch [95/346], Loss: 1.7388\n",
            "Epoch [96/100], Batch [96/346], Loss: 1.0484\n",
            "Epoch [96/100], Batch [97/346], Loss: 1.3131\n",
            "Epoch [96/100], Batch [98/346], Loss: 1.2071\n",
            "Epoch [96/100], Batch [99/346], Loss: 2.0367\n",
            "Epoch [96/100], Batch [100/346], Loss: 1.4426\n",
            "Epoch [96/100], Batch [101/346], Loss: 1.5470\n",
            "Epoch [96/100], Batch [102/346], Loss: 1.5544\n",
            "Epoch [96/100], Batch [103/346], Loss: 1.3217\n",
            "Epoch [96/100], Batch [104/346], Loss: 1.4823\n",
            "Epoch [96/100], Batch [105/346], Loss: 1.2040\n",
            "Epoch [96/100], Batch [106/346], Loss: 1.4810\n",
            "Epoch [96/100], Batch [107/346], Loss: 1.4375\n",
            "Epoch [96/100], Batch [108/346], Loss: 1.6979\n",
            "Epoch [96/100], Batch [109/346], Loss: 1.2838\n",
            "Epoch [96/100], Batch [110/346], Loss: 1.7389\n",
            "Epoch [96/100], Batch [111/346], Loss: 1.4672\n",
            "Epoch [96/100], Batch [112/346], Loss: 1.4055\n",
            "Epoch [96/100], Batch [113/346], Loss: 1.4608\n",
            "Epoch [96/100], Batch [114/346], Loss: 1.5882\n",
            "Epoch [96/100], Batch [115/346], Loss: 1.4922\n",
            "Epoch [96/100], Batch [116/346], Loss: 1.8466\n",
            "Epoch [96/100], Batch [117/346], Loss: 1.3708\n",
            "Epoch [96/100], Batch [118/346], Loss: 1.3836\n",
            "Epoch [96/100], Batch [119/346], Loss: 1.4133\n",
            "Epoch [96/100], Batch [120/346], Loss: 1.4241\n",
            "Epoch [96/100], Batch [121/346], Loss: 1.2278\n",
            "Epoch [96/100], Batch [122/346], Loss: 1.5477\n",
            "Epoch [96/100], Batch [123/346], Loss: 1.2458\n",
            "Epoch [96/100], Batch [124/346], Loss: 1.2217\n",
            "Epoch [96/100], Batch [125/346], Loss: 1.7439\n",
            "Epoch [96/100], Batch [126/346], Loss: 1.8821\n",
            "Epoch [96/100], Batch [127/346], Loss: 1.5570\n",
            "Epoch [96/100], Batch [128/346], Loss: 1.6569\n",
            "Epoch [96/100], Batch [129/346], Loss: 1.2965\n",
            "Epoch [96/100], Batch [130/346], Loss: 1.7371\n",
            "Epoch [96/100], Batch [131/346], Loss: 1.4555\n",
            "Epoch [96/100], Batch [132/346], Loss: 1.6954\n",
            "Epoch [96/100], Batch [133/346], Loss: 1.6014\n",
            "Epoch [96/100], Batch [134/346], Loss: 1.0787\n",
            "Epoch [96/100], Batch [135/346], Loss: 1.6908\n",
            "Epoch [96/100], Batch [136/346], Loss: 1.6920\n",
            "Epoch [96/100], Batch [137/346], Loss: 1.5304\n",
            "Epoch [96/100], Batch [138/346], Loss: 1.5958\n",
            "Epoch [96/100], Batch [139/346], Loss: 1.4100\n",
            "Epoch [96/100], Batch [140/346], Loss: 1.6750\n",
            "Epoch [96/100], Batch [141/346], Loss: 1.4659\n",
            "Epoch [96/100], Batch [142/346], Loss: 1.5015\n",
            "Epoch [96/100], Batch [143/346], Loss: 1.6707\n",
            "Epoch [96/100], Batch [144/346], Loss: 1.3939\n",
            "Epoch [96/100], Batch [145/346], Loss: 1.2306\n",
            "Epoch [96/100], Batch [146/346], Loss: 1.3803\n",
            "Epoch [96/100], Batch [147/346], Loss: 1.5034\n",
            "Epoch [96/100], Batch [148/346], Loss: 1.3745\n",
            "Epoch [96/100], Batch [149/346], Loss: 1.3970\n",
            "Epoch [96/100], Batch [150/346], Loss: 1.6759\n",
            "Epoch [96/100], Batch [151/346], Loss: 1.4362\n",
            "Epoch [96/100], Batch [152/346], Loss: 1.4225\n",
            "Epoch [96/100], Batch [153/346], Loss: 2.3988\n",
            "Epoch [96/100], Batch [154/346], Loss: 1.8407\n",
            "Epoch [96/100], Batch [155/346], Loss: 1.4529\n",
            "Epoch [96/100], Batch [156/346], Loss: 1.4903\n",
            "Epoch [96/100], Batch [157/346], Loss: 1.5104\n",
            "Epoch [96/100], Batch [158/346], Loss: 1.8158\n",
            "Epoch [96/100], Batch [159/346], Loss: 1.5548\n",
            "Epoch [96/100], Batch [160/346], Loss: 1.5571\n",
            "Epoch [96/100], Batch [161/346], Loss: 1.5657\n",
            "Epoch [96/100], Batch [162/346], Loss: 2.0183\n",
            "Epoch [96/100], Batch [163/346], Loss: 1.7430\n",
            "Epoch [96/100], Batch [164/346], Loss: 1.5410\n",
            "Epoch [96/100], Batch [165/346], Loss: 1.5857\n",
            "Epoch [96/100], Batch [166/346], Loss: 1.2414\n",
            "Epoch [96/100], Batch [167/346], Loss: 1.0787\n",
            "Epoch [96/100], Batch [168/346], Loss: 2.1833\n",
            "Epoch [96/100], Batch [169/346], Loss: 1.9215\n",
            "Epoch [96/100], Batch [170/346], Loss: 1.3549\n",
            "Epoch [96/100], Batch [171/346], Loss: 1.4634\n",
            "Epoch [96/100], Batch [172/346], Loss: 1.5245\n",
            "Epoch [96/100], Batch [173/346], Loss: 1.5295\n",
            "Epoch [96/100], Batch [174/346], Loss: 1.4928\n",
            "Epoch [96/100], Batch [175/346], Loss: 1.2681\n",
            "Epoch [96/100], Batch [176/346], Loss: 1.4216\n",
            "Epoch [96/100], Batch [177/346], Loss: 1.5171\n",
            "Epoch [96/100], Batch [178/346], Loss: 1.5597\n",
            "Epoch [96/100], Batch [179/346], Loss: 1.5802\n",
            "Epoch [96/100], Batch [180/346], Loss: 1.5358\n",
            "Epoch [96/100], Batch [181/346], Loss: 1.5378\n",
            "Epoch [96/100], Batch [182/346], Loss: 1.5747\n",
            "Epoch [96/100], Batch [183/346], Loss: 1.4578\n",
            "Epoch [96/100], Batch [184/346], Loss: 1.6674\n",
            "Epoch [96/100], Batch [185/346], Loss: 1.4487\n",
            "Epoch [96/100], Batch [186/346], Loss: 1.6211\n",
            "Epoch [96/100], Batch [187/346], Loss: 1.6161\n",
            "Epoch [96/100], Batch [188/346], Loss: 1.5625\n",
            "Epoch [96/100], Batch [189/346], Loss: 1.4335\n",
            "Epoch [96/100], Batch [190/346], Loss: 1.4232\n",
            "Epoch [96/100], Batch [191/346], Loss: 1.2402\n",
            "Epoch [96/100], Batch [192/346], Loss: 1.4467\n",
            "Epoch [96/100], Batch [193/346], Loss: 0.8337\n",
            "Epoch [96/100], Batch [194/346], Loss: 1.7588\n",
            "Epoch [96/100], Batch [195/346], Loss: 1.9769\n",
            "Epoch [96/100], Batch [196/346], Loss: 1.3625\n",
            "Epoch [96/100], Batch [197/346], Loss: 1.7753\n",
            "Epoch [96/100], Batch [198/346], Loss: 1.7708\n",
            "Epoch [96/100], Batch [199/346], Loss: 1.4497\n",
            "Epoch [96/100], Batch [200/346], Loss: 1.6374\n",
            "Epoch [96/100], Batch [201/346], Loss: 1.5128\n",
            "Epoch [96/100], Batch [202/346], Loss: 1.4015\n",
            "Epoch [96/100], Batch [203/346], Loss: 1.5785\n",
            "Epoch [96/100], Batch [204/346], Loss: 1.4302\n",
            "Epoch [96/100], Batch [205/346], Loss: 1.2814\n",
            "Epoch [96/100], Batch [206/346], Loss: 0.9406\n",
            "Epoch [96/100], Batch [207/346], Loss: 1.2675\n",
            "Epoch [96/100], Batch [208/346], Loss: 1.2487\n",
            "Epoch [96/100], Batch [209/346], Loss: 1.3650\n",
            "Epoch [96/100], Batch [210/346], Loss: 1.5141\n",
            "Epoch [96/100], Batch [211/346], Loss: 1.7363\n",
            "Epoch [96/100], Batch [212/346], Loss: 1.7112\n",
            "Epoch [96/100], Batch [213/346], Loss: 1.6847\n",
            "Epoch [96/100], Batch [214/346], Loss: 1.2809\n",
            "Epoch [96/100], Batch [215/346], Loss: 1.3627\n",
            "Epoch [96/100], Batch [216/346], Loss: 1.0786\n",
            "Epoch [96/100], Batch [217/346], Loss: 1.5989\n",
            "Epoch [96/100], Batch [218/346], Loss: 1.2018\n",
            "Epoch [96/100], Batch [219/346], Loss: 1.8725\n",
            "Epoch [96/100], Batch [220/346], Loss: 1.7963\n",
            "Epoch [96/100], Batch [221/346], Loss: 1.3929\n",
            "Epoch [96/100], Batch [222/346], Loss: 1.5276\n",
            "Epoch [96/100], Batch [223/346], Loss: 1.8983\n",
            "Epoch [96/100], Batch [224/346], Loss: 1.6882\n",
            "Epoch [96/100], Batch [225/346], Loss: 1.1823\n",
            "Epoch [96/100], Batch [226/346], Loss: 1.4729\n",
            "Epoch [96/100], Batch [227/346], Loss: 1.9390\n",
            "Epoch [96/100], Batch [228/346], Loss: 1.4387\n",
            "Epoch [96/100], Batch [229/346], Loss: 1.5332\n",
            "Epoch [96/100], Batch [230/346], Loss: 1.7782\n",
            "Epoch [96/100], Batch [231/346], Loss: 1.5220\n",
            "Epoch [96/100], Batch [232/346], Loss: 1.6043\n",
            "Epoch [96/100], Batch [233/346], Loss: 1.3098\n",
            "Epoch [96/100], Batch [234/346], Loss: 1.7330\n",
            "Epoch [96/100], Batch [235/346], Loss: 1.3200\n",
            "Epoch [96/100], Batch [236/346], Loss: 1.5401\n",
            "Epoch [96/100], Batch [237/346], Loss: 1.5561\n",
            "Epoch [96/100], Batch [238/346], Loss: 1.5446\n",
            "Epoch [96/100], Batch [239/346], Loss: 1.7968\n",
            "Epoch [96/100], Batch [240/346], Loss: 1.4406\n",
            "Epoch [96/100], Batch [241/346], Loss: 1.4814\n",
            "Epoch [96/100], Batch [242/346], Loss: 1.5065\n",
            "Epoch [96/100], Batch [243/346], Loss: 1.8324\n",
            "Epoch [96/100], Batch [244/346], Loss: 1.5026\n",
            "Epoch [96/100], Batch [245/346], Loss: 1.8450\n",
            "Epoch [96/100], Batch [246/346], Loss: 1.5042\n",
            "Epoch [96/100], Batch [247/346], Loss: 1.5268\n",
            "Epoch [96/100], Batch [248/346], Loss: 1.6989\n",
            "Epoch [96/100], Batch [249/346], Loss: 1.4452\n",
            "Epoch [96/100], Batch [250/346], Loss: 1.7287\n",
            "Epoch [96/100], Batch [251/346], Loss: 1.7202\n",
            "Epoch [96/100], Batch [252/346], Loss: 1.5330\n",
            "Epoch [96/100], Batch [253/346], Loss: 1.5856\n",
            "Epoch [96/100], Batch [254/346], Loss: 2.2112\n",
            "Epoch [96/100], Batch [255/346], Loss: 1.5084\n",
            "Epoch [96/100], Batch [256/346], Loss: 1.4630\n",
            "Epoch [96/100], Batch [257/346], Loss: 1.2104\n",
            "Epoch [96/100], Batch [258/346], Loss: 1.7903\n",
            "Epoch [96/100], Batch [259/346], Loss: 1.3362\n",
            "Epoch [96/100], Batch [260/346], Loss: 1.2841\n",
            "Epoch [96/100], Batch [261/346], Loss: 1.2794\n",
            "Epoch [96/100], Batch [262/346], Loss: 1.7100\n",
            "Epoch [96/100], Batch [263/346], Loss: 1.7422\n",
            "Epoch [96/100], Batch [264/346], Loss: 1.4341\n",
            "Epoch [96/100], Batch [265/346], Loss: 1.4481\n",
            "Epoch [96/100], Batch [266/346], Loss: 1.5794\n",
            "Epoch [96/100], Batch [267/346], Loss: 1.4195\n",
            "Epoch [96/100], Batch [268/346], Loss: 1.0897\n",
            "Epoch [96/100], Batch [269/346], Loss: 1.2624\n",
            "Epoch [96/100], Batch [270/346], Loss: 1.7640\n",
            "Epoch [96/100], Batch [271/346], Loss: 1.4700\n",
            "Epoch [96/100], Batch [272/346], Loss: 1.8489\n",
            "Epoch [96/100], Batch [273/346], Loss: 1.1793\n",
            "Epoch [96/100], Batch [274/346], Loss: 1.9978\n",
            "Epoch [96/100], Batch [275/346], Loss: 1.6335\n",
            "Epoch [96/100], Batch [276/346], Loss: 1.7110\n",
            "Epoch [96/100], Batch [277/346], Loss: 1.4687\n",
            "Epoch [96/100], Batch [278/346], Loss: 1.5386\n",
            "Epoch [96/100], Batch [279/346], Loss: 1.6417\n",
            "Epoch [96/100], Batch [280/346], Loss: 1.2393\n",
            "Epoch [96/100], Batch [281/346], Loss: 1.3681\n",
            "Epoch [96/100], Batch [282/346], Loss: 1.3777\n",
            "Epoch [96/100], Batch [283/346], Loss: 1.5417\n",
            "Epoch [96/100], Batch [284/346], Loss: 1.8304\n",
            "Epoch [96/100], Batch [285/346], Loss: 1.7569\n",
            "Epoch [96/100], Batch [286/346], Loss: 1.5343\n",
            "Epoch [96/100], Batch [287/346], Loss: 1.5245\n",
            "Epoch [96/100], Batch [288/346], Loss: 1.3886\n",
            "Epoch [96/100], Batch [289/346], Loss: 1.6798\n",
            "Epoch [96/100], Batch [290/346], Loss: 1.4950\n",
            "Epoch [96/100], Batch [291/346], Loss: 1.3385\n",
            "Epoch [96/100], Batch [292/346], Loss: 1.5771\n",
            "Epoch [96/100], Batch [293/346], Loss: 1.4575\n",
            "Epoch [96/100], Batch [294/346], Loss: 1.4034\n",
            "Epoch [96/100], Batch [295/346], Loss: 1.4887\n",
            "Epoch [96/100], Batch [296/346], Loss: 1.5055\n",
            "Epoch [96/100], Batch [297/346], Loss: 1.8809\n",
            "Epoch [96/100], Batch [298/346], Loss: 1.8023\n",
            "Epoch [96/100], Batch [299/346], Loss: 1.3530\n",
            "Epoch [96/100], Batch [300/346], Loss: 2.2164\n",
            "Epoch [96/100], Batch [301/346], Loss: 1.6701\n",
            "Epoch [96/100], Batch [302/346], Loss: 1.4694\n",
            "Epoch [96/100], Batch [303/346], Loss: 1.4331\n",
            "Epoch [96/100], Batch [304/346], Loss: 1.7257\n",
            "Epoch [96/100], Batch [305/346], Loss: 1.4383\n",
            "Epoch [96/100], Batch [306/346], Loss: 1.6196\n",
            "Epoch [96/100], Batch [307/346], Loss: 1.6811\n",
            "Epoch [96/100], Batch [308/346], Loss: 1.3248\n",
            "Epoch [96/100], Batch [309/346], Loss: 2.0082\n",
            "Epoch [96/100], Batch [310/346], Loss: 1.4618\n",
            "Epoch [96/100], Batch [311/346], Loss: 1.5097\n",
            "Epoch [96/100], Batch [312/346], Loss: 1.5856\n",
            "Epoch [96/100], Batch [313/346], Loss: 1.8501\n",
            "Epoch [96/100], Batch [314/346], Loss: 1.5351\n",
            "Epoch [96/100], Batch [315/346], Loss: 1.2546\n",
            "Epoch [96/100], Batch [316/346], Loss: 1.3481\n",
            "Epoch [96/100], Batch [317/346], Loss: 1.0902\n",
            "Epoch [96/100], Batch [318/346], Loss: 1.4753\n",
            "Epoch [96/100], Batch [319/346], Loss: 1.2570\n",
            "Epoch [96/100], Batch [320/346], Loss: 1.3655\n",
            "Epoch [96/100], Batch [321/346], Loss: 1.4370\n",
            "Epoch [96/100], Batch [322/346], Loss: 1.5295\n",
            "Epoch [96/100], Batch [323/346], Loss: 1.4290\n",
            "Epoch [96/100], Batch [324/346], Loss: 1.5253\n",
            "Epoch [96/100], Batch [325/346], Loss: 1.8668\n",
            "Epoch [96/100], Batch [326/346], Loss: 1.6589\n",
            "Epoch [96/100], Batch [327/346], Loss: 1.3851\n",
            "Epoch [96/100], Batch [328/346], Loss: 1.4527\n",
            "Epoch [96/100], Batch [329/346], Loss: 1.4856\n",
            "Epoch [96/100], Batch [330/346], Loss: 1.5060\n",
            "Epoch [96/100], Batch [331/346], Loss: 1.4203\n",
            "Epoch [96/100], Batch [332/346], Loss: 1.1739\n",
            "Epoch [96/100], Batch [333/346], Loss: 1.3006\n",
            "Epoch [96/100], Batch [334/346], Loss: 1.5156\n",
            "Epoch [96/100], Batch [335/346], Loss: 1.5535\n",
            "Epoch [96/100], Batch [336/346], Loss: 1.8757\n",
            "Epoch [96/100], Batch [337/346], Loss: 1.7337\n",
            "Epoch [96/100], Batch [338/346], Loss: 1.6058\n",
            "Epoch [96/100], Batch [339/346], Loss: 1.5771\n",
            "Epoch [96/100], Batch [340/346], Loss: 1.6044\n",
            "Epoch [96/100], Batch [341/346], Loss: 1.3504\n",
            "Epoch [96/100], Batch [342/346], Loss: 1.2195\n",
            "Epoch [96/100], Batch [343/346], Loss: 1.6413\n",
            "Epoch [96/100], Batch [344/346], Loss: 1.2849\n",
            "Epoch [96/100], Batch [345/346], Loss: 1.6149\n",
            "Epoch [96/100], Batch [346/346], Loss: 1.7887\n",
            "Epoch [97/100], Batch [1/346], Loss: 1.0981\n",
            "Epoch [97/100], Batch [2/346], Loss: 1.5173\n",
            "Epoch [97/100], Batch [3/346], Loss: 1.9917\n",
            "Epoch [97/100], Batch [4/346], Loss: 1.4808\n",
            "Epoch [97/100], Batch [5/346], Loss: 1.6005\n",
            "Epoch [97/100], Batch [6/346], Loss: 1.2234\n",
            "Epoch [97/100], Batch [7/346], Loss: 1.4403\n",
            "Epoch [97/100], Batch [8/346], Loss: 1.2645\n",
            "Epoch [97/100], Batch [9/346], Loss: 1.5601\n",
            "Epoch [97/100], Batch [10/346], Loss: 1.4309\n",
            "Epoch [97/100], Batch [11/346], Loss: 1.7273\n",
            "Epoch [97/100], Batch [12/346], Loss: 1.3152\n",
            "Epoch [97/100], Batch [13/346], Loss: 1.6879\n",
            "Epoch [97/100], Batch [14/346], Loss: 1.2030\n",
            "Epoch [97/100], Batch [15/346], Loss: 1.5951\n",
            "Epoch [97/100], Batch [16/346], Loss: 1.7968\n",
            "Epoch [97/100], Batch [17/346], Loss: 1.6159\n",
            "Epoch [97/100], Batch [18/346], Loss: 1.5281\n",
            "Epoch [97/100], Batch [19/346], Loss: 1.3030\n",
            "Epoch [97/100], Batch [20/346], Loss: 1.4814\n",
            "Epoch [97/100], Batch [21/346], Loss: 1.4611\n",
            "Epoch [97/100], Batch [22/346], Loss: 1.6522\n",
            "Epoch [97/100], Batch [23/346], Loss: 1.3614\n",
            "Epoch [97/100], Batch [24/346], Loss: 1.7459\n",
            "Epoch [97/100], Batch [25/346], Loss: 1.5285\n",
            "Epoch [97/100], Batch [26/346], Loss: 1.4944\n",
            "Epoch [97/100], Batch [27/346], Loss: 1.3776\n",
            "Epoch [97/100], Batch [28/346], Loss: 1.5536\n",
            "Epoch [97/100], Batch [29/346], Loss: 1.4198\n",
            "Epoch [97/100], Batch [30/346], Loss: 1.5506\n",
            "Epoch [97/100], Batch [31/346], Loss: 1.5021\n",
            "Epoch [97/100], Batch [32/346], Loss: 1.3637\n",
            "Epoch [97/100], Batch [33/346], Loss: 1.1820\n",
            "Epoch [97/100], Batch [34/346], Loss: 1.4849\n",
            "Epoch [97/100], Batch [35/346], Loss: 1.4023\n",
            "Epoch [97/100], Batch [36/346], Loss: 1.2211\n",
            "Epoch [97/100], Batch [37/346], Loss: 1.3458\n",
            "Epoch [97/100], Batch [38/346], Loss: 1.2109\n",
            "Epoch [97/100], Batch [39/346], Loss: 1.5658\n",
            "Epoch [97/100], Batch [40/346], Loss: 1.4170\n",
            "Epoch [97/100], Batch [41/346], Loss: 1.3606\n",
            "Epoch [97/100], Batch [42/346], Loss: 1.4831\n",
            "Epoch [97/100], Batch [43/346], Loss: 1.3684\n",
            "Epoch [97/100], Batch [44/346], Loss: 1.6480\n",
            "Epoch [97/100], Batch [45/346], Loss: 1.2342\n",
            "Epoch [97/100], Batch [46/346], Loss: 1.7582\n",
            "Epoch [97/100], Batch [47/346], Loss: 1.4193\n",
            "Epoch [97/100], Batch [48/346], Loss: 1.2432\n",
            "Epoch [97/100], Batch [49/346], Loss: 1.3379\n",
            "Epoch [97/100], Batch [50/346], Loss: 1.4175\n",
            "Epoch [97/100], Batch [51/346], Loss: 1.0793\n",
            "Epoch [97/100], Batch [52/346], Loss: 1.5234\n",
            "Epoch [97/100], Batch [53/346], Loss: 1.4066\n",
            "Epoch [97/100], Batch [54/346], Loss: 1.4755\n",
            "Epoch [97/100], Batch [55/346], Loss: 1.4457\n",
            "Epoch [97/100], Batch [56/346], Loss: 1.6079\n",
            "Epoch [97/100], Batch [57/346], Loss: 1.7869\n",
            "Epoch [97/100], Batch [58/346], Loss: 1.3519\n",
            "Epoch [97/100], Batch [59/346], Loss: 1.3032\n",
            "Epoch [97/100], Batch [60/346], Loss: 1.6064\n",
            "Epoch [97/100], Batch [61/346], Loss: 1.7914\n",
            "Epoch [97/100], Batch [62/346], Loss: 1.4772\n",
            "Epoch [97/100], Batch [63/346], Loss: 1.6538\n",
            "Epoch [97/100], Batch [64/346], Loss: 1.6964\n",
            "Epoch [97/100], Batch [65/346], Loss: 1.2056\n",
            "Epoch [97/100], Batch [66/346], Loss: 1.8626\n",
            "Epoch [97/100], Batch [67/346], Loss: 1.7557\n",
            "Epoch [97/100], Batch [68/346], Loss: 1.6586\n",
            "Epoch [97/100], Batch [69/346], Loss: 1.7882\n",
            "Epoch [97/100], Batch [70/346], Loss: 1.7954\n",
            "Epoch [97/100], Batch [71/346], Loss: 1.1916\n",
            "Epoch [97/100], Batch [72/346], Loss: 1.6537\n",
            "Epoch [97/100], Batch [73/346], Loss: 1.7112\n",
            "Epoch [97/100], Batch [74/346], Loss: 1.8438\n",
            "Epoch [97/100], Batch [75/346], Loss: 1.5063\n",
            "Epoch [97/100], Batch [76/346], Loss: 1.5517\n",
            "Epoch [97/100], Batch [77/346], Loss: 1.6506\n",
            "Epoch [97/100], Batch [78/346], Loss: 1.3596\n",
            "Epoch [97/100], Batch [79/346], Loss: 1.7850\n",
            "Epoch [97/100], Batch [80/346], Loss: 1.3820\n",
            "Epoch [97/100], Batch [81/346], Loss: 1.2854\n",
            "Epoch [97/100], Batch [82/346], Loss: 1.2693\n",
            "Epoch [97/100], Batch [83/346], Loss: 1.7996\n",
            "Epoch [97/100], Batch [84/346], Loss: 1.7763\n",
            "Epoch [97/100], Batch [85/346], Loss: 1.1506\n",
            "Epoch [97/100], Batch [86/346], Loss: 1.5627\n",
            "Epoch [97/100], Batch [87/346], Loss: 1.6323\n",
            "Epoch [97/100], Batch [88/346], Loss: 1.5035\n",
            "Epoch [97/100], Batch [89/346], Loss: 1.5674\n",
            "Epoch [97/100], Batch [90/346], Loss: 1.6665\n",
            "Epoch [97/100], Batch [91/346], Loss: 1.8477\n",
            "Epoch [97/100], Batch [92/346], Loss: 1.4352\n",
            "Epoch [97/100], Batch [93/346], Loss: 1.9181\n",
            "Epoch [97/100], Batch [94/346], Loss: 1.2689\n",
            "Epoch [97/100], Batch [95/346], Loss: 1.6909\n",
            "Epoch [97/100], Batch [96/346], Loss: 1.5295\n",
            "Epoch [97/100], Batch [97/346], Loss: 1.6874\n",
            "Epoch [97/100], Batch [98/346], Loss: 1.9910\n",
            "Epoch [97/100], Batch [99/346], Loss: 1.6568\n",
            "Epoch [97/100], Batch [100/346], Loss: 1.4100\n",
            "Epoch [97/100], Batch [101/346], Loss: 1.5381\n",
            "Epoch [97/100], Batch [102/346], Loss: 1.6322\n",
            "Epoch [97/100], Batch [103/346], Loss: 1.7172\n",
            "Epoch [97/100], Batch [104/346], Loss: 1.4948\n",
            "Epoch [97/100], Batch [105/346], Loss: 1.3973\n",
            "Epoch [97/100], Batch [106/346], Loss: 1.4101\n",
            "Epoch [97/100], Batch [107/346], Loss: 1.8149\n",
            "Epoch [97/100], Batch [108/346], Loss: 1.7344\n",
            "Epoch [97/100], Batch [109/346], Loss: 1.4894\n",
            "Epoch [97/100], Batch [110/346], Loss: 1.4813\n",
            "Epoch [97/100], Batch [111/346], Loss: 1.6893\n",
            "Epoch [97/100], Batch [112/346], Loss: 1.7344\n",
            "Epoch [97/100], Batch [113/346], Loss: 1.3589\n",
            "Epoch [97/100], Batch [114/346], Loss: 1.7923\n",
            "Epoch [97/100], Batch [115/346], Loss: 2.0858\n",
            "Epoch [97/100], Batch [116/346], Loss: 1.6044\n",
            "Epoch [97/100], Batch [117/346], Loss: 1.5328\n",
            "Epoch [97/100], Batch [118/346], Loss: 1.5764\n",
            "Epoch [97/100], Batch [119/346], Loss: 1.4599\n",
            "Epoch [97/100], Batch [120/346], Loss: 1.4357\n",
            "Epoch [97/100], Batch [121/346], Loss: 1.5321\n",
            "Epoch [97/100], Batch [122/346], Loss: 1.5008\n",
            "Epoch [97/100], Batch [123/346], Loss: 1.2533\n",
            "Epoch [97/100], Batch [124/346], Loss: 1.6377\n",
            "Epoch [97/100], Batch [125/346], Loss: 1.7299\n",
            "Epoch [97/100], Batch [126/346], Loss: 1.4875\n",
            "Epoch [97/100], Batch [127/346], Loss: 1.5165\n",
            "Epoch [97/100], Batch [128/346], Loss: 1.5870\n",
            "Epoch [97/100], Batch [129/346], Loss: 1.2189\n",
            "Epoch [97/100], Batch [130/346], Loss: 1.7912\n",
            "Epoch [97/100], Batch [131/346], Loss: 1.6082\n",
            "Epoch [97/100], Batch [132/346], Loss: 1.5052\n",
            "Epoch [97/100], Batch [133/346], Loss: 1.4101\n",
            "Epoch [97/100], Batch [134/346], Loss: 1.3996\n",
            "Epoch [97/100], Batch [135/346], Loss: 1.5942\n",
            "Epoch [97/100], Batch [136/346], Loss: 1.5640\n",
            "Epoch [97/100], Batch [137/346], Loss: 1.5154\n",
            "Epoch [97/100], Batch [138/346], Loss: 1.5116\n",
            "Epoch [97/100], Batch [139/346], Loss: 1.2419\n",
            "Epoch [97/100], Batch [140/346], Loss: 1.6898\n",
            "Epoch [97/100], Batch [141/346], Loss: 2.0433\n",
            "Epoch [97/100], Batch [142/346], Loss: 1.3078\n",
            "Epoch [97/100], Batch [143/346], Loss: 1.6509\n",
            "Epoch [97/100], Batch [144/346], Loss: 1.7071\n",
            "Epoch [97/100], Batch [145/346], Loss: 1.6685\n",
            "Epoch [97/100], Batch [146/346], Loss: 2.2128\n",
            "Epoch [97/100], Batch [147/346], Loss: 1.5871\n",
            "Epoch [97/100], Batch [148/346], Loss: 1.6878\n",
            "Epoch [97/100], Batch [149/346], Loss: 1.9892\n",
            "Epoch [97/100], Batch [150/346], Loss: 1.6256\n",
            "Epoch [97/100], Batch [151/346], Loss: 1.4567\n",
            "Epoch [97/100], Batch [152/346], Loss: 1.6380\n",
            "Epoch [97/100], Batch [153/346], Loss: 1.6264\n",
            "Epoch [97/100], Batch [154/346], Loss: 1.5593\n",
            "Epoch [97/100], Batch [155/346], Loss: 1.4045\n",
            "Epoch [97/100], Batch [156/346], Loss: 1.8893\n",
            "Epoch [97/100], Batch [157/346], Loss: 1.5299\n",
            "Epoch [97/100], Batch [158/346], Loss: 1.5503\n",
            "Epoch [97/100], Batch [159/346], Loss: 1.4377\n",
            "Epoch [97/100], Batch [160/346], Loss: 1.6369\n",
            "Epoch [97/100], Batch [161/346], Loss: 1.5237\n",
            "Epoch [97/100], Batch [162/346], Loss: 1.1380\n",
            "Epoch [97/100], Batch [163/346], Loss: 1.1098\n",
            "Epoch [97/100], Batch [164/346], Loss: 1.3876\n",
            "Epoch [97/100], Batch [165/346], Loss: 1.3963\n",
            "Epoch [97/100], Batch [166/346], Loss: 1.3654\n",
            "Epoch [97/100], Batch [167/346], Loss: 1.3377\n",
            "Epoch [97/100], Batch [168/346], Loss: 1.8316\n",
            "Epoch [97/100], Batch [169/346], Loss: 1.6528\n",
            "Epoch [97/100], Batch [170/346], Loss: 1.2270\n",
            "Epoch [97/100], Batch [171/346], Loss: 1.2854\n",
            "Epoch [97/100], Batch [172/346], Loss: 1.5164\n",
            "Epoch [97/100], Batch [173/346], Loss: 1.3824\n",
            "Epoch [97/100], Batch [174/346], Loss: 2.0115\n",
            "Epoch [97/100], Batch [175/346], Loss: 1.7805\n",
            "Epoch [97/100], Batch [176/346], Loss: 1.5341\n",
            "Epoch [97/100], Batch [177/346], Loss: 1.4523\n",
            "Epoch [97/100], Batch [178/346], Loss: 1.7567\n",
            "Epoch [97/100], Batch [179/346], Loss: 1.6290\n",
            "Epoch [97/100], Batch [180/346], Loss: 1.5396\n",
            "Epoch [97/100], Batch [181/346], Loss: 1.8215\n",
            "Epoch [97/100], Batch [182/346], Loss: 1.0532\n",
            "Epoch [97/100], Batch [183/346], Loss: 1.4224\n",
            "Epoch [97/100], Batch [184/346], Loss: 1.4468\n",
            "Epoch [97/100], Batch [185/346], Loss: 1.4636\n",
            "Epoch [97/100], Batch [186/346], Loss: 1.7783\n",
            "Epoch [97/100], Batch [187/346], Loss: 1.7950\n",
            "Epoch [97/100], Batch [188/346], Loss: 1.5825\n",
            "Epoch [97/100], Batch [189/346], Loss: 1.6944\n",
            "Epoch [97/100], Batch [190/346], Loss: 1.7098\n",
            "Epoch [97/100], Batch [191/346], Loss: 1.4407\n",
            "Epoch [97/100], Batch [192/346], Loss: 1.6206\n",
            "Epoch [97/100], Batch [193/346], Loss: 1.5850\n",
            "Epoch [97/100], Batch [194/346], Loss: 1.9993\n",
            "Epoch [97/100], Batch [195/346], Loss: 1.6256\n",
            "Epoch [97/100], Batch [196/346], Loss: 1.2819\n",
            "Epoch [97/100], Batch [197/346], Loss: 1.9378\n",
            "Epoch [97/100], Batch [198/346], Loss: 1.2252\n",
            "Epoch [97/100], Batch [199/346], Loss: 1.3265\n",
            "Epoch [97/100], Batch [200/346], Loss: 1.6636\n",
            "Epoch [97/100], Batch [201/346], Loss: 1.6053\n",
            "Epoch [97/100], Batch [202/346], Loss: 1.3817\n",
            "Epoch [97/100], Batch [203/346], Loss: 2.0202\n",
            "Epoch [97/100], Batch [204/346], Loss: 2.1502\n",
            "Epoch [97/100], Batch [205/346], Loss: 1.8767\n",
            "Epoch [97/100], Batch [206/346], Loss: 1.4038\n",
            "Epoch [97/100], Batch [207/346], Loss: 1.2445\n",
            "Epoch [97/100], Batch [208/346], Loss: 1.5651\n",
            "Epoch [97/100], Batch [209/346], Loss: 1.5227\n",
            "Epoch [97/100], Batch [210/346], Loss: 1.6087\n",
            "Epoch [97/100], Batch [211/346], Loss: 1.5238\n",
            "Epoch [97/100], Batch [212/346], Loss: 2.0215\n",
            "Epoch [97/100], Batch [213/346], Loss: 1.8327\n",
            "Epoch [97/100], Batch [214/346], Loss: 1.6243\n",
            "Epoch [97/100], Batch [215/346], Loss: 1.4042\n",
            "Epoch [97/100], Batch [216/346], Loss: 1.3019\n",
            "Epoch [97/100], Batch [217/346], Loss: 1.3911\n",
            "Epoch [97/100], Batch [218/346], Loss: 1.6108\n",
            "Epoch [97/100], Batch [219/346], Loss: 1.6058\n",
            "Epoch [97/100], Batch [220/346], Loss: 1.2577\n",
            "Epoch [97/100], Batch [221/346], Loss: 1.4717\n",
            "Epoch [97/100], Batch [222/346], Loss: 1.3366\n",
            "Epoch [97/100], Batch [223/346], Loss: 1.5553\n",
            "Epoch [97/100], Batch [224/346], Loss: 1.4006\n",
            "Epoch [97/100], Batch [225/346], Loss: 1.5581\n",
            "Epoch [97/100], Batch [226/346], Loss: 1.4701\n",
            "Epoch [97/100], Batch [227/346], Loss: 1.3168\n",
            "Epoch [97/100], Batch [228/346], Loss: 1.3448\n",
            "Epoch [97/100], Batch [229/346], Loss: 1.4384\n",
            "Epoch [97/100], Batch [230/346], Loss: 1.2390\n",
            "Epoch [97/100], Batch [231/346], Loss: 1.8809\n",
            "Epoch [97/100], Batch [232/346], Loss: 1.3365\n",
            "Epoch [97/100], Batch [233/346], Loss: 1.3007\n",
            "Epoch [97/100], Batch [234/346], Loss: 1.2501\n",
            "Epoch [97/100], Batch [235/346], Loss: 1.3435\n",
            "Epoch [97/100], Batch [236/346], Loss: 1.3715\n",
            "Epoch [97/100], Batch [237/346], Loss: 1.3166\n",
            "Epoch [97/100], Batch [238/346], Loss: 1.4436\n",
            "Epoch [97/100], Batch [239/346], Loss: 1.5975\n",
            "Epoch [97/100], Batch [240/346], Loss: 1.3373\n",
            "Epoch [97/100], Batch [241/346], Loss: 1.4635\n",
            "Epoch [97/100], Batch [242/346], Loss: 1.8292\n",
            "Epoch [97/100], Batch [243/346], Loss: 1.3829\n",
            "Epoch [97/100], Batch [244/346], Loss: 1.3930\n",
            "Epoch [97/100], Batch [245/346], Loss: 1.3990\n",
            "Epoch [97/100], Batch [246/346], Loss: 1.9026\n",
            "Epoch [97/100], Batch [247/346], Loss: 1.4634\n",
            "Epoch [97/100], Batch [248/346], Loss: 1.5377\n",
            "Epoch [97/100], Batch [249/346], Loss: 1.3785\n",
            "Epoch [97/100], Batch [250/346], Loss: 1.5502\n",
            "Epoch [97/100], Batch [251/346], Loss: 1.6309\n",
            "Epoch [97/100], Batch [252/346], Loss: 1.2939\n",
            "Epoch [97/100], Batch [253/346], Loss: 2.0598\n",
            "Epoch [97/100], Batch [254/346], Loss: 1.4059\n",
            "Epoch [97/100], Batch [255/346], Loss: 1.6794\n",
            "Epoch [97/100], Batch [256/346], Loss: 1.5401\n",
            "Epoch [97/100], Batch [257/346], Loss: 1.2120\n",
            "Epoch [97/100], Batch [258/346], Loss: 1.4229\n",
            "Epoch [97/100], Batch [259/346], Loss: 1.6571\n",
            "Epoch [97/100], Batch [260/346], Loss: 1.7149\n",
            "Epoch [97/100], Batch [261/346], Loss: 1.8567\n",
            "Epoch [97/100], Batch [262/346], Loss: 1.2122\n",
            "Epoch [97/100], Batch [263/346], Loss: 1.2886\n",
            "Epoch [97/100], Batch [264/346], Loss: 1.5140\n",
            "Epoch [97/100], Batch [265/346], Loss: 1.2219\n",
            "Epoch [97/100], Batch [266/346], Loss: 1.6404\n",
            "Epoch [97/100], Batch [267/346], Loss: 1.6575\n",
            "Epoch [97/100], Batch [268/346], Loss: 1.3630\n",
            "Epoch [97/100], Batch [269/346], Loss: 1.8246\n",
            "Epoch [97/100], Batch [270/346], Loss: 1.6671\n",
            "Epoch [97/100], Batch [271/346], Loss: 1.7138\n",
            "Epoch [97/100], Batch [272/346], Loss: 1.5623\n",
            "Epoch [97/100], Batch [273/346], Loss: 1.4967\n",
            "Epoch [97/100], Batch [274/346], Loss: 1.5843\n",
            "Epoch [97/100], Batch [275/346], Loss: 1.5924\n",
            "Epoch [97/100], Batch [276/346], Loss: 1.4881\n",
            "Epoch [97/100], Batch [277/346], Loss: 1.7666\n",
            "Epoch [97/100], Batch [278/346], Loss: 1.4539\n",
            "Epoch [97/100], Batch [279/346], Loss: 1.6503\n",
            "Epoch [97/100], Batch [280/346], Loss: 1.7268\n",
            "Epoch [97/100], Batch [281/346], Loss: 1.5338\n",
            "Epoch [97/100], Batch [282/346], Loss: 1.7184\n",
            "Epoch [97/100], Batch [283/346], Loss: 1.5161\n",
            "Epoch [97/100], Batch [284/346], Loss: 1.4302\n",
            "Epoch [97/100], Batch [285/346], Loss: 1.5718\n",
            "Epoch [97/100], Batch [286/346], Loss: 1.3798\n",
            "Epoch [97/100], Batch [287/346], Loss: 1.6810\n",
            "Epoch [97/100], Batch [288/346], Loss: 1.3694\n",
            "Epoch [97/100], Batch [289/346], Loss: 1.6843\n",
            "Epoch [97/100], Batch [290/346], Loss: 1.5263\n",
            "Epoch [97/100], Batch [291/346], Loss: 1.6421\n",
            "Epoch [97/100], Batch [292/346], Loss: 1.6361\n",
            "Epoch [97/100], Batch [293/346], Loss: 1.5545\n",
            "Epoch [97/100], Batch [294/346], Loss: 1.5705\n",
            "Epoch [97/100], Batch [295/346], Loss: 1.4492\n",
            "Epoch [97/100], Batch [296/346], Loss: 1.6397\n",
            "Epoch [97/100], Batch [297/346], Loss: 1.5572\n",
            "Epoch [97/100], Batch [298/346], Loss: 1.3168\n",
            "Epoch [97/100], Batch [299/346], Loss: 1.3503\n",
            "Epoch [97/100], Batch [300/346], Loss: 1.4345\n",
            "Epoch [97/100], Batch [301/346], Loss: 1.2303\n",
            "Epoch [97/100], Batch [302/346], Loss: 1.5807\n",
            "Epoch [97/100], Batch [303/346], Loss: 1.1383\n",
            "Epoch [97/100], Batch [304/346], Loss: 1.5769\n",
            "Epoch [97/100], Batch [305/346], Loss: 1.5719\n",
            "Epoch [97/100], Batch [306/346], Loss: 1.2734\n",
            "Epoch [97/100], Batch [307/346], Loss: 1.5680\n",
            "Epoch [97/100], Batch [308/346], Loss: 1.6876\n",
            "Epoch [97/100], Batch [309/346], Loss: 1.4607\n",
            "Epoch [97/100], Batch [310/346], Loss: 1.7302\n",
            "Epoch [97/100], Batch [311/346], Loss: 1.2817\n",
            "Epoch [97/100], Batch [312/346], Loss: 1.6754\n",
            "Epoch [97/100], Batch [313/346], Loss: 1.7940\n",
            "Epoch [97/100], Batch [314/346], Loss: 1.6160\n",
            "Epoch [97/100], Batch [315/346], Loss: 1.2825\n",
            "Epoch [97/100], Batch [316/346], Loss: 1.4257\n",
            "Epoch [97/100], Batch [317/346], Loss: 1.5078\n",
            "Epoch [97/100], Batch [318/346], Loss: 1.3837\n",
            "Epoch [97/100], Batch [319/346], Loss: 1.2276\n",
            "Epoch [97/100], Batch [320/346], Loss: 1.7509\n",
            "Epoch [97/100], Batch [321/346], Loss: 1.2977\n",
            "Epoch [97/100], Batch [322/346], Loss: 1.1750\n",
            "Epoch [97/100], Batch [323/346], Loss: 1.3530\n",
            "Epoch [97/100], Batch [324/346], Loss: 1.4120\n",
            "Epoch [97/100], Batch [325/346], Loss: 1.2488\n",
            "Epoch [97/100], Batch [326/346], Loss: 1.5126\n",
            "Epoch [97/100], Batch [327/346], Loss: 1.2533\n",
            "Epoch [97/100], Batch [328/346], Loss: 1.5387\n",
            "Epoch [97/100], Batch [329/346], Loss: 1.5767\n",
            "Epoch [97/100], Batch [330/346], Loss: 1.3718\n",
            "Epoch [97/100], Batch [331/346], Loss: 1.4852\n",
            "Epoch [97/100], Batch [332/346], Loss: 1.2561\n",
            "Epoch [97/100], Batch [333/346], Loss: 1.2944\n",
            "Epoch [97/100], Batch [334/346], Loss: 1.5762\n",
            "Epoch [97/100], Batch [335/346], Loss: 1.7816\n",
            "Epoch [97/100], Batch [336/346], Loss: 1.0671\n",
            "Epoch [97/100], Batch [337/346], Loss: 1.4408\n",
            "Epoch [97/100], Batch [338/346], Loss: 1.2961\n",
            "Epoch [97/100], Batch [339/346], Loss: 2.1958\n",
            "Epoch [97/100], Batch [340/346], Loss: 1.4110\n",
            "Epoch [97/100], Batch [341/346], Loss: 1.1479\n",
            "Epoch [97/100], Batch [342/346], Loss: 1.3450\n",
            "Epoch [97/100], Batch [343/346], Loss: 1.6014\n",
            "Epoch [97/100], Batch [344/346], Loss: 1.4721\n",
            "Epoch [97/100], Batch [345/346], Loss: 1.4743\n",
            "Epoch [97/100], Batch [346/346], Loss: 1.7266\n",
            "Epoch [98/100], Batch [1/346], Loss: 1.6291\n",
            "Epoch [98/100], Batch [2/346], Loss: 1.2338\n",
            "Epoch [98/100], Batch [3/346], Loss: 1.1847\n",
            "Epoch [98/100], Batch [4/346], Loss: 1.2728\n",
            "Epoch [98/100], Batch [5/346], Loss: 1.7643\n",
            "Epoch [98/100], Batch [6/346], Loss: 1.4537\n",
            "Epoch [98/100], Batch [7/346], Loss: 1.1252\n",
            "Epoch [98/100], Batch [8/346], Loss: 1.0174\n",
            "Epoch [98/100], Batch [9/346], Loss: 1.7107\n",
            "Epoch [98/100], Batch [10/346], Loss: 1.8691\n",
            "Epoch [98/100], Batch [11/346], Loss: 0.7823\n",
            "Epoch [98/100], Batch [12/346], Loss: 1.4321\n",
            "Epoch [98/100], Batch [13/346], Loss: 1.7554\n",
            "Epoch [98/100], Batch [14/346], Loss: 1.5912\n",
            "Epoch [98/100], Batch [15/346], Loss: 1.5456\n",
            "Epoch [98/100], Batch [16/346], Loss: 1.6893\n",
            "Epoch [98/100], Batch [17/346], Loss: 1.6155\n",
            "Epoch [98/100], Batch [18/346], Loss: 1.3473\n",
            "Epoch [98/100], Batch [19/346], Loss: 1.2879\n",
            "Epoch [98/100], Batch [20/346], Loss: 1.7078\n",
            "Epoch [98/100], Batch [21/346], Loss: 1.8270\n",
            "Epoch [98/100], Batch [22/346], Loss: 1.4627\n",
            "Epoch [98/100], Batch [23/346], Loss: 1.3792\n",
            "Epoch [98/100], Batch [24/346], Loss: 1.5111\n",
            "Epoch [98/100], Batch [25/346], Loss: 1.8107\n",
            "Epoch [98/100], Batch [26/346], Loss: 1.4506\n",
            "Epoch [98/100], Batch [27/346], Loss: 1.7874\n",
            "Epoch [98/100], Batch [28/346], Loss: 1.8058\n",
            "Epoch [98/100], Batch [29/346], Loss: 1.4802\n",
            "Epoch [98/100], Batch [30/346], Loss: 1.4503\n",
            "Epoch [98/100], Batch [31/346], Loss: 1.6507\n",
            "Epoch [98/100], Batch [32/346], Loss: 1.5180\n",
            "Epoch [98/100], Batch [33/346], Loss: 1.5186\n",
            "Epoch [98/100], Batch [34/346], Loss: 1.7731\n",
            "Epoch [98/100], Batch [35/346], Loss: 1.3853\n",
            "Epoch [98/100], Batch [36/346], Loss: 1.3986\n",
            "Epoch [98/100], Batch [37/346], Loss: 1.4170\n",
            "Epoch [98/100], Batch [38/346], Loss: 1.6759\n",
            "Epoch [98/100], Batch [39/346], Loss: 1.5864\n",
            "Epoch [98/100], Batch [40/346], Loss: 1.6062\n",
            "Epoch [98/100], Batch [41/346], Loss: 1.5492\n",
            "Epoch [98/100], Batch [42/346], Loss: 1.4694\n",
            "Epoch [98/100], Batch [43/346], Loss: 1.2948\n",
            "Epoch [98/100], Batch [44/346], Loss: 1.6587\n",
            "Epoch [98/100], Batch [45/346], Loss: 1.6593\n",
            "Epoch [98/100], Batch [46/346], Loss: 1.1202\n",
            "Epoch [98/100], Batch [47/346], Loss: 1.2274\n",
            "Epoch [98/100], Batch [48/346], Loss: 1.5163\n",
            "Epoch [98/100], Batch [49/346], Loss: 1.6781\n",
            "Epoch [98/100], Batch [50/346], Loss: 1.3579\n",
            "Epoch [98/100], Batch [51/346], Loss: 1.3345\n",
            "Epoch [98/100], Batch [52/346], Loss: 1.2808\n",
            "Epoch [98/100], Batch [53/346], Loss: 1.6045\n",
            "Epoch [98/100], Batch [54/346], Loss: 1.1619\n",
            "Epoch [98/100], Batch [55/346], Loss: 1.4896\n",
            "Epoch [98/100], Batch [56/346], Loss: 1.5430\n",
            "Epoch [98/100], Batch [57/346], Loss: 1.5138\n",
            "Epoch [98/100], Batch [58/346], Loss: 1.6815\n",
            "Epoch [98/100], Batch [59/346], Loss: 1.4055\n",
            "Epoch [98/100], Batch [60/346], Loss: 1.5634\n",
            "Epoch [98/100], Batch [61/346], Loss: 1.4581\n",
            "Epoch [98/100], Batch [62/346], Loss: 1.2498\n",
            "Epoch [98/100], Batch [63/346], Loss: 1.3500\n",
            "Epoch [98/100], Batch [64/346], Loss: 1.3240\n",
            "Epoch [98/100], Batch [65/346], Loss: 1.4835\n",
            "Epoch [98/100], Batch [66/346], Loss: 1.8181\n",
            "Epoch [98/100], Batch [67/346], Loss: 1.4827\n",
            "Epoch [98/100], Batch [68/346], Loss: 1.3335\n",
            "Epoch [98/100], Batch [69/346], Loss: 1.4122\n",
            "Epoch [98/100], Batch [70/346], Loss: 1.5182\n",
            "Epoch [98/100], Batch [71/346], Loss: 1.5099\n",
            "Epoch [98/100], Batch [72/346], Loss: 1.6845\n",
            "Epoch [98/100], Batch [73/346], Loss: 1.3849\n",
            "Epoch [98/100], Batch [74/346], Loss: 1.2859\n",
            "Epoch [98/100], Batch [75/346], Loss: 2.0635\n",
            "Epoch [98/100], Batch [76/346], Loss: 1.0546\n",
            "Epoch [98/100], Batch [77/346], Loss: 1.4976\n",
            "Epoch [98/100], Batch [78/346], Loss: 1.8832\n",
            "Epoch [98/100], Batch [79/346], Loss: 1.3583\n",
            "Epoch [98/100], Batch [80/346], Loss: 1.4679\n",
            "Epoch [98/100], Batch [81/346], Loss: 1.6754\n",
            "Epoch [98/100], Batch [82/346], Loss: 1.3423\n",
            "Epoch [98/100], Batch [83/346], Loss: 1.7835\n",
            "Epoch [98/100], Batch [84/346], Loss: 1.8283\n",
            "Epoch [98/100], Batch [85/346], Loss: 1.7768\n",
            "Epoch [98/100], Batch [86/346], Loss: 1.4260\n",
            "Epoch [98/100], Batch [87/346], Loss: 1.5961\n",
            "Epoch [98/100], Batch [88/346], Loss: 1.1962\n",
            "Epoch [98/100], Batch [89/346], Loss: 1.7455\n",
            "Epoch [98/100], Batch [90/346], Loss: 1.4841\n",
            "Epoch [98/100], Batch [91/346], Loss: 1.1802\n",
            "Epoch [98/100], Batch [92/346], Loss: 2.1071\n",
            "Epoch [98/100], Batch [93/346], Loss: 1.6664\n",
            "Epoch [98/100], Batch [94/346], Loss: 1.2602\n",
            "Epoch [98/100], Batch [95/346], Loss: 1.2991\n",
            "Epoch [98/100], Batch [96/346], Loss: 1.7352\n",
            "Epoch [98/100], Batch [97/346], Loss: 1.5527\n",
            "Epoch [98/100], Batch [98/346], Loss: 1.5025\n",
            "Epoch [98/100], Batch [99/346], Loss: 1.6767\n",
            "Epoch [98/100], Batch [100/346], Loss: 1.6946\n",
            "Epoch [98/100], Batch [101/346], Loss: 1.5123\n",
            "Epoch [98/100], Batch [102/346], Loss: 1.6726\n",
            "Epoch [98/100], Batch [103/346], Loss: 1.2779\n",
            "Epoch [98/100], Batch [104/346], Loss: 2.1898\n",
            "Epoch [98/100], Batch [105/346], Loss: 1.5763\n",
            "Epoch [98/100], Batch [106/346], Loss: 1.5329\n",
            "Epoch [98/100], Batch [107/346], Loss: 1.3148\n",
            "Epoch [98/100], Batch [108/346], Loss: 1.4922\n",
            "Epoch [98/100], Batch [109/346], Loss: 1.6323\n",
            "Epoch [98/100], Batch [110/346], Loss: 1.4180\n",
            "Epoch [98/100], Batch [111/346], Loss: 1.7286\n",
            "Epoch [98/100], Batch [112/346], Loss: 1.2897\n",
            "Epoch [98/100], Batch [113/346], Loss: 1.5176\n",
            "Epoch [98/100], Batch [114/346], Loss: 1.7324\n",
            "Epoch [98/100], Batch [115/346], Loss: 1.4543\n",
            "Epoch [98/100], Batch [116/346], Loss: 1.8085\n",
            "Epoch [98/100], Batch [117/346], Loss: 1.3328\n",
            "Epoch [98/100], Batch [118/346], Loss: 1.3521\n",
            "Epoch [98/100], Batch [119/346], Loss: 1.3765\n",
            "Epoch [98/100], Batch [120/346], Loss: 1.5199\n",
            "Epoch [98/100], Batch [121/346], Loss: 1.2459\n",
            "Epoch [98/100], Batch [122/346], Loss: 1.4318\n",
            "Epoch [98/100], Batch [123/346], Loss: 1.2516\n",
            "Epoch [98/100], Batch [124/346], Loss: 1.3755\n",
            "Epoch [98/100], Batch [125/346], Loss: 1.4347\n",
            "Epoch [98/100], Batch [126/346], Loss: 1.4971\n",
            "Epoch [98/100], Batch [127/346], Loss: 1.1814\n",
            "Epoch [98/100], Batch [128/346], Loss: 1.8699\n",
            "Epoch [98/100], Batch [129/346], Loss: 1.4357\n",
            "Epoch [98/100], Batch [130/346], Loss: 1.5004\n",
            "Epoch [98/100], Batch [131/346], Loss: 1.8069\n",
            "Epoch [98/100], Batch [132/346], Loss: 1.7594\n",
            "Epoch [98/100], Batch [133/346], Loss: 1.3531\n",
            "Epoch [98/100], Batch [134/346], Loss: 1.5341\n",
            "Epoch [98/100], Batch [135/346], Loss: 1.4137\n",
            "Epoch [98/100], Batch [136/346], Loss: 1.1884\n",
            "Epoch [98/100], Batch [137/346], Loss: 2.2235\n",
            "Epoch [98/100], Batch [138/346], Loss: 1.4837\n",
            "Epoch [98/100], Batch [139/346], Loss: 1.4218\n",
            "Epoch [98/100], Batch [140/346], Loss: 1.5276\n",
            "Epoch [98/100], Batch [141/346], Loss: 1.6688\n",
            "Epoch [98/100], Batch [142/346], Loss: 1.6251\n",
            "Epoch [98/100], Batch [143/346], Loss: 1.8127\n",
            "Epoch [98/100], Batch [144/346], Loss: 1.7105\n",
            "Epoch [98/100], Batch [145/346], Loss: 1.2880\n",
            "Epoch [98/100], Batch [146/346], Loss: 1.6925\n",
            "Epoch [98/100], Batch [147/346], Loss: 1.6197\n",
            "Epoch [98/100], Batch [148/346], Loss: 1.8805\n",
            "Epoch [98/100], Batch [149/346], Loss: 1.4201\n",
            "Epoch [98/100], Batch [150/346], Loss: 1.5691\n",
            "Epoch [98/100], Batch [151/346], Loss: 1.6296\n",
            "Epoch [98/100], Batch [152/346], Loss: 1.6301\n",
            "Epoch [98/100], Batch [153/346], Loss: 2.9878\n",
            "Epoch [98/100], Batch [154/346], Loss: 1.2865\n",
            "Epoch [98/100], Batch [155/346], Loss: 1.2273\n",
            "Epoch [98/100], Batch [156/346], Loss: 2.1664\n",
            "Epoch [98/100], Batch [157/346], Loss: 1.6121\n",
            "Epoch [98/100], Batch [158/346], Loss: 1.7831\n",
            "Epoch [98/100], Batch [159/346], Loss: 1.7463\n",
            "Epoch [98/100], Batch [160/346], Loss: 1.7955\n",
            "Epoch [98/100], Batch [161/346], Loss: 1.8519\n",
            "Epoch [98/100], Batch [162/346], Loss: 1.5848\n",
            "Epoch [98/100], Batch [163/346], Loss: 1.6884\n",
            "Epoch [98/100], Batch [164/346], Loss: 1.2285\n",
            "Epoch [98/100], Batch [165/346], Loss: 1.8167\n",
            "Epoch [98/100], Batch [166/346], Loss: 1.7202\n",
            "Epoch [98/100], Batch [167/346], Loss: 1.4836\n",
            "Epoch [98/100], Batch [168/346], Loss: 1.3195\n",
            "Epoch [98/100], Batch [169/346], Loss: 1.9818\n",
            "Epoch [98/100], Batch [170/346], Loss: 1.2407\n",
            "Epoch [98/100], Batch [171/346], Loss: 1.3942\n",
            "Epoch [98/100], Batch [172/346], Loss: 1.5322\n",
            "Epoch [98/100], Batch [173/346], Loss: 1.3069\n",
            "Epoch [98/100], Batch [174/346], Loss: 1.4371\n",
            "Epoch [98/100], Batch [175/346], Loss: 1.3686\n",
            "Epoch [98/100], Batch [176/346], Loss: 1.2138\n",
            "Epoch [98/100], Batch [177/346], Loss: 1.6903\n",
            "Epoch [98/100], Batch [178/346], Loss: 1.3894\n",
            "Epoch [98/100], Batch [179/346], Loss: 2.0152\n",
            "Epoch [98/100], Batch [180/346], Loss: 1.7845\n",
            "Epoch [98/100], Batch [181/346], Loss: 1.8681\n",
            "Epoch [98/100], Batch [182/346], Loss: 1.1053\n",
            "Epoch [98/100], Batch [183/346], Loss: 1.3350\n",
            "Epoch [98/100], Batch [184/346], Loss: 1.7985\n",
            "Epoch [98/100], Batch [185/346], Loss: 1.6558\n",
            "Epoch [98/100], Batch [186/346], Loss: 1.9729\n",
            "Epoch [98/100], Batch [187/346], Loss: 1.0552\n",
            "Epoch [98/100], Batch [188/346], Loss: 2.0670\n",
            "Epoch [98/100], Batch [189/346], Loss: 1.6098\n",
            "Epoch [98/100], Batch [190/346], Loss: 1.6431\n",
            "Epoch [98/100], Batch [191/346], Loss: 1.3190\n",
            "Epoch [98/100], Batch [192/346], Loss: 1.4985\n",
            "Epoch [98/100], Batch [193/346], Loss: 1.8179\n",
            "Epoch [98/100], Batch [194/346], Loss: 1.5983\n",
            "Epoch [98/100], Batch [195/346], Loss: 1.7588\n",
            "Epoch [98/100], Batch [196/346], Loss: 1.7058\n",
            "Epoch [98/100], Batch [197/346], Loss: 1.4030\n",
            "Epoch [98/100], Batch [198/346], Loss: 1.6250\n",
            "Epoch [98/100], Batch [199/346], Loss: 1.5547\n",
            "Epoch [98/100], Batch [200/346], Loss: 1.6353\n",
            "Epoch [98/100], Batch [201/346], Loss: 1.2694\n",
            "Epoch [98/100], Batch [202/346], Loss: 1.7672\n",
            "Epoch [98/100], Batch [203/346], Loss: 1.3575\n",
            "Epoch [98/100], Batch [204/346], Loss: 1.3825\n",
            "Epoch [98/100], Batch [205/346], Loss: 1.4278\n",
            "Epoch [98/100], Batch [206/346], Loss: 1.5384\n",
            "Epoch [98/100], Batch [207/346], Loss: 1.3444\n",
            "Epoch [98/100], Batch [208/346], Loss: 1.4938\n",
            "Epoch [98/100], Batch [209/346], Loss: 1.3878\n",
            "Epoch [98/100], Batch [210/346], Loss: 1.5567\n",
            "Epoch [98/100], Batch [211/346], Loss: 1.3697\n",
            "Epoch [98/100], Batch [212/346], Loss: 1.4204\n",
            "Epoch [98/100], Batch [213/346], Loss: 1.5119\n",
            "Epoch [98/100], Batch [214/346], Loss: 1.4058\n",
            "Epoch [98/100], Batch [215/346], Loss: 1.1395\n",
            "Epoch [98/100], Batch [216/346], Loss: 1.7679\n",
            "Epoch [98/100], Batch [217/346], Loss: 1.6288\n",
            "Epoch [98/100], Batch [218/346], Loss: 1.7249\n",
            "Epoch [98/100], Batch [219/346], Loss: 1.5816\n",
            "Epoch [98/100], Batch [220/346], Loss: 1.3019\n",
            "Epoch [98/100], Batch [221/346], Loss: 1.5880\n",
            "Epoch [98/100], Batch [222/346], Loss: 1.3567\n",
            "Epoch [98/100], Batch [223/346], Loss: 1.9631\n",
            "Epoch [98/100], Batch [224/346], Loss: 1.5942\n",
            "Epoch [98/100], Batch [225/346], Loss: 1.6137\n",
            "Epoch [98/100], Batch [226/346], Loss: 1.5163\n",
            "Epoch [98/100], Batch [227/346], Loss: 1.5894\n",
            "Epoch [98/100], Batch [228/346], Loss: 1.5707\n",
            "Epoch [98/100], Batch [229/346], Loss: 1.6454\n",
            "Epoch [98/100], Batch [230/346], Loss: 1.6747\n",
            "Epoch [98/100], Batch [231/346], Loss: 1.3573\n",
            "Epoch [98/100], Batch [232/346], Loss: 1.6168\n",
            "Epoch [98/100], Batch [233/346], Loss: 1.5582\n",
            "Epoch [98/100], Batch [234/346], Loss: 1.6954\n",
            "Epoch [98/100], Batch [235/346], Loss: 1.4672\n",
            "Epoch [98/100], Batch [236/346], Loss: 1.1810\n",
            "Epoch [98/100], Batch [237/346], Loss: 1.2080\n",
            "Epoch [98/100], Batch [238/346], Loss: 1.4516\n",
            "Epoch [98/100], Batch [239/346], Loss: 1.6840\n",
            "Epoch [98/100], Batch [240/346], Loss: 1.7841\n",
            "Epoch [98/100], Batch [241/346], Loss: 1.3935\n",
            "Epoch [98/100], Batch [242/346], Loss: 1.3516\n",
            "Epoch [98/100], Batch [243/346], Loss: 1.4901\n",
            "Epoch [98/100], Batch [244/346], Loss: 1.9039\n",
            "Epoch [98/100], Batch [245/346], Loss: 1.4057\n",
            "Epoch [98/100], Batch [246/346], Loss: 1.1878\n",
            "Epoch [98/100], Batch [247/346], Loss: 1.7916\n",
            "Epoch [98/100], Batch [248/346], Loss: 1.6153\n",
            "Epoch [98/100], Batch [249/346], Loss: 1.4157\n",
            "Epoch [98/100], Batch [250/346], Loss: 1.5850\n",
            "Epoch [98/100], Batch [251/346], Loss: 1.3686\n",
            "Epoch [98/100], Batch [252/346], Loss: 1.6940\n",
            "Epoch [98/100], Batch [253/346], Loss: 1.5028\n",
            "Epoch [98/100], Batch [254/346], Loss: 1.3414\n",
            "Epoch [98/100], Batch [255/346], Loss: 1.5131\n",
            "Epoch [98/100], Batch [256/346], Loss: 1.3710\n",
            "Epoch [98/100], Batch [257/346], Loss: 1.3584\n",
            "Epoch [98/100], Batch [258/346], Loss: 1.2441\n",
            "Epoch [98/100], Batch [259/346], Loss: 1.4571\n",
            "Epoch [98/100], Batch [260/346], Loss: 1.3034\n",
            "Epoch [98/100], Batch [261/346], Loss: 1.4770\n",
            "Epoch [98/100], Batch [262/346], Loss: 1.5684\n",
            "Epoch [98/100], Batch [263/346], Loss: 1.3824\n",
            "Epoch [98/100], Batch [264/346], Loss: 1.4878\n",
            "Epoch [98/100], Batch [265/346], Loss: 1.6097\n",
            "Epoch [98/100], Batch [266/346], Loss: 1.5027\n",
            "Epoch [98/100], Batch [267/346], Loss: 1.1837\n",
            "Epoch [98/100], Batch [268/346], Loss: 1.3742\n",
            "Epoch [98/100], Batch [269/346], Loss: 1.1841\n",
            "Epoch [98/100], Batch [270/346], Loss: 1.6059\n",
            "Epoch [98/100], Batch [271/346], Loss: 2.0572\n",
            "Epoch [98/100], Batch [272/346], Loss: 1.2726\n",
            "Epoch [98/100], Batch [273/346], Loss: 1.1229\n",
            "Epoch [98/100], Batch [274/346], Loss: 1.5449\n",
            "Epoch [98/100], Batch [275/346], Loss: 1.4716\n",
            "Epoch [98/100], Batch [276/346], Loss: 1.2322\n",
            "Epoch [98/100], Batch [277/346], Loss: 1.3645\n",
            "Epoch [98/100], Batch [278/346], Loss: 1.6278\n",
            "Epoch [98/100], Batch [279/346], Loss: 1.4369\n",
            "Epoch [98/100], Batch [280/346], Loss: 1.5862\n",
            "Epoch [98/100], Batch [281/346], Loss: 1.4992\n",
            "Epoch [98/100], Batch [282/346], Loss: 1.4859\n",
            "Epoch [98/100], Batch [283/346], Loss: 1.3331\n",
            "Epoch [98/100], Batch [284/346], Loss: 1.6292\n",
            "Epoch [98/100], Batch [285/346], Loss: 1.4401\n",
            "Epoch [98/100], Batch [286/346], Loss: 1.4379\n",
            "Epoch [98/100], Batch [287/346], Loss: 1.4029\n",
            "Epoch [98/100], Batch [288/346], Loss: 1.6826\n",
            "Epoch [98/100], Batch [289/346], Loss: 1.6376\n",
            "Epoch [98/100], Batch [290/346], Loss: 1.6752\n",
            "Epoch [98/100], Batch [291/346], Loss: 1.4740\n",
            "Epoch [98/100], Batch [292/346], Loss: 1.9965\n",
            "Epoch [98/100], Batch [293/346], Loss: 1.6409\n",
            "Epoch [98/100], Batch [294/346], Loss: 1.6240\n",
            "Epoch [98/100], Batch [295/346], Loss: 1.6918\n",
            "Epoch [98/100], Batch [296/346], Loss: 1.4381\n",
            "Epoch [98/100], Batch [297/346], Loss: 1.5585\n",
            "Epoch [98/100], Batch [298/346], Loss: 1.3736\n",
            "Epoch [98/100], Batch [299/346], Loss: 1.5514\n",
            "Epoch [98/100], Batch [300/346], Loss: 1.7454\n",
            "Epoch [98/100], Batch [301/346], Loss: 1.7472\n",
            "Epoch [98/100], Batch [302/346], Loss: 1.2668\n",
            "Epoch [98/100], Batch [303/346], Loss: 1.0582\n",
            "Epoch [98/100], Batch [304/346], Loss: 1.4949\n",
            "Epoch [98/100], Batch [305/346], Loss: 1.3922\n",
            "Epoch [98/100], Batch [306/346], Loss: 1.7741\n",
            "Epoch [98/100], Batch [307/346], Loss: 1.2952\n",
            "Epoch [98/100], Batch [308/346], Loss: 1.7376\n",
            "Epoch [98/100], Batch [309/346], Loss: 1.6991\n",
            "Epoch [98/100], Batch [310/346], Loss: 1.5700\n",
            "Epoch [98/100], Batch [311/346], Loss: 1.3709\n",
            "Epoch [98/100], Batch [312/346], Loss: 1.7137\n",
            "Epoch [98/100], Batch [313/346], Loss: 1.4194\n",
            "Epoch [98/100], Batch [314/346], Loss: 1.3313\n",
            "Epoch [98/100], Batch [315/346], Loss: 1.9838\n",
            "Epoch [98/100], Batch [316/346], Loss: 1.5060\n",
            "Epoch [98/100], Batch [317/346], Loss: 1.6546\n",
            "Epoch [98/100], Batch [318/346], Loss: 1.8926\n",
            "Epoch [98/100], Batch [319/346], Loss: 1.7625\n",
            "Epoch [98/100], Batch [320/346], Loss: 1.5958\n",
            "Epoch [98/100], Batch [321/346], Loss: 1.4346\n",
            "Epoch [98/100], Batch [322/346], Loss: 1.1223\n",
            "Epoch [98/100], Batch [323/346], Loss: 1.6679\n",
            "Epoch [98/100], Batch [324/346], Loss: 2.1417\n",
            "Epoch [98/100], Batch [325/346], Loss: 1.2775\n",
            "Epoch [98/100], Batch [326/346], Loss: 1.3999\n",
            "Epoch [98/100], Batch [327/346], Loss: 1.5227\n",
            "Epoch [98/100], Batch [328/346], Loss: 1.3155\n",
            "Epoch [98/100], Batch [329/346], Loss: 1.2789\n",
            "Epoch [98/100], Batch [330/346], Loss: 1.7112\n",
            "Epoch [98/100], Batch [331/346], Loss: 1.4058\n",
            "Epoch [98/100], Batch [332/346], Loss: 1.5283\n",
            "Epoch [98/100], Batch [333/346], Loss: 1.1164\n",
            "Epoch [98/100], Batch [334/346], Loss: 1.6137\n",
            "Epoch [98/100], Batch [335/346], Loss: 1.1486\n",
            "Epoch [98/100], Batch [336/346], Loss: 1.3706\n",
            "Epoch [98/100], Batch [337/346], Loss: 2.0021\n",
            "Epoch [98/100], Batch [338/346], Loss: 1.5497\n",
            "Epoch [98/100], Batch [339/346], Loss: 1.6253\n",
            "Epoch [98/100], Batch [340/346], Loss: 1.2146\n",
            "Epoch [98/100], Batch [341/346], Loss: 1.3444\n",
            "Epoch [98/100], Batch [342/346], Loss: 1.6357\n",
            "Epoch [98/100], Batch [343/346], Loss: 1.5744\n",
            "Epoch [98/100], Batch [344/346], Loss: 1.3528\n",
            "Epoch [98/100], Batch [345/346], Loss: 1.5982\n",
            "Epoch [98/100], Batch [346/346], Loss: 1.2991\n",
            "Epoch [99/100], Batch [1/346], Loss: 1.2709\n",
            "Epoch [99/100], Batch [2/346], Loss: 2.3489\n",
            "Epoch [99/100], Batch [3/346], Loss: 1.1618\n",
            "Epoch [99/100], Batch [4/346], Loss: 1.5092\n",
            "Epoch [99/100], Batch [5/346], Loss: 1.5132\n",
            "Epoch [99/100], Batch [6/346], Loss: 1.2847\n",
            "Epoch [99/100], Batch [7/346], Loss: 1.7424\n",
            "Epoch [99/100], Batch [8/346], Loss: 1.6485\n",
            "Epoch [99/100], Batch [9/346], Loss: 1.5229\n",
            "Epoch [99/100], Batch [10/346], Loss: 1.6297\n",
            "Epoch [99/100], Batch [11/346], Loss: 1.3169\n",
            "Epoch [99/100], Batch [12/346], Loss: 1.5713\n",
            "Epoch [99/100], Batch [13/346], Loss: 1.3510\n",
            "Epoch [99/100], Batch [14/346], Loss: 1.5568\n",
            "Epoch [99/100], Batch [15/346], Loss: 1.5951\n",
            "Epoch [99/100], Batch [16/346], Loss: 1.4810\n",
            "Epoch [99/100], Batch [17/346], Loss: 1.9515\n",
            "Epoch [99/100], Batch [18/346], Loss: 1.5232\n",
            "Epoch [99/100], Batch [19/346], Loss: 1.6169\n",
            "Epoch [99/100], Batch [20/346], Loss: 1.6567\n",
            "Epoch [99/100], Batch [21/346], Loss: 1.5736\n",
            "Epoch [99/100], Batch [22/346], Loss: 1.5801\n",
            "Epoch [99/100], Batch [23/346], Loss: 1.3508\n",
            "Epoch [99/100], Batch [24/346], Loss: 1.4979\n",
            "Epoch [99/100], Batch [25/346], Loss: 1.4000\n",
            "Epoch [99/100], Batch [26/346], Loss: 1.4529\n",
            "Epoch [99/100], Batch [27/346], Loss: 1.4297\n",
            "Epoch [99/100], Batch [28/346], Loss: 1.4902\n",
            "Epoch [99/100], Batch [29/346], Loss: 1.4066\n",
            "Epoch [99/100], Batch [30/346], Loss: 1.8669\n",
            "Epoch [99/100], Batch [31/346], Loss: 1.2219\n",
            "Epoch [99/100], Batch [32/346], Loss: 1.4528\n",
            "Epoch [99/100], Batch [33/346], Loss: 1.6551\n",
            "Epoch [99/100], Batch [34/346], Loss: 1.6952\n",
            "Epoch [99/100], Batch [35/346], Loss: 1.6752\n",
            "Epoch [99/100], Batch [36/346], Loss: 1.5333\n",
            "Epoch [99/100], Batch [37/346], Loss: 1.4364\n",
            "Epoch [99/100], Batch [38/346], Loss: 1.5035\n",
            "Epoch [99/100], Batch [39/346], Loss: 1.3358\n",
            "Epoch [99/100], Batch [40/346], Loss: 1.3265\n",
            "Epoch [99/100], Batch [41/346], Loss: 1.8145\n",
            "Epoch [99/100], Batch [42/346], Loss: 1.9364\n",
            "Epoch [99/100], Batch [43/346], Loss: 1.4092\n",
            "Epoch [99/100], Batch [44/346], Loss: 1.6933\n",
            "Epoch [99/100], Batch [45/346], Loss: 1.4727\n",
            "Epoch [99/100], Batch [46/346], Loss: 1.7464\n",
            "Epoch [99/100], Batch [47/346], Loss: 1.8797\n",
            "Epoch [99/100], Batch [48/346], Loss: 1.6771\n",
            "Epoch [99/100], Batch [49/346], Loss: 1.5528\n",
            "Epoch [99/100], Batch [50/346], Loss: 1.5421\n",
            "Epoch [99/100], Batch [51/346], Loss: 1.8379\n",
            "Epoch [99/100], Batch [52/346], Loss: 1.7059\n",
            "Epoch [99/100], Batch [53/346], Loss: 1.4639\n",
            "Epoch [99/100], Batch [54/346], Loss: 1.3700\n",
            "Epoch [99/100], Batch [55/346], Loss: 1.5966\n",
            "Epoch [99/100], Batch [56/346], Loss: 1.6457\n",
            "Epoch [99/100], Batch [57/346], Loss: 1.4381\n",
            "Epoch [99/100], Batch [58/346], Loss: 1.3198\n",
            "Epoch [99/100], Batch [59/346], Loss: 1.6829\n",
            "Epoch [99/100], Batch [60/346], Loss: 1.6207\n",
            "Epoch [99/100], Batch [61/346], Loss: 1.4365\n",
            "Epoch [99/100], Batch [62/346], Loss: 1.2508\n",
            "Epoch [99/100], Batch [63/346], Loss: 1.6718\n",
            "Epoch [99/100], Batch [64/346], Loss: 1.3310\n",
            "Epoch [99/100], Batch [65/346], Loss: 1.6982\n",
            "Epoch [99/100], Batch [66/346], Loss: 1.3473\n",
            "Epoch [99/100], Batch [67/346], Loss: 1.3113\n",
            "Epoch [99/100], Batch [68/346], Loss: 1.3047\n",
            "Epoch [99/100], Batch [69/346], Loss: 1.3069\n",
            "Epoch [99/100], Batch [70/346], Loss: 1.5889\n",
            "Epoch [99/100], Batch [71/346], Loss: 1.5231\n",
            "Epoch [99/100], Batch [72/346], Loss: 1.5346\n",
            "Epoch [99/100], Batch [73/346], Loss: 1.6197\n",
            "Epoch [99/100], Batch [74/346], Loss: 1.1271\n",
            "Epoch [99/100], Batch [75/346], Loss: 1.6274\n",
            "Epoch [99/100], Batch [76/346], Loss: 1.6833\n",
            "Epoch [99/100], Batch [77/346], Loss: 1.2194\n",
            "Epoch [99/100], Batch [78/346], Loss: 1.2823\n",
            "Epoch [99/100], Batch [79/346], Loss: 1.3818\n",
            "Epoch [99/100], Batch [80/346], Loss: 1.4710\n",
            "Epoch [99/100], Batch [81/346], Loss: 1.6151\n",
            "Epoch [99/100], Batch [82/346], Loss: 1.5078\n",
            "Epoch [99/100], Batch [83/346], Loss: 1.2566\n",
            "Epoch [99/100], Batch [84/346], Loss: 1.6314\n",
            "Epoch [99/100], Batch [85/346], Loss: 1.6710\n",
            "Epoch [99/100], Batch [86/346], Loss: 1.5735\n",
            "Epoch [99/100], Batch [87/346], Loss: 1.3481\n",
            "Epoch [99/100], Batch [88/346], Loss: 1.2372\n",
            "Epoch [99/100], Batch [89/346], Loss: 2.0581\n",
            "Epoch [99/100], Batch [90/346], Loss: 1.6504\n",
            "Epoch [99/100], Batch [91/346], Loss: 1.5787\n",
            "Epoch [99/100], Batch [92/346], Loss: 1.4582\n",
            "Epoch [99/100], Batch [93/346], Loss: 1.7389\n",
            "Epoch [99/100], Batch [94/346], Loss: 1.5926\n",
            "Epoch [99/100], Batch [95/346], Loss: 1.7260\n",
            "Epoch [99/100], Batch [96/346], Loss: 1.3040\n",
            "Epoch [99/100], Batch [97/346], Loss: 1.2305\n",
            "Epoch [99/100], Batch [98/346], Loss: 1.3014\n",
            "Epoch [99/100], Batch [99/346], Loss: 1.2825\n",
            "Epoch [99/100], Batch [100/346], Loss: 1.4510\n",
            "Epoch [99/100], Batch [101/346], Loss: 1.3300\n",
            "Epoch [99/100], Batch [102/346], Loss: 1.5747\n",
            "Epoch [99/100], Batch [103/346], Loss: 1.8612\n",
            "Epoch [99/100], Batch [104/346], Loss: 1.7199\n",
            "Epoch [99/100], Batch [105/346], Loss: 1.3307\n",
            "Epoch [99/100], Batch [106/346], Loss: 1.4760\n",
            "Epoch [99/100], Batch [107/346], Loss: 1.9524\n",
            "Epoch [99/100], Batch [108/346], Loss: 1.8021\n",
            "Epoch [99/100], Batch [109/346], Loss: 1.6857\n",
            "Epoch [99/100], Batch [110/346], Loss: 1.3466\n",
            "Epoch [99/100], Batch [111/346], Loss: 1.6615\n",
            "Epoch [99/100], Batch [112/346], Loss: 1.7817\n",
            "Epoch [99/100], Batch [113/346], Loss: 1.6700\n",
            "Epoch [99/100], Batch [114/346], Loss: 1.6424\n",
            "Epoch [99/100], Batch [115/346], Loss: 1.4382\n",
            "Epoch [99/100], Batch [116/346], Loss: 1.6838\n",
            "Epoch [99/100], Batch [117/346], Loss: 1.4555\n",
            "Epoch [99/100], Batch [118/346], Loss: 1.7684\n",
            "Epoch [99/100], Batch [119/346], Loss: 1.3437\n",
            "Epoch [99/100], Batch [120/346], Loss: 2.0069\n",
            "Epoch [99/100], Batch [121/346], Loss: 1.1858\n",
            "Epoch [99/100], Batch [122/346], Loss: 1.5295\n",
            "Epoch [99/100], Batch [123/346], Loss: 1.5122\n",
            "Epoch [99/100], Batch [124/346], Loss: 1.3398\n",
            "Epoch [99/100], Batch [125/346], Loss: 1.4369\n",
            "Epoch [99/100], Batch [126/346], Loss: 1.4158\n",
            "Epoch [99/100], Batch [127/346], Loss: 1.3881\n",
            "Epoch [99/100], Batch [128/346], Loss: 1.7673\n",
            "Epoch [99/100], Batch [129/346], Loss: 1.4100\n",
            "Epoch [99/100], Batch [130/346], Loss: 1.7579\n",
            "Epoch [99/100], Batch [131/346], Loss: 1.3468\n",
            "Epoch [99/100], Batch [132/346], Loss: 1.3933\n",
            "Epoch [99/100], Batch [133/346], Loss: 1.6567\n",
            "Epoch [99/100], Batch [134/346], Loss: 1.5235\n",
            "Epoch [99/100], Batch [135/346], Loss: 1.3123\n",
            "Epoch [99/100], Batch [136/346], Loss: 1.3780\n",
            "Epoch [99/100], Batch [137/346], Loss: 1.5070\n",
            "Epoch [99/100], Batch [138/346], Loss: 1.7156\n",
            "Epoch [99/100], Batch [139/346], Loss: 1.4363\n",
            "Epoch [99/100], Batch [140/346], Loss: 1.4036\n",
            "Epoch [99/100], Batch [141/346], Loss: 1.5536\n",
            "Epoch [99/100], Batch [142/346], Loss: 1.3371\n",
            "Epoch [99/100], Batch [143/346], Loss: 1.5373\n",
            "Epoch [99/100], Batch [144/346], Loss: 1.4259\n",
            "Epoch [99/100], Batch [145/346], Loss: 1.3317\n",
            "Epoch [99/100], Batch [146/346], Loss: 1.4849\n",
            "Epoch [99/100], Batch [147/346], Loss: 1.4941\n",
            "Epoch [99/100], Batch [148/346], Loss: 1.4608\n",
            "Epoch [99/100], Batch [149/346], Loss: 2.0719\n",
            "Epoch [99/100], Batch [150/346], Loss: 1.8355\n",
            "Epoch [99/100], Batch [151/346], Loss: 1.4901\n",
            "Epoch [99/100], Batch [152/346], Loss: 1.2880\n",
            "Epoch [99/100], Batch [153/346], Loss: 1.2917\n",
            "Epoch [99/100], Batch [154/346], Loss: 1.4005\n",
            "Epoch [99/100], Batch [155/346], Loss: 1.9528\n",
            "Epoch [99/100], Batch [156/346], Loss: 1.5076\n",
            "Epoch [99/100], Batch [157/346], Loss: 1.3839\n",
            "Epoch [99/100], Batch [158/346], Loss: 1.5777\n",
            "Epoch [99/100], Batch [159/346], Loss: 1.1880\n",
            "Epoch [99/100], Batch [160/346], Loss: 1.5573\n",
            "Epoch [99/100], Batch [161/346], Loss: 1.4655\n",
            "Epoch [99/100], Batch [162/346], Loss: 1.1726\n",
            "Epoch [99/100], Batch [163/346], Loss: 1.2843\n",
            "Epoch [99/100], Batch [164/346], Loss: 1.6256\n",
            "Epoch [99/100], Batch [165/346], Loss: 1.2381\n",
            "Epoch [99/100], Batch [166/346], Loss: 1.5549\n",
            "Epoch [99/100], Batch [167/346], Loss: 1.5240\n",
            "Epoch [99/100], Batch [168/346], Loss: 1.2327\n",
            "Epoch [99/100], Batch [169/346], Loss: 1.5626\n",
            "Epoch [99/100], Batch [170/346], Loss: 1.2173\n",
            "Epoch [99/100], Batch [171/346], Loss: 1.5460\n",
            "Epoch [99/100], Batch [172/346], Loss: 1.9046\n",
            "Epoch [99/100], Batch [173/346], Loss: 1.4567\n",
            "Epoch [99/100], Batch [174/346], Loss: 1.5829\n",
            "Epoch [99/100], Batch [175/346], Loss: 1.5280\n",
            "Epoch [99/100], Batch [176/346], Loss: 1.7242\n",
            "Epoch [99/100], Batch [177/346], Loss: 1.7093\n",
            "Epoch [99/100], Batch [178/346], Loss: 1.4365\n",
            "Epoch [99/100], Batch [179/346], Loss: 1.3471\n",
            "Epoch [99/100], Batch [180/346], Loss: 1.3539\n",
            "Epoch [99/100], Batch [181/346], Loss: 1.6437\n",
            "Epoch [99/100], Batch [182/346], Loss: 1.8400\n",
            "Epoch [99/100], Batch [183/346], Loss: 1.5866\n",
            "Epoch [99/100], Batch [184/346], Loss: 1.8218\n",
            "Epoch [99/100], Batch [185/346], Loss: 1.6512\n",
            "Epoch [99/100], Batch [186/346], Loss: 1.4994\n",
            "Epoch [99/100], Batch [187/346], Loss: 1.4717\n",
            "Epoch [99/100], Batch [188/346], Loss: 1.3879\n",
            "Epoch [99/100], Batch [189/346], Loss: 1.3018\n",
            "Epoch [99/100], Batch [190/346], Loss: 1.7946\n",
            "Epoch [99/100], Batch [191/346], Loss: 1.8912\n",
            "Epoch [99/100], Batch [192/346], Loss: 1.3304\n",
            "Epoch [99/100], Batch [193/346], Loss: 1.6785\n",
            "Epoch [99/100], Batch [194/346], Loss: 2.3154\n",
            "Epoch [99/100], Batch [195/346], Loss: 1.9830\n",
            "Epoch [99/100], Batch [196/346], Loss: 1.3835\n",
            "Epoch [99/100], Batch [197/346], Loss: 1.9201\n",
            "Epoch [99/100], Batch [198/346], Loss: 1.5953\n",
            "Epoch [99/100], Batch [199/346], Loss: 1.8027\n",
            "Epoch [99/100], Batch [200/346], Loss: 1.6417\n",
            "Epoch [99/100], Batch [201/346], Loss: 1.6862\n",
            "Epoch [99/100], Batch [202/346], Loss: 1.7353\n",
            "Epoch [99/100], Batch [203/346], Loss: 1.9627\n",
            "Epoch [99/100], Batch [204/346], Loss: 1.8127\n",
            "Epoch [99/100], Batch [205/346], Loss: 1.4608\n",
            "Epoch [99/100], Batch [206/346], Loss: 1.6553\n",
            "Epoch [99/100], Batch [207/346], Loss: 1.9292\n",
            "Epoch [99/100], Batch [208/346], Loss: 1.6696\n",
            "Epoch [99/100], Batch [209/346], Loss: 1.0682\n",
            "Epoch [99/100], Batch [210/346], Loss: 1.4855\n",
            "Epoch [99/100], Batch [211/346], Loss: 1.6284\n",
            "Epoch [99/100], Batch [212/346], Loss: 1.7773\n",
            "Epoch [99/100], Batch [213/346], Loss: 1.6836\n",
            "Epoch [99/100], Batch [214/346], Loss: 1.2950\n",
            "Epoch [99/100], Batch [215/346], Loss: 1.5622\n",
            "Epoch [99/100], Batch [216/346], Loss: 1.6234\n",
            "Epoch [99/100], Batch [217/346], Loss: 1.6758\n",
            "Epoch [99/100], Batch [218/346], Loss: 1.5650\n",
            "Epoch [99/100], Batch [219/346], Loss: 1.3835\n",
            "Epoch [99/100], Batch [220/346], Loss: 1.6293\n",
            "Epoch [99/100], Batch [221/346], Loss: 1.3378\n",
            "Epoch [99/100], Batch [222/346], Loss: 1.5264\n",
            "Epoch [99/100], Batch [223/346], Loss: 1.4432\n",
            "Epoch [99/100], Batch [224/346], Loss: 1.2602\n",
            "Epoch [99/100], Batch [225/346], Loss: 1.5281\n",
            "Epoch [99/100], Batch [226/346], Loss: 1.5819\n",
            "Epoch [99/100], Batch [227/346], Loss: 1.3838\n",
            "Epoch [99/100], Batch [228/346], Loss: 1.6876\n",
            "Epoch [99/100], Batch [229/346], Loss: 2.2818\n",
            "Epoch [99/100], Batch [230/346], Loss: 1.4501\n",
            "Epoch [99/100], Batch [231/346], Loss: 1.1146\n",
            "Epoch [99/100], Batch [232/346], Loss: 1.2731\n",
            "Epoch [99/100], Batch [233/346], Loss: 1.3372\n",
            "Epoch [99/100], Batch [234/346], Loss: 2.1371\n",
            "Epoch [99/100], Batch [235/346], Loss: 1.4084\n",
            "Epoch [99/100], Batch [236/346], Loss: 1.3750\n",
            "Epoch [99/100], Batch [237/346], Loss: 1.5060\n",
            "Epoch [99/100], Batch [238/346], Loss: 1.4240\n",
            "Epoch [99/100], Batch [239/346], Loss: 1.3159\n",
            "Epoch [99/100], Batch [240/346], Loss: 1.9434\n",
            "Epoch [99/100], Batch [241/346], Loss: 1.4639\n",
            "Epoch [99/100], Batch [242/346], Loss: 1.3222\n",
            "Epoch [99/100], Batch [243/346], Loss: 1.4439\n",
            "Epoch [99/100], Batch [244/346], Loss: 1.3863\n",
            "Epoch [99/100], Batch [245/346], Loss: 1.3793\n",
            "Epoch [99/100], Batch [246/346], Loss: 1.6111\n",
            "Epoch [99/100], Batch [247/346], Loss: 1.5209\n",
            "Epoch [99/100], Batch [248/346], Loss: 1.5066\n",
            "Epoch [99/100], Batch [249/346], Loss: 1.0963\n",
            "Epoch [99/100], Batch [250/346], Loss: 1.6293\n",
            "Epoch [99/100], Batch [251/346], Loss: 1.5793\n",
            "Epoch [99/100], Batch [252/346], Loss: 1.9735\n",
            "Epoch [99/100], Batch [253/346], Loss: 1.3403\n",
            "Epoch [99/100], Batch [254/346], Loss: 1.8440\n",
            "Epoch [99/100], Batch [255/346], Loss: 1.8847\n",
            "Epoch [99/100], Batch [256/346], Loss: 1.6557\n",
            "Epoch [99/100], Batch [257/346], Loss: 1.3940\n",
            "Epoch [99/100], Batch [258/346], Loss: 1.6817\n",
            "Epoch [99/100], Batch [259/346], Loss: 1.5489\n",
            "Epoch [99/100], Batch [260/346], Loss: 1.5037\n",
            "Epoch [99/100], Batch [261/346], Loss: 0.9852\n",
            "Epoch [99/100], Batch [262/346], Loss: 1.8284\n",
            "Epoch [99/100], Batch [263/346], Loss: 1.2904\n",
            "Epoch [99/100], Batch [264/346], Loss: 1.4828\n",
            "Epoch [99/100], Batch [265/346], Loss: 1.6441\n",
            "Epoch [99/100], Batch [266/346], Loss: 1.5731\n",
            "Epoch [99/100], Batch [267/346], Loss: 1.6351\n",
            "Epoch [99/100], Batch [268/346], Loss: 1.3316\n",
            "Epoch [99/100], Batch [269/346], Loss: 1.1995\n",
            "Epoch [99/100], Batch [270/346], Loss: 1.1718\n",
            "Epoch [99/100], Batch [271/346], Loss: 1.0895\n",
            "Epoch [99/100], Batch [272/346], Loss: 1.6030\n",
            "Epoch [99/100], Batch [273/346], Loss: 1.6945\n",
            "Epoch [99/100], Batch [274/346], Loss: 1.7694\n",
            "Epoch [99/100], Batch [275/346], Loss: 1.7449\n",
            "Epoch [99/100], Batch [276/346], Loss: 1.7219\n",
            "Epoch [99/100], Batch [277/346], Loss: 1.4731\n",
            "Epoch [99/100], Batch [278/346], Loss: 0.9667\n",
            "Epoch [99/100], Batch [279/346], Loss: 1.1440\n",
            "Epoch [99/100], Batch [280/346], Loss: 1.7057\n",
            "Epoch [99/100], Batch [281/346], Loss: 2.0317\n",
            "Epoch [99/100], Batch [282/346], Loss: 2.1153\n",
            "Epoch [99/100], Batch [283/346], Loss: 1.5101\n",
            "Epoch [99/100], Batch [284/346], Loss: 1.5346\n",
            "Epoch [99/100], Batch [285/346], Loss: 1.7279\n",
            "Epoch [99/100], Batch [286/346], Loss: 1.7073\n",
            "Epoch [99/100], Batch [287/346], Loss: 1.8451\n",
            "Epoch [99/100], Batch [288/346], Loss: 1.6643\n",
            "Epoch [99/100], Batch [289/346], Loss: 1.1801\n",
            "Epoch [99/100], Batch [290/346], Loss: 1.5373\n",
            "Epoch [99/100], Batch [291/346], Loss: 1.5182\n",
            "Epoch [99/100], Batch [292/346], Loss: 1.7779\n",
            "Epoch [99/100], Batch [293/346], Loss: 1.5368\n",
            "Epoch [99/100], Batch [294/346], Loss: 2.1249\n",
            "Epoch [99/100], Batch [295/346], Loss: 1.8001\n",
            "Epoch [99/100], Batch [296/346], Loss: 1.7967\n",
            "Epoch [99/100], Batch [297/346], Loss: 1.9742\n",
            "Epoch [99/100], Batch [298/346], Loss: 2.2383\n",
            "Epoch [99/100], Batch [299/346], Loss: 1.3444\n",
            "Epoch [99/100], Batch [300/346], Loss: 1.3962\n",
            "Epoch [99/100], Batch [301/346], Loss: 1.3087\n",
            "Epoch [99/100], Batch [302/346], Loss: 1.3681\n",
            "Epoch [99/100], Batch [303/346], Loss: 1.4005\n",
            "Epoch [99/100], Batch [304/346], Loss: 1.2627\n",
            "Epoch [99/100], Batch [305/346], Loss: 1.5239\n",
            "Epoch [99/100], Batch [306/346], Loss: 1.2887\n",
            "Epoch [99/100], Batch [307/346], Loss: 1.9719\n",
            "Epoch [99/100], Batch [308/346], Loss: 1.3351\n",
            "Epoch [99/100], Batch [309/346], Loss: 1.8656\n",
            "Epoch [99/100], Batch [310/346], Loss: 1.4960\n",
            "Epoch [99/100], Batch [311/346], Loss: 1.4719\n",
            "Epoch [99/100], Batch [312/346], Loss: 1.4011\n",
            "Epoch [99/100], Batch [313/346], Loss: 1.7182\n",
            "Epoch [99/100], Batch [314/346], Loss: 1.4971\n",
            "Epoch [99/100], Batch [315/346], Loss: 1.4032\n",
            "Epoch [99/100], Batch [316/346], Loss: 1.7556\n",
            "Epoch [99/100], Batch [317/346], Loss: 1.6750\n",
            "Epoch [99/100], Batch [318/346], Loss: 1.3604\n",
            "Epoch [99/100], Batch [319/346], Loss: 1.4844\n",
            "Epoch [99/100], Batch [320/346], Loss: 1.7521\n",
            "Epoch [99/100], Batch [321/346], Loss: 1.6119\n",
            "Epoch [99/100], Batch [322/346], Loss: 1.2505\n",
            "Epoch [99/100], Batch [323/346], Loss: 1.4730\n",
            "Epoch [99/100], Batch [324/346], Loss: 1.3266\n",
            "Epoch [99/100], Batch [325/346], Loss: 1.5045\n",
            "Epoch [99/100], Batch [326/346], Loss: 1.5164\n",
            "Epoch [99/100], Batch [327/346], Loss: 1.4626\n",
            "Epoch [99/100], Batch [328/346], Loss: 1.9171\n",
            "Epoch [99/100], Batch [329/346], Loss: 1.4382\n",
            "Epoch [99/100], Batch [330/346], Loss: 1.6403\n",
            "Epoch [99/100], Batch [331/346], Loss: 1.1067\n",
            "Epoch [99/100], Batch [332/346], Loss: 1.7453\n",
            "Epoch [99/100], Batch [333/346], Loss: 1.5494\n",
            "Epoch [99/100], Batch [334/346], Loss: 1.6141\n",
            "Epoch [99/100], Batch [335/346], Loss: 1.6271\n",
            "Epoch [99/100], Batch [336/346], Loss: 1.2705\n",
            "Epoch [99/100], Batch [337/346], Loss: 1.2786\n",
            "Epoch [99/100], Batch [338/346], Loss: 1.5914\n",
            "Epoch [99/100], Batch [339/346], Loss: 1.6434\n",
            "Epoch [99/100], Batch [340/346], Loss: 1.3232\n",
            "Epoch [99/100], Batch [341/346], Loss: 1.1566\n",
            "Epoch [99/100], Batch [342/346], Loss: 1.4470\n",
            "Epoch [99/100], Batch [343/346], Loss: 1.5866\n",
            "Epoch [99/100], Batch [344/346], Loss: 1.4262\n",
            "Epoch [99/100], Batch [345/346], Loss: 1.4469\n",
            "Epoch [99/100], Batch [346/346], Loss: 1.5140\n",
            "Epoch [100/100], Batch [1/346], Loss: 1.1546\n",
            "Epoch [100/100], Batch [2/346], Loss: 1.4174\n",
            "Epoch [100/100], Batch [3/346], Loss: 1.4215\n",
            "Epoch [100/100], Batch [4/346], Loss: 1.4097\n",
            "Epoch [100/100], Batch [5/346], Loss: 1.4583\n",
            "Epoch [100/100], Batch [6/346], Loss: 1.3182\n",
            "Epoch [100/100], Batch [7/346], Loss: 1.0739\n",
            "Epoch [100/100], Batch [8/346], Loss: 1.2408\n",
            "Epoch [100/100], Batch [9/346], Loss: 1.7666\n",
            "Epoch [100/100], Batch [10/346], Loss: 1.3282\n",
            "Epoch [100/100], Batch [11/346], Loss: 1.4199\n",
            "Epoch [100/100], Batch [12/346], Loss: 1.3955\n",
            "Epoch [100/100], Batch [13/346], Loss: 1.7227\n",
            "Epoch [100/100], Batch [14/346], Loss: 1.3932\n",
            "Epoch [100/100], Batch [15/346], Loss: 1.4810\n",
            "Epoch [100/100], Batch [16/346], Loss: 1.7476\n",
            "Epoch [100/100], Batch [17/346], Loss: 1.5270\n",
            "Epoch [100/100], Batch [18/346], Loss: 1.4120\n",
            "Epoch [100/100], Batch [19/346], Loss: 1.4583\n",
            "Epoch [100/100], Batch [20/346], Loss: 1.3996\n",
            "Epoch [100/100], Batch [21/346], Loss: 1.5151\n",
            "Epoch [100/100], Batch [22/346], Loss: 1.2783\n",
            "Epoch [100/100], Batch [23/346], Loss: 1.2256\n",
            "Epoch [100/100], Batch [24/346], Loss: 1.2850\n",
            "Epoch [100/100], Batch [25/346], Loss: 1.5633\n",
            "Epoch [100/100], Batch [26/346], Loss: 1.3051\n",
            "Epoch [100/100], Batch [27/346], Loss: 1.1953\n",
            "Epoch [100/100], Batch [28/346], Loss: 1.3778\n",
            "Epoch [100/100], Batch [29/346], Loss: 1.6914\n",
            "Epoch [100/100], Batch [30/346], Loss: 1.2478\n",
            "Epoch [100/100], Batch [31/346], Loss: 1.2715\n",
            "Epoch [100/100], Batch [32/346], Loss: 1.4212\n",
            "Epoch [100/100], Batch [33/346], Loss: 1.1748\n",
            "Epoch [100/100], Batch [34/346], Loss: 1.5178\n",
            "Epoch [100/100], Batch [35/346], Loss: 1.2405\n",
            "Epoch [100/100], Batch [36/346], Loss: 1.2035\n",
            "Epoch [100/100], Batch [37/346], Loss: 1.6217\n",
            "Epoch [100/100], Batch [38/346], Loss: 1.6408\n",
            "Epoch [100/100], Batch [39/346], Loss: 1.2036\n",
            "Epoch [100/100], Batch [40/346], Loss: 1.6340\n",
            "Epoch [100/100], Batch [41/346], Loss: 1.5842\n",
            "Epoch [100/100], Batch [42/346], Loss: 1.8704\n",
            "Epoch [100/100], Batch [43/346], Loss: 1.3477\n",
            "Epoch [100/100], Batch [44/346], Loss: 1.7805\n",
            "Epoch [100/100], Batch [45/346], Loss: 1.6799\n",
            "Epoch [100/100], Batch [46/346], Loss: 1.8303\n",
            "Epoch [100/100], Batch [47/346], Loss: 1.4393\n",
            "Epoch [100/100], Batch [48/346], Loss: 1.5968\n",
            "Epoch [100/100], Batch [49/346], Loss: 1.7568\n",
            "Epoch [100/100], Batch [50/346], Loss: 1.5620\n",
            "Epoch [100/100], Batch [51/346], Loss: 1.2935\n",
            "Epoch [100/100], Batch [52/346], Loss: 1.4419\n",
            "Epoch [100/100], Batch [53/346], Loss: 1.4629\n",
            "Epoch [100/100], Batch [54/346], Loss: 1.5898\n",
            "Epoch [100/100], Batch [55/346], Loss: 1.4591\n",
            "Epoch [100/100], Batch [56/346], Loss: 1.3980\n",
            "Epoch [100/100], Batch [57/346], Loss: 1.4130\n",
            "Epoch [100/100], Batch [58/346], Loss: 1.7889\n",
            "Epoch [100/100], Batch [59/346], Loss: 1.3238\n",
            "Epoch [100/100], Batch [60/346], Loss: 1.5640\n",
            "Epoch [100/100], Batch [61/346], Loss: 1.4967\n",
            "Epoch [100/100], Batch [62/346], Loss: 1.8072\n",
            "Epoch [100/100], Batch [63/346], Loss: 1.4065\n",
            "Epoch [100/100], Batch [64/346], Loss: 1.2979\n",
            "Epoch [100/100], Batch [65/346], Loss: 1.5439\n",
            "Epoch [100/100], Batch [66/346], Loss: 1.3336\n",
            "Epoch [100/100], Batch [67/346], Loss: 1.3918\n",
            "Epoch [100/100], Batch [68/346], Loss: 1.6172\n",
            "Epoch [100/100], Batch [69/346], Loss: 1.4981\n",
            "Epoch [100/100], Batch [70/346], Loss: 1.3733\n",
            "Epoch [100/100], Batch [71/346], Loss: 1.2478\n",
            "Epoch [100/100], Batch [72/346], Loss: 1.3340\n",
            "Epoch [100/100], Batch [73/346], Loss: 1.4746\n",
            "Epoch [100/100], Batch [74/346], Loss: 2.0499\n",
            "Epoch [100/100], Batch [75/346], Loss: 1.4678\n",
            "Epoch [100/100], Batch [76/346], Loss: 1.4841\n",
            "Epoch [100/100], Batch [77/346], Loss: 1.4766\n",
            "Epoch [100/100], Batch [78/346], Loss: 1.4889\n",
            "Epoch [100/100], Batch [79/346], Loss: 1.4145\n",
            "Epoch [100/100], Batch [80/346], Loss: 1.6310\n",
            "Epoch [100/100], Batch [81/346], Loss: 1.5757\n",
            "Epoch [100/100], Batch [82/346], Loss: 1.2678\n",
            "Epoch [100/100], Batch [83/346], Loss: 1.5530\n",
            "Epoch [100/100], Batch [84/346], Loss: 1.5906\n",
            "Epoch [100/100], Batch [85/346], Loss: 1.4472\n",
            "Epoch [100/100], Batch [86/346], Loss: 1.8044\n",
            "Epoch [100/100], Batch [87/346], Loss: 1.2037\n",
            "Epoch [100/100], Batch [88/346], Loss: 1.5446\n",
            "Epoch [100/100], Batch [89/346], Loss: 1.0942\n",
            "Epoch [100/100], Batch [90/346], Loss: 1.3125\n",
            "Epoch [100/100], Batch [91/346], Loss: 1.8011\n",
            "Epoch [100/100], Batch [92/346], Loss: 1.4723\n",
            "Epoch [100/100], Batch [93/346], Loss: 1.3910\n",
            "Epoch [100/100], Batch [94/346], Loss: 1.6530\n",
            "Epoch [100/100], Batch [95/346], Loss: 1.4353\n",
            "Epoch [100/100], Batch [96/346], Loss: 1.4987\n",
            "Epoch [100/100], Batch [97/346], Loss: 1.5740\n",
            "Epoch [100/100], Batch [98/346], Loss: 1.4464\n",
            "Epoch [100/100], Batch [99/346], Loss: 1.4818\n",
            "Epoch [100/100], Batch [100/346], Loss: 1.1467\n",
            "Epoch [100/100], Batch [101/346], Loss: 1.3194\n",
            "Epoch [100/100], Batch [102/346], Loss: 1.4144\n",
            "Epoch [100/100], Batch [103/346], Loss: 1.6006\n",
            "Epoch [100/100], Batch [104/346], Loss: 1.2437\n",
            "Epoch [100/100], Batch [105/346], Loss: 2.0220\n",
            "Epoch [100/100], Batch [106/346], Loss: 1.6730\n",
            "Epoch [100/100], Batch [107/346], Loss: 1.4228\n",
            "Epoch [100/100], Batch [108/346], Loss: 1.5007\n",
            "Epoch [100/100], Batch [109/346], Loss: 1.3256\n",
            "Epoch [100/100], Batch [110/346], Loss: 1.6373\n",
            "Epoch [100/100], Batch [111/346], Loss: 1.3751\n",
            "Epoch [100/100], Batch [112/346], Loss: 1.3780\n",
            "Epoch [100/100], Batch [113/346], Loss: 1.2710\n",
            "Epoch [100/100], Batch [114/346], Loss: 1.8164\n",
            "Epoch [100/100], Batch [115/346], Loss: 1.3076\n",
            "Epoch [100/100], Batch [116/346], Loss: 2.0529\n",
            "Epoch [100/100], Batch [117/346], Loss: 2.4393\n",
            "Epoch [100/100], Batch [118/346], Loss: 1.7405\n",
            "Epoch [100/100], Batch [119/346], Loss: 1.1054\n",
            "Epoch [100/100], Batch [120/346], Loss: 1.4547\n",
            "Epoch [100/100], Batch [121/346], Loss: 1.7878\n",
            "Epoch [100/100], Batch [122/346], Loss: 1.6039\n",
            "Epoch [100/100], Batch [123/346], Loss: 1.2186\n",
            "Epoch [100/100], Batch [124/346], Loss: 1.9654\n",
            "Epoch [100/100], Batch [125/346], Loss: 1.4952\n",
            "Epoch [100/100], Batch [126/346], Loss: 1.2644\n",
            "Epoch [100/100], Batch [127/346], Loss: 1.1396\n",
            "Epoch [100/100], Batch [128/346], Loss: 1.6581\n",
            "Epoch [100/100], Batch [129/346], Loss: 1.4380\n",
            "Epoch [100/100], Batch [130/346], Loss: 1.2272\n",
            "Epoch [100/100], Batch [131/346], Loss: 1.4974\n",
            "Epoch [100/100], Batch [132/346], Loss: 1.7552\n",
            "Epoch [100/100], Batch [133/346], Loss: 1.8065\n",
            "Epoch [100/100], Batch [134/346], Loss: 1.2896\n",
            "Epoch [100/100], Batch [135/346], Loss: 1.2791\n",
            "Epoch [100/100], Batch [136/346], Loss: 1.7543\n",
            "Epoch [100/100], Batch [137/346], Loss: 1.5806\n",
            "Epoch [100/100], Batch [138/346], Loss: 1.2990\n",
            "Epoch [100/100], Batch [139/346], Loss: 1.5701\n",
            "Epoch [100/100], Batch [140/346], Loss: 1.5943\n",
            "Epoch [100/100], Batch [141/346], Loss: 1.1591\n",
            "Epoch [100/100], Batch [142/346], Loss: 1.7625\n",
            "Epoch [100/100], Batch [143/346], Loss: 1.6071\n",
            "Epoch [100/100], Batch [144/346], Loss: 1.6681\n",
            "Epoch [100/100], Batch [145/346], Loss: 1.3170\n",
            "Epoch [100/100], Batch [146/346], Loss: 1.2621\n",
            "Epoch [100/100], Batch [147/346], Loss: 1.5732\n",
            "Epoch [100/100], Batch [148/346], Loss: 1.8763\n",
            "Epoch [100/100], Batch [149/346], Loss: 1.3411\n",
            "Epoch [100/100], Batch [150/346], Loss: 1.7149\n",
            "Epoch [100/100], Batch [151/346], Loss: 2.0061\n",
            "Epoch [100/100], Batch [152/346], Loss: 2.1237\n",
            "Epoch [100/100], Batch [153/346], Loss: 1.3931\n",
            "Epoch [100/100], Batch [154/346], Loss: 1.3650\n",
            "Epoch [100/100], Batch [155/346], Loss: 1.4427\n",
            "Epoch [100/100], Batch [156/346], Loss: 1.0932\n",
            "Epoch [100/100], Batch [157/346], Loss: 1.9533\n",
            "Epoch [100/100], Batch [158/346], Loss: 1.1137\n",
            "Epoch [100/100], Batch [159/346], Loss: 1.3374\n",
            "Epoch [100/100], Batch [160/346], Loss: 1.1801\n",
            "Epoch [100/100], Batch [161/346], Loss: 1.6720\n",
            "Epoch [100/100], Batch [162/346], Loss: 1.9231\n",
            "Epoch [100/100], Batch [163/346], Loss: 1.5681\n",
            "Epoch [100/100], Batch [164/346], Loss: 1.7567\n",
            "Epoch [100/100], Batch [165/346], Loss: 1.5149\n",
            "Epoch [100/100], Batch [166/346], Loss: 1.8185\n",
            "Epoch [100/100], Batch [167/346], Loss: 1.8205\n",
            "Epoch [100/100], Batch [168/346], Loss: 1.4534\n",
            "Epoch [100/100], Batch [169/346], Loss: 1.4283\n",
            "Epoch [100/100], Batch [170/346], Loss: 1.6351\n",
            "Epoch [100/100], Batch [171/346], Loss: 1.8460\n",
            "Epoch [100/100], Batch [172/346], Loss: 1.7431\n",
            "Epoch [100/100], Batch [173/346], Loss: 1.9386\n",
            "Epoch [100/100], Batch [174/346], Loss: 1.8053\n",
            "Epoch [100/100], Batch [175/346], Loss: 1.3758\n",
            "Epoch [100/100], Batch [176/346], Loss: 1.4547\n",
            "Epoch [100/100], Batch [177/346], Loss: 1.3722\n",
            "Epoch [100/100], Batch [178/346], Loss: 1.3737\n",
            "Epoch [100/100], Batch [179/346], Loss: 1.7409\n",
            "Epoch [100/100], Batch [180/346], Loss: 1.0714\n",
            "Epoch [100/100], Batch [181/346], Loss: 2.2657\n",
            "Epoch [100/100], Batch [182/346], Loss: 1.1789\n",
            "Epoch [100/100], Batch [183/346], Loss: 1.5314\n",
            "Epoch [100/100], Batch [184/346], Loss: 1.4682\n",
            "Epoch [100/100], Batch [185/346], Loss: 1.6760\n",
            "Epoch [100/100], Batch [186/346], Loss: 1.4231\n",
            "Epoch [100/100], Batch [187/346], Loss: 1.4647\n",
            "Epoch [100/100], Batch [188/346], Loss: 1.9755\n",
            "Epoch [100/100], Batch [189/346], Loss: 1.7739\n",
            "Epoch [100/100], Batch [190/346], Loss: 1.8500\n",
            "Epoch [100/100], Batch [191/346], Loss: 1.5864\n",
            "Epoch [100/100], Batch [192/346], Loss: 1.5155\n",
            "Epoch [100/100], Batch [193/346], Loss: 1.3888\n",
            "Epoch [100/100], Batch [194/346], Loss: 1.2441\n",
            "Epoch [100/100], Batch [195/346], Loss: 1.4864\n",
            "Epoch [100/100], Batch [196/346], Loss: 1.6244\n",
            "Epoch [100/100], Batch [197/346], Loss: 2.2389\n",
            "Epoch [100/100], Batch [198/346], Loss: 1.4719\n",
            "Epoch [100/100], Batch [199/346], Loss: 1.3846\n",
            "Epoch [100/100], Batch [200/346], Loss: 1.8057\n",
            "Epoch [100/100], Batch [201/346], Loss: 1.4937\n",
            "Epoch [100/100], Batch [202/346], Loss: 1.6668\n",
            "Epoch [100/100], Batch [203/346], Loss: 1.1805\n",
            "Epoch [100/100], Batch [204/346], Loss: 1.1292\n",
            "Epoch [100/100], Batch [205/346], Loss: 1.4149\n",
            "Epoch [100/100], Batch [206/346], Loss: 1.6637\n",
            "Epoch [100/100], Batch [207/346], Loss: 1.6010\n",
            "Epoch [100/100], Batch [208/346], Loss: 2.3024\n",
            "Epoch [100/100], Batch [209/346], Loss: 1.3391\n",
            "Epoch [100/100], Batch [210/346], Loss: 1.3416\n",
            "Epoch [100/100], Batch [211/346], Loss: 1.4515\n",
            "Epoch [100/100], Batch [212/346], Loss: 1.5583\n",
            "Epoch [100/100], Batch [213/346], Loss: 1.9702\n",
            "Epoch [100/100], Batch [214/346], Loss: 1.7306\n",
            "Epoch [100/100], Batch [215/346], Loss: 1.7183\n",
            "Epoch [100/100], Batch [216/346], Loss: 1.8356\n",
            "Epoch [100/100], Batch [217/346], Loss: 1.5769\n",
            "Epoch [100/100], Batch [218/346], Loss: 1.2904\n",
            "Epoch [100/100], Batch [219/346], Loss: 1.7895\n",
            "Epoch [100/100], Batch [220/346], Loss: 1.7777\n",
            "Epoch [100/100], Batch [221/346], Loss: 1.9535\n",
            "Epoch [100/100], Batch [222/346], Loss: 1.8896\n",
            "Epoch [100/100], Batch [223/346], Loss: 1.2594\n",
            "Epoch [100/100], Batch [224/346], Loss: 1.4983\n",
            "Epoch [100/100], Batch [225/346], Loss: 1.5008\n",
            "Epoch [100/100], Batch [226/346], Loss: 1.7483\n",
            "Epoch [100/100], Batch [227/346], Loss: 1.6605\n",
            "Epoch [100/100], Batch [228/346], Loss: 1.7393\n",
            "Epoch [100/100], Batch [229/346], Loss: 1.4264\n",
            "Epoch [100/100], Batch [230/346], Loss: 1.5184\n",
            "Epoch [100/100], Batch [231/346], Loss: 1.4396\n",
            "Epoch [100/100], Batch [232/346], Loss: 1.6104\n",
            "Epoch [100/100], Batch [233/346], Loss: 1.7854\n",
            "Epoch [100/100], Batch [234/346], Loss: 1.6143\n",
            "Epoch [100/100], Batch [235/346], Loss: 1.1223\n",
            "Epoch [100/100], Batch [236/346], Loss: 1.3392\n",
            "Epoch [100/100], Batch [237/346], Loss: 1.1246\n",
            "Epoch [100/100], Batch [238/346], Loss: 1.7307\n",
            "Epoch [100/100], Batch [239/346], Loss: 1.2256\n",
            "Epoch [100/100], Batch [240/346], Loss: 1.4434\n",
            "Epoch [100/100], Batch [241/346], Loss: 1.9710\n",
            "Epoch [100/100], Batch [242/346], Loss: 1.3939\n",
            "Epoch [100/100], Batch [243/346], Loss: 1.6247\n",
            "Epoch [100/100], Batch [244/346], Loss: 1.4425\n",
            "Epoch [100/100], Batch [245/346], Loss: 1.4736\n",
            "Epoch [100/100], Batch [246/346], Loss: 2.6053\n",
            "Epoch [100/100], Batch [247/346], Loss: 1.1563\n",
            "Epoch [100/100], Batch [248/346], Loss: 1.9672\n",
            "Epoch [100/100], Batch [249/346], Loss: 1.4263\n",
            "Epoch [100/100], Batch [250/346], Loss: 1.4402\n",
            "Epoch [100/100], Batch [251/346], Loss: 1.7650\n",
            "Epoch [100/100], Batch [252/346], Loss: 1.5419\n",
            "Epoch [100/100], Batch [253/346], Loss: 1.7238\n",
            "Epoch [100/100], Batch [254/346], Loss: 1.6663\n",
            "Epoch [100/100], Batch [255/346], Loss: 1.5398\n",
            "Epoch [100/100], Batch [256/346], Loss: 1.5696\n",
            "Epoch [100/100], Batch [257/346], Loss: 1.0158\n",
            "Epoch [100/100], Batch [258/346], Loss: 1.3833\n",
            "Epoch [100/100], Batch [259/346], Loss: 1.2023\n",
            "Epoch [100/100], Batch [260/346], Loss: 1.4897\n",
            "Epoch [100/100], Batch [261/346], Loss: 1.6653\n",
            "Epoch [100/100], Batch [262/346], Loss: 1.4067\n",
            "Epoch [100/100], Batch [263/346], Loss: 1.4358\n",
            "Epoch [100/100], Batch [264/346], Loss: 1.2471\n",
            "Epoch [100/100], Batch [265/346], Loss: 1.4415\n",
            "Epoch [100/100], Batch [266/346], Loss: 1.6227\n",
            "Epoch [100/100], Batch [267/346], Loss: 1.3540\n",
            "Epoch [100/100], Batch [268/346], Loss: 1.3652\n",
            "Epoch [100/100], Batch [269/346], Loss: 1.6000\n",
            "Epoch [100/100], Batch [270/346], Loss: 1.5796\n",
            "Epoch [100/100], Batch [271/346], Loss: 1.5981\n",
            "Epoch [100/100], Batch [272/346], Loss: 1.3862\n",
            "Epoch [100/100], Batch [273/346], Loss: 1.5118\n",
            "Epoch [100/100], Batch [274/346], Loss: 1.4116\n",
            "Epoch [100/100], Batch [275/346], Loss: 1.5349\n",
            "Epoch [100/100], Batch [276/346], Loss: 1.1570\n",
            "Epoch [100/100], Batch [277/346], Loss: 1.4195\n",
            "Epoch [100/100], Batch [278/346], Loss: 1.7549\n",
            "Epoch [100/100], Batch [279/346], Loss: 1.2221\n",
            "Epoch [100/100], Batch [280/346], Loss: 1.2899\n",
            "Epoch [100/100], Batch [281/346], Loss: 1.2830\n",
            "Epoch [100/100], Batch [282/346], Loss: 1.4907\n",
            "Epoch [100/100], Batch [283/346], Loss: 1.8849\n",
            "Epoch [100/100], Batch [284/346], Loss: 1.5514\n",
            "Epoch [100/100], Batch [285/346], Loss: 1.2336\n",
            "Epoch [100/100], Batch [286/346], Loss: 1.9436\n",
            "Epoch [100/100], Batch [287/346], Loss: 1.3316\n",
            "Epoch [100/100], Batch [288/346], Loss: 1.3954\n",
            "Epoch [100/100], Batch [289/346], Loss: 1.4241\n",
            "Epoch [100/100], Batch [290/346], Loss: 1.3746\n",
            "Epoch [100/100], Batch [291/346], Loss: 1.5025\n",
            "Epoch [100/100], Batch [292/346], Loss: 1.5412\n",
            "Epoch [100/100], Batch [293/346], Loss: 1.6117\n",
            "Epoch [100/100], Batch [294/346], Loss: 1.5907\n",
            "Epoch [100/100], Batch [295/346], Loss: 1.4320\n",
            "Epoch [100/100], Batch [296/346], Loss: 1.2834\n",
            "Epoch [100/100], Batch [297/346], Loss: 1.5617\n",
            "Epoch [100/100], Batch [298/346], Loss: 1.4232\n",
            "Epoch [100/100], Batch [299/346], Loss: 1.6249\n",
            "Epoch [100/100], Batch [300/346], Loss: 1.7270\n",
            "Epoch [100/100], Batch [301/346], Loss: 1.3026\n",
            "Epoch [100/100], Batch [302/346], Loss: 1.6360\n",
            "Epoch [100/100], Batch [303/346], Loss: 1.5296\n",
            "Epoch [100/100], Batch [304/346], Loss: 1.2908\n",
            "Epoch [100/100], Batch [305/346], Loss: 1.2382\n",
            "Epoch [100/100], Batch [306/346], Loss: 1.6651\n",
            "Epoch [100/100], Batch [307/346], Loss: 1.8654\n",
            "Epoch [100/100], Batch [308/346], Loss: 1.7158\n",
            "Epoch [100/100], Batch [309/346], Loss: 2.2231\n",
            "Epoch [100/100], Batch [310/346], Loss: 1.3337\n",
            "Epoch [100/100], Batch [311/346], Loss: 2.0398\n",
            "Epoch [100/100], Batch [312/346], Loss: 1.8096\n",
            "Epoch [100/100], Batch [313/346], Loss: 1.7247\n",
            "Epoch [100/100], Batch [314/346], Loss: 1.6111\n",
            "Epoch [100/100], Batch [315/346], Loss: 1.7691\n",
            "Epoch [100/100], Batch [316/346], Loss: 1.8325\n",
            "Epoch [100/100], Batch [317/346], Loss: 1.0793\n",
            "Epoch [100/100], Batch [318/346], Loss: 2.6348\n",
            "Epoch [100/100], Batch [319/346], Loss: 1.7917\n",
            "Epoch [100/100], Batch [320/346], Loss: 1.6024\n",
            "Epoch [100/100], Batch [321/346], Loss: 1.6307\n",
            "Epoch [100/100], Batch [322/346], Loss: 1.6637\n",
            "Epoch [100/100], Batch [323/346], Loss: 1.7482\n",
            "Epoch [100/100], Batch [324/346], Loss: 1.8800\n",
            "Epoch [100/100], Batch [325/346], Loss: 1.3851\n",
            "Epoch [100/100], Batch [326/346], Loss: 1.3371\n",
            "Epoch [100/100], Batch [327/346], Loss: 1.0799\n",
            "Epoch [100/100], Batch [328/346], Loss: 1.3332\n",
            "Epoch [100/100], Batch [329/346], Loss: 1.6006\n",
            "Epoch [100/100], Batch [330/346], Loss: 1.6923\n",
            "Epoch [100/100], Batch [331/346], Loss: 1.7185\n",
            "Epoch [100/100], Batch [332/346], Loss: 1.4297\n",
            "Epoch [100/100], Batch [333/346], Loss: 1.4590\n",
            "Epoch [100/100], Batch [334/346], Loss: 1.4730\n",
            "Epoch [100/100], Batch [335/346], Loss: 1.3520\n",
            "Epoch [100/100], Batch [336/346], Loss: 2.0728\n",
            "Epoch [100/100], Batch [337/346], Loss: 1.5782\n",
            "Epoch [100/100], Batch [338/346], Loss: 1.4198\n",
            "Epoch [100/100], Batch [339/346], Loss: 1.6776\n",
            "Epoch [100/100], Batch [340/346], Loss: 1.6997\n",
            "Epoch [100/100], Batch [341/346], Loss: 1.1769\n",
            "Epoch [100/100], Batch [342/346], Loss: 1.5880\n",
            "Epoch [100/100], Batch [343/346], Loss: 1.5330\n",
            "Epoch [100/100], Batch [344/346], Loss: 1.8853\n",
            "Epoch [100/100], Batch [345/346], Loss: 1.4525\n",
            "Epoch [100/100], Batch [346/346], Loss: 1.5783\n",
            "Finishing training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "\n",
        "# Instantiate the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1)  # loss 2.6101\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # 2.5928\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0) # loss 2.6425\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=1) # loss 2.5945\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1) # loss 2..5951\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)#2.6039\n",
        "\n",
        "\n",
        "# Initialize the best validation loss and best_model_wts before the training loop\n",
        "best_val_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "num_epochs = 200\n",
        "patience = 50  # Number of epochs with no improvement after which training will be stopped\n",
        "no_improve_epochs = 0  # Number of epochs with no improvement\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for batch_idx, (hsi_batch, lidar_batch, label_batch) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        hsi_batch = hsi_batch.to(device)\n",
        "        lidar_batch = lidar_batch.to(device)\n",
        "        label_batch = label_batch.to(device)  # Reshape labels\n",
        "\n",
        "        # Forward pass\n",
        "        output, attn_scores  = model(lidar_batch, hsi_batch)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.transpose(1, 2), label_batch.squeeze(2))\n",
        "        running_train_loss += loss.item() * hsi_batch.size(0)  # Multiply by batch size\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate epoch training loss\n",
        "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for hsi_batch, lidar_batch, label_batch in val_loader:\n",
        "            # Move tensors to the configured device\n",
        "            hsi_batch = hsi_batch.to(device)\n",
        "            lidar_batch = lidar_batch.to(device)\n",
        "            label_batch = label_batch.to(device)  # Reshape labels\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(lidar_batch, hsi_batch)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output[0].transpose(1, 2), label_batch.squeeze(2))\n",
        "            running_val_loss += loss.item() * hsi_batch.size(0)  # Multiply by batch size\n",
        "\n",
        "    # Calculate epoch validation loss\n",
        "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
        "\n",
        "    # If the validation loss is lower than the current best, save the model's state\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        print(f'Validation Loss Decreased({best_val_loss:.6f}--->{epoch_val_loss:.6f}) \\t Saving The Model')\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        no_improve_epochs = 0  # reset count\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "\n",
        "    # Check early stopping condition\n",
        "    if no_improve_epochs > patience:\n",
        "        print('Early stopping!')\n",
        "        model.load_state_dict(best_model_wts)  # load best model\n",
        "        break\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    #scheduler.step(epoch_val_loss)\n",
        "\n",
        "print('Finished training')\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_wts, path+'muufl_best_model_weights.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP23mD1wx63c",
        "outputId": "23439169-a226-4b0c-8fa6-f44c8cb93b54"
      },
      "id": "DP23mD1wx63c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Train Loss: 70.8700, Val Loss: 51.4951\n",
            "Validation Loss Decreased(inf--->51.495129) \t Saving The Model\n",
            "Epoch [2/200], Train Loss: 42.0242, Val Loss: 29.4395\n",
            "Validation Loss Decreased(51.495129--->29.439477) \t Saving The Model\n",
            "Epoch [3/200], Train Loss: 25.6073, Val Loss: 19.7197\n",
            "Validation Loss Decreased(29.439477--->19.719674) \t Saving The Model\n",
            "Epoch [4/200], Train Loss: 17.3049, Val Loss: 14.2144\n",
            "Validation Loss Decreased(19.719674--->14.214400) \t Saving The Model\n",
            "Epoch [5/200], Train Loss: 13.1908, Val Loss: 11.5645\n",
            "Validation Loss Decreased(14.214400--->11.564485) \t Saving The Model\n",
            "Epoch [6/200], Train Loss: 11.3333, Val Loss: 10.8401\n",
            "Validation Loss Decreased(11.564485--->10.840082) \t Saving The Model\n",
            "Epoch [7/200], Train Loss: 10.7802, Val Loss: 10.2955\n",
            "Validation Loss Decreased(10.840082--->10.295460) \t Saving The Model\n",
            "Epoch [8/200], Train Loss: 10.1617, Val Loss: 9.8754\n",
            "Validation Loss Decreased(10.295460--->9.875372) \t Saving The Model\n",
            "Epoch [9/200], Train Loss: 9.6309, Val Loss: 9.4810\n",
            "Validation Loss Decreased(9.875372--->9.480984) \t Saving The Model\n",
            "Epoch [10/200], Train Loss: 9.1928, Val Loss: 9.1863\n",
            "Validation Loss Decreased(9.480984--->9.186290) \t Saving The Model\n",
            "Epoch [11/200], Train Loss: 8.8448, Val Loss: 8.8793\n",
            "Validation Loss Decreased(9.186290--->8.879298) \t Saving The Model\n",
            "Epoch [12/200], Train Loss: 8.4769, Val Loss: 8.5780\n",
            "Validation Loss Decreased(8.879298--->8.577983) \t Saving The Model\n",
            "Epoch [13/200], Train Loss: 8.1121, Val Loss: 8.2983\n",
            "Validation Loss Decreased(8.577983--->8.298268) \t Saving The Model\n",
            "Epoch [14/200], Train Loss: 7.7271, Val Loss: 8.1188\n",
            "Validation Loss Decreased(8.298268--->8.118770) \t Saving The Model\n",
            "Epoch [15/200], Train Loss: 7.4436, Val Loss: 7.8410\n",
            "Validation Loss Decreased(8.118770--->7.841031) \t Saving The Model\n",
            "Epoch [16/200], Train Loss: 7.1356, Val Loss: 7.8016\n",
            "Validation Loss Decreased(7.841031--->7.801602) \t Saving The Model\n",
            "Epoch [17/200], Train Loss: 6.9586, Val Loss: 7.5963\n",
            "Validation Loss Decreased(7.801602--->7.596343) \t Saving The Model\n",
            "Epoch [18/200], Train Loss: 6.7547, Val Loss: 7.2995\n",
            "Validation Loss Decreased(7.596343--->7.299503) \t Saving The Model\n",
            "Epoch [19/200], Train Loss: 6.6055, Val Loss: 7.2906\n",
            "Validation Loss Decreased(7.299503--->7.290605) \t Saving The Model\n",
            "Epoch [20/200], Train Loss: 6.3936, Val Loss: 7.0307\n",
            "Validation Loss Decreased(7.290605--->7.030704) \t Saving The Model\n",
            "Epoch [21/200], Train Loss: 6.3070, Val Loss: 6.9914\n",
            "Validation Loss Decreased(7.030704--->6.991362) \t Saving The Model\n",
            "Epoch [22/200], Train Loss: 6.0978, Val Loss: 6.7760\n",
            "Validation Loss Decreased(6.991362--->6.776041) \t Saving The Model\n",
            "Epoch [23/200], Train Loss: 6.1185, Val Loss: 6.7117\n",
            "Validation Loss Decreased(6.776041--->6.711706) \t Saving The Model\n",
            "Epoch [24/200], Train Loss: 5.9141, Val Loss: 6.5849\n",
            "Validation Loss Decreased(6.711706--->6.584935) \t Saving The Model\n",
            "Epoch [25/200], Train Loss: 5.7254, Val Loss: 6.5313\n",
            "Validation Loss Decreased(6.584935--->6.531327) \t Saving The Model\n",
            "Epoch [26/200], Train Loss: 5.6825, Val Loss: 6.3333\n",
            "Validation Loss Decreased(6.531327--->6.333325) \t Saving The Model\n",
            "Epoch [27/200], Train Loss: 5.4635, Val Loss: 6.1552\n",
            "Validation Loss Decreased(6.333325--->6.155219) \t Saving The Model\n",
            "Epoch [28/200], Train Loss: 5.3421, Val Loss: 6.2120\n",
            "Epoch [29/200], Train Loss: 5.3723, Val Loss: 6.1613\n",
            "Epoch [30/200], Train Loss: 5.3092, Val Loss: 6.0451\n",
            "Validation Loss Decreased(6.155219--->6.045053) \t Saving The Model\n",
            "Epoch [31/200], Train Loss: 5.2397, Val Loss: 5.9980\n",
            "Validation Loss Decreased(6.045053--->5.997993) \t Saving The Model\n",
            "Epoch [32/200], Train Loss: 4.9916, Val Loss: 5.8945\n",
            "Validation Loss Decreased(5.997993--->5.894528) \t Saving The Model\n",
            "Epoch [33/200], Train Loss: 4.8947, Val Loss: 5.8154\n",
            "Validation Loss Decreased(5.894528--->5.815385) \t Saving The Model\n",
            "Epoch [34/200], Train Loss: 4.7641, Val Loss: 5.7381\n",
            "Validation Loss Decreased(5.815385--->5.738146) \t Saving The Model\n",
            "Epoch [35/200], Train Loss: 4.6617, Val Loss: 5.6173\n",
            "Validation Loss Decreased(5.738146--->5.617283) \t Saving The Model\n",
            "Epoch [36/200], Train Loss: 4.6449, Val Loss: 5.4632\n",
            "Validation Loss Decreased(5.617283--->5.463198) \t Saving The Model\n",
            "Epoch [37/200], Train Loss: 4.5899, Val Loss: 5.3935\n",
            "Validation Loss Decreased(5.463198--->5.393490) \t Saving The Model\n",
            "Epoch [38/200], Train Loss: 4.4309, Val Loss: 5.3437\n",
            "Validation Loss Decreased(5.393490--->5.343694) \t Saving The Model\n",
            "Epoch [39/200], Train Loss: 4.4134, Val Loss: 5.3571\n",
            "Epoch [40/200], Train Loss: 4.3733, Val Loss: 5.3214\n",
            "Validation Loss Decreased(5.343694--->5.321434) \t Saving The Model\n",
            "Epoch [41/200], Train Loss: 4.2883, Val Loss: 5.3165\n",
            "Validation Loss Decreased(5.321434--->5.316533) \t Saving The Model\n",
            "Epoch [42/200], Train Loss: 4.2351, Val Loss: 5.2470\n",
            "Validation Loss Decreased(5.316533--->5.247013) \t Saving The Model\n",
            "Epoch [43/200], Train Loss: 4.2423, Val Loss: 5.2377\n",
            "Validation Loss Decreased(5.247013--->5.237716) \t Saving The Model\n",
            "Epoch [44/200], Train Loss: 4.0722, Val Loss: 5.0581\n",
            "Validation Loss Decreased(5.237716--->5.058072) \t Saving The Model\n",
            "Epoch [45/200], Train Loss: 4.0096, Val Loss: 4.9958\n",
            "Validation Loss Decreased(5.058072--->4.995819) \t Saving The Model\n",
            "Epoch [46/200], Train Loss: 4.0064, Val Loss: 4.9208\n",
            "Validation Loss Decreased(4.995819--->4.920828) \t Saving The Model\n",
            "Epoch [47/200], Train Loss: 3.9392, Val Loss: 4.8538\n",
            "Validation Loss Decreased(4.920828--->4.853807) \t Saving The Model\n",
            "Epoch [48/200], Train Loss: 3.8887, Val Loss: 4.8696\n",
            "Epoch [49/200], Train Loss: 3.8107, Val Loss: 4.8199\n",
            "Validation Loss Decreased(4.853807--->4.819864) \t Saving The Model\n",
            "Epoch [50/200], Train Loss: 3.7925, Val Loss: 4.7840\n",
            "Validation Loss Decreased(4.819864--->4.784037) \t Saving The Model\n",
            "Epoch [51/200], Train Loss: 3.7035, Val Loss: 4.7646\n",
            "Validation Loss Decreased(4.784037--->4.764577) \t Saving The Model\n",
            "Epoch [52/200], Train Loss: 3.7386, Val Loss: 4.8607\n",
            "Epoch [53/200], Train Loss: 3.7704, Val Loss: 4.6825\n",
            "Validation Loss Decreased(4.764577--->4.682480) \t Saving The Model\n",
            "Epoch [54/200], Train Loss: 3.5557, Val Loss: 4.6791\n",
            "Validation Loss Decreased(4.682480--->4.679142) \t Saving The Model\n",
            "Epoch [55/200], Train Loss: 3.5445, Val Loss: 4.6761\n",
            "Validation Loss Decreased(4.679142--->4.676126) \t Saving The Model\n",
            "Epoch [56/200], Train Loss: 3.5517, Val Loss: 4.5490\n",
            "Validation Loss Decreased(4.676126--->4.549028) \t Saving The Model\n",
            "Epoch [57/200], Train Loss: 3.4359, Val Loss: 4.5398\n",
            "Validation Loss Decreased(4.549028--->4.539792) \t Saving The Model\n",
            "Epoch [58/200], Train Loss: 3.4595, Val Loss: 4.4799\n",
            "Validation Loss Decreased(4.539792--->4.479939) \t Saving The Model\n",
            "Epoch [59/200], Train Loss: 3.4308, Val Loss: 4.3924\n",
            "Validation Loss Decreased(4.479939--->4.392380) \t Saving The Model\n",
            "Epoch [60/200], Train Loss: 3.3488, Val Loss: 4.3699\n",
            "Validation Loss Decreased(4.392380--->4.369900) \t Saving The Model\n",
            "Epoch [61/200], Train Loss: 3.2992, Val Loss: 4.4219\n",
            "Epoch [62/200], Train Loss: 3.3133, Val Loss: 4.4317\n",
            "Epoch [63/200], Train Loss: 3.2679, Val Loss: 4.3597\n",
            "Validation Loss Decreased(4.369900--->4.359714) \t Saving The Model\n",
            "Epoch [64/200], Train Loss: 3.2618, Val Loss: 4.2604\n",
            "Validation Loss Decreased(4.359714--->4.260350) \t Saving The Model\n",
            "Epoch [65/200], Train Loss: 3.1859, Val Loss: 4.2517\n",
            "Validation Loss Decreased(4.260350--->4.251685) \t Saving The Model\n",
            "Epoch [66/200], Train Loss: 3.1731, Val Loss: 4.1522\n",
            "Validation Loss Decreased(4.251685--->4.152150) \t Saving The Model\n",
            "Epoch [67/200], Train Loss: 3.1757, Val Loss: 4.3185\n",
            "Epoch [68/200], Train Loss: 3.1373, Val Loss: 4.2309\n",
            "Epoch [69/200], Train Loss: 3.1275, Val Loss: 4.3407\n",
            "Epoch [70/200], Train Loss: 3.1326, Val Loss: 4.1741\n",
            "Epoch [71/200], Train Loss: 3.1450, Val Loss: 4.0829\n",
            "Validation Loss Decreased(4.152150--->4.082895) \t Saving The Model\n",
            "Epoch [72/200], Train Loss: 3.0584, Val Loss: 4.2060\n",
            "Epoch [73/200], Train Loss: 2.9785, Val Loss: 3.9968\n",
            "Validation Loss Decreased(4.082895--->3.996839) \t Saving The Model\n",
            "Epoch [74/200], Train Loss: 2.9723, Val Loss: 4.0825\n",
            "Epoch [75/200], Train Loss: 2.9235, Val Loss: 3.9812\n",
            "Validation Loss Decreased(3.996839--->3.981209) \t Saving The Model\n",
            "Epoch [76/200], Train Loss: 2.9148, Val Loss: 3.9947\n",
            "Epoch [77/200], Train Loss: 2.9079, Val Loss: 3.9716\n",
            "Validation Loss Decreased(3.981209--->3.971635) \t Saving The Model\n",
            "Epoch [78/200], Train Loss: 2.8649, Val Loss: 4.0395\n",
            "Epoch [79/200], Train Loss: 2.8834, Val Loss: 3.9481\n",
            "Validation Loss Decreased(3.971635--->3.948145) \t Saving The Model\n",
            "Epoch [80/200], Train Loss: 2.9358, Val Loss: 3.9549\n",
            "Epoch [81/200], Train Loss: 2.8367, Val Loss: 3.8151\n",
            "Validation Loss Decreased(3.948145--->3.815144) \t Saving The Model\n",
            "Epoch [82/200], Train Loss: 2.7740, Val Loss: 3.8910\n",
            "Epoch [83/200], Train Loss: 2.7256, Val Loss: 3.8268\n",
            "Epoch [84/200], Train Loss: 2.7744, Val Loss: 3.8456\n",
            "Epoch [85/200], Train Loss: 2.7753, Val Loss: 3.6998\n",
            "Validation Loss Decreased(3.815144--->3.699821) \t Saving The Model\n",
            "Epoch [86/200], Train Loss: 2.7278, Val Loss: 3.7317\n",
            "Epoch [87/200], Train Loss: 2.6944, Val Loss: 3.7455\n",
            "Epoch [88/200], Train Loss: 2.6794, Val Loss: 3.6918\n",
            "Validation Loss Decreased(3.699821--->3.691806) \t Saving The Model\n",
            "Epoch [89/200], Train Loss: 2.6785, Val Loss: 3.8226\n",
            "Epoch [90/200], Train Loss: 2.7434, Val Loss: 3.7223\n",
            "Epoch [91/200], Train Loss: 2.6430, Val Loss: 3.6352\n",
            "Validation Loss Decreased(3.691806--->3.635190) \t Saving The Model\n",
            "Epoch [92/200], Train Loss: 2.6460, Val Loss: 3.7634\n",
            "Epoch [93/200], Train Loss: 2.7456, Val Loss: 3.6056\n",
            "Validation Loss Decreased(3.635190--->3.605648) \t Saving The Model\n",
            "Epoch [94/200], Train Loss: 2.5732, Val Loss: 3.5969\n",
            "Validation Loss Decreased(3.605648--->3.596890) \t Saving The Model\n",
            "Epoch [95/200], Train Loss: 2.5627, Val Loss: 3.6528\n",
            "Epoch [96/200], Train Loss: 2.5820, Val Loss: 3.5785\n",
            "Validation Loss Decreased(3.596890--->3.578460) \t Saving The Model\n",
            "Epoch [97/200], Train Loss: 2.5716, Val Loss: 3.6599\n",
            "Epoch [98/200], Train Loss: 2.5934, Val Loss: 3.5422\n",
            "Validation Loss Decreased(3.578460--->3.542213) \t Saving The Model\n",
            "Epoch [99/200], Train Loss: 2.5774, Val Loss: 3.4958\n",
            "Validation Loss Decreased(3.542213--->3.495756) \t Saving The Model\n",
            "Epoch [100/200], Train Loss: 2.5456, Val Loss: 3.6013\n",
            "Epoch [101/200], Train Loss: 2.5048, Val Loss: 3.5078\n",
            "Epoch [102/200], Train Loss: 2.5401, Val Loss: 3.5674\n",
            "Epoch [103/200], Train Loss: 2.5809, Val Loss: 3.4700\n",
            "Validation Loss Decreased(3.495756--->3.469970) \t Saving The Model\n",
            "Epoch [104/200], Train Loss: 2.4608, Val Loss: 3.4325\n",
            "Validation Loss Decreased(3.469970--->3.432483) \t Saving The Model\n",
            "Epoch [105/200], Train Loss: 2.4433, Val Loss: 3.4318\n",
            "Validation Loss Decreased(3.432483--->3.431769) \t Saving The Model\n",
            "Epoch [106/200], Train Loss: 2.4846, Val Loss: 3.3835\n",
            "Validation Loss Decreased(3.431769--->3.383506) \t Saving The Model\n",
            "Epoch [107/200], Train Loss: 2.4574, Val Loss: 3.3191\n",
            "Validation Loss Decreased(3.383506--->3.319134) \t Saving The Model\n",
            "Epoch [108/200], Train Loss: 2.4279, Val Loss: 3.4887\n",
            "Epoch [109/200], Train Loss: 2.4502, Val Loss: 3.3116\n",
            "Validation Loss Decreased(3.319134--->3.311645) \t Saving The Model\n",
            "Epoch [110/200], Train Loss: 2.3859, Val Loss: 3.3930\n",
            "Epoch [111/200], Train Loss: 2.3820, Val Loss: 3.3514\n",
            "Epoch [112/200], Train Loss: 2.3631, Val Loss: 3.3621\n",
            "Epoch [113/200], Train Loss: 2.3501, Val Loss: 3.4719\n",
            "Epoch [114/200], Train Loss: 2.3662, Val Loss: 3.3208\n",
            "Epoch [115/200], Train Loss: 2.3419, Val Loss: 3.3259\n",
            "Epoch [116/200], Train Loss: 2.3534, Val Loss: 3.2386\n",
            "Validation Loss Decreased(3.311645--->3.238602) \t Saving The Model\n",
            "Epoch [117/200], Train Loss: 2.3092, Val Loss: 3.3509\n",
            "Epoch [118/200], Train Loss: 2.3521, Val Loss: 3.2649\n",
            "Epoch [119/200], Train Loss: 2.3943, Val Loss: 3.2401\n",
            "Epoch [120/200], Train Loss: 2.3322, Val Loss: 3.3328\n",
            "Epoch [121/200], Train Loss: 2.3280, Val Loss: 3.2829\n",
            "Epoch [122/200], Train Loss: 2.3107, Val Loss: 3.2376\n",
            "Validation Loss Decreased(3.238602--->3.237573) \t Saving The Model\n",
            "Epoch [123/200], Train Loss: 2.2888, Val Loss: 3.2642\n",
            "Epoch [124/200], Train Loss: 2.2650, Val Loss: 3.1919\n",
            "Validation Loss Decreased(3.237573--->3.191871) \t Saving The Model\n",
            "Epoch [125/200], Train Loss: 2.3162, Val Loss: 3.2215\n",
            "Epoch [126/200], Train Loss: 2.2528, Val Loss: 3.1548\n",
            "Validation Loss Decreased(3.191871--->3.154826) \t Saving The Model\n",
            "Epoch [127/200], Train Loss: 2.2307, Val Loss: 3.1515\n",
            "Validation Loss Decreased(3.154826--->3.151484) \t Saving The Model\n",
            "Epoch [128/200], Train Loss: 2.2560, Val Loss: 3.1753\n",
            "Epoch [129/200], Train Loss: 2.2188, Val Loss: 3.0514\n",
            "Validation Loss Decreased(3.151484--->3.051362) \t Saving The Model\n",
            "Epoch [130/200], Train Loss: 2.2289, Val Loss: 3.0588\n",
            "Epoch [131/200], Train Loss: 2.2647, Val Loss: 3.2305\n",
            "Epoch [132/200], Train Loss: 2.2463, Val Loss: 3.1210\n",
            "Epoch [133/200], Train Loss: 2.2210, Val Loss: 3.1541\n",
            "Epoch [134/200], Train Loss: 2.2674, Val Loss: 3.0636\n",
            "Epoch [135/200], Train Loss: 2.1580, Val Loss: 3.0067\n",
            "Validation Loss Decreased(3.051362--->3.006683) \t Saving The Model\n",
            "Epoch [136/200], Train Loss: 2.1824, Val Loss: 3.1271\n",
            "Epoch [137/200], Train Loss: 2.2025, Val Loss: 3.2113\n",
            "Epoch [138/200], Train Loss: 2.3689, Val Loss: 3.2649\n",
            "Epoch [139/200], Train Loss: 2.2880, Val Loss: 3.0372\n",
            "Epoch [140/200], Train Loss: 2.2260, Val Loss: 3.1063\n",
            "Epoch [141/200], Train Loss: 2.1955, Val Loss: 3.0442\n",
            "Epoch [142/200], Train Loss: 2.1499, Val Loss: 2.9688\n",
            "Validation Loss Decreased(3.006683--->2.968803) \t Saving The Model\n",
            "Epoch [143/200], Train Loss: 2.2000, Val Loss: 3.0069\n",
            "Epoch [144/200], Train Loss: 2.1178, Val Loss: 2.9611\n",
            "Validation Loss Decreased(2.968803--->2.961071) \t Saving The Model\n",
            "Epoch [145/200], Train Loss: 2.1617, Val Loss: 3.0492\n",
            "Epoch [146/200], Train Loss: 2.1425, Val Loss: 3.0158\n",
            "Epoch [147/200], Train Loss: 2.1815, Val Loss: 2.9446\n",
            "Validation Loss Decreased(2.961071--->2.944565) \t Saving The Model\n",
            "Epoch [148/200], Train Loss: 2.1519, Val Loss: 2.9706\n",
            "Epoch [149/200], Train Loss: 2.1316, Val Loss: 2.9791\n",
            "Epoch [150/200], Train Loss: 2.1548, Val Loss: 2.9481\n",
            "Epoch [151/200], Train Loss: 2.1205, Val Loss: 2.9973\n",
            "Epoch [152/200], Train Loss: 2.2024, Val Loss: 3.0287\n",
            "Epoch [153/200], Train Loss: 2.1603, Val Loss: 3.1006\n",
            "Epoch [154/200], Train Loss: 2.1679, Val Loss: 3.0308\n",
            "Epoch [155/200], Train Loss: 2.1640, Val Loss: 3.0055\n",
            "Epoch [156/200], Train Loss: 2.1976, Val Loss: 2.8312\n",
            "Validation Loss Decreased(2.944565--->2.831199) \t Saving The Model\n",
            "Epoch [157/200], Train Loss: 2.0849, Val Loss: 2.9219\n",
            "Epoch [158/200], Train Loss: 2.1427, Val Loss: 2.9743\n",
            "Epoch [159/200], Train Loss: 2.0960, Val Loss: 2.8572\n",
            "Epoch [160/200], Train Loss: 2.1118, Val Loss: 2.8304\n",
            "Validation Loss Decreased(2.831199--->2.830408) \t Saving The Model\n",
            "Epoch [161/200], Train Loss: 2.1049, Val Loss: 2.9138\n",
            "Epoch [162/200], Train Loss: 2.1117, Val Loss: 2.8585\n",
            "Epoch [163/200], Train Loss: 2.0599, Val Loss: 2.8476\n",
            "Epoch [164/200], Train Loss: 2.0568, Val Loss: 2.8335\n",
            "Epoch [165/200], Train Loss: 2.0559, Val Loss: 2.8633\n",
            "Epoch [166/200], Train Loss: 2.0162, Val Loss: 2.8528\n",
            "Epoch [167/200], Train Loss: 2.0282, Val Loss: 2.9226\n",
            "Epoch [168/200], Train Loss: 2.1233, Val Loss: 2.9562\n",
            "Epoch [169/200], Train Loss: 2.0693, Val Loss: 2.8708\n",
            "Epoch [170/200], Train Loss: 2.0659, Val Loss: 2.7517\n",
            "Validation Loss Decreased(2.830408--->2.751672) \t Saving The Model\n",
            "Epoch [171/200], Train Loss: 2.0552, Val Loss: 2.8917\n",
            "Epoch [172/200], Train Loss: 2.1413, Val Loss: 2.8137\n",
            "Epoch [173/200], Train Loss: 2.0918, Val Loss: 2.7892\n",
            "Epoch [174/200], Train Loss: 2.0839, Val Loss: 2.9651\n",
            "Epoch [175/200], Train Loss: 2.0665, Val Loss: 2.8889\n",
            "Epoch [176/200], Train Loss: 2.0533, Val Loss: 2.8697\n",
            "Epoch [177/200], Train Loss: 2.0431, Val Loss: 2.8192\n",
            "Epoch [178/200], Train Loss: 2.0203, Val Loss: 2.7730\n",
            "Epoch [179/200], Train Loss: 1.9981, Val Loss: 2.7379\n",
            "Validation Loss Decreased(2.751672--->2.737946) \t Saving The Model\n",
            "Epoch [180/200], Train Loss: 1.9887, Val Loss: 2.8055\n",
            "Epoch [181/200], Train Loss: 2.0147, Val Loss: 2.8439\n",
            "Epoch [182/200], Train Loss: 2.0652, Val Loss: 2.7704\n",
            "Epoch [183/200], Train Loss: 2.0720, Val Loss: 2.7994\n",
            "Epoch [184/200], Train Loss: 2.0871, Val Loss: 2.7371\n",
            "Validation Loss Decreased(2.737946--->2.737109) \t Saving The Model\n",
            "Epoch [185/200], Train Loss: 2.0228, Val Loss: 2.7728\n",
            "Epoch [186/200], Train Loss: 2.0083, Val Loss: 2.7765\n",
            "Epoch [187/200], Train Loss: 2.0685, Val Loss: 2.7487\n",
            "Epoch [188/200], Train Loss: 2.0504, Val Loss: 2.7970\n",
            "Epoch [189/200], Train Loss: 2.0824, Val Loss: 2.7365\n",
            "Validation Loss Decreased(2.737109--->2.736480) \t Saving The Model\n",
            "Epoch [190/200], Train Loss: 2.0423, Val Loss: 2.7521\n",
            "Epoch [191/200], Train Loss: 1.9832, Val Loss: 2.7135\n",
            "Validation Loss Decreased(2.736480--->2.713548) \t Saving The Model\n",
            "Epoch [192/200], Train Loss: 1.9871, Val Loss: 2.8222\n",
            "Epoch [193/200], Train Loss: 1.9952, Val Loss: 2.8145\n",
            "Epoch [194/200], Train Loss: 2.0285, Val Loss: 2.8164\n",
            "Epoch [195/200], Train Loss: 2.0569, Val Loss: 2.8767\n",
            "Epoch [196/200], Train Loss: 2.0779, Val Loss: 2.7014\n",
            "Validation Loss Decreased(2.713548--->2.701354) \t Saving The Model\n",
            "Epoch [197/200], Train Loss: 1.9832, Val Loss: 2.7145\n",
            "Epoch [198/200], Train Loss: 1.9915, Val Loss: 2.7324\n",
            "Epoch [199/200], Train Loss: 1.9876, Val Loss: 2.7807\n",
            "Epoch [200/200], Train Loss: 2.0457, Val Loss: 2.7406\n",
            "Finished training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model state\n",
        "torch.save(best_model_wts, path+'best_model_weights.pth')\n"
      ],
      "metadata": {
        "id": "8G-EKMlepL2t"
      },
      "id": "8G-EKMlepL2t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "best_model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "\n",
        "# Load the weights\n",
        "best_model.load_state_dict(torch.load(path+'best_model_weights.pth'))\n",
        "\n",
        "# Move the model to the GPU\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "# Now your model is ready for making predictions on GPU\n"
      ],
      "metadata": {
        "id": "jOc5jb92pL6V"
      },
      "id": "jOc5jb92pL6V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.0 Load Trained Model"
      ],
      "metadata": {
        "id": "oSKh2t4gJcGU"
      },
      "id": "oSKh2t4gJcGU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "model.load_state_dict(torch.load(path+'muufl_p9_best_model_weights.pth'))\n",
        "model.eval()  # set the model to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntLf47LkpmgD",
        "outputId": "065c17ae-0961-4fd9-ef07-6cfe225b4855"
      },
      "id": "ntLf47LkpmgD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossAttentionModel(\n",
              "  (hsi_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(81, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (lidar_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(81, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (cross_attention): CrossAttention(\n",
              "    (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
              "    (class_layer): Linear(in_features=64, out_features=11, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "model.load_state_dict(torch.load(path+'muufl_p9_best_model_weights.pth'))\n",
        "model.eval()  # set the model to evaluation mode\n",
        "\n",
        "# Move the model to the GPU\n",
        "#\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrRolVFaKZy_",
        "outputId": "2c7109bf-f1e0-4a74-ea23-305bef44b4b0"
      },
      "id": "IrRolVFaKZy_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "hsi_patch_embedding.pos_embedding \t torch.Size([1, 65, 128])\n",
            "hsi_patch_embedding.cls_token \t torch.Size([1, 1, 128])\n",
            "hsi_patch_embedding.proj.weight \t torch.Size([128, 81, 1, 1])\n",
            "hsi_patch_embedding.proj.bias \t torch.Size([128])\n",
            "lidar_patch_embedding.pos_embedding \t torch.Size([1, 3, 128])\n",
            "lidar_patch_embedding.cls_token \t torch.Size([1, 1, 128])\n",
            "lidar_patch_embedding.proj.weight \t torch.Size([128, 81, 1, 1])\n",
            "lidar_patch_embedding.proj.bias \t torch.Size([128])\n",
            "cross_attention.to_q.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_k.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_v.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_out.weight \t torch.Size([64, 512])\n",
            "cross_attention.to_out.bias \t torch.Size([64])\n",
            "cross_attention.class_layer.weight \t torch.Size([11, 64])\n",
            "cross_attention.class_layer.bias \t torch.Size([11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n"
      ],
      "metadata": {
        "id": "2OAS0mPPKi9O"
      },
      "id": "2OAS0mPPKi9O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model=model # load your best model\n",
        "\n",
        "# Move the model to the correct device\n",
        "best_model = model.to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "best_model.eval()\n",
        "\n",
        "# Initialize a list to hold all the predictions and attention scores\n",
        "all_predictions = []\n",
        "all_attn_scores = []\n",
        "\n",
        "# Loop over the validation set\n",
        "for hsi_batch, lidar_batch, _ in val_loader:  # we don't need the labels for predictions\n",
        "    # Move the batch to the desired device\n",
        "    hsi_batch = hsi_batch.to(device)\n",
        "    lidar_batch = lidar_batch.to(device)\n",
        "\n",
        "    # Pass the batch through the model\n",
        "    with torch.no_grad():\n",
        "        output, attn_scores = best_model(lidar_batch, hsi_batch)\n",
        "\n",
        "        # Add the predictions and attention scores to our lists\n",
        "        all_predictions.append(output.cpu().numpy())\n",
        "        all_attn_scores.append(attn_scores.cpu().numpy())\n",
        "\n",
        "# Concatenate all predictions and attention scores into a single numpy array\n",
        "all_predictions = np.concatenate(all_predictions)\n",
        "all_attn_scores = np.concatenate(all_attn_scores)\n",
        "\n",
        "# Now you can use the predictions and attention scores as needed\n",
        "print('all_predictions shape:', all_predictions.shape)\n",
        "print('all_attn_scores shape:', all_attn_scores.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "S2xqXWkOndVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9dede85-065b-4819-d334-8962dac3a31a"
      },
      "id": "S2xqXWkOndVP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_predictions shape: (16601, 64, 512)\n",
            "all_attn_scores shape: (16601, 64, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Band Selection"
      ],
      "metadata": {
        "id": "7o9FUM2ILtVK"
      },
      "id": "7o9FUM2ILtVK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the variance of the feature vectors for each band across all samples and then find the bands with the maximum variance"
      ],
      "metadata": {
        "id": "oqc6nZNfXKEN"
      },
      "id": "oqc6nZNfXKEN"
    },
    {
      "cell_type": "code",
      "source": [
        "# First, take the mean across all samples and heads. This will result in a single 144-dim vector.\n",
        "mean_attention = np.mean(all_attn_scores, axis=(0, 1))\n",
        "print('mean_attention:',mean_attention.shape)\n",
        "\n",
        "# Then, sort the bands by attention. This will give you the band indices in descending order of attention.\n",
        "sorted_band_indices = np.argsort(mean_attention)[::-1]\n",
        "print('sorted_band_indices :',sorted_band_indices.shape)\n",
        "\n",
        "# If you want to see the attention values as well, you can sort the mean_attention array in the same order.\n",
        "sorted_attention_values = mean_attention[sorted_band_indices]\n",
        "print('sorted_attention_values :',sorted_attention_values.shape)\n",
        "\n",
        "N = 50  # Top N bands\n",
        "top_N_bands = sorted_band_indices[:N]\n",
        "top_N_attention_values = sorted_attention_values[:N]\n",
        "\n",
        "# Print the top N band indices with their attention scores\n",
        "print(f\"Top {N} bands based on the mean attention score:\")\n",
        "for band_index, attention_value in zip(top_N_bands, top_N_attention_values):\n",
        "    print(f\"Band index: {band_index}, Mean attention score: {attention_value}\")\n",
        "\n",
        "# Print the list of top N band indices\n",
        "print(\"\\nList of top {} band indices:\".format(N))\n",
        "print(top_N_bands.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EH-ULRdlVVW",
        "outputId": "2fec7dd0-5e54-4b5b-9e40-e6fa1a7eff2d"
      },
      "id": "4EH-ULRdlVVW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_attention: (64,)\n",
            "sorted_band_indices : (64,)\n",
            "sorted_attention_values : (64,)\n",
            "Top 50 bands based on the mean attention score:\n",
            "Band index: 21, Mean attention score: 63.02592086791992\n",
            "Band index: 62, Mean attention score: 60.26506805419922\n",
            "Band index: 60, Mean attention score: 55.706642150878906\n",
            "Band index: 4, Mean attention score: 50.740474700927734\n",
            "Band index: 34, Mean attention score: 47.5483512878418\n",
            "Band index: 23, Mean attention score: 44.8641357421875\n",
            "Band index: 20, Mean attention score: 44.70694351196289\n",
            "Band index: 10, Mean attention score: 34.23902893066406\n",
            "Band index: 45, Mean attention score: 33.21865463256836\n",
            "Band index: 19, Mean attention score: 32.920753479003906\n",
            "Band index: 51, Mean attention score: 32.7322883605957\n",
            "Band index: 27, Mean attention score: 28.580551147460938\n",
            "Band index: 39, Mean attention score: 25.99041748046875\n",
            "Band index: 46, Mean attention score: 24.747472763061523\n",
            "Band index: 8, Mean attention score: 22.008190155029297\n",
            "Band index: 63, Mean attention score: 20.538185119628906\n",
            "Band index: 2, Mean attention score: 18.2613582611084\n",
            "Band index: 33, Mean attention score: 17.044530868530273\n",
            "Band index: 24, Mean attention score: 15.95011043548584\n",
            "Band index: 35, Mean attention score: 15.11439037322998\n",
            "Band index: 59, Mean attention score: 14.600854873657227\n",
            "Band index: 18, Mean attention score: 14.451109886169434\n",
            "Band index: 15, Mean attention score: 14.10416030883789\n",
            "Band index: 5, Mean attention score: 13.153228759765625\n",
            "Band index: 9, Mean attention score: 12.244707107543945\n",
            "Band index: 30, Mean attention score: 10.20620346069336\n",
            "Band index: 44, Mean attention score: 9.192863464355469\n",
            "Band index: 1, Mean attention score: 6.221868515014648\n",
            "Band index: 22, Mean attention score: 6.1180853843688965\n",
            "Band index: 37, Mean attention score: 5.648428440093994\n",
            "Band index: 56, Mean attention score: 4.314061164855957\n",
            "Band index: 14, Mean attention score: 4.137597560882568\n",
            "Band index: 50, Mean attention score: 1.739251732826233\n",
            "Band index: 57, Mean attention score: 1.363297462463379\n",
            "Band index: 36, Mean attention score: -0.8823817372322083\n",
            "Band index: 16, Mean attention score: -0.9494529962539673\n",
            "Band index: 17, Mean attention score: -1.9780910015106201\n",
            "Band index: 28, Mean attention score: -2.5408904552459717\n",
            "Band index: 26, Mean attention score: -2.638371229171753\n",
            "Band index: 49, Mean attention score: -8.891718864440918\n",
            "Band index: 42, Mean attention score: -8.953989028930664\n",
            "Band index: 53, Mean attention score: -9.097631454467773\n",
            "Band index: 11, Mean attention score: -9.1132173538208\n",
            "Band index: 25, Mean attention score: -21.447484970092773\n",
            "Band index: 58, Mean attention score: -23.119409561157227\n",
            "Band index: 40, Mean attention score: -24.58136558532715\n",
            "Band index: 6, Mean attention score: -25.0190486907959\n",
            "Band index: 54, Mean attention score: -31.801149368286133\n",
            "Band index: 32, Mean attention score: -31.89775848388672\n",
            "Band index: 13, Mean attention score: -32.84770202636719\n",
            "\n",
            "List of top 50 band indices:\n",
            "[21, 62, 60, 4, 34, 23, 20, 10, 45, 19, 51, 27, 39, 46, 8, 63, 2, 33, 24, 35, 59, 18, 15, 5, 9, 30, 44, 1, 22, 37, 56, 14, 50, 57, 36, 16, 17, 28, 26, 49, 42, 53, 11, 25, 58, 40, 6, 54, 32, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "band_50=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20, 4, 57, 53, 7, 3, 37, 16, 11, 54, 1, 39, 34, 43, 25, 6, 56, 27, 23, 0, 30, 52, 17, 26, 41, 29, 28, 49, 35, 12, 44, 50, 22, 42, 2, 63, 59, 61, 15, 32, 40]\n",
        "band_45=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20, 4, 57, 53, 7, 3, 37, 16, 11, 54, 1, 39, 34, 43, 25, 6, 56, 27, 23, 0, 30, 52, 17, 26, 41, 29, 28, 49, 35, 12, 44, 50, 22, 42, 2, 63]\n",
        "band_40=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20, 4, 57, 53, 7, 3, 37, 16, 11, 54, 1, 39, 34, 43, 25, 6, 56, 27, 23, 0, 30, 52, 17, 26, 41, 29, 28, 49, 35, 12, 44]\n",
        "band_35=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20, 4, 57, 53, 7, 3, 37, 16, 11, 54, 1, 39, 34, 43, 25, 6, 56, 27, 23, 0, 30, 52, 17, 26, 41, 29]\n",
        "band_30=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20, 4, 57, 53, 7, 3, 37, 16, 11, 54, 1, 39, 34, 43, 25, 6, 56, 27, 23, 0, 30]\n",
        "band_25=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20, 4, 57, 53, 7, 3, 37, 16, 11, 54, 1, 39, 34, 43, 25, 6]\n",
        "band_20=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20, 4, 57, 53, 7, 3, 37, 16, 11, 54, 1]\n",
        "band_15=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20, 4, 57, 53, 7, 3]\n",
        "band_10=[47, 38, 51, 8, 14, 36, 55, 10, 9, 20]\n",
        "band_5=[47, 38, 51, 8, 14]\n"
      ],
      "metadata": {
        "id": "1E-TAAxz8X7e"
      },
      "id": "1E-TAAxz8X7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "band_50_p11=[32, 28, 58, 40, 8, 37, 25, 34, 62, 24, 44, 3, 23, 21, 27, 56, 60, 63, 41, 50, 22, 55, 31, 51, 2, 16, 15, 5, 0, 12, 49, 46, 57, 30, 35, 4, 7, 33, 29, 47, 20, 48, 42, 39, 26, 13, 45, 61, 10, 11]"
      ],
      "metadata": {
        "id": "8TdumuTVo6PE"
      },
      "id": "8TdumuTVo6PE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "band_50_p9=[21, 62, 60, 4, 34, 23, 20, 10, 45, 19, 51, 27, 39, 46, 8, 63, 2, 33, 24, 35, 59, 18, 15, 5, 9, 30, 44, 1, 22, 37, 56, 14, 50, 57, 36, 16, 17, 28, 26, 49, 42, 53, 11, 25, 58, 40, 6, 54, 32, 13]"
      ],
      "metadata": {
        "id": "yNBFu0zVo6Tg"
      },
      "id": "yNBFu0zVo6Tg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}