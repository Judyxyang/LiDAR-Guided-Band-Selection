{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "14525fd7",
      "metadata": {
        "id": "14525fd7"
      },
      "source": [
        "# 0.0 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vuX4sZpytah9",
      "metadata": {
        "id": "vuX4sZpytah9"
      },
      "outputs": [],
      "source": [
        "pip install spectral mat73  einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12826627",
      "metadata": {
        "id": "12826627"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "\n",
        "from einops import rearrange\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from scipy import io\n",
        "import torch.utils.data\n",
        "import scipy.io as sio\n",
        "import mat73\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dRG3WratmCN",
      "metadata": {
        "id": "6dRG3WratmCN"
      },
      "source": [
        "# 1.0 Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vpwq4Yi-tgjs",
      "metadata": {
        "id": "Vpwq4Yi-tgjs"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vMin2GFJtorP",
      "metadata": {
        "id": "vMin2GFJtorP"
      },
      "outputs": [],
      "source": [
        "#! ls '/content/drive/MyDrive/A02_RemoteSensingData/TrentoDataSet/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4NMT_vf3t3Bg",
      "metadata": {
        "id": "4NMT_vf3t3Bg"
      },
      "outputs": [],
      "source": [
        "# path\n",
        "#path ='/content/drive/MyDrive/A02_RemoteSensingData/TrentoDataSet/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VebGxkrY2sbp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VebGxkrY2sbp",
        "outputId": "7a10d6b9-6ed6-4026-a5c3-96153e249291"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(166, 600, 63)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loader HSI TR_map_2018\n",
        "trento_data=sio.loadmat('trento_data.mat')['HSI_data']\n",
        "trento_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D-bNzvYHnxS1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-bNzvYHnxS1",
        "outputId": "0a312a91-c949-45df-a50b-727e70a2079f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trento_hsi_data shape: (166, 600, 63)\n",
            "trento_lidar_data shape: (166, 600, 1)\n",
            "trento_gt shape: (166, 600)\n",
            "trento_hsi_trset shape: (2832, 144)\n",
            "trento_trlabel shape: (2832, 1)\n",
            "trento_hsi_teset shape: (12197, 144)\n",
            "trento_telabel shape: (12197, 1)\n"
          ]
        }
      ],
      "source": [
        "# Loader HSI TR_map_2018\n",
        "trento_data=sio.loadmat('trento_data.mat')\n",
        "\n",
        "# Extract the HSI_data and LiDAR_data arrays\n",
        "trento_hsi_data = trento_data['HSI_data']\n",
        "trento_lidar_data = trento_data['LiDAR_data']\n",
        "\n",
        "# Reshape the data\n",
        "trento_lidar_data = np.reshape(trento_lidar_data, (166, 600, 1))\n",
        "\n",
        "trento_gt=trento_data['ground']\n",
        "print('trento_hsi_data shape:', trento_hsi_data.shape)\n",
        "print('trento_lidar_data shape:', trento_lidar_data.shape)\n",
        "print('trento_gt shape:', trento_gt.shape)\n",
        "\n",
        "# Load HSITrSet\n",
        "trento_hsi_trset=sio.loadmat('HSI_TrSet.mat')['HSI_TrSet']\n",
        "trento_trlabel=sio.loadmat('TrLabel.mat')['TrLabel']\n",
        "print('trento_hsi_trset shape:', trento_hsi_trset.shape)\n",
        "print('trento_trlabel shape:', trento_trlabel.shape)\n",
        "\n",
        "#Load HSITeset\n",
        "trento_hsi_teset=sio.loadmat('HSI_TeSet.mat')['HSI_TeSet']\n",
        "trento_telabel=sio.loadmat('TeLabel.mat')['TeLabel']\n",
        "print('trento_hsi_teset shape:', trento_hsi_teset.shape)\n",
        "print('trento_telabel shape:', trento_telabel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690eba9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "690eba9c",
        "outputId": "53524e5e-bc65-444f-990b-97268e1ee7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class 1: Apple trees\n",
            "Training Samples: 129\n",
            "Test Samples: 3905\n",
            "Total Samples: 4034\n",
            "\n",
            "Class 2: Buildings \n",
            "Training Samples: 125\n",
            "Test Samples: 2778\n",
            "Total Samples: 2903\n",
            "\n",
            "Class 3: Ground\n",
            "Training Samples: 105\n",
            "Test Samples: 374\n",
            "Total Samples: 479\n",
            "\n",
            "Class 4: Wood\n",
            "Training Samples: 154\n",
            "Test Samples: 8969\n",
            "Total Samples: 9123\n",
            "\n",
            "Class 5: Vineyard\n",
            "Training Samples: 184\n",
            "Test Samples: 10317\n",
            "Total Samples: 10501\n",
            "\n",
            "Class 6: Roads\n",
            "Training Samples: 122\n",
            "Test Samples: 3052\n",
            "Total Samples: 3174\n",
            "\n",
            "{1: {'class_name': 'Apple trees', 'training': 129, 'test': 3905, 'samples': 4034}, 2: {'class_name': 'Buildings ', 'training': 125, 'test': 2778, 'samples': 2903}, 3: {'class_name': 'Ground', 'training': 105, 'test': 374, 'samples': 479}, 4: {'class_name': 'Wood', 'training': 154, 'test': 8969, 'samples': 9123}, 5: {'class_name': 'Vineyard', 'training': 184, 'test': 10317, 'samples': 10501}, 6: {'class_name': 'Roads', 'training': 122, 'test': 3052, 'samples': 3174}}\n"
          ]
        }
      ],
      "source": [
        "# 2.1 Define the class information\n",
        "class_info = [\n",
        "    (1, \"Apple trees\", 129, 3905, 4034),\n",
        "    (2, \"Buildings \", 125, 2778, 2903),\n",
        "    (3, \"Ground\", 105, 374, 479),\n",
        "    (4, \"Wood\", 154, 8969, 9123),\n",
        "    (5, \"Vineyard\", 184, 10317, 10501),  # Corrected this line\n",
        "    (6, \"Roads\", 122, 3052, 3174)\n",
        "]\n",
        "\n",
        "class_dict = {class_number: {\"class_name\": class_name, \"training\": training, \"test\": test, \"samples\": samples} for class_number, class_name, training, test, samples in class_info}\n",
        "\n",
        "for class_number, class_info in class_dict.items():\n",
        "    print(f\"Class {class_number}: {class_info['class_name']}\")\n",
        "    print(f\"Training Samples: {class_info['training']}\")\n",
        "    print(f\"Test Samples: {class_info['test']}\")\n",
        "    print(f\"Total Samples: {class_info['samples']}\")\n",
        "    print()\n",
        "\n",
        "print(class_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002fcb4c",
      "metadata": {
        "id": "002fcb4c"
      },
      "source": [
        "# 2.0 Data Preprocessing & Dataloader Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8D-fNrGQIQFT",
      "metadata": {
        "id": "8D-fNrGQIQFT"
      },
      "source": [
        "### 2.1  Samples Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sedpbkBm4j7A",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "sedpbkBm4j7A",
        "outputId": "1dc438fd-badb-4393-d470-0f397cfa770b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_samples shape: (30214, 9, 9, 63)\n",
            "lidar_samples shape: (30214, 9, 9, 1)\n",
            "labels shape: (30214,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Create a mask with all class labels\n",
        "# mask = np.copy(trento_gt)\n",
        "\n",
        "# # Set the background class to 0\n",
        "# mask[mask == 0] = 0\n",
        "\n",
        "# Define patch size and stride\n",
        "patch_size = 9\n",
        "stride = 1\n",
        "\n",
        "# Create an empty list to store patches and labels\n",
        "hsi_samples = []\n",
        "lidar_samples = []\n",
        "labels = []\n",
        "\n",
        "# Initialize a dictionary to store class count\n",
        "class_count = {i: 0 for i in class_dict.keys()}\n",
        "\n",
        "# Function to check if all classes have the required number of samples\n",
        "def all_classes_completed(class_count, class_dict):\n",
        "    return all(class_count[class_num] == class_dict[class_num][\"samples\"] for class_num in class_dict.keys())\n",
        "\n",
        "while not all_classes_completed(class_count, class_dict):\n",
        "    # Loop through the ground truth data\n",
        "    for label in class_dict.keys():\n",
        "        # Get the coordinates of the ground truth pixels\n",
        "        #coords = np.argwhere((trento_gt == label) & (mask > 0))\n",
        "        coords = np.argwhere((trento_gt == label) )\n",
        "\n",
        "        # Shuffle the coordinates to randomize the patch extraction\n",
        "        np.random.shuffle(coords)\n",
        "\n",
        "        for coord in coords:\n",
        "            i, j = coord\n",
        "            # Calculate the patch indices\n",
        "            i_start, i_end = i - patch_size // 2, i + patch_size // 2 + 1\n",
        "            j_start, j_end = j - patch_size // 2, j + patch_size // 2 + 1\n",
        "\n",
        "            # Check if the indices are within the bounds of the HSI data\n",
        "            if i_start >= 0 and i_end <= trento_hsi_data.shape[0] and j_start >= 0 and j_end <= trento_hsi_data.shape[1]:\n",
        "                # Extract the patch\n",
        "                hsi_patch = trento_hsi_data[i_start:i_end, j_start:j_end, :]\n",
        "\n",
        "                # Extract the LiDAR patch\n",
        "                lidar_patch = trento_lidar_data[i_start:i_end, j_start:j_end]\n",
        "\n",
        "                # If the class count is less than the required samples\n",
        "                if class_count[label] < class_dict[label][\"samples\"]:\n",
        "                    # Append the patch and its label to the list\n",
        "                    hsi_samples.append(hsi_patch)\n",
        "                    lidar_samples.append(lidar_patch)\n",
        "                    labels.append(label)\n",
        "                    class_count[label] += 1\n",
        "\n",
        "                    # If all classes have the required number of samples, exit the loop\n",
        "                    if all_classes_completed(class_count, class_dict):\n",
        "                        break\n",
        "\n",
        "# Convert the list of patches and labels into arrays\n",
        "hsi_samples = np.array(hsi_samples)\n",
        "lidar_samples = np.array(lidar_samples)\n",
        "labels = np.array(labels)\n",
        "print('hsi_samples shape:', hsi_samples.shape)\n",
        "print('lidar_samples shape:', lidar_samples.shape)\n",
        "print('labels shape:', labels.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ifIwfC-C9ON9",
      "metadata": {
        "id": "ifIwfC-C9ON9"
      },
      "source": [
        "### 2.2 Training samples extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ade945",
      "metadata": {
        "id": "40ade945",
        "outputId": "996aad6f-8d43-43a6-ab4d-22736574efae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_train shape: (819, 9, 9, 63)\n",
            "hsi_test shape: (29395, 9, 9, 63)\n",
            "lidar_train shape: (819, 9, 9, 1)\n",
            "lidar_test shape: (29395, 9, 9, 1)\n",
            "y_train shape: (819,)\n",
            "y_test shape: (29395,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize empty lists to store the train and test samples\n",
        "hsi_train = []\n",
        "hsi_test = []\n",
        "lidar_train = []\n",
        "lidar_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "# Split the data for each class\n",
        "for class_number, class_info in class_dict.items():\n",
        "    # Get the indices of the samples for this class\n",
        "    class_indices = np.where(labels == class_number)[0]\n",
        "\n",
        "    # Shuffle the class indices\n",
        "    np.random.shuffle(class_indices)\n",
        "\n",
        "    # Split the class indices into training and test indices\n",
        "    train_indices = class_indices[:class_info[\"training\"]]\n",
        "    test_indices = class_indices[class_info[\"training\"]:class_info[\"samples\"]]\n",
        "\n",
        "    # Add the selected training samples to the train lists\n",
        "    hsi_train.append(hsi_samples[train_indices])\n",
        "    lidar_train.append(lidar_samples[train_indices])\n",
        "    y_train.append(labels[train_indices])\n",
        "\n",
        "    # Add the remaining samples to the test lists\n",
        "    hsi_test.append(hsi_samples[test_indices])\n",
        "    lidar_test.append(lidar_samples[test_indices])\n",
        "    y_test.append(labels[test_indices])\n",
        "\n",
        "# Concatenate the train and test lists to create the train and test arrays\n",
        "hsi_train = np.concatenate(hsi_train)\n",
        "hsi_test = np.concatenate(hsi_test)\n",
        "lidar_train = np.concatenate(lidar_train)\n",
        "lidar_test = np.concatenate(lidar_test)\n",
        "y_train = np.concatenate(y_train)\n",
        "y_test = np.concatenate(y_test)\n",
        "\n",
        "# Print the shapes of the train and test arrays\n",
        "print('hsi_train shape:', hsi_train.shape)\n",
        "print('hsi_test shape:', hsi_test.shape)\n",
        "print('lidar_train shape:', lidar_train.shape)\n",
        "print('lidar_test shape:', lidar_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7f3009",
      "metadata": {
        "id": "4c7f3009",
        "outputId": "67bd8a6b-af5f-4836-c5cc-25c65d2237fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ch1 shape: (819, 9, 9)\n",
            "ch2 shape: (819, 9, 9)\n",
            "ch3 shape: (29395, 9, 9)\n",
            "ch4 shape: (29395, 9, 9)\n"
          ]
        }
      ],
      "source": [
        "# Normalize channel 1(Height) of the train data\n",
        "ch1 = hsi_train[:, :, :, 0]\n",
        "pmin = np.amin(ch1)\n",
        "pmax = np.amax(ch1)\n",
        "ch1 = (ch1-pmin) / (pmax- pmin)\n",
        "\n",
        "# Normalize channel 2(Intensity) of the train data\n",
        "ch2 = hsi_train[:, :, :, 1]\n",
        "pmin1 = np.amin(ch2)\n",
        "pmax1 = np.amax(ch2)\n",
        "ch2 = (ch2-pmin1) / (pmax1- pmin1)\n",
        "\n",
        "hsi_train[:,:,:,0] = ch1\n",
        "hsi_train[:,:,:,1] = ch2\n",
        "\n",
        "# Normalize channel 1(Height) the test data\n",
        "ch3 = hsi_test[:, :, :, 0]\n",
        "ch3 = (ch3-pmin) / (pmax- pmin)\n",
        "\n",
        "# Normalize channel 2(Intensity) of the test data\n",
        "ch4 = hsi_test[:, :, :, 1]\n",
        "ch4 = (ch4-pmin1) / (pmax1- pmin1)\n",
        "\n",
        "hsi_test[:,:,:,0] = ch3\n",
        "hsi_test[:,:,:,1] = ch4\n",
        "\n",
        "print('ch1 shape:',ch1.shape)\n",
        "print('ch2 shape:',ch2.shape)\n",
        "print('ch3 shape:',ch3.shape)\n",
        "print('ch4 shape:',ch4.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec408eb",
      "metadata": {
        "id": "4ec408eb",
        "outputId": "a1ff5b87-e930-479a-b130-ff7741933c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmented HSI training samples shape: (4914, 9, 9, 63)\n",
            "Augmented LiDAR training samples shape: (4914, 9, 9, 1)\n",
            "Augmented training labels shape: (4914,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "def augment_training_data(hsi_training_data, lidar_training_data, training_labels, rotations=[45, 90, 135], flip_up_down=True, flip_left_right=True):\n",
        "    augmented_hsi = []\n",
        "    augmented_lidar = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for hsi, lidar, label in zip(hsi_training_data, lidar_training_data, training_labels):\n",
        "        # Original data\n",
        "        augmented_hsi.append(hsi)\n",
        "        augmented_lidar.append(lidar)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "        # Rotations\n",
        "        for angle in rotations:\n",
        "            hsi_rotated = rotate(hsi, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "            lidar_rotated = rotate(lidar, angle, axes=(0, 1), reshape=False, mode='nearest')\n",
        "\n",
        "            augmented_hsi.append(hsi_rotated)\n",
        "            augmented_lidar.append(lidar_rotated)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip up-down\n",
        "        if flip_up_down:\n",
        "            hsi_flipped_ud = np.flipud(hsi)\n",
        "            lidar_flipped_ud = np.flipud(lidar)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_ud)\n",
        "            augmented_lidar.append(lidar_flipped_ud)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "        # Flip left-right\n",
        "        if flip_left_right:\n",
        "            hsi_flipped_lr = np.fliplr(hsi)\n",
        "            lidar_flipped_lr = np.fliplr(lidar)\n",
        "\n",
        "            augmented_hsi.append(hsi_flipped_lr)\n",
        "            augmented_lidar.append(lidar_flipped_lr)\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    return np.array(augmented_hsi), np.array(augmented_lidar), np.array(augmented_labels)\n",
        "\n",
        "# Augmenting the training samples\n",
        "augmented_hsi_training_samples, augmented_lidar_training_samples, augmented_training_labels = augment_training_data(hsi_train, lidar_train, y_train)\n",
        "\n",
        "# Print shapes to verify the augmented training data\n",
        "print('Augmented HSI training samples shape:', augmented_hsi_training_samples.shape)\n",
        "print('Augmented LiDAR training samples shape:', augmented_lidar_training_samples.shape)\n",
        "print('Augmented training labels shape:', augmented_training_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a99df1",
      "metadata": {
        "id": "28a99df1",
        "outputId": "3c22a68d-b0b8-421c-f00a-0ca0d4a3fa66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_train_samples shape: (4914, 9, 9, 63)\n",
            "lidar_train_samples shape: (4914, 9, 9, 1)\n",
            "train_labels shape: (4914,)\n"
          ]
        }
      ],
      "source": [
        "hsi_train=augmented_hsi_training_samples\n",
        "lidar_train=augmented_lidar_training_samples\n",
        "y_train=augmented_training_labels\n",
        "print('hsi_train_samples shape:', hsi_train.shape)\n",
        "print('lidar_train_samples shape:', lidar_train.shape)\n",
        "print('train_labels shape:', y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca013500",
      "metadata": {
        "id": "ca013500",
        "outputId": "cf7a8125-4d21-4cae-d7fe-688d1c5dacf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_test_samples shape: (29395, 9, 9, 63)\n",
            "lidar_test_samples shape: (29395, 9, 9, 1)\n",
            "y_test shape: (29395,)\n"
          ]
        }
      ],
      "source": [
        "print('hsi_test_samples shape:', hsi_test.shape)\n",
        "print('lidar_test_samples shape:', lidar_test.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c8988a",
      "metadata": {
        "id": "a5c8988a",
        "outputId": "d119750c-e321-45e4-955a-1b1612247364"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-29 09:26:51.393522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_train.shape: (4914, 6)\n",
            "y_test.shape: (29395, 6)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# One hot encoding of labels\n",
        "# Substract 1 from labels\n",
        "y_train_adj = augmented_training_labels - 1\n",
        "y_test_adj = y_test - 1\n",
        "\n",
        "# One hot encoding of labels\n",
        "y_train = to_categorical(y_train_adj, num_classes = 6, dtype =\"int32\")\n",
        "y_test = to_categorical(y_test_adj, num_classes = 6, dtype =\"int32\")\n",
        "\n",
        "print('y_train.shape:',y_train.shape)\n",
        "print('y_test.shape:',y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03398278",
      "metadata": {
        "id": "03398278"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# # One hot encoding of labels\n",
        "# # Substract 1 from labels\n",
        "# y_train_adj = y_train - 1\n",
        "# y_test_adj = y_test - 1\n",
        "\n",
        "# # One hot encoding of labels\n",
        "# y_train = to_categorical(y_train_adj, num_classes = 6, dtype =\"int32\")\n",
        "# y_test = to_categorical(y_test_adj, num_classes = 6, dtype =\"int32\")\n",
        "\n",
        "# print('y_train.shape:',y_train.shape)\n",
        "# print('y_test.shape:',y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OGoTB07uUZKP",
      "metadata": {
        "id": "OGoTB07uUZKP"
      },
      "source": [
        "### 2.3 Test samples extracting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J7Ko9uYgIf2D",
      "metadata": {
        "id": "J7Ko9uYgIf2D"
      },
      "source": [
        "#3.0 Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u1rQcEYmVP_W",
      "metadata": {
        "id": "u1rQcEYmVP_W"
      },
      "source": [
        "### 3.1 Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35qAdvCxIPJ8",
      "metadata": {
        "id": "35qAdvCxIPJ8"
      },
      "outputs": [],
      "source": [
        "# 3.1 Configuration\n",
        "class Config:\n",
        "    def __init__(self,in_channels,num_patches,kernel_size,patch_size,emb_size, dim,depth,heads,dim_head,mlp_dim,num_classes,dropout,pos_emb_size,class_emb_size,stride, ):\n",
        "        self.in_channels = in_channels\n",
        "        self.num_patches = num_patches\n",
        "        self.kernel_size = kernel_size\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "        self.pos_emb_size = pos_emb_size\n",
        "        self.class_emb_size = class_emb_size\n",
        "        self.stride = stride\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U5T_YmiQespq",
      "metadata": {
        "id": "U5T_YmiQespq"
      },
      "source": [
        "### 3.2 EmbeddingPatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H0MaMR75xcKE",
      "metadata": {
        "id": "H0MaMR75xcKE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(\n",
        "            config.in_channels,\n",
        "            config.emb_size,\n",
        "            kernel_size=config.patch_size,\n",
        "            stride=config.stride,\n",
        "        )\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, config.num_patches + 1, config.emb_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.emb_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n",
        "        x = x + self.pos_embedding[:, :x.size(1)]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7DyoAhpwF1WO",
      "metadata": {
        "id": "7DyoAhpwF1WO"
      },
      "source": [
        "### 3.2 Optianl Adding bandoutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba439b0",
      "metadata": {
        "id": "3ba439b0"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, lidar_config, hsi_config):\n",
        "        super(CrossAttention, self).__init__()\n",
        "\n",
        "        # Define module parameters\n",
        "        self.dim_head = lidar_config.dim_head\n",
        "        self.num_patches=hsi_config.num_patches\n",
        "        self.num_heads = lidar_config.heads\n",
        "        self.sqrt_dim_head = math.sqrt(self.dim_head)\n",
        "\n",
        "        # Define linear layers for transforming Q, K, and V\n",
        "        self.to_q = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "        self.to_k = nn.Linear(hsi_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "        self.to_v = nn.Linear(lidar_config.dim, self.dim_head * self.num_heads, bias=False)\n",
        "\n",
        "        self.to_out = nn.Linear( self.num_heads* self.dim_head,self.num_patches )  # added\n",
        "\n",
        "\n",
        "    def forward(self, lidar, hsi):\n",
        "        B, N_lidar, _ = lidar.size()\n",
        "        _, N_hsi, _ = hsi.size()\n",
        "\n",
        "        outputs = []\n",
        "        attn_scores = []  # List to store attention scores\n",
        "\n",
        "       # Iterate over lidar and hsi patches\n",
        "        for i in range(1, N_lidar):\n",
        "        #for i in range(N_lidar):\n",
        "\n",
        "            lidar_patch = lidar[:, i].unsqueeze(1)  # Add a dimension for number of patches\n",
        "            for j in range(1, N_hsi):\n",
        "            #for j in range(N_hsi):\n",
        "\n",
        "                hsi_patch = hsi[:, j].unsqueeze(1)  # Add a dimension for number of patches\n",
        "                Q = self.to_q(lidar_patch)\n",
        "                K = self.to_k(hsi_patch)\n",
        "                V = self.to_v(lidar_patch)\n",
        "\n",
        "                Q = Q / self.sqrt_dim_head\n",
        "                attn_weights = F.softmax(Q @ K.transpose(-2, -1), dim=-1)\n",
        "\n",
        "                attn_output = attn_weights @ V\n",
        "                attn_score = self.to_out(attn_output)  # added\n",
        "                outputs.append(attn_output)\n",
        "                attn_scores.append(attn_score)  # Store the attention scores\n",
        "\n",
        "         # Concatenate all the outputs\n",
        "        output = torch.cat(outputs, dim=1)\n",
        "        attn_scores = torch.cat(attn_scores, dim=1)  # Concatenate all the attention scores\n",
        "\n",
        "        return output, attn_scores  # Return both output and attention scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "at2xTwGv91ng",
      "metadata": {
        "id": "at2xTwGv91ng"
      },
      "outputs": [],
      "source": [
        "class CrossAttentionModel(nn.Module):\n",
        "    def __init__(self, hsi_config, lidar_config):\n",
        "        super().__init__()\n",
        "        self.hsi_patch_embedding = PatchEmbedding(hsi_config)\n",
        "        self.lidar_patch_embedding = PatchEmbedding(lidar_config)\n",
        "        self.cross_attention = CrossAttention(lidar_config, hsi_config)\n",
        "\n",
        "    def forward(self, lidar_data, hsi_data):\n",
        "        # Apply PatchEmbedding\n",
        "        lidar_emb = self.lidar_patch_embedding(lidar_data)\n",
        "        hsi_emb = self.hsi_patch_embedding(hsi_data)\n",
        "\n",
        "        # Apply CrossAttention\n",
        "        output, attn_scores = self.cross_attention(lidar_emb, hsi_emb)\n",
        "\n",
        "        return output, attn_scores  # Return both output and attention scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca7a261",
      "metadata": {
        "id": "5ca7a261"
      },
      "outputs": [],
      "source": [
        "#3.1.1 Parameters Setting\n",
        "# Hsi configuration\n",
        "hsi_config = Config(\n",
        "    in_channels=81,  # Each sample covers 144 bands\n",
        "    num_patches=63,  # 25*1*144 bands are grouped into 3600 groups\n",
        "    kernel_size=1,  # Adjusted to match new patch size\n",
        "    patch_size=1,  # Adjusted to match new patch size (5*5, 144/24=6)\n",
        "    emb_size=128,  # Embedding size, this can be changed\n",
        "    dim=128,  # Dimension of the transformer, this can be changed\n",
        "    depth=6,  # Number of transformer layers, this can be changed\n",
        "    heads=8,  # Number of attention heads, this can be changed\n",
        "    dim_head=64,  # Dimension of each attention head, this can be changed\n",
        "    mlp_dim=256,  # Dimension of the MLP layer, this can be changed\n",
        "    num_classes=15,  # Number of classes, this can be changed\n",
        "    dropout=0.4,  # Dropout rate, this can be changed\n",
        "    pos_emb_size=128,  # Position embedding size, this can be changed\n",
        "    class_emb_size=128,  # Class embedding size, this can be changed\n",
        "    stride=1  # Stride for the convolution, this can be changed\n",
        ")\n",
        "\n",
        "\n",
        "# Lidara configuration\n",
        "lidar_config = Config(\n",
        "    in_channels=81,  # lidar group has 1 channels\n",
        "    num_patches=1,  # 1 band for Lidar\n",
        "    kernel_size=1,  # Adjusted to match new patch size\n",
        "    patch_size=1, # Adjusted to match new patch size\n",
        "    emb_size=128,  # Embedding size, this can be changed\n",
        "    dim=128,  # Dimension of the transformer, this can be changed\n",
        "    depth=6,  # Number of transformer layers, this can be changed\n",
        "    heads=8,  # Number of attention heads, this can be changed\n",
        "    dim_head=64,  # Dimension of each attention head, this can be changed\n",
        "    mlp_dim=256,  # Dimension of the MLP layer, this can be changed\n",
        "    num_classes=15,  # Number of classes, this can be changed\n",
        "    dropout=0.4,  # Dropout rate, this can be changed\n",
        "    pos_emb_size=128,  # Position embedding size, this can be changed\n",
        "    class_emb_size=128,  # Class embedding size, this can be changed\n",
        "    stride=1  # Stride for the convolution, this can be changed\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b_WjbDjBeODf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_WjbDjBeODf",
        "outputId": "75c38836-91e9-4f32-d877-9c75d98908f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_batch sahpe before transpose: (4914, 9, 9, 63)\n",
            "lidar_batch sahpe before transpose: (4914, 9, 9, 1)\n",
            "hsi_batch sahpe after transpose: (4914, 63, 9, 9)\n",
            "lidar_batch shape after transpose: (4914, 1, 9, 9)\n"
          ]
        }
      ],
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch = hsi_train  # Shape: (2832, 5, 5, 144)\n",
        "lidar_batch = lidar_train # Shape: (2832, 5, 5, 1)\n",
        "print('hsi_batch sahpe before transpose:', hsi_batch.shape)\n",
        "print('lidar_batch sahpe before transpose:', lidar_batch.shape)\n",
        "# Transpose the data to have the channel dimension at the correct place\n",
        "hsi_batch = hsi_batch.transpose(0, 3, 1, 2)  # New shape: (2832, 144, 5, 5)\n",
        "lidar_batch = lidar_batch.transpose(0, 3, 1, 2)  # New shape: (2832, 1, 5, 5)\n",
        "print('hsi_batch sahpe after transpose:', hsi_batch.shape)\n",
        "print('lidar_batch shape after transpose:', lidar_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GWEESEK3a-hX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWEESEK3a-hX",
        "outputId": "bca8fc48-633c-4810-b784-4558bf9a1e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one_hsi_batch sahpe before transpose: (1, 9, 9, 63)\n",
            "one_lidar_batch sahpe before transpose: (1, 9, 9, 1)\n",
            "hsi_batch sahpe after transpose: (1, 63, 9, 9)\n",
            "lidar_batch shape after transpose: (1, 1, 9, 9)\n"
          ]
        }
      ],
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "one_hsi_batch = hsi_train[:1]  # Shape: (1, 5, 5, 144)\n",
        "one_lidar_batch = lidar_train[:1]  # Shape: (1, 5, 5, 1)\n",
        "print('one_hsi_batch sahpe before transpose:', one_hsi_batch.shape)\n",
        "print('one_lidar_batch sahpe before transpose:', one_lidar_batch.shape)\n",
        "# Transpose the data to have the channel dimension at the correct place\n",
        "one_hsi_batch = one_hsi_batch.transpose(0, 3, 1, 2)  # New shape: (1, 144, 5, 5)\n",
        "one_lidar_batch = one_lidar_batch.transpose(0, 3,1, 2)  # New shape: (1, 1, 5, 5)\n",
        "print('hsi_batch sahpe after transpose:', one_hsi_batch.shape)\n",
        "print('lidar_batch shape after transpose:', one_lidar_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7fbd0a6",
      "metadata": {
        "id": "b7fbd0a6",
        "outputId": "a8643f6b-2d78-4b28-e058-8f044793dc7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one_hsi_batch_embedded shape: torch.Size([1, 64, 128])\n",
            "one_lidar_batch_embedded shape: torch.Size([1, 2, 128])\n"
          ]
        }
      ],
      "source": [
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "one_hsi_batch = hsi_train[:1]  # Shape: (1, 5, 5, 144)\n",
        "one_lidar_batch = lidar_train[:1]  # Shape: (1, 5, 5, 1)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "one_hsi_batch_flat = torch.from_numpy(one_hsi_batch.astype(np.float32).reshape(1, hsi_config.in_channels, 63, 1)).to(device)\n",
        "one_lidar_batch_flat = torch.from_numpy(one_lidar_batch.astype(np.float32).reshape(1, lidar_config.in_channels,1, 1)).to(device)\n",
        "\n",
        "# Initialize the patch embedding module\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "one_hsi_batch_embedded = hsi_patch_embedding(one_hsi_batch_flat).to(device)\n",
        "one_lidar_batch_embedded = lidar_patch_embedding(one_lidar_batch_flat).to(device)\n",
        "\n",
        "print('one_hsi_batch_embedded shape:', one_hsi_batch_embedded.shape)\n",
        "print('one_lidar_batch_embedded shape:', one_lidar_batch_embedded.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gpa3oU5ndrhn",
      "metadata": {
        "id": "gpa3oU5ndrhn"
      },
      "source": [
        "### Intialisation CrossAttention Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eb951f9",
      "metadata": {
        "id": "6eb951f9",
        "outputId": "3e187389-e605-40bb-b144-d58470feb98e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'cudatoolkit=11.1'\n",
            "Hint: = is not a valid operator. Did you mean == ?\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision torchaudio cudatoolkit=11.1 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4eefbb4",
      "metadata": {
        "id": "e4eefbb4",
        "outputId": "d7d7d141-1ea2-4631-c230-64b0d9c93821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_batch_6  shape: (6, 9, 9, 63)\n",
            "lidar_batch_6 shape: (6, 9, 9, 1)\n",
            "hsi_batch_6_flat  shape: torch.Size([6, 81, 63, 1])\n",
            "lidar_batch_6_flat shape: torch.Size([6, 81, 1, 1])\n",
            "hsi_batch_6_embedded  shape: torch.Size([6, 64, 128])\n",
            "lidar_batch_6_embedded shape: torch.Size([6, 2, 128])\n",
            "Cross attention output shape: torch.Size([6, 63, 63])\n",
            "Output shape: torch.Size([6, 63, 512])\n"
          ]
        }
      ],
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch_6 = hsi_train[:6]  # Shape: (6, 5, 5, 144)\n",
        "lidar_batch_6 = lidar_train[:6]  # Shape: (6, 5, 5, 1)\n",
        "\n",
        "print(\"hsi_batch_6  shape:\", hsi_batch_6 .shape)\n",
        "print(\"lidar_batch_6 shape:\", lidar_batch_6.shape)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "hsi_batch_6_flat = torch.from_numpy(hsi_batch_6.astype(np.float32).reshape(6, hsi_config.in_channels, 63, 1))\n",
        "lidar_batch_6_flat = torch.from_numpy(lidar_batch_6.astype(np.float32).reshape(6, lidar_config.in_channels, 1, 1))\n",
        "\n",
        "print(\"hsi_batch_6_flat  shape:\", hsi_batch_6_flat .shape)\n",
        "print(\"lidar_batch_6_flat shape:\", lidar_batch_6_flat.shape)\n",
        "\n",
        "hsi_batch_6_flat = hsi_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_6_flat = lidar_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "hsi_batch_6_embedded  = hsi_patch_embedding(hsi_batch_6_flat).to(device)\n",
        "lidar_batch_6_embedded = lidar_patch_embedding(lidar_batch_6_flat).to(device)\n",
        "\n",
        "#device = torch.device(\"cuda:0\")  # Define the device (GPU)\n",
        "\n",
        "# Move the tensors to the desired device\n",
        "hsi_batch_6_embedded = hsi_batch_6_embedded.to(device)\n",
        "lidar_batch_6_embedded = lidar_batch_6_embedded.to(device)\n",
        "print(\"hsi_batch_6_embedded  shape:\", hsi_batch_6_embedded .shape)\n",
        "print(\"lidar_batch_6_embedded shape:\", lidar_batch_6_embedded.shape)\n",
        "\n",
        "# Define the dimension of the model and the number of heads\n",
        "d_model = hsi_config.emb_size  # the output dimension of PatchEmbedding\n",
        "num_heads = hsi_config.heads  # the number of attention heads in the transformer\n",
        "\n",
        "# Initialize CrossAttention module\n",
        "cross_attention = CrossAttention(lidar_config, hsi_config).to(device)\n",
        "\n",
        "# Apply the cross attention\n",
        "output,attn_scores = cross_attention(lidar_batch_6_embedded, hsi_batch_6_embedded)\n",
        "\n",
        "\n",
        "print(\"Cross attention output shape:\", attn_scores.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XYlhrA99ncUq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYlhrA99ncUq",
        "outputId": "d6f95d96-0c40-4120-96d2-9eed2b10b5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_batch_6  shape: (6, 9, 9, 63)\n",
            "lidar_batch_6 shape: (6, 9, 9, 1)\n",
            "hsi_batch_6_flat  shape: torch.Size([6, 81, 63, 1])\n",
            "lidar_batch_6_flat shape: torch.Size([6, 81, 1, 1])\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[118], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhsi_batch_6_flat  shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, hsi_batch_6_flat \u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlidar_batch_6_flat shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lidar_batch_6_flat\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 17\u001b[0m hsi_batch_6_flat \u001b[38;5;241m=\u001b[39m \u001b[43mhsi_batch_6_flat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move the tensor to GPU\u001b[39;00m\n\u001b[1;32m     18\u001b[0m lidar_batch_6_flat \u001b[38;5;241m=\u001b[39m lidar_batch_6_flat\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move the tensor to GPU\u001b[39;00m\n\u001b[1;32m     20\u001b[0m hsi_patch_embedding \u001b[38;5;241m=\u001b[39m PatchEmbedding(hsi_config)\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "hsi_batch_6 = hsi_train[:6]  # Shape: (6, 5, 5, 144)\n",
        "lidar_batch_6 = lidar_train[:6]  # Shape: (6, 5, 5, 1)\n",
        "\n",
        "print(\"hsi_batch_6  shape:\", hsi_batch_6 .shape)\n",
        "print(\"lidar_batch_6 shape:\", lidar_batch_6.shape)\n",
        "\n",
        "# Now reshape HSI data such that spatial dimensions (5x5) are flattened and treated as channels\n",
        "#hsi_batch_6_flat = torch.from_numpy(hsi_batch_6.astype(np.float32).reshape(6, 25, 63, 1))\n",
        "#lidar_batch_6_flat = torch.from_numpy(lidar_batch_6.astype(np.float32).reshape(6, 25, 1, 1))\n",
        "hsi_batch_6_flat = torch.from_numpy(hsi_batch_6.astype(np.float32).reshape(6, hsi_config.in_channels, hsi_config.num_patches, 1))\n",
        "lidar_batch_6_flat = torch.from_numpy(lidar_batch_6.astype(np.float32).reshape(6, lidar_config.in_channels, lidar_config.num_patches, 1))\n",
        "\n",
        "print(\"hsi_batch_6_flat  shape:\", hsi_batch_6_flat .shape)\n",
        "print(\"lidar_batch_6_flat shape:\", lidar_batch_6_flat.shape)\n",
        "\n",
        "hsi_batch_6_flat = hsi_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_6_flat = lidar_batch_6_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "hsi_patch_embedding = PatchEmbedding(hsi_config).to(device)\n",
        "lidar_patch_embedding = PatchEmbedding(lidar_config).to(device)\n",
        "\n",
        "# Pass the data through the patch embedding module\n",
        "hsi_batch_6_embedded  = hsi_patch_embedding(hsi_batch_6_flat)\n",
        "lidar_batch_6_embedded = lidar_patch_embedding(lidar_batch_6_flat)\n",
        "\n",
        "device = torch.device(\"cuda:0\")  # Define the device (GPU)\n",
        "\n",
        "# Move the tensors to the desired device\n",
        "hsi_batch_6_embedded = hsi_batch_6_embedded.to(device)\n",
        "lidar_batch_6_embedded = lidar_batch_6_embedded.to(device)\n",
        "print(\"hsi_batch_6_embedded  shape:\", hsi_batch_6_embedded .shape)\n",
        "print(\"lidar_batch_6_embedded shape:\", lidar_batch_6_embedded.shape)\n",
        "\n",
        "# Define the dimension of the model and the number of heads\n",
        "d_model = hsi_config.emb_size  # the output dimension of PatchEmbedding\n",
        "num_heads = hsi_config.heads  # the number of attention heads in the transformer\n",
        "\n",
        "# Initialize CrossAttention module\n",
        "cross_attention = CrossAttention(lidar_config, hsi_config).to(device)\n",
        "\n",
        "# Apply the cross attention\n",
        "output,attn_scores = cross_attention(lidar_batch_6_embedded, hsi_batch_6_embedded)\n",
        "\n",
        "\n",
        "print(\"Cross attention output shape:\", attn_scores.shape)\n",
        "print(\"Output shape:\", output.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dDySwOVzY7AE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDySwOVzY7AE",
        "outputId": "f04c1c8f-2de1-4ec2-f26a-677ca34e38c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CrossAttention(\n",
              "  (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (to_out): Linear(in_features=512, out_features=63, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cross_attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nJTi3xzCYzd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJTi3xzCYzd1",
        "outputId": "c96fd7da-874b-4448-ddcd-35a789fea053"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CrossAttentionModel(\n",
              "  (hsi_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(81, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (lidar_patch_embedding): PatchEmbedding(\n",
              "    (proj): Conv2d(81, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (cross_attention): CrossAttention(\n",
              "    (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
              "    (to_out): Linear(in_features=512, out_features=63, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yem48RAvJSLH",
      "metadata": {
        "id": "yem48RAvJSLH"
      },
      "source": [
        "# 4.1  Training DataLoader for Cross Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06efbee4",
      "metadata": {
        "id": "06efbee4"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Initialize empty lists to store the train and test samples\n",
        "# hsi_train = []\n",
        "# hsi_test = []\n",
        "# lidar_train = []\n",
        "# lidar_test = []\n",
        "# y_train = []\n",
        "# y_test = []\n",
        "\n",
        "# # Split the data for each class\n",
        "# for class_number, class_info in class_dict.items():\n",
        "#     # Get the indices of the samples for this class\n",
        "#     class_indices = np.where(labels == class_number)[0]\n",
        "\n",
        "#     # Shuffle the class indices\n",
        "#     np.random.shuffle(class_indices)\n",
        "\n",
        "#     # Split the class indices into training and test indices\n",
        "#     train_indices = class_indices[:class_info[\"training\"]]\n",
        "#     test_indices = class_indices[class_info[\"training\"]:class_info[\"samples\"]]\n",
        "\n",
        "#     # Add the selected training samples to the train lists\n",
        "#     hsi_train.append(hsi_samples[train_indices])\n",
        "#     lidar_train.append(lidar_samples[train_indices])\n",
        "#     y_train.append(labels[train_indices])\n",
        "\n",
        "#     # Add the remaining samples to the test lists\n",
        "#     hsi_test.append(hsi_samples[test_indices])\n",
        "#     lidar_test.append(lidar_samples[test_indices])\n",
        "#     y_test.append(labels[test_indices])\n",
        "\n",
        "# # Concatenate the train and test lists to create the train and test arrays\n",
        "# hsi_train = np.concatenate(hsi_train)\n",
        "# hsi_test = np.concatenate(hsi_test)\n",
        "# lidar_train = np.concatenate(lidar_train)\n",
        "# lidar_test = np.concatenate(lidar_test)\n",
        "# y_train = np.concatenate(y_train)\n",
        "# y_test = np.concatenate(y_test)\n",
        "\n",
        "# # Print the shapes of the train and test arrays\n",
        "# print('hsi_train shape:', hsi_train.shape)\n",
        "# print('hsi_test shape:', hsi_test.shape)\n",
        "# print('lidar_train shape:', lidar_train.shape)\n",
        "# print('lidar_test shape:', lidar_test.shape)\n",
        "# print('y_train shape:', y_train.shape)\n",
        "# print('y_test shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Sw5JbJ23X3n",
      "metadata": {
        "id": "1Sw5JbJ23X3n"
      },
      "outputs": [],
      "source": [
        "class HyperspectralDataset(Dataset):\n",
        "    def __init__(self, hsi_samples, lidar_samples, labels):\n",
        "        self.hsi_samples = hsi_samples\n",
        "        self.lidar_samples = lidar_samples\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hsi_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hsi_patch = self.hsi_samples[idx].float().to(device)\n",
        "        lidar_patch = self.lidar_samples[idx].float().to(device)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert label to a tensor and reshape to match the model's output shape\n",
        "        label = torch.tensor(label).repeat(hsi_patch.shape[1], 1).to(device)\n",
        "\n",
        "        return hsi_patch, lidar_patch, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a3014a",
      "metadata": {
        "id": "c0a3014a",
        "outputId": "518d0066-b1bb-4e5f-9a11-6f97ef478684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "labels_tensor: torch.Size([4914, 6])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8353/1321332885.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_tensor = torch.tensor(y_train)\n"
          ]
        }
      ],
      "source": [
        "labels_tensor = torch.tensor(y_train)\n",
        "labels_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "print('labels_tensor:',labels_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a00236bf",
      "metadata": {
        "id": "a00236bf",
        "outputId": "c80bbb8f-e30a-4af7-fb0a-6c739d11940a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4914])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming labels_tensor is one-hot encoded or contains probabilities\n",
        "labels_tensor = torch.argmax(labels_tensor, dim=1)\n",
        "labels_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ec406a",
      "metadata": {
        "id": "e3ec406a",
        "outputId": "707d39c1-80b6-45df-9f7d-3d544e57d775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_train shape: torch.Size([4914])\n"
          ]
        }
      ],
      "source": [
        "y_train=labels_tensor\n",
        "print('y_train shape:',y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59448309",
      "metadata": {
        "id": "59448309",
        "outputId": "810aef86-8505-408a-d355-c33fbb1da430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_test_tensor shape: torch.Size([29395, 6])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8353/3569926731.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_test_tensor= torch.tensor(y_test)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "y_test_tensor= torch.tensor(y_test)\n",
        "y_test_tensor -= 1  # Shift label values to the range [0, C-1]\n",
        "print('y_test_tensor shape:',y_test_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1db9eb8",
      "metadata": {
        "id": "d1db9eb8",
        "outputId": "29d32891-6e5a-4d01-f064-500265bf64ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_test_tensor shape: torch.Size([29395])\n"
          ]
        }
      ],
      "source": [
        "# Assuming labels_tensor is one-hot encoded or contains probabilities\n",
        "y_test_tensor = torch.argmax(y_test_tensor, dim=1)\n",
        "print('y_test_tensor shape:',y_test_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc61bc8",
      "metadata": {
        "id": "9bc61bc8"
      },
      "outputs": [],
      "source": [
        "y_test= y_test_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-72dBlA1TbBj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-72dBlA1TbBj",
        "outputId": "2d0df72f-dcd9-4e6d-e8d9-12d05650dd80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique classes: tensor([0, 1, 2, 3, 4, 5])\n"
          ]
        }
      ],
      "source": [
        "unique_classes = torch.unique(labels_tensor)\n",
        "print(\"Unique classes:\", unique_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905dbc07",
      "metadata": {
        "id": "905dbc07",
        "outputId": "4640756a-9524-49bb-9d63-457e55916a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hsi_batch shape: torch.Size([4914, 81, 63, 1])\n",
            "lidar_batch shape: torch.Size([4914, 81, 1, 1])\n",
            "hsi_samples_train shape: torch.Size([3931, 81, 63, 1])\n",
            "hsi_samples_val shape: torch.Size([983, 81, 63, 1])\n",
            "lidar_samples_train shape: torch.Size([3931, 81, 1, 1])\n",
            "lidar_samples_valshape: torch.Size([983, 81, 1, 1])\n",
            "labels_train, shape: torch.Size([3931])\n",
            "labels_val, shape: torch.Size([983])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Resahpe the data\n",
        "\n",
        "# Move the input data to the desired device\n",
        "# Assume we have loaded one batch of HSI and LiDAR data\n",
        "# hsi_batch = hsi_train # Shape: (2832, 5, 5, 144)\n",
        "# lidar_batch= lidar_train# Shape: (2832, 5, 5, 1)\n",
        "# label_batch=y_train\n",
        "# print(\"hsi_batch shape:\", hsi_batch.shape)\n",
        "# print(\"lidar_batch shape:\", lidar_batch.shape)\n",
        "# print(\"label_batch shape:\", label_batch.shape)\n",
        "\n",
        "## Reshape the data\n",
        "hsi_batch_flat = torch.from_numpy(hsi_train.astype(np.float32).reshape(4914, hsi_config.in_channels, 63, 1))\n",
        "lidar_batch_flat = torch.from_numpy(lidar_train.astype(np.float32).reshape(4914, lidar_config.in_channels, 1, 1))\n",
        "\n",
        "print(\"hsi_batch shape:\", hsi_batch_flat .shape)\n",
        "print(\"lidar_batch shape:\", lidar_batch_flat.shape)\n",
        "\n",
        "hsi_batch_flat = hsi_batch_flat.to(device)  # Move the tensor to GPU\n",
        "lidar_batch_flat = lidar_batch_flat.to(device)  # Move the tensor to GPU\n",
        "\n",
        "# # Split into train and test\n",
        "# hsi_samples_train, hsi_samples_test, lidar_samples_train, lidar_samples_test, labels_train, labels_test = train_test_split(\n",
        "#     hsi_batch_flat, lidar_batch_flat, label_batch, test_size=0.05, random_state=42)\n",
        "# print('labels_train shape:',labels_train.shape)\n",
        "\n",
        "# Split train into train and validation\n",
        "hsi_samples_train, hsi_samples_val, lidar_samples_train, lidar_samples_val, labels_train, labels_val = train_test_split(\n",
        "    hsi_batch_flat, lidar_batch_flat, y_train, test_size=0.20, random_state=42)\n",
        "print('hsi_samples_train shape:',hsi_samples_train.shape)\n",
        "print('hsi_samples_val shape:',hsi_samples_val.shape)\n",
        "print('lidar_samples_train shape:',lidar_samples_train.shape)\n",
        "print('lidar_samples_valshape:',lidar_samples_val.shape)\n",
        "print('labels_train, shape:',labels_train.shape)\n",
        "print('labels_val, shape:',labels_val.shape)\n",
        "\n",
        "# Now test data.\n",
        "## Reshape the data\n",
        "hsi_test_batch_flat = torch.from_numpy(hsi_test.astype(np.float32).reshape(29395, hsi_config.in_channels, 63, 1))\n",
        "lidar_test_batch_flat = torch.from_numpy(lidar_test.astype(np.float32).reshape(29395, lidar_config.in_channels, 1, 1))\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = HyperspectralDataset(hsi_samples_train, lidar_samples_train, labels_train)\n",
        "val_dataset = HyperspectralDataset(hsi_samples_val, lidar_samples_val, labels_val)\n",
        "test_dataset = HyperspectralDataset(hsi_test_batch_flat, lidar_test_batch_flat, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_MBeu13kDLc6",
      "metadata": {
        "id": "_MBeu13kDLc6"
      },
      "source": [
        "# 4.2 Training CorssAttentionMode ltraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w2D4wFzJ5ams",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2D4wFzJ5ams",
        "outputId": "7f057e62-b3ca-4c39-d3a8-fa1fd9b14987"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8353/3897225702.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  label = torch.tensor(label).repeat(hsi_patch.shape[1], 1).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Batch [1/123], Loss: 86.2442\n",
            "Epoch [1/200], Batch [2/123], Loss: 73.4336\n",
            "Epoch [1/200], Batch [3/123], Loss: 70.7271\n",
            "Epoch [1/200], Batch [4/123], Loss: 63.4063\n",
            "Epoch [1/200], Batch [5/123], Loss: 41.7166\n",
            "Epoch [1/200], Batch [6/123], Loss: 73.7784\n",
            "Epoch [1/200], Batch [7/123], Loss: 53.7144\n",
            "Epoch [1/200], Batch [8/123], Loss: 54.1906\n",
            "Epoch [1/200], Batch [9/123], Loss: 76.5028\n",
            "Epoch [1/200], Batch [10/123], Loss: 47.3111\n",
            "Epoch [1/200], Batch [11/123], Loss: 39.5063\n",
            "Epoch [1/200], Batch [12/123], Loss: 52.8524\n",
            "Epoch [1/200], Batch [13/123], Loss: 66.4406\n",
            "Epoch [1/200], Batch [14/123], Loss: 35.8733\n",
            "Epoch [1/200], Batch [15/123], Loss: 35.9469\n",
            "Epoch [1/200], Batch [16/123], Loss: 32.6988\n",
            "Epoch [1/200], Batch [17/123], Loss: 38.9532\n",
            "Epoch [1/200], Batch [18/123], Loss: 36.6473\n",
            "Epoch [1/200], Batch [19/123], Loss: 27.8115\n",
            "Epoch [1/200], Batch [20/123], Loss: 33.7839\n",
            "Epoch [1/200], Batch [21/123], Loss: 27.0236\n",
            "Epoch [1/200], Batch [22/123], Loss: 36.0117\n",
            "Epoch [1/200], Batch [23/123], Loss: 21.2150\n",
            "Epoch [1/200], Batch [24/123], Loss: 28.6098\n",
            "Epoch [1/200], Batch [25/123], Loss: 24.0882\n",
            "Epoch [1/200], Batch [26/123], Loss: 17.8738\n",
            "Epoch [1/200], Batch [27/123], Loss: 23.2817\n",
            "Epoch [1/200], Batch [28/123], Loss: 16.0755\n",
            "Epoch [1/200], Batch [29/123], Loss: 20.9495\n",
            "Epoch [1/200], Batch [30/123], Loss: 16.6641\n",
            "Epoch [1/200], Batch [31/123], Loss: 14.0619\n",
            "Epoch [1/200], Batch [32/123], Loss: 12.2635\n",
            "Epoch [1/200], Batch [33/123], Loss: 14.9729\n",
            "Epoch [1/200], Batch [34/123], Loss: 13.2207\n",
            "Epoch [1/200], Batch [35/123], Loss: 13.5480\n",
            "Epoch [1/200], Batch [36/123], Loss: 9.2380\n",
            "Epoch [1/200], Batch [37/123], Loss: 12.1543\n",
            "Epoch [1/200], Batch [38/123], Loss: 18.1160\n",
            "Epoch [1/200], Batch [39/123], Loss: 13.2001\n",
            "Epoch [1/200], Batch [40/123], Loss: 12.4588\n",
            "Epoch [1/200], Batch [41/123], Loss: 11.4890\n",
            "Epoch [1/200], Batch [42/123], Loss: 7.2127\n",
            "Epoch [1/200], Batch [43/123], Loss: 13.6716\n",
            "Epoch [1/200], Batch [44/123], Loss: 8.8027\n",
            "Epoch [1/200], Batch [45/123], Loss: 12.7715\n",
            "Epoch [1/200], Batch [46/123], Loss: 11.0329\n",
            "Epoch [1/200], Batch [47/123], Loss: 12.4520\n",
            "Epoch [1/200], Batch [48/123], Loss: 12.0293\n",
            "Epoch [1/200], Batch [49/123], Loss: 10.4304\n",
            "Epoch [1/200], Batch [50/123], Loss: 16.5978\n",
            "Epoch [1/200], Batch [51/123], Loss: 7.9475\n",
            "Epoch [1/200], Batch [52/123], Loss: 13.5416\n",
            "Epoch [1/200], Batch [53/123], Loss: 11.9656\n",
            "Epoch [1/200], Batch [54/123], Loss: 8.7896\n",
            "Epoch [1/200], Batch [55/123], Loss: 7.7674\n",
            "Epoch [1/200], Batch [56/123], Loss: 10.5471\n",
            "Epoch [1/200], Batch [57/123], Loss: 11.4867\n",
            "Epoch [1/200], Batch [58/123], Loss: 10.4543\n",
            "Epoch [1/200], Batch [59/123], Loss: 9.0415\n",
            "Epoch [1/200], Batch [60/123], Loss: 6.0210\n",
            "Epoch [1/200], Batch [61/123], Loss: 9.4810\n",
            "Epoch [1/200], Batch [62/123], Loss: 10.2149\n",
            "Epoch [1/200], Batch [63/123], Loss: 10.3891\n",
            "Epoch [1/200], Batch [64/123], Loss: 12.6097\n",
            "Epoch [1/200], Batch [65/123], Loss: 7.2421\n",
            "Epoch [1/200], Batch [66/123], Loss: 12.3105\n",
            "Epoch [1/200], Batch [67/123], Loss: 9.2295\n",
            "Epoch [1/200], Batch [68/123], Loss: 6.6933\n",
            "Epoch [1/200], Batch [69/123], Loss: 7.5211\n",
            "Epoch [1/200], Batch [70/123], Loss: 7.4362\n",
            "Epoch [1/200], Batch [71/123], Loss: 12.2302\n",
            "Epoch [1/200], Batch [72/123], Loss: 10.3082\n",
            "Epoch [1/200], Batch [73/123], Loss: 13.9935\n",
            "Epoch [1/200], Batch [74/123], Loss: 6.4855\n",
            "Epoch [1/200], Batch [75/123], Loss: 9.4034\n",
            "Epoch [1/200], Batch [76/123], Loss: 6.6129\n",
            "Epoch [1/200], Batch [77/123], Loss: 7.5109\n",
            "Epoch [1/200], Batch [78/123], Loss: 11.5771\n",
            "Epoch [1/200], Batch [79/123], Loss: 8.1559\n",
            "Epoch [1/200], Batch [80/123], Loss: 6.9584\n",
            "Epoch [1/200], Batch [81/123], Loss: 5.5844\n",
            "Epoch [1/200], Batch [82/123], Loss: 5.1264\n",
            "Epoch [1/200], Batch [83/123], Loss: 10.3694\n",
            "Epoch [1/200], Batch [84/123], Loss: 9.8018\n",
            "Epoch [1/200], Batch [85/123], Loss: 4.2744\n",
            "Epoch [1/200], Batch [86/123], Loss: 6.2401\n",
            "Epoch [1/200], Batch [87/123], Loss: 5.7773\n",
            "Epoch [1/200], Batch [88/123], Loss: 6.5202\n",
            "Epoch [1/200], Batch [89/123], Loss: 9.4215\n",
            "Epoch [1/200], Batch [90/123], Loss: 6.8361\n",
            "Epoch [1/200], Batch [91/123], Loss: 6.0694\n",
            "Epoch [1/200], Batch [92/123], Loss: 9.0630\n",
            "Epoch [1/200], Batch [93/123], Loss: 6.4462\n",
            "Epoch [1/200], Batch [94/123], Loss: 7.7556\n",
            "Epoch [1/200], Batch [95/123], Loss: 8.1808\n",
            "Epoch [1/200], Batch [96/123], Loss: 6.0040\n",
            "Epoch [1/200], Batch [97/123], Loss: 8.4982\n",
            "Epoch [1/200], Batch [98/123], Loss: 9.5272\n",
            "Epoch [1/200], Batch [99/123], Loss: 5.7313\n",
            "Epoch [1/200], Batch [100/123], Loss: 5.5926\n",
            "Epoch [1/200], Batch [101/123], Loss: 8.4650\n",
            "Epoch [1/200], Batch [102/123], Loss: 6.8865\n",
            "Epoch [1/200], Batch [103/123], Loss: 9.5675\n",
            "Epoch [1/200], Batch [104/123], Loss: 5.8977\n",
            "Epoch [1/200], Batch [105/123], Loss: 5.5481\n",
            "Epoch [1/200], Batch [106/123], Loss: 6.6411\n",
            "Epoch [1/200], Batch [107/123], Loss: 6.9196\n",
            "Epoch [1/200], Batch [108/123], Loss: 7.2376\n",
            "Epoch [1/200], Batch [109/123], Loss: 5.5749\n",
            "Epoch [1/200], Batch [110/123], Loss: 7.6820\n",
            "Epoch [1/200], Batch [111/123], Loss: 7.4804\n",
            "Epoch [1/200], Batch [112/123], Loss: 5.6228\n",
            "Epoch [1/200], Batch [113/123], Loss: 9.0540\n",
            "Epoch [1/200], Batch [114/123], Loss: 6.3586\n",
            "Epoch [1/200], Batch [115/123], Loss: 5.9044\n",
            "Epoch [1/200], Batch [116/123], Loss: 4.7076\n",
            "Epoch [1/200], Batch [117/123], Loss: 8.5295\n",
            "Epoch [1/200], Batch [118/123], Loss: 10.8390\n",
            "Epoch [1/200], Batch [119/123], Loss: 5.6377\n",
            "Epoch [1/200], Batch [120/123], Loss: 8.1601\n",
            "Epoch [1/200], Batch [121/123], Loss: 8.5586\n",
            "Epoch [1/200], Batch [122/123], Loss: 7.0914\n",
            "Epoch [1/200], Batch [123/123], Loss: 6.6223\n",
            "Epoch [2/200], Batch [1/123], Loss: 4.4330\n",
            "Epoch [2/200], Batch [2/123], Loss: 5.7484\n",
            "Epoch [2/200], Batch [3/123], Loss: 7.5637\n",
            "Epoch [2/200], Batch [4/123], Loss: 8.2033\n",
            "Epoch [2/200], Batch [5/123], Loss: 7.1324\n",
            "Epoch [2/200], Batch [6/123], Loss: 8.0147\n",
            "Epoch [2/200], Batch [7/123], Loss: 6.5534\n",
            "Epoch [2/200], Batch [8/123], Loss: 6.5549\n",
            "Epoch [2/200], Batch [9/123], Loss: 7.8625\n",
            "Epoch [2/200], Batch [10/123], Loss: 6.0244\n",
            "Epoch [2/200], Batch [11/123], Loss: 7.2999\n",
            "Epoch [2/200], Batch [12/123], Loss: 7.3444\n",
            "Epoch [2/200], Batch [13/123], Loss: 7.9210\n",
            "Epoch [2/200], Batch [14/123], Loss: 4.4985\n",
            "Epoch [2/200], Batch [15/123], Loss: 5.7292\n",
            "Epoch [2/200], Batch [16/123], Loss: 6.9178\n",
            "Epoch [2/200], Batch [17/123], Loss: 7.9102\n",
            "Epoch [2/200], Batch [18/123], Loss: 5.6344\n",
            "Epoch [2/200], Batch [19/123], Loss: 7.2753\n",
            "Epoch [2/200], Batch [20/123], Loss: 3.9205\n",
            "Epoch [2/200], Batch [21/123], Loss: 6.4724\n",
            "Epoch [2/200], Batch [22/123], Loss: 4.1420\n",
            "Epoch [2/200], Batch [23/123], Loss: 6.5326\n",
            "Epoch [2/200], Batch [24/123], Loss: 4.8384\n",
            "Epoch [2/200], Batch [25/123], Loss: 4.7844\n",
            "Epoch [2/200], Batch [26/123], Loss: 7.2914\n",
            "Epoch [2/200], Batch [27/123], Loss: 6.3684\n",
            "Epoch [2/200], Batch [28/123], Loss: 7.6940\n",
            "Epoch [2/200], Batch [29/123], Loss: 7.2783\n",
            "Epoch [2/200], Batch [30/123], Loss: 6.6864\n",
            "Epoch [2/200], Batch [31/123], Loss: 5.2524\n",
            "Epoch [2/200], Batch [32/123], Loss: 5.7574\n",
            "Epoch [2/200], Batch [33/123], Loss: 8.5334\n",
            "Epoch [2/200], Batch [34/123], Loss: 7.5270\n",
            "Epoch [2/200], Batch [35/123], Loss: 5.6038\n",
            "Epoch [2/200], Batch [36/123], Loss: 6.9424\n",
            "Epoch [2/200], Batch [37/123], Loss: 5.9519\n",
            "Epoch [2/200], Batch [38/123], Loss: 5.0974\n",
            "Epoch [2/200], Batch [39/123], Loss: 7.8656\n",
            "Epoch [2/200], Batch [40/123], Loss: 4.7016\n",
            "Epoch [2/200], Batch [41/123], Loss: 5.5150\n",
            "Epoch [2/200], Batch [42/123], Loss: 5.6641\n",
            "Epoch [2/200], Batch [43/123], Loss: 6.2446\n",
            "Epoch [2/200], Batch [44/123], Loss: 7.2149\n",
            "Epoch [2/200], Batch [45/123], Loss: 5.8603\n",
            "Epoch [2/200], Batch [46/123], Loss: 5.8371\n",
            "Epoch [2/200], Batch [47/123], Loss: 4.8495\n",
            "Epoch [2/200], Batch [48/123], Loss: 5.9070\n",
            "Epoch [2/200], Batch [49/123], Loss: 5.4601\n",
            "Epoch [2/200], Batch [50/123], Loss: 6.1922\n",
            "Epoch [2/200], Batch [51/123], Loss: 6.2906\n",
            "Epoch [2/200], Batch [52/123], Loss: 4.3321\n",
            "Epoch [2/200], Batch [53/123], Loss: 5.1237\n",
            "Epoch [2/200], Batch [54/123], Loss: 4.8919\n",
            "Epoch [2/200], Batch [55/123], Loss: 5.7514\n",
            "Epoch [2/200], Batch [56/123], Loss: 6.7874\n",
            "Epoch [2/200], Batch [57/123], Loss: 6.3360\n",
            "Epoch [2/200], Batch [58/123], Loss: 7.8633\n",
            "Epoch [2/200], Batch [59/123], Loss: 7.3082\n",
            "Epoch [2/200], Batch [60/123], Loss: 6.9454\n",
            "Epoch [2/200], Batch [61/123], Loss: 5.5869\n",
            "Epoch [2/200], Batch [62/123], Loss: 3.7552\n",
            "Epoch [2/200], Batch [63/123], Loss: 6.9941\n",
            "Epoch [2/200], Batch [64/123], Loss: 4.0495\n",
            "Epoch [2/200], Batch [65/123], Loss: 3.5358\n",
            "Epoch [2/200], Batch [66/123], Loss: 6.2522\n",
            "Epoch [2/200], Batch [67/123], Loss: 5.2058\n",
            "Epoch [2/200], Batch [68/123], Loss: 6.1728\n",
            "Epoch [2/200], Batch [69/123], Loss: 4.7697\n",
            "Epoch [2/200], Batch [70/123], Loss: 6.9292\n",
            "Epoch [2/200], Batch [71/123], Loss: 4.7699\n",
            "Epoch [2/200], Batch [72/123], Loss: 3.7304\n",
            "Epoch [2/200], Batch [73/123], Loss: 6.2282\n",
            "Epoch [2/200], Batch [74/123], Loss: 6.3350\n",
            "Epoch [2/200], Batch [75/123], Loss: 5.6179\n",
            "Epoch [2/200], Batch [76/123], Loss: 5.1594\n",
            "Epoch [2/200], Batch [77/123], Loss: 7.0600\n",
            "Epoch [2/200], Batch [78/123], Loss: 5.5026\n",
            "Epoch [2/200], Batch [79/123], Loss: 5.9430\n",
            "Epoch [2/200], Batch [80/123], Loss: 4.2464\n",
            "Epoch [2/200], Batch [81/123], Loss: 3.6628\n",
            "Epoch [2/200], Batch [82/123], Loss: 5.7973\n",
            "Epoch [2/200], Batch [83/123], Loss: 4.8377\n",
            "Epoch [2/200], Batch [84/123], Loss: 2.2662\n",
            "Epoch [2/200], Batch [85/123], Loss: 4.5585\n",
            "Epoch [2/200], Batch [86/123], Loss: 5.4467\n",
            "Epoch [2/200], Batch [87/123], Loss: 5.3173\n",
            "Epoch [2/200], Batch [88/123], Loss: 5.7839\n",
            "Epoch [2/200], Batch [89/123], Loss: 6.2758\n",
            "Epoch [2/200], Batch [90/123], Loss: 4.7829\n",
            "Epoch [2/200], Batch [91/123], Loss: 3.6662\n",
            "Epoch [2/200], Batch [92/123], Loss: 4.1757\n",
            "Epoch [2/200], Batch [93/123], Loss: 5.3016\n",
            "Epoch [2/200], Batch [94/123], Loss: 4.3811\n",
            "Epoch [2/200], Batch [95/123], Loss: 6.1555\n",
            "Epoch [2/200], Batch [96/123], Loss: 5.2943\n",
            "Epoch [2/200], Batch [97/123], Loss: 4.1588\n",
            "Epoch [2/200], Batch [98/123], Loss: 4.0613\n",
            "Epoch [2/200], Batch [99/123], Loss: 5.0339\n",
            "Epoch [2/200], Batch [100/123], Loss: 4.5537\n",
            "Epoch [2/200], Batch [101/123], Loss: 3.5075\n",
            "Epoch [2/200], Batch [102/123], Loss: 6.2504\n",
            "Epoch [2/200], Batch [103/123], Loss: 4.0422\n",
            "Epoch [2/200], Batch [104/123], Loss: 4.3644\n",
            "Epoch [2/200], Batch [105/123], Loss: 3.2359\n",
            "Epoch [2/200], Batch [106/123], Loss: 6.0085\n",
            "Epoch [2/200], Batch [107/123], Loss: 4.1393\n",
            "Epoch [2/200], Batch [108/123], Loss: 5.7445\n",
            "Epoch [2/200], Batch [109/123], Loss: 4.5009\n",
            "Epoch [2/200], Batch [110/123], Loss: 6.6071\n",
            "Epoch [2/200], Batch [111/123], Loss: 5.3818\n",
            "Epoch [2/200], Batch [112/123], Loss: 9.3771\n",
            "Epoch [2/200], Batch [113/123], Loss: 6.5891\n",
            "Epoch [2/200], Batch [114/123], Loss: 3.0955\n",
            "Epoch [2/200], Batch [115/123], Loss: 4.0402\n",
            "Epoch [2/200], Batch [116/123], Loss: 5.1047\n",
            "Epoch [2/200], Batch [117/123], Loss: 7.8500\n",
            "Epoch [2/200], Batch [118/123], Loss: 3.4165\n",
            "Epoch [2/200], Batch [119/123], Loss: 6.1508\n",
            "Epoch [2/200], Batch [120/123], Loss: 6.2924\n",
            "Epoch [2/200], Batch [121/123], Loss: 3.5787\n",
            "Epoch [2/200], Batch [122/123], Loss: 5.4535\n",
            "Epoch [2/200], Batch [123/123], Loss: 3.9429\n",
            "Epoch [3/200], Batch [1/123], Loss: 3.4184\n",
            "Epoch [3/200], Batch [2/123], Loss: 4.3763\n",
            "Epoch [3/200], Batch [3/123], Loss: 4.8560\n",
            "Epoch [3/200], Batch [4/123], Loss: 6.2549\n",
            "Epoch [3/200], Batch [5/123], Loss: 5.6931\n",
            "Epoch [3/200], Batch [6/123], Loss: 3.0430\n",
            "Epoch [3/200], Batch [7/123], Loss: 3.6421\n",
            "Epoch [3/200], Batch [8/123], Loss: 5.1314\n",
            "Epoch [3/200], Batch [9/123], Loss: 5.4779\n",
            "Epoch [3/200], Batch [10/123], Loss: 4.8474\n",
            "Epoch [3/200], Batch [11/123], Loss: 4.8717\n",
            "Epoch [3/200], Batch [12/123], Loss: 6.0483\n",
            "Epoch [3/200], Batch [13/123], Loss: 5.7376\n",
            "Epoch [3/200], Batch [14/123], Loss: 5.9239\n",
            "Epoch [3/200], Batch [15/123], Loss: 5.8193\n",
            "Epoch [3/200], Batch [16/123], Loss: 5.8979\n",
            "Epoch [3/200], Batch [17/123], Loss: 3.4035\n",
            "Epoch [3/200], Batch [18/123], Loss: 4.0048\n",
            "Epoch [3/200], Batch [19/123], Loss: 5.3294\n",
            "Epoch [3/200], Batch [20/123], Loss: 4.9058\n",
            "Epoch [3/200], Batch [21/123], Loss: 4.5398\n",
            "Epoch [3/200], Batch [22/123], Loss: 3.4779\n",
            "Epoch [3/200], Batch [23/123], Loss: 4.4974\n",
            "Epoch [3/200], Batch [24/123], Loss: 4.4039\n",
            "Epoch [3/200], Batch [25/123], Loss: 4.5371\n",
            "Epoch [3/200], Batch [26/123], Loss: 3.4554\n",
            "Epoch [3/200], Batch [27/123], Loss: 4.5772\n",
            "Epoch [3/200], Batch [28/123], Loss: 6.4031\n",
            "Epoch [3/200], Batch [29/123], Loss: 6.0109\n",
            "Epoch [3/200], Batch [30/123], Loss: 5.0927\n",
            "Epoch [3/200], Batch [31/123], Loss: 2.3311\n",
            "Epoch [3/200], Batch [32/123], Loss: 5.2650\n",
            "Epoch [3/200], Batch [33/123], Loss: 4.3307\n",
            "Epoch [3/200], Batch [34/123], Loss: 4.7272\n",
            "Epoch [3/200], Batch [35/123], Loss: 5.4574\n",
            "Epoch [3/200], Batch [36/123], Loss: 3.6069\n",
            "Epoch [3/200], Batch [37/123], Loss: 5.6710\n",
            "Epoch [3/200], Batch [38/123], Loss: 3.2248\n",
            "Epoch [3/200], Batch [39/123], Loss: 4.5357\n",
            "Epoch [3/200], Batch [40/123], Loss: 5.1170\n",
            "Epoch [3/200], Batch [41/123], Loss: 5.5092\n",
            "Epoch [3/200], Batch [42/123], Loss: 7.0907\n",
            "Epoch [3/200], Batch [43/123], Loss: 5.7146\n",
            "Epoch [3/200], Batch [44/123], Loss: 3.3039\n",
            "Epoch [3/200], Batch [45/123], Loss: 7.0092\n",
            "Epoch [3/200], Batch [46/123], Loss: 4.7761\n",
            "Epoch [3/200], Batch [47/123], Loss: 4.5459\n",
            "Epoch [3/200], Batch [48/123], Loss: 4.3216\n",
            "Epoch [3/200], Batch [49/123], Loss: 3.4660\n",
            "Epoch [3/200], Batch [50/123], Loss: 3.2407\n",
            "Epoch [3/200], Batch [51/123], Loss: 4.4267\n",
            "Epoch [3/200], Batch [52/123], Loss: 4.4273\n",
            "Epoch [3/200], Batch [53/123], Loss: 4.7175\n",
            "Epoch [3/200], Batch [54/123], Loss: 5.5647\n",
            "Epoch [3/200], Batch [55/123], Loss: 4.3496\n",
            "Epoch [3/200], Batch [56/123], Loss: 3.5944\n",
            "Epoch [3/200], Batch [57/123], Loss: 2.8459\n",
            "Epoch [3/200], Batch [58/123], Loss: 3.0416\n",
            "Epoch [3/200], Batch [59/123], Loss: 3.5843\n",
            "Epoch [3/200], Batch [60/123], Loss: 4.8406\n",
            "Epoch [3/200], Batch [61/123], Loss: 4.2567\n",
            "Epoch [3/200], Batch [62/123], Loss: 3.9787\n",
            "Epoch [3/200], Batch [63/123], Loss: 3.8793\n",
            "Epoch [3/200], Batch [64/123], Loss: 4.4373\n",
            "Epoch [3/200], Batch [65/123], Loss: 4.1810\n",
            "Epoch [3/200], Batch [66/123], Loss: 4.6093\n",
            "Epoch [3/200], Batch [67/123], Loss: 6.9790\n",
            "Epoch [3/200], Batch [68/123], Loss: 3.9579\n",
            "Epoch [3/200], Batch [69/123], Loss: 4.9019\n",
            "Epoch [3/200], Batch [70/123], Loss: 3.4945\n",
            "Epoch [3/200], Batch [71/123], Loss: 5.6415\n",
            "Epoch [3/200], Batch [72/123], Loss: 4.5743\n",
            "Epoch [3/200], Batch [73/123], Loss: 6.3005\n",
            "Epoch [3/200], Batch [74/123], Loss: 3.4827\n",
            "Epoch [3/200], Batch [75/123], Loss: 5.2830\n",
            "Epoch [3/200], Batch [76/123], Loss: 4.3198\n",
            "Epoch [3/200], Batch [77/123], Loss: 4.9413\n",
            "Epoch [3/200], Batch [78/123], Loss: 2.7960\n",
            "Epoch [3/200], Batch [79/123], Loss: 6.2466\n",
            "Epoch [3/200], Batch [80/123], Loss: 2.7524\n",
            "Epoch [3/200], Batch [81/123], Loss: 3.8666\n",
            "Epoch [3/200], Batch [82/123], Loss: 5.7619\n",
            "Epoch [3/200], Batch [83/123], Loss: 5.3674\n",
            "Epoch [3/200], Batch [84/123], Loss: 4.4903\n",
            "Epoch [3/200], Batch [85/123], Loss: 5.2961\n",
            "Epoch [3/200], Batch [86/123], Loss: 3.5265\n",
            "Epoch [3/200], Batch [87/123], Loss: 3.5155\n",
            "Epoch [3/200], Batch [88/123], Loss: 2.3762\n",
            "Epoch [3/200], Batch [89/123], Loss: 4.9517\n",
            "Epoch [3/200], Batch [90/123], Loss: 5.0988\n",
            "Epoch [3/200], Batch [91/123], Loss: 5.0753\n",
            "Epoch [3/200], Batch [92/123], Loss: 3.7029\n",
            "Epoch [3/200], Batch [93/123], Loss: 4.8217\n",
            "Epoch [3/200], Batch [94/123], Loss: 2.8094\n",
            "Epoch [3/200], Batch [95/123], Loss: 4.1530\n",
            "Epoch [3/200], Batch [96/123], Loss: 3.4427\n",
            "Epoch [3/200], Batch [97/123], Loss: 4.4580\n",
            "Epoch [3/200], Batch [98/123], Loss: 4.0815\n",
            "Epoch [3/200], Batch [99/123], Loss: 4.6891\n",
            "Epoch [3/200], Batch [100/123], Loss: 3.3123\n",
            "Epoch [3/200], Batch [101/123], Loss: 3.4521\n",
            "Epoch [3/200], Batch [102/123], Loss: 3.8830\n",
            "Epoch [3/200], Batch [103/123], Loss: 3.7493\n",
            "Epoch [3/200], Batch [104/123], Loss: 9.4224\n",
            "Epoch [3/200], Batch [105/123], Loss: 6.7441\n",
            "Epoch [3/200], Batch [106/123], Loss: 4.3894\n",
            "Epoch [3/200], Batch [107/123], Loss: 4.7369\n",
            "Epoch [3/200], Batch [108/123], Loss: 2.7411\n",
            "Epoch [3/200], Batch [109/123], Loss: 4.4632\n",
            "Epoch [3/200], Batch [110/123], Loss: 3.2266\n",
            "Epoch [3/200], Batch [111/123], Loss: 4.0977\n",
            "Epoch [3/200], Batch [112/123], Loss: 3.8221\n",
            "Epoch [3/200], Batch [113/123], Loss: 3.4591\n",
            "Epoch [3/200], Batch [114/123], Loss: 3.9949\n",
            "Epoch [3/200], Batch [115/123], Loss: 2.8767\n",
            "Epoch [3/200], Batch [116/123], Loss: 5.6774\n",
            "Epoch [3/200], Batch [117/123], Loss: 3.7901\n",
            "Epoch [3/200], Batch [118/123], Loss: 3.9628\n",
            "Epoch [3/200], Batch [119/123], Loss: 3.2307\n",
            "Epoch [3/200], Batch [120/123], Loss: 3.6876\n",
            "Epoch [3/200], Batch [121/123], Loss: 4.7644\n",
            "Epoch [3/200], Batch [122/123], Loss: 3.6368\n",
            "Epoch [3/200], Batch [123/123], Loss: 3.1445\n",
            "Epoch [4/200], Batch [1/123], Loss: 3.2197\n",
            "Epoch [4/200], Batch [2/123], Loss: 3.9182\n",
            "Epoch [4/200], Batch [3/123], Loss: 3.4269\n",
            "Epoch [4/200], Batch [4/123], Loss: 3.4730\n",
            "Epoch [4/200], Batch [5/123], Loss: 3.9562\n",
            "Epoch [4/200], Batch [6/123], Loss: 3.6788\n",
            "Epoch [4/200], Batch [7/123], Loss: 3.1794\n",
            "Epoch [4/200], Batch [8/123], Loss: 3.5957\n",
            "Epoch [4/200], Batch [9/123], Loss: 4.1892\n",
            "Epoch [4/200], Batch [10/123], Loss: 4.7475\n",
            "Epoch [4/200], Batch [11/123], Loss: 4.7089\n",
            "Epoch [4/200], Batch [12/123], Loss: 3.0416\n",
            "Epoch [4/200], Batch [13/123], Loss: 3.6752\n",
            "Epoch [4/200], Batch [14/123], Loss: 4.3165\n",
            "Epoch [4/200], Batch [15/123], Loss: 4.1476\n",
            "Epoch [4/200], Batch [16/123], Loss: 4.0510\n",
            "Epoch [4/200], Batch [17/123], Loss: 3.7994\n",
            "Epoch [4/200], Batch [18/123], Loss: 3.3705\n",
            "Epoch [4/200], Batch [19/123], Loss: 5.3767\n",
            "Epoch [4/200], Batch [20/123], Loss: 4.1341\n",
            "Epoch [4/200], Batch [21/123], Loss: 3.1280\n",
            "Epoch [4/200], Batch [22/123], Loss: 5.4950\n",
            "Epoch [4/200], Batch [23/123], Loss: 4.9956\n",
            "Epoch [4/200], Batch [24/123], Loss: 3.2660\n",
            "Epoch [4/200], Batch [25/123], Loss: 3.4664\n",
            "Epoch [4/200], Batch [26/123], Loss: 4.5531\n",
            "Epoch [4/200], Batch [27/123], Loss: 3.6712\n",
            "Epoch [4/200], Batch [28/123], Loss: 4.2005\n",
            "Epoch [4/200], Batch [29/123], Loss: 3.6863\n",
            "Epoch [4/200], Batch [30/123], Loss: 6.7164\n",
            "Epoch [4/200], Batch [31/123], Loss: 3.8890\n",
            "Epoch [4/200], Batch [32/123], Loss: 3.5861\n",
            "Epoch [4/200], Batch [33/123], Loss: 3.4600\n",
            "Epoch [4/200], Batch [34/123], Loss: 3.6926\n",
            "Epoch [4/200], Batch [35/123], Loss: 4.4538\n",
            "Epoch [4/200], Batch [36/123], Loss: 3.6496\n",
            "Epoch [4/200], Batch [37/123], Loss: 4.7596\n",
            "Epoch [4/200], Batch [38/123], Loss: 2.9844\n",
            "Epoch [4/200], Batch [39/123], Loss: 3.0593\n",
            "Epoch [4/200], Batch [40/123], Loss: 4.3417\n",
            "Epoch [4/200], Batch [41/123], Loss: 3.4962\n",
            "Epoch [4/200], Batch [42/123], Loss: 2.7413\n",
            "Epoch [4/200], Batch [43/123], Loss: 3.4557\n",
            "Epoch [4/200], Batch [44/123], Loss: 3.1590\n",
            "Epoch [4/200], Batch [45/123], Loss: 3.6333\n",
            "Epoch [4/200], Batch [46/123], Loss: 2.0929\n",
            "Epoch [4/200], Batch [47/123], Loss: 3.0908\n",
            "Epoch [4/200], Batch [48/123], Loss: 4.8050\n",
            "Epoch [4/200], Batch [49/123], Loss: 3.8920\n",
            "Epoch [4/200], Batch [50/123], Loss: 3.2107\n",
            "Epoch [4/200], Batch [51/123], Loss: 3.3089\n",
            "Epoch [4/200], Batch [52/123], Loss: 5.0605\n",
            "Epoch [4/200], Batch [53/123], Loss: 3.3786\n",
            "Epoch [4/200], Batch [54/123], Loss: 3.1608\n",
            "Epoch [4/200], Batch [55/123], Loss: 4.6472\n",
            "Epoch [4/200], Batch [56/123], Loss: 2.9600\n",
            "Epoch [4/200], Batch [57/123], Loss: 3.9523\n",
            "Epoch [4/200], Batch [58/123], Loss: 3.7909\n",
            "Epoch [4/200], Batch [59/123], Loss: 3.7196\n",
            "Epoch [4/200], Batch [60/123], Loss: 3.1696\n",
            "Epoch [4/200], Batch [61/123], Loss: 3.8296\n",
            "Epoch [4/200], Batch [62/123], Loss: 3.6485\n",
            "Epoch [4/200], Batch [63/123], Loss: 3.2576\n",
            "Epoch [4/200], Batch [64/123], Loss: 5.6282\n",
            "Epoch [4/200], Batch [65/123], Loss: 3.8844\n",
            "Epoch [4/200], Batch [66/123], Loss: 3.4440\n",
            "Epoch [4/200], Batch [67/123], Loss: 3.9879\n",
            "Epoch [4/200], Batch [68/123], Loss: 3.3266\n",
            "Epoch [4/200], Batch [69/123], Loss: 3.7888\n",
            "Epoch [4/200], Batch [70/123], Loss: 4.7545\n",
            "Epoch [4/200], Batch [71/123], Loss: 3.8673\n",
            "Epoch [4/200], Batch [72/123], Loss: 3.4444\n",
            "Epoch [4/200], Batch [73/123], Loss: 3.9987\n",
            "Epoch [4/200], Batch [74/123], Loss: 3.7905\n",
            "Epoch [4/200], Batch [75/123], Loss: 4.6201\n",
            "Epoch [4/200], Batch [76/123], Loss: 4.7359\n",
            "Epoch [4/200], Batch [77/123], Loss: 3.4416\n",
            "Epoch [4/200], Batch [78/123], Loss: 4.6965\n",
            "Epoch [4/200], Batch [79/123], Loss: 3.1100\n",
            "Epoch [4/200], Batch [80/123], Loss: 4.6386\n",
            "Epoch [4/200], Batch [81/123], Loss: 4.4330\n",
            "Epoch [4/200], Batch [82/123], Loss: 3.0884\n",
            "Epoch [4/200], Batch [83/123], Loss: 3.3011\n",
            "Epoch [4/200], Batch [84/123], Loss: 4.7743\n",
            "Epoch [4/200], Batch [85/123], Loss: 4.5240\n",
            "Epoch [4/200], Batch [86/123], Loss: 2.2990\n",
            "Epoch [4/200], Batch [87/123], Loss: 4.1327\n",
            "Epoch [4/200], Batch [88/123], Loss: 2.8562\n",
            "Epoch [4/200], Batch [89/123], Loss: 4.0627\n",
            "Epoch [4/200], Batch [90/123], Loss: 3.6632\n",
            "Epoch [4/200], Batch [91/123], Loss: 4.0444\n",
            "Epoch [4/200], Batch [92/123], Loss: 4.6476\n",
            "Epoch [4/200], Batch [93/123], Loss: 3.3303\n",
            "Epoch [4/200], Batch [94/123], Loss: 2.7316\n",
            "Epoch [4/200], Batch [95/123], Loss: 2.6719\n",
            "Epoch [4/200], Batch [96/123], Loss: 3.6077\n",
            "Epoch [4/200], Batch [97/123], Loss: 3.6855\n",
            "Epoch [4/200], Batch [98/123], Loss: 3.4322\n",
            "Epoch [4/200], Batch [99/123], Loss: 3.1941\n",
            "Epoch [4/200], Batch [100/123], Loss: 4.3418\n",
            "Epoch [4/200], Batch [101/123], Loss: 3.0317\n",
            "Epoch [4/200], Batch [102/123], Loss: 3.7602\n",
            "Epoch [4/200], Batch [103/123], Loss: 3.3138\n",
            "Epoch [4/200], Batch [104/123], Loss: 4.3671\n",
            "Epoch [4/200], Batch [105/123], Loss: 4.3410\n",
            "Epoch [4/200], Batch [106/123], Loss: 5.7318\n",
            "Epoch [4/200], Batch [107/123], Loss: 3.6422\n",
            "Epoch [4/200], Batch [108/123], Loss: 3.4238\n",
            "Epoch [4/200], Batch [109/123], Loss: 3.4924\n",
            "Epoch [4/200], Batch [110/123], Loss: 3.5571\n",
            "Epoch [4/200], Batch [111/123], Loss: 3.0374\n",
            "Epoch [4/200], Batch [112/123], Loss: 4.0075\n",
            "Epoch [4/200], Batch [113/123], Loss: 4.4520\n",
            "Epoch [4/200], Batch [114/123], Loss: 3.7679\n",
            "Epoch [4/200], Batch [115/123], Loss: 3.1392\n",
            "Epoch [4/200], Batch [116/123], Loss: 4.8879\n",
            "Epoch [4/200], Batch [117/123], Loss: 4.3151\n",
            "Epoch [4/200], Batch [118/123], Loss: 2.5937\n",
            "Epoch [4/200], Batch [119/123], Loss: 3.2444\n",
            "Epoch [4/200], Batch [120/123], Loss: 3.2913\n",
            "Epoch [4/200], Batch [121/123], Loss: 3.5033\n",
            "Epoch [4/200], Batch [122/123], Loss: 3.8106\n",
            "Epoch [4/200], Batch [123/123], Loss: 4.7669\n",
            "Epoch [5/200], Batch [1/123], Loss: 3.0536\n",
            "Epoch [5/200], Batch [2/123], Loss: 3.8283\n",
            "Epoch [5/200], Batch [3/123], Loss: 4.3685\n",
            "Epoch [5/200], Batch [4/123], Loss: 2.5453\n",
            "Epoch [5/200], Batch [5/123], Loss: 3.1248\n",
            "Epoch [5/200], Batch [6/123], Loss: 4.7485\n",
            "Epoch [5/200], Batch [7/123], Loss: 3.4083\n",
            "Epoch [5/200], Batch [8/123], Loss: 4.5474\n",
            "Epoch [5/200], Batch [9/123], Loss: 3.7478\n",
            "Epoch [5/200], Batch [10/123], Loss: 3.5250\n",
            "Epoch [5/200], Batch [11/123], Loss: 4.5412\n",
            "Epoch [5/200], Batch [12/123], Loss: 3.7115\n",
            "Epoch [5/200], Batch [13/123], Loss: 3.6062\n",
            "Epoch [5/200], Batch [14/123], Loss: 4.7204\n",
            "Epoch [5/200], Batch [15/123], Loss: 2.8296\n",
            "Epoch [5/200], Batch [16/123], Loss: 2.1456\n",
            "Epoch [5/200], Batch [17/123], Loss: 2.9380\n",
            "Epoch [5/200], Batch [18/123], Loss: 2.6736\n",
            "Epoch [5/200], Batch [19/123], Loss: 3.9067\n",
            "Epoch [5/200], Batch [20/123], Loss: 3.6439\n",
            "Epoch [5/200], Batch [21/123], Loss: 3.5787\n",
            "Epoch [5/200], Batch [22/123], Loss: 3.9468\n",
            "Epoch [5/200], Batch [23/123], Loss: 2.9913\n",
            "Epoch [5/200], Batch [24/123], Loss: 4.1665\n",
            "Epoch [5/200], Batch [25/123], Loss: 3.1327\n",
            "Epoch [5/200], Batch [26/123], Loss: 3.7128\n",
            "Epoch [5/200], Batch [27/123], Loss: 4.2782\n",
            "Epoch [5/200], Batch [28/123], Loss: 3.7140\n",
            "Epoch [5/200], Batch [29/123], Loss: 3.4816\n",
            "Epoch [5/200], Batch [30/123], Loss: 3.3910\n",
            "Epoch [5/200], Batch [31/123], Loss: 4.3439\n",
            "Epoch [5/200], Batch [32/123], Loss: 3.4761\n",
            "Epoch [5/200], Batch [33/123], Loss: 2.4835\n",
            "Epoch [5/200], Batch [34/123], Loss: 3.0535\n",
            "Epoch [5/200], Batch [35/123], Loss: 3.9655\n",
            "Epoch [5/200], Batch [36/123], Loss: 2.9989\n",
            "Epoch [5/200], Batch [37/123], Loss: 3.3472\n",
            "Epoch [5/200], Batch [38/123], Loss: 4.0706\n",
            "Epoch [5/200], Batch [39/123], Loss: 3.1813\n",
            "Epoch [5/200], Batch [40/123], Loss: 3.2935\n",
            "Epoch [5/200], Batch [41/123], Loss: 4.3068\n",
            "Epoch [5/200], Batch [42/123], Loss: 4.7756\n",
            "Epoch [5/200], Batch [43/123], Loss: 3.9294\n",
            "Epoch [5/200], Batch [44/123], Loss: 3.8449\n",
            "Epoch [5/200], Batch [45/123], Loss: 2.4779\n",
            "Epoch [5/200], Batch [46/123], Loss: 4.1243\n",
            "Epoch [5/200], Batch [47/123], Loss: 3.5006\n",
            "Epoch [5/200], Batch [48/123], Loss: 4.3789\n",
            "Epoch [5/200], Batch [49/123], Loss: 3.4274\n",
            "Epoch [5/200], Batch [50/123], Loss: 3.3915\n",
            "Epoch [5/200], Batch [51/123], Loss: 2.7227\n",
            "Epoch [5/200], Batch [52/123], Loss: 2.6697\n",
            "Epoch [5/200], Batch [53/123], Loss: 3.0908\n",
            "Epoch [5/200], Batch [54/123], Loss: 2.9835\n",
            "Epoch [5/200], Batch [55/123], Loss: 4.3203\n",
            "Epoch [5/200], Batch [56/123], Loss: 2.6906\n",
            "Epoch [5/200], Batch [57/123], Loss: 2.8127\n",
            "Epoch [5/200], Batch [58/123], Loss: 3.9132\n",
            "Epoch [5/200], Batch [59/123], Loss: 3.7538\n",
            "Epoch [5/200], Batch [60/123], Loss: 3.6420\n",
            "Epoch [5/200], Batch [61/123], Loss: 2.8275\n",
            "Epoch [5/200], Batch [62/123], Loss: 3.0155\n",
            "Epoch [5/200], Batch [63/123], Loss: 3.8803\n",
            "Epoch [5/200], Batch [64/123], Loss: 4.5314\n",
            "Epoch [5/200], Batch [65/123], Loss: 4.6458\n",
            "Epoch [5/200], Batch [66/123], Loss: 2.1074\n",
            "Epoch [5/200], Batch [67/123], Loss: 3.9004\n",
            "Epoch [5/200], Batch [68/123], Loss: 3.7688\n",
            "Epoch [5/200], Batch [69/123], Loss: 3.8799\n",
            "Epoch [5/200], Batch [70/123], Loss: 4.6091\n",
            "Epoch [5/200], Batch [71/123], Loss: 3.7399\n",
            "Epoch [5/200], Batch [72/123], Loss: 3.6821\n",
            "Epoch [5/200], Batch [73/123], Loss: 2.2441\n",
            "Epoch [5/200], Batch [74/123], Loss: 2.6144\n",
            "Epoch [5/200], Batch [75/123], Loss: 5.8558\n",
            "Epoch [5/200], Batch [76/123], Loss: 4.8633\n",
            "Epoch [5/200], Batch [77/123], Loss: 3.8948\n",
            "Epoch [5/200], Batch [78/123], Loss: 2.7628\n",
            "Epoch [5/200], Batch [79/123], Loss: 5.7052\n",
            "Epoch [5/200], Batch [80/123], Loss: 3.0319\n",
            "Epoch [5/200], Batch [81/123], Loss: 2.9560\n",
            "Epoch [5/200], Batch [82/123], Loss: 3.0945\n",
            "Epoch [5/200], Batch [83/123], Loss: 3.1881\n",
            "Epoch [5/200], Batch [84/123], Loss: 3.0632\n",
            "Epoch [5/200], Batch [85/123], Loss: 4.0269\n",
            "Epoch [5/200], Batch [86/123], Loss: 2.2355\n",
            "Epoch [5/200], Batch [87/123], Loss: 2.1906\n",
            "Epoch [5/200], Batch [88/123], Loss: 5.1309\n",
            "Epoch [5/200], Batch [89/123], Loss: 4.4797\n",
            "Epoch [5/200], Batch [90/123], Loss: 3.2719\n",
            "Epoch [5/200], Batch [91/123], Loss: 4.0601\n",
            "Epoch [5/200], Batch [92/123], Loss: 2.5618\n",
            "Epoch [5/200], Batch [93/123], Loss: 2.4124\n",
            "Epoch [5/200], Batch [94/123], Loss: 3.9843\n",
            "Epoch [5/200], Batch [95/123], Loss: 3.3193\n",
            "Epoch [5/200], Batch [96/123], Loss: 4.2494\n",
            "Epoch [5/200], Batch [97/123], Loss: 4.6078\n",
            "Epoch [5/200], Batch [98/123], Loss: 2.3701\n",
            "Epoch [5/200], Batch [99/123], Loss: 2.3172\n",
            "Epoch [5/200], Batch [100/123], Loss: 2.0626\n",
            "Epoch [5/200], Batch [101/123], Loss: 4.8306\n",
            "Epoch [5/200], Batch [102/123], Loss: 2.3362\n",
            "Epoch [5/200], Batch [103/123], Loss: 3.2555\n",
            "Epoch [5/200], Batch [104/123], Loss: 4.5462\n",
            "Epoch [5/200], Batch [105/123], Loss: 3.8942\n",
            "Epoch [5/200], Batch [106/123], Loss: 3.5532\n",
            "Epoch [5/200], Batch [107/123], Loss: 2.7396\n",
            "Epoch [5/200], Batch [108/123], Loss: 3.5029\n",
            "Epoch [5/200], Batch [109/123], Loss: 2.8296\n",
            "Epoch [5/200], Batch [110/123], Loss: 3.5572\n",
            "Epoch [5/200], Batch [111/123], Loss: 2.5529\n",
            "Epoch [5/200], Batch [112/123], Loss: 3.5266\n",
            "Epoch [5/200], Batch [113/123], Loss: 4.1690\n",
            "Epoch [5/200], Batch [114/123], Loss: 3.0515\n",
            "Epoch [5/200], Batch [115/123], Loss: 3.4516\n",
            "Epoch [5/200], Batch [116/123], Loss: 4.2801\n",
            "Epoch [5/200], Batch [117/123], Loss: 2.8563\n",
            "Epoch [5/200], Batch [118/123], Loss: 4.2391\n",
            "Epoch [5/200], Batch [119/123], Loss: 3.9291\n",
            "Epoch [5/200], Batch [120/123], Loss: 3.4551\n",
            "Epoch [5/200], Batch [121/123], Loss: 3.4796\n",
            "Epoch [5/200], Batch [122/123], Loss: 2.9836\n",
            "Epoch [5/200], Batch [123/123], Loss: 2.4897\n",
            "Epoch [6/200], Batch [1/123], Loss: 3.8536\n",
            "Epoch [6/200], Batch [2/123], Loss: 3.6440\n",
            "Epoch [6/200], Batch [3/123], Loss: 3.4816\n",
            "Epoch [6/200], Batch [4/123], Loss: 2.3554\n",
            "Epoch [6/200], Batch [5/123], Loss: 3.6438\n",
            "Epoch [6/200], Batch [6/123], Loss: 3.6708\n",
            "Epoch [6/200], Batch [7/123], Loss: 2.3591\n",
            "Epoch [6/200], Batch [8/123], Loss: 3.0758\n",
            "Epoch [6/200], Batch [9/123], Loss: 2.7657\n",
            "Epoch [6/200], Batch [10/123], Loss: 2.6743\n",
            "Epoch [6/200], Batch [11/123], Loss: 3.1608\n",
            "Epoch [6/200], Batch [12/123], Loss: 1.9233\n",
            "Epoch [6/200], Batch [13/123], Loss: 2.8639\n",
            "Epoch [6/200], Batch [14/123], Loss: 2.5885\n",
            "Epoch [6/200], Batch [15/123], Loss: 2.5560\n",
            "Epoch [6/200], Batch [16/123], Loss: 2.3166\n",
            "Epoch [6/200], Batch [17/123], Loss: 3.1378\n",
            "Epoch [6/200], Batch [18/123], Loss: 5.1489\n",
            "Epoch [6/200], Batch [19/123], Loss: 4.1297\n",
            "Epoch [6/200], Batch [20/123], Loss: 2.0386\n",
            "Epoch [6/200], Batch [21/123], Loss: 3.0678\n",
            "Epoch [6/200], Batch [22/123], Loss: 2.0987\n",
            "Epoch [6/200], Batch [23/123], Loss: 2.7278\n",
            "Epoch [6/200], Batch [24/123], Loss: 3.0259\n",
            "Epoch [6/200], Batch [25/123], Loss: 2.2334\n",
            "Epoch [6/200], Batch [26/123], Loss: 3.3755\n",
            "Epoch [6/200], Batch [27/123], Loss: 3.5520\n",
            "Epoch [6/200], Batch [28/123], Loss: 2.0313\n",
            "Epoch [6/200], Batch [29/123], Loss: 4.2189\n",
            "Epoch [6/200], Batch [30/123], Loss: 2.8692\n",
            "Epoch [6/200], Batch [31/123], Loss: 4.1155\n",
            "Epoch [6/200], Batch [32/123], Loss: 2.8981\n",
            "Epoch [6/200], Batch [33/123], Loss: 2.8872\n",
            "Epoch [6/200], Batch [34/123], Loss: 2.4648\n",
            "Epoch [6/200], Batch [35/123], Loss: 4.2066\n",
            "Epoch [6/200], Batch [36/123], Loss: 3.5550\n",
            "Epoch [6/200], Batch [37/123], Loss: 2.1267\n",
            "Epoch [6/200], Batch [38/123], Loss: 2.4569\n",
            "Epoch [6/200], Batch [39/123], Loss: 2.8327\n",
            "Epoch [6/200], Batch [40/123], Loss: 3.8470\n",
            "Epoch [6/200], Batch [41/123], Loss: 3.9928\n",
            "Epoch [6/200], Batch [42/123], Loss: 2.3470\n",
            "Epoch [6/200], Batch [43/123], Loss: 3.4485\n",
            "Epoch [6/200], Batch [44/123], Loss: 4.1651\n",
            "Epoch [6/200], Batch [45/123], Loss: 2.8118\n",
            "Epoch [6/200], Batch [46/123], Loss: 3.6045\n",
            "Epoch [6/200], Batch [47/123], Loss: 2.3122\n",
            "Epoch [6/200], Batch [48/123], Loss: 2.7789\n",
            "Epoch [6/200], Batch [49/123], Loss: 3.2325\n",
            "Epoch [6/200], Batch [50/123], Loss: 4.7366\n",
            "Epoch [6/200], Batch [51/123], Loss: 1.9867\n",
            "Epoch [6/200], Batch [52/123], Loss: 3.7876\n",
            "Epoch [6/200], Batch [53/123], Loss: 3.2978\n",
            "Epoch [6/200], Batch [54/123], Loss: 3.6709\n",
            "Epoch [6/200], Batch [55/123], Loss: 4.1302\n",
            "Epoch [6/200], Batch [56/123], Loss: 3.7068\n",
            "Epoch [6/200], Batch [57/123], Loss: 3.7523\n",
            "Epoch [6/200], Batch [58/123], Loss: 2.3287\n",
            "Epoch [6/200], Batch [59/123], Loss: 3.9415\n",
            "Epoch [6/200], Batch [60/123], Loss: 2.9214\n",
            "Epoch [6/200], Batch [61/123], Loss: 3.8432\n",
            "Epoch [6/200], Batch [62/123], Loss: 3.3553\n",
            "Epoch [6/200], Batch [63/123], Loss: 3.0684\n",
            "Epoch [6/200], Batch [64/123], Loss: 3.4543\n",
            "Epoch [6/200], Batch [65/123], Loss: 2.4588\n",
            "Epoch [6/200], Batch [66/123], Loss: 4.7135\n",
            "Epoch [6/200], Batch [67/123], Loss: 5.1161\n",
            "Epoch [6/200], Batch [68/123], Loss: 2.1787\n",
            "Epoch [6/200], Batch [69/123], Loss: 3.2178\n",
            "Epoch [6/200], Batch [70/123], Loss: 2.9087\n",
            "Epoch [6/200], Batch [71/123], Loss: 2.6680\n",
            "Epoch [6/200], Batch [72/123], Loss: 3.8255\n",
            "Epoch [6/200], Batch [73/123], Loss: 3.5985\n",
            "Epoch [6/200], Batch [74/123], Loss: 3.7247\n",
            "Epoch [6/200], Batch [75/123], Loss: 2.8383\n",
            "Epoch [6/200], Batch [76/123], Loss: 3.5299\n",
            "Epoch [6/200], Batch [77/123], Loss: 6.9037\n",
            "Epoch [6/200], Batch [78/123], Loss: 2.7278\n",
            "Epoch [6/200], Batch [79/123], Loss: 2.0931\n",
            "Epoch [6/200], Batch [80/123], Loss: 3.8152\n",
            "Epoch [6/200], Batch [81/123], Loss: 4.3743\n",
            "Epoch [6/200], Batch [82/123], Loss: 2.4748\n",
            "Epoch [6/200], Batch [83/123], Loss: 2.4583\n",
            "Epoch [6/200], Batch [84/123], Loss: 2.0650\n",
            "Epoch [6/200], Batch [85/123], Loss: 3.8312\n",
            "Epoch [6/200], Batch [86/123], Loss: 3.7248\n",
            "Epoch [6/200], Batch [87/123], Loss: 2.2991\n",
            "Epoch [6/200], Batch [88/123], Loss: 2.4372\n",
            "Epoch [6/200], Batch [89/123], Loss: 2.6881\n",
            "Epoch [6/200], Batch [90/123], Loss: 3.1713\n",
            "Epoch [6/200], Batch [91/123], Loss: 2.2478\n",
            "Epoch [6/200], Batch [92/123], Loss: 2.2025\n",
            "Epoch [6/200], Batch [93/123], Loss: 2.4318\n",
            "Epoch [6/200], Batch [94/123], Loss: 2.7400\n",
            "Epoch [6/200], Batch [95/123], Loss: 4.4124\n",
            "Epoch [6/200], Batch [96/123], Loss: 2.7005\n",
            "Epoch [6/200], Batch [97/123], Loss: 1.7429\n",
            "Epoch [6/200], Batch [98/123], Loss: 2.3719\n",
            "Epoch [6/200], Batch [99/123], Loss: 2.5333\n",
            "Epoch [6/200], Batch [100/123], Loss: 3.6629\n",
            "Epoch [6/200], Batch [101/123], Loss: 2.7953\n",
            "Epoch [6/200], Batch [102/123], Loss: 4.2095\n",
            "Epoch [6/200], Batch [103/123], Loss: 3.5391\n",
            "Epoch [6/200], Batch [104/123], Loss: 2.9936\n",
            "Epoch [6/200], Batch [105/123], Loss: 4.3979\n",
            "Epoch [6/200], Batch [106/123], Loss: 3.9108\n",
            "Epoch [6/200], Batch [107/123], Loss: 2.7526\n",
            "Epoch [6/200], Batch [108/123], Loss: 2.9707\n",
            "Epoch [6/200], Batch [109/123], Loss: 3.3262\n",
            "Epoch [6/200], Batch [110/123], Loss: 2.9258\n",
            "Epoch [6/200], Batch [111/123], Loss: 4.7771\n",
            "Epoch [6/200], Batch [112/123], Loss: 3.8987\n",
            "Epoch [6/200], Batch [113/123], Loss: 2.6608\n",
            "Epoch [6/200], Batch [114/123], Loss: 3.3586\n",
            "Epoch [6/200], Batch [115/123], Loss: 2.8750\n",
            "Epoch [6/200], Batch [116/123], Loss: 3.7676\n",
            "Epoch [6/200], Batch [117/123], Loss: 3.0160\n",
            "Epoch [6/200], Batch [118/123], Loss: 2.7865\n",
            "Epoch [6/200], Batch [119/123], Loss: 3.2108\n",
            "Epoch [6/200], Batch [120/123], Loss: 3.2307\n",
            "Epoch [6/200], Batch [121/123], Loss: 4.1505\n",
            "Epoch [6/200], Batch [122/123], Loss: 3.3468\n",
            "Epoch [6/200], Batch [123/123], Loss: 1.9956\n",
            "Epoch [7/200], Batch [1/123], Loss: 3.6284\n",
            "Epoch [7/200], Batch [2/123], Loss: 3.6457\n",
            "Epoch [7/200], Batch [3/123], Loss: 3.1069\n",
            "Epoch [7/200], Batch [4/123], Loss: 3.7405\n",
            "Epoch [7/200], Batch [5/123], Loss: 2.3992\n",
            "Epoch [7/200], Batch [6/123], Loss: 3.3011\n",
            "Epoch [7/200], Batch [7/123], Loss: 3.7907\n",
            "Epoch [7/200], Batch [8/123], Loss: 2.7287\n",
            "Epoch [7/200], Batch [9/123], Loss: 2.7914\n",
            "Epoch [7/200], Batch [10/123], Loss: 3.3220\n",
            "Epoch [7/200], Batch [11/123], Loss: 2.7048\n",
            "Epoch [7/200], Batch [12/123], Loss: 3.2438\n",
            "Epoch [7/200], Batch [13/123], Loss: 3.2096\n",
            "Epoch [7/200], Batch [14/123], Loss: 2.3744\n",
            "Epoch [7/200], Batch [15/123], Loss: 3.2066\n",
            "Epoch [7/200], Batch [16/123], Loss: 3.0594\n",
            "Epoch [7/200], Batch [17/123], Loss: 2.4456\n",
            "Epoch [7/200], Batch [18/123], Loss: 2.6891\n",
            "Epoch [7/200], Batch [19/123], Loss: 2.7061\n",
            "Epoch [7/200], Batch [20/123], Loss: 3.3933\n",
            "Epoch [7/200], Batch [21/123], Loss: 2.8494\n",
            "Epoch [7/200], Batch [22/123], Loss: 2.0743\n",
            "Epoch [7/200], Batch [23/123], Loss: 2.8871\n",
            "Epoch [7/200], Batch [24/123], Loss: 2.6108\n",
            "Epoch [7/200], Batch [25/123], Loss: 2.5712\n",
            "Epoch [7/200], Batch [26/123], Loss: 2.9634\n",
            "Epoch [7/200], Batch [27/123], Loss: 2.7463\n",
            "Epoch [7/200], Batch [28/123], Loss: 2.5700\n",
            "Epoch [7/200], Batch [29/123], Loss: 3.3532\n",
            "Epoch [7/200], Batch [30/123], Loss: 3.6219\n",
            "Epoch [7/200], Batch [31/123], Loss: 3.2574\n",
            "Epoch [7/200], Batch [32/123], Loss: 3.6737\n",
            "Epoch [7/200], Batch [33/123], Loss: 3.5312\n",
            "Epoch [7/200], Batch [34/123], Loss: 2.8307\n",
            "Epoch [7/200], Batch [35/123], Loss: 3.0195\n",
            "Epoch [7/200], Batch [36/123], Loss: 2.6951\n",
            "Epoch [7/200], Batch [37/123], Loss: 3.1569\n",
            "Epoch [7/200], Batch [38/123], Loss: 3.5207\n",
            "Epoch [7/200], Batch [39/123], Loss: 2.1447\n",
            "Epoch [7/200], Batch [40/123], Loss: 2.1171\n",
            "Epoch [7/200], Batch [41/123], Loss: 3.3004\n",
            "Epoch [7/200], Batch [42/123], Loss: 3.3803\n",
            "Epoch [7/200], Batch [43/123], Loss: 2.8783\n",
            "Epoch [7/200], Batch [44/123], Loss: 3.7529\n",
            "Epoch [7/200], Batch [45/123], Loss: 2.5314\n",
            "Epoch [7/200], Batch [46/123], Loss: 3.1747\n",
            "Epoch [7/200], Batch [47/123], Loss: 2.3131\n",
            "Epoch [7/200], Batch [48/123], Loss: 4.1726\n",
            "Epoch [7/200], Batch [49/123], Loss: 3.1943\n",
            "Epoch [7/200], Batch [50/123], Loss: 2.8387\n",
            "Epoch [7/200], Batch [51/123], Loss: 2.9784\n",
            "Epoch [7/200], Batch [52/123], Loss: 3.5124\n",
            "Epoch [7/200], Batch [53/123], Loss: 3.6916\n",
            "Epoch [7/200], Batch [54/123], Loss: 2.9057\n",
            "Epoch [7/200], Batch [55/123], Loss: 3.8458\n",
            "Epoch [7/200], Batch [56/123], Loss: 1.9807\n",
            "Epoch [7/200], Batch [57/123], Loss: 3.5481\n",
            "Epoch [7/200], Batch [58/123], Loss: 2.9056\n",
            "Epoch [7/200], Batch [59/123], Loss: 2.6081\n",
            "Epoch [7/200], Batch [60/123], Loss: 3.0782\n",
            "Epoch [7/200], Batch [61/123], Loss: 2.7286\n",
            "Epoch [7/200], Batch [62/123], Loss: 2.4749\n",
            "Epoch [7/200], Batch [63/123], Loss: 3.4396\n",
            "Epoch [7/200], Batch [64/123], Loss: 2.3761\n",
            "Epoch [7/200], Batch [65/123], Loss: 2.7753\n",
            "Epoch [7/200], Batch [66/123], Loss: 2.4103\n",
            "Epoch [7/200], Batch [67/123], Loss: 2.8494\n",
            "Epoch [7/200], Batch [68/123], Loss: 2.3633\n",
            "Epoch [7/200], Batch [69/123], Loss: 1.9938\n",
            "Epoch [7/200], Batch [70/123], Loss: 3.5747\n",
            "Epoch [7/200], Batch [71/123], Loss: 5.5642\n",
            "Epoch [7/200], Batch [72/123], Loss: 5.4201\n",
            "Epoch [7/200], Batch [73/123], Loss: 3.0060\n",
            "Epoch [7/200], Batch [74/123], Loss: 2.4546\n",
            "Epoch [7/200], Batch [75/123], Loss: 2.9941\n",
            "Epoch [7/200], Batch [76/123], Loss: 2.4759\n",
            "Epoch [7/200], Batch [77/123], Loss: 2.4047\n",
            "Epoch [7/200], Batch [78/123], Loss: 2.6892\n",
            "Epoch [7/200], Batch [79/123], Loss: 2.5301\n",
            "Epoch [7/200], Batch [80/123], Loss: 2.0340\n",
            "Epoch [7/200], Batch [81/123], Loss: 2.7841\n",
            "Epoch [7/200], Batch [82/123], Loss: 2.1776\n",
            "Epoch [7/200], Batch [83/123], Loss: 2.6614\n",
            "Epoch [7/200], Batch [84/123], Loss: 2.8772\n",
            "Epoch [7/200], Batch [85/123], Loss: 3.5565\n",
            "Epoch [7/200], Batch [86/123], Loss: 1.7318\n",
            "Epoch [7/200], Batch [87/123], Loss: 2.5558\n",
            "Epoch [7/200], Batch [88/123], Loss: 2.7316\n",
            "Epoch [7/200], Batch [89/123], Loss: 1.9573\n",
            "Epoch [7/200], Batch [90/123], Loss: 3.9370\n",
            "Epoch [7/200], Batch [91/123], Loss: 1.9891\n",
            "Epoch [7/200], Batch [92/123], Loss: 2.4079\n",
            "Epoch [7/200], Batch [93/123], Loss: 3.3431\n",
            "Epoch [7/200], Batch [94/123], Loss: 3.3927\n",
            "Epoch [7/200], Batch [95/123], Loss: 2.5333\n",
            "Epoch [7/200], Batch [96/123], Loss: 2.0889\n",
            "Epoch [7/200], Batch [97/123], Loss: 2.2365\n",
            "Epoch [7/200], Batch [98/123], Loss: 2.0398\n",
            "Epoch [7/200], Batch [99/123], Loss: 2.5960\n",
            "Epoch [7/200], Batch [100/123], Loss: 3.3916\n",
            "Epoch [7/200], Batch [101/123], Loss: 2.9041\n",
            "Epoch [7/200], Batch [102/123], Loss: 2.9872\n",
            "Epoch [7/200], Batch [103/123], Loss: 1.8163\n",
            "Epoch [7/200], Batch [104/123], Loss: 2.1941\n",
            "Epoch [7/200], Batch [105/123], Loss: 2.2584\n",
            "Epoch [7/200], Batch [106/123], Loss: 2.7741\n",
            "Epoch [7/200], Batch [107/123], Loss: 1.9562\n",
            "Epoch [7/200], Batch [108/123], Loss: 2.8770\n",
            "Epoch [7/200], Batch [109/123], Loss: 2.2726\n",
            "Epoch [7/200], Batch [110/123], Loss: 2.1228\n",
            "Epoch [7/200], Batch [111/123], Loss: 2.8904\n",
            "Epoch [7/200], Batch [112/123], Loss: 3.6262\n",
            "Epoch [7/200], Batch [113/123], Loss: 3.7040\n",
            "Epoch [7/200], Batch [114/123], Loss: 3.6296\n",
            "Epoch [7/200], Batch [115/123], Loss: 2.4526\n",
            "Epoch [7/200], Batch [116/123], Loss: 2.6067\n",
            "Epoch [7/200], Batch [117/123], Loss: 4.3442\n",
            "Epoch [7/200], Batch [118/123], Loss: 2.5622\n",
            "Epoch [7/200], Batch [119/123], Loss: 1.9887\n",
            "Epoch [7/200], Batch [120/123], Loss: 2.5465\n",
            "Epoch [7/200], Batch [121/123], Loss: 3.5357\n",
            "Epoch [7/200], Batch [122/123], Loss: 3.3071\n",
            "Epoch [7/200], Batch [123/123], Loss: 3.0710\n",
            "Epoch [8/200], Batch [1/123], Loss: 1.8408\n",
            "Epoch [8/200], Batch [2/123], Loss: 3.5875\n",
            "Epoch [8/200], Batch [3/123], Loss: 3.1513\n",
            "Epoch [8/200], Batch [4/123], Loss: 1.8275\n",
            "Epoch [8/200], Batch [5/123], Loss: 2.1008\n",
            "Epoch [8/200], Batch [6/123], Loss: 1.5619\n",
            "Epoch [8/200], Batch [7/123], Loss: 2.5847\n",
            "Epoch [8/200], Batch [8/123], Loss: 2.4855\n",
            "Epoch [8/200], Batch [9/123], Loss: 4.3386\n",
            "Epoch [8/200], Batch [10/123], Loss: 5.2597\n",
            "Epoch [8/200], Batch [11/123], Loss: 2.9119\n",
            "Epoch [8/200], Batch [12/123], Loss: 1.9083\n",
            "Epoch [8/200], Batch [13/123], Loss: 2.3971\n",
            "Epoch [8/200], Batch [14/123], Loss: 2.5383\n",
            "Epoch [8/200], Batch [15/123], Loss: 5.0333\n",
            "Epoch [8/200], Batch [16/123], Loss: 2.1685\n",
            "Epoch [8/200], Batch [17/123], Loss: 3.2081\n",
            "Epoch [8/200], Batch [18/123], Loss: 2.7454\n",
            "Epoch [8/200], Batch [19/123], Loss: 2.1597\n",
            "Epoch [8/200], Batch [20/123], Loss: 3.0417\n",
            "Epoch [8/200], Batch [21/123], Loss: 4.1078\n",
            "Epoch [8/200], Batch [22/123], Loss: 3.5646\n",
            "Epoch [8/200], Batch [23/123], Loss: 1.9916\n",
            "Epoch [8/200], Batch [24/123], Loss: 3.6879\n",
            "Epoch [8/200], Batch [25/123], Loss: 1.7226\n",
            "Epoch [8/200], Batch [26/123], Loss: 3.0907\n",
            "Epoch [8/200], Batch [27/123], Loss: 2.9981\n",
            "Epoch [8/200], Batch [28/123], Loss: 2.4097\n",
            "Epoch [8/200], Batch [29/123], Loss: 2.8952\n",
            "Epoch [8/200], Batch [30/123], Loss: 2.6546\n",
            "Epoch [8/200], Batch [31/123], Loss: 2.9536\n",
            "Epoch [8/200], Batch [32/123], Loss: 2.8765\n",
            "Epoch [8/200], Batch [33/123], Loss: 2.4239\n",
            "Epoch [8/200], Batch [34/123], Loss: 2.9661\n",
            "Epoch [8/200], Batch [35/123], Loss: 2.8993\n",
            "Epoch [8/200], Batch [36/123], Loss: 2.2083\n",
            "Epoch [8/200], Batch [37/123], Loss: 2.7296\n",
            "Epoch [8/200], Batch [38/123], Loss: 2.8640\n",
            "Epoch [8/200], Batch [39/123], Loss: 3.5202\n",
            "Epoch [8/200], Batch [40/123], Loss: 4.9185\n",
            "Epoch [8/200], Batch [41/123], Loss: 2.7628\n",
            "Epoch [8/200], Batch [42/123], Loss: 3.1191\n",
            "Epoch [8/200], Batch [43/123], Loss: 2.3186\n",
            "Epoch [8/200], Batch [44/123], Loss: 2.9191\n",
            "Epoch [8/200], Batch [45/123], Loss: 2.6114\n",
            "Epoch [8/200], Batch [46/123], Loss: 2.6397\n",
            "Epoch [8/200], Batch [47/123], Loss: 2.8250\n",
            "Epoch [8/200], Batch [48/123], Loss: 2.5871\n",
            "Epoch [8/200], Batch [49/123], Loss: 3.5976\n",
            "Epoch [8/200], Batch [50/123], Loss: 2.5146\n",
            "Epoch [8/200], Batch [51/123], Loss: 3.0971\n",
            "Epoch [8/200], Batch [52/123], Loss: 3.1516\n",
            "Epoch [8/200], Batch [53/123], Loss: 2.2783\n",
            "Epoch [8/200], Batch [54/123], Loss: 2.1201\n",
            "Epoch [8/200], Batch [55/123], Loss: 2.5703\n",
            "Epoch [8/200], Batch [56/123], Loss: 2.0521\n",
            "Epoch [8/200], Batch [57/123], Loss: 1.4459\n",
            "Epoch [8/200], Batch [58/123], Loss: 2.0414\n",
            "Epoch [8/200], Batch [59/123], Loss: 2.7728\n",
            "Epoch [8/200], Batch [60/123], Loss: 3.1047\n",
            "Epoch [8/200], Batch [61/123], Loss: 2.0551\n",
            "Epoch [8/200], Batch [62/123], Loss: 2.1427\n",
            "Epoch [8/200], Batch [63/123], Loss: 2.7447\n",
            "Epoch [8/200], Batch [64/123], Loss: 2.8549\n",
            "Epoch [8/200], Batch [65/123], Loss: 2.3182\n",
            "Epoch [8/200], Batch [66/123], Loss: 2.0055\n",
            "Epoch [8/200], Batch [67/123], Loss: 2.8057\n",
            "Epoch [8/200], Batch [68/123], Loss: 2.5882\n",
            "Epoch [8/200], Batch [69/123], Loss: 2.1347\n",
            "Epoch [8/200], Batch [70/123], Loss: 2.8506\n",
            "Epoch [8/200], Batch [71/123], Loss: 2.6428\n",
            "Epoch [8/200], Batch [72/123], Loss: 2.1491\n",
            "Epoch [8/200], Batch [73/123], Loss: 1.3944\n",
            "Epoch [8/200], Batch [74/123], Loss: 3.5532\n",
            "Epoch [8/200], Batch [75/123], Loss: 2.1991\n",
            "Epoch [8/200], Batch [76/123], Loss: 2.0705\n",
            "Epoch [8/200], Batch [77/123], Loss: 2.6682\n",
            "Epoch [8/200], Batch [78/123], Loss: 2.5907\n",
            "Epoch [8/200], Batch [79/123], Loss: 3.4587\n",
            "Epoch [8/200], Batch [80/123], Loss: 2.3101\n",
            "Epoch [8/200], Batch [81/123], Loss: 2.3066\n",
            "Epoch [8/200], Batch [82/123], Loss: 3.5314\n",
            "Epoch [8/200], Batch [83/123], Loss: 1.9119\n",
            "Epoch [8/200], Batch [84/123], Loss: 1.4920\n",
            "Epoch [8/200], Batch [85/123], Loss: 2.3450\n",
            "Epoch [8/200], Batch [86/123], Loss: 2.4801\n",
            "Epoch [8/200], Batch [87/123], Loss: 1.9957\n",
            "Epoch [8/200], Batch [88/123], Loss: 3.3173\n",
            "Epoch [8/200], Batch [89/123], Loss: 2.8631\n",
            "Epoch [8/200], Batch [90/123], Loss: 2.8972\n",
            "Epoch [8/200], Batch [91/123], Loss: 3.0705\n",
            "Epoch [8/200], Batch [92/123], Loss: 2.5199\n",
            "Epoch [8/200], Batch [93/123], Loss: 2.2618\n",
            "Epoch [8/200], Batch [94/123], Loss: 2.9668\n",
            "Epoch [8/200], Batch [95/123], Loss: 3.2277\n",
            "Epoch [8/200], Batch [96/123], Loss: 2.2680\n",
            "Epoch [8/200], Batch [97/123], Loss: 1.6078\n",
            "Epoch [8/200], Batch [98/123], Loss: 1.9300\n",
            "Epoch [8/200], Batch [99/123], Loss: 3.7579\n",
            "Epoch [8/200], Batch [100/123], Loss: 3.2939\n",
            "Epoch [8/200], Batch [101/123], Loss: 2.1530\n",
            "Epoch [8/200], Batch [102/123], Loss: 2.6864\n",
            "Epoch [8/200], Batch [103/123], Loss: 3.5900\n",
            "Epoch [8/200], Batch [104/123], Loss: 3.2490\n",
            "Epoch [8/200], Batch [105/123], Loss: 2.5578\n",
            "Epoch [8/200], Batch [106/123], Loss: 1.9793\n",
            "Epoch [8/200], Batch [107/123], Loss: 3.0401\n",
            "Epoch [8/200], Batch [108/123], Loss: 2.2303\n",
            "Epoch [8/200], Batch [109/123], Loss: 2.5930\n",
            "Epoch [8/200], Batch [110/123], Loss: 2.1469\n",
            "Epoch [8/200], Batch [111/123], Loss: 3.3783\n",
            "Epoch [8/200], Batch [112/123], Loss: 3.4620\n",
            "Epoch [8/200], Batch [113/123], Loss: 1.8808\n",
            "Epoch [8/200], Batch [114/123], Loss: 1.7393\n",
            "Epoch [8/200], Batch [115/123], Loss: 3.8910\n",
            "Epoch [8/200], Batch [116/123], Loss: 1.7769\n",
            "Epoch [8/200], Batch [117/123], Loss: 1.8961\n",
            "Epoch [8/200], Batch [118/123], Loss: 2.3783\n",
            "Epoch [8/200], Batch [119/123], Loss: 2.8734\n",
            "Epoch [8/200], Batch [120/123], Loss: 2.8414\n",
            "Epoch [8/200], Batch [121/123], Loss: 2.6328\n",
            "Epoch [8/200], Batch [122/123], Loss: 3.6724\n",
            "Epoch [8/200], Batch [123/123], Loss: 2.4092\n",
            "Epoch [9/200], Batch [1/123], Loss: 2.3898\n",
            "Epoch [9/200], Batch [2/123], Loss: 2.5536\n",
            "Epoch [9/200], Batch [3/123], Loss: 2.2667\n",
            "Epoch [9/200], Batch [4/123], Loss: 2.1715\n",
            "Epoch [9/200], Batch [5/123], Loss: 3.2556\n",
            "Epoch [9/200], Batch [6/123], Loss: 1.8042\n",
            "Epoch [9/200], Batch [7/123], Loss: 2.3297\n",
            "Epoch [9/200], Batch [8/123], Loss: 2.7958\n",
            "Epoch [9/200], Batch [9/123], Loss: 2.1521\n",
            "Epoch [9/200], Batch [10/123], Loss: 1.8582\n",
            "Epoch [9/200], Batch [11/123], Loss: 1.8922\n",
            "Epoch [9/200], Batch [12/123], Loss: 2.7918\n",
            "Epoch [9/200], Batch [13/123], Loss: 1.9529\n",
            "Epoch [9/200], Batch [14/123], Loss: 2.5600\n",
            "Epoch [9/200], Batch [15/123], Loss: 2.4302\n",
            "Epoch [9/200], Batch [16/123], Loss: 2.2344\n",
            "Epoch [9/200], Batch [17/123], Loss: 4.4159\n",
            "Epoch [9/200], Batch [18/123], Loss: 2.9690\n",
            "Epoch [9/200], Batch [19/123], Loss: 2.1235\n",
            "Epoch [9/200], Batch [20/123], Loss: 1.9666\n",
            "Epoch [9/200], Batch [21/123], Loss: 1.9327\n",
            "Epoch [9/200], Batch [22/123], Loss: 2.6819\n",
            "Epoch [9/200], Batch [23/123], Loss: 2.2292\n",
            "Epoch [9/200], Batch [24/123], Loss: 2.4394\n",
            "Epoch [9/200], Batch [25/123], Loss: 3.5197\n",
            "Epoch [9/200], Batch [26/123], Loss: 2.5717\n",
            "Epoch [9/200], Batch [27/123], Loss: 2.7473\n",
            "Epoch [9/200], Batch [28/123], Loss: 2.7933\n",
            "Epoch [9/200], Batch [29/123], Loss: 3.0101\n",
            "Epoch [9/200], Batch [30/123], Loss: 1.7970\n",
            "Epoch [9/200], Batch [31/123], Loss: 2.1931\n",
            "Epoch [9/200], Batch [32/123], Loss: 2.5975\n",
            "Epoch [9/200], Batch [33/123], Loss: 3.2065\n",
            "Epoch [9/200], Batch [34/123], Loss: 3.3821\n",
            "Epoch [9/200], Batch [35/123], Loss: 2.9453\n",
            "Epoch [9/200], Batch [36/123], Loss: 2.2820\n",
            "Epoch [9/200], Batch [37/123], Loss: 1.9791\n",
            "Epoch [9/200], Batch [38/123], Loss: 2.5600\n",
            "Epoch [9/200], Batch [39/123], Loss: 2.9119\n",
            "Epoch [9/200], Batch [40/123], Loss: 2.9568\n",
            "Epoch [9/200], Batch [41/123], Loss: 2.2026\n",
            "Epoch [9/200], Batch [42/123], Loss: 3.8870\n",
            "Epoch [9/200], Batch [43/123], Loss: 1.4544\n",
            "Epoch [9/200], Batch [44/123], Loss: 2.9172\n",
            "Epoch [9/200], Batch [45/123], Loss: 1.9934\n",
            "Epoch [9/200], Batch [46/123], Loss: 2.8035\n",
            "Epoch [9/200], Batch [47/123], Loss: 2.1692\n",
            "Epoch [9/200], Batch [48/123], Loss: 2.3661\n",
            "Epoch [9/200], Batch [49/123], Loss: 2.4873\n",
            "Epoch [9/200], Batch [50/123], Loss: 2.3060\n",
            "Epoch [9/200], Batch [51/123], Loss: 2.4927\n",
            "Epoch [9/200], Batch [52/123], Loss: 1.7248\n",
            "Epoch [9/200], Batch [53/123], Loss: 1.6467\n",
            "Epoch [9/200], Batch [54/123], Loss: 1.9855\n",
            "Epoch [9/200], Batch [55/123], Loss: 2.2270\n",
            "Epoch [9/200], Batch [56/123], Loss: 2.3461\n",
            "Epoch [9/200], Batch [57/123], Loss: 2.4384\n",
            "Epoch [9/200], Batch [58/123], Loss: 3.8677\n",
            "Epoch [9/200], Batch [59/123], Loss: 2.9810\n",
            "Epoch [9/200], Batch [60/123], Loss: 2.3338\n",
            "Epoch [9/200], Batch [61/123], Loss: 2.0832\n",
            "Epoch [9/200], Batch [62/123], Loss: 1.2174\n",
            "Epoch [9/200], Batch [63/123], Loss: 2.6526\n",
            "Epoch [9/200], Batch [64/123], Loss: 2.9038\n",
            "Epoch [9/200], Batch [65/123], Loss: 1.7449\n",
            "Epoch [9/200], Batch [66/123], Loss: 4.1971\n",
            "Epoch [9/200], Batch [67/123], Loss: 2.9806\n",
            "Epoch [9/200], Batch [68/123], Loss: 3.6098\n",
            "Epoch [9/200], Batch [69/123], Loss: 2.2151\n",
            "Epoch [9/200], Batch [70/123], Loss: 2.3912\n",
            "Epoch [9/200], Batch [71/123], Loss: 2.2346\n",
            "Epoch [9/200], Batch [72/123], Loss: 2.5637\n",
            "Epoch [9/200], Batch [73/123], Loss: 2.7788\n",
            "Epoch [9/200], Batch [74/123], Loss: 2.5277\n",
            "Epoch [9/200], Batch [75/123], Loss: 2.2728\n",
            "Epoch [9/200], Batch [76/123], Loss: 1.8089\n",
            "Epoch [9/200], Batch [77/123], Loss: 2.8105\n",
            "Epoch [9/200], Batch [78/123], Loss: 2.3154\n",
            "Epoch [9/200], Batch [79/123], Loss: 3.1810\n",
            "Epoch [9/200], Batch [80/123], Loss: 2.3716\n",
            "Epoch [9/200], Batch [81/123], Loss: 1.6490\n",
            "Epoch [9/200], Batch [82/123], Loss: 3.0483\n",
            "Epoch [9/200], Batch [83/123], Loss: 2.5647\n",
            "Epoch [9/200], Batch [84/123], Loss: 1.6791\n",
            "Epoch [9/200], Batch [85/123], Loss: 2.2728\n",
            "Epoch [9/200], Batch [86/123], Loss: 2.3861\n",
            "Epoch [9/200], Batch [87/123], Loss: 2.2567\n",
            "Epoch [9/200], Batch [88/123], Loss: 2.1112\n",
            "Epoch [9/200], Batch [89/123], Loss: 2.1849\n",
            "Epoch [9/200], Batch [90/123], Loss: 2.0017\n",
            "Epoch [9/200], Batch [91/123], Loss: 2.0160\n",
            "Epoch [9/200], Batch [92/123], Loss: 3.1304\n",
            "Epoch [9/200], Batch [93/123], Loss: 2.4584\n",
            "Epoch [9/200], Batch [94/123], Loss: 2.4283\n",
            "Epoch [9/200], Batch [95/123], Loss: 2.3145\n",
            "Epoch [9/200], Batch [96/123], Loss: 2.4414\n",
            "Epoch [9/200], Batch [97/123], Loss: 2.8749\n",
            "Epoch [9/200], Batch [98/123], Loss: 1.4885\n",
            "Epoch [9/200], Batch [99/123], Loss: 2.1846\n",
            "Epoch [9/200], Batch [100/123], Loss: 3.0526\n",
            "Epoch [9/200], Batch [101/123], Loss: 2.0894\n",
            "Epoch [9/200], Batch [102/123], Loss: 1.8267\n",
            "Epoch [9/200], Batch [103/123], Loss: 2.0256\n",
            "Epoch [9/200], Batch [104/123], Loss: 1.7516\n",
            "Epoch [9/200], Batch [105/123], Loss: 2.6036\n",
            "Epoch [9/200], Batch [106/123], Loss: 1.8444\n",
            "Epoch [9/200], Batch [107/123], Loss: 3.2694\n",
            "Epoch [9/200], Batch [108/123], Loss: 2.1980\n",
            "Epoch [9/200], Batch [109/123], Loss: 2.2044\n",
            "Epoch [9/200], Batch [110/123], Loss: 1.7982\n",
            "Epoch [9/200], Batch [111/123], Loss: 2.8102\n",
            "Epoch [9/200], Batch [112/123], Loss: 2.3260\n",
            "Epoch [9/200], Batch [113/123], Loss: 1.8509\n",
            "Epoch [9/200], Batch [114/123], Loss: 2.5105\n",
            "Epoch [9/200], Batch [115/123], Loss: 4.4458\n",
            "Epoch [9/200], Batch [116/123], Loss: 2.8047\n",
            "Epoch [9/200], Batch [117/123], Loss: 2.5999\n",
            "Epoch [9/200], Batch [118/123], Loss: 1.6461\n",
            "Epoch [9/200], Batch [119/123], Loss: 2.2201\n",
            "Epoch [9/200], Batch [120/123], Loss: 2.7639\n",
            "Epoch [9/200], Batch [121/123], Loss: 1.8917\n",
            "Epoch [9/200], Batch [122/123], Loss: 2.0122\n",
            "Epoch [9/200], Batch [123/123], Loss: 2.2274\n",
            "Epoch [10/200], Batch [1/123], Loss: 2.7955\n",
            "Epoch [10/200], Batch [2/123], Loss: 2.3160\n",
            "Epoch [10/200], Batch [3/123], Loss: 2.9060\n",
            "Epoch [10/200], Batch [4/123], Loss: 2.4637\n",
            "Epoch [10/200], Batch [5/123], Loss: 2.5759\n",
            "Epoch [10/200], Batch [6/123], Loss: 2.0366\n",
            "Epoch [10/200], Batch [7/123], Loss: 2.9612\n",
            "Epoch [10/200], Batch [8/123], Loss: 2.0410\n",
            "Epoch [10/200], Batch [9/123], Loss: 3.1304\n",
            "Epoch [10/200], Batch [10/123], Loss: 2.6921\n",
            "Epoch [10/200], Batch [11/123], Loss: 2.5578\n",
            "Epoch [10/200], Batch [12/123], Loss: 2.5131\n",
            "Epoch [10/200], Batch [13/123], Loss: 1.7969\n",
            "Epoch [10/200], Batch [14/123], Loss: 1.9925\n",
            "Epoch [10/200], Batch [15/123], Loss: 1.8899\n",
            "Epoch [10/200], Batch [16/123], Loss: 2.9649\n",
            "Epoch [10/200], Batch [17/123], Loss: 1.6003\n",
            "Epoch [10/200], Batch [18/123], Loss: 1.8704\n",
            "Epoch [10/200], Batch [19/123], Loss: 3.1384\n",
            "Epoch [10/200], Batch [20/123], Loss: 2.4842\n",
            "Epoch [10/200], Batch [21/123], Loss: 1.9111\n",
            "Epoch [10/200], Batch [22/123], Loss: 2.5056\n",
            "Epoch [10/200], Batch [23/123], Loss: 2.4869\n",
            "Epoch [10/200], Batch [24/123], Loss: 2.2973\n",
            "Epoch [10/200], Batch [25/123], Loss: 1.7305\n",
            "Epoch [10/200], Batch [26/123], Loss: 2.6942\n",
            "Epoch [10/200], Batch [27/123], Loss: 2.6860\n",
            "Epoch [10/200], Batch [28/123], Loss: 1.9508\n",
            "Epoch [10/200], Batch [29/123], Loss: 1.9896\n",
            "Epoch [10/200], Batch [30/123], Loss: 2.3877\n",
            "Epoch [10/200], Batch [31/123], Loss: 2.4059\n",
            "Epoch [10/200], Batch [32/123], Loss: 2.6168\n",
            "Epoch [10/200], Batch [33/123], Loss: 2.4479\n",
            "Epoch [10/200], Batch [34/123], Loss: 2.4899\n",
            "Epoch [10/200], Batch [35/123], Loss: 2.2888\n",
            "Epoch [10/200], Batch [36/123], Loss: 1.7471\n",
            "Epoch [10/200], Batch [37/123], Loss: 2.9199\n",
            "Epoch [10/200], Batch [38/123], Loss: 2.5665\n",
            "Epoch [10/200], Batch [39/123], Loss: 2.3944\n",
            "Epoch [10/200], Batch [40/123], Loss: 2.2030\n",
            "Epoch [10/200], Batch [41/123], Loss: 2.2409\n",
            "Epoch [10/200], Batch [42/123], Loss: 2.5886\n",
            "Epoch [10/200], Batch [43/123], Loss: 2.4127\n",
            "Epoch [10/200], Batch [44/123], Loss: 1.7360\n",
            "Epoch [10/200], Batch [45/123], Loss: 1.9901\n",
            "Epoch [10/200], Batch [46/123], Loss: 2.7276\n",
            "Epoch [10/200], Batch [47/123], Loss: 2.6148\n",
            "Epoch [10/200], Batch [48/123], Loss: 3.3166\n",
            "Epoch [10/200], Batch [49/123], Loss: 1.7947\n",
            "Epoch [10/200], Batch [50/123], Loss: 2.4720\n",
            "Epoch [10/200], Batch [51/123], Loss: 3.6778\n",
            "Epoch [10/200], Batch [52/123], Loss: 2.7746\n",
            "Epoch [10/200], Batch [53/123], Loss: 1.7594\n",
            "Epoch [10/200], Batch [54/123], Loss: 2.8849\n",
            "Epoch [10/200], Batch [55/123], Loss: 2.7771\n",
            "Epoch [10/200], Batch [56/123], Loss: 1.9901\n",
            "Epoch [10/200], Batch [57/123], Loss: 2.3210\n",
            "Epoch [10/200], Batch [58/123], Loss: 2.3471\n",
            "Epoch [10/200], Batch [59/123], Loss: 2.5445\n",
            "Epoch [10/200], Batch [60/123], Loss: 1.9087\n",
            "Epoch [10/200], Batch [61/123], Loss: 1.6583\n",
            "Epoch [10/200], Batch [62/123], Loss: 1.9390\n",
            "Epoch [10/200], Batch [63/123], Loss: 2.0654\n",
            "Epoch [10/200], Batch [64/123], Loss: 2.0732\n",
            "Epoch [10/200], Batch [65/123], Loss: 2.5629\n",
            "Epoch [10/200], Batch [66/123], Loss: 1.7662\n",
            "Epoch [10/200], Batch [67/123], Loss: 2.7165\n",
            "Epoch [10/200], Batch [68/123], Loss: 1.9393\n",
            "Epoch [10/200], Batch [69/123], Loss: 1.6992\n",
            "Epoch [10/200], Batch [70/123], Loss: 2.7412\n",
            "Epoch [10/200], Batch [71/123], Loss: 2.5391\n",
            "Epoch [10/200], Batch [72/123], Loss: 4.2212\n",
            "Epoch [10/200], Batch [73/123], Loss: 2.4425\n",
            "Epoch [10/200], Batch [74/123], Loss: 2.0381\n",
            "Epoch [10/200], Batch [75/123], Loss: 2.9711\n",
            "Epoch [10/200], Batch [76/123], Loss: 1.9244\n",
            "Epoch [10/200], Batch [77/123], Loss: 3.0819\n",
            "Epoch [10/200], Batch [78/123], Loss: 1.9164\n",
            "Epoch [10/200], Batch [79/123], Loss: 1.8687\n",
            "Epoch [10/200], Batch [80/123], Loss: 2.7127\n",
            "Epoch [10/200], Batch [81/123], Loss: 1.7559\n",
            "Epoch [10/200], Batch [82/123], Loss: 3.0052\n",
            "Epoch [10/200], Batch [83/123], Loss: 1.7929\n",
            "Epoch [10/200], Batch [84/123], Loss: 1.9196\n",
            "Epoch [10/200], Batch [85/123], Loss: 2.6481\n",
            "Epoch [10/200], Batch [86/123], Loss: 2.9753\n",
            "Epoch [10/200], Batch [87/123], Loss: 1.7378\n",
            "Epoch [10/200], Batch [88/123], Loss: 2.4655\n",
            "Epoch [10/200], Batch [89/123], Loss: 3.0604\n",
            "Epoch [10/200], Batch [90/123], Loss: 3.3077\n",
            "Epoch [10/200], Batch [91/123], Loss: 2.5206\n",
            "Epoch [10/200], Batch [92/123], Loss: 4.3718\n",
            "Epoch [10/200], Batch [93/123], Loss: 2.3249\n",
            "Epoch [10/200], Batch [94/123], Loss: 3.7894\n",
            "Epoch [10/200], Batch [95/123], Loss: 2.4094\n",
            "Epoch [10/200], Batch [96/123], Loss: 2.0238\n",
            "Epoch [10/200], Batch [97/123], Loss: 1.6983\n",
            "Epoch [10/200], Batch [98/123], Loss: 2.3524\n",
            "Epoch [10/200], Batch [99/123], Loss: 1.7263\n",
            "Epoch [10/200], Batch [100/123], Loss: 2.2807\n",
            "Epoch [10/200], Batch [101/123], Loss: 1.6593\n",
            "Epoch [10/200], Batch [102/123], Loss: 2.3305\n",
            "Epoch [10/200], Batch [103/123], Loss: 2.1168\n",
            "Epoch [10/200], Batch [104/123], Loss: 3.1356\n",
            "Epoch [10/200], Batch [105/123], Loss: 2.9181\n",
            "Epoch [10/200], Batch [106/123], Loss: 1.4774\n",
            "Epoch [10/200], Batch [107/123], Loss: 1.9019\n",
            "Epoch [10/200], Batch [108/123], Loss: 1.6314\n",
            "Epoch [10/200], Batch [109/123], Loss: 2.3304\n",
            "Epoch [10/200], Batch [110/123], Loss: 2.5733\n",
            "Epoch [10/200], Batch [111/123], Loss: 2.0630\n",
            "Epoch [10/200], Batch [112/123], Loss: 1.6792\n",
            "Epoch [10/200], Batch [113/123], Loss: 3.5659\n",
            "Epoch [10/200], Batch [114/123], Loss: 1.7023\n",
            "Epoch [10/200], Batch [115/123], Loss: 1.3802\n",
            "Epoch [10/200], Batch [116/123], Loss: 2.7274\n",
            "Epoch [10/200], Batch [117/123], Loss: 2.0655\n",
            "Epoch [10/200], Batch [118/123], Loss: 2.2012\n",
            "Epoch [10/200], Batch [119/123], Loss: 1.7894\n",
            "Epoch [10/200], Batch [120/123], Loss: 1.6406\n",
            "Epoch [10/200], Batch [121/123], Loss: 2.0059\n",
            "Epoch [10/200], Batch [122/123], Loss: 1.8436\n",
            "Epoch [10/200], Batch [123/123], Loss: 3.2368\n",
            "Epoch [11/200], Batch [1/123], Loss: 1.2585\n",
            "Epoch [11/200], Batch [2/123], Loss: 2.4505\n",
            "Epoch [11/200], Batch [3/123], Loss: 2.4794\n",
            "Epoch [11/200], Batch [4/123], Loss: 2.9198\n",
            "Epoch [11/200], Batch [5/123], Loss: 1.7847\n",
            "Epoch [11/200], Batch [6/123], Loss: 1.8860\n",
            "Epoch [11/200], Batch [7/123], Loss: 2.5538\n",
            "Epoch [11/200], Batch [8/123], Loss: 2.4543\n",
            "Epoch [11/200], Batch [9/123], Loss: 2.2196\n",
            "Epoch [11/200], Batch [10/123], Loss: 2.6746\n",
            "Epoch [11/200], Batch [11/123], Loss: 1.9564\n",
            "Epoch [11/200], Batch [12/123], Loss: 1.8051\n",
            "Epoch [11/200], Batch [13/123], Loss: 1.6901\n",
            "Epoch [11/200], Batch [14/123], Loss: 2.1762\n",
            "Epoch [11/200], Batch [15/123], Loss: 1.7812\n",
            "Epoch [11/200], Batch [16/123], Loss: 2.0839\n",
            "Epoch [11/200], Batch [17/123], Loss: 2.3803\n",
            "Epoch [11/200], Batch [18/123], Loss: 1.7789\n",
            "Epoch [11/200], Batch [19/123], Loss: 1.6317\n",
            "Epoch [11/200], Batch [20/123], Loss: 2.0273\n",
            "Epoch [11/200], Batch [21/123], Loss: 2.0758\n",
            "Epoch [11/200], Batch [22/123], Loss: 1.3618\n",
            "Epoch [11/200], Batch [23/123], Loss: 1.8983\n",
            "Epoch [11/200], Batch [24/123], Loss: 3.3348\n",
            "Epoch [11/200], Batch [25/123], Loss: 2.5279\n",
            "Epoch [11/200], Batch [26/123], Loss: 2.1102\n",
            "Epoch [11/200], Batch [27/123], Loss: 2.2120\n",
            "Epoch [11/200], Batch [28/123], Loss: 5.1331\n",
            "Epoch [11/200], Batch [29/123], Loss: 2.0131\n",
            "Epoch [11/200], Batch [30/123], Loss: 2.9718\n",
            "Epoch [11/200], Batch [31/123], Loss: 2.9235\n",
            "Epoch [11/200], Batch [32/123], Loss: 2.4217\n",
            "Epoch [11/200], Batch [33/123], Loss: 2.4272\n",
            "Epoch [11/200], Batch [34/123], Loss: 1.9569\n",
            "Epoch [11/200], Batch [35/123], Loss: 1.3755\n",
            "Epoch [11/200], Batch [36/123], Loss: 3.1151\n",
            "Epoch [11/200], Batch [37/123], Loss: 2.3658\n",
            "Epoch [11/200], Batch [38/123], Loss: 2.0168\n",
            "Epoch [11/200], Batch [39/123], Loss: 1.9535\n",
            "Epoch [11/200], Batch [40/123], Loss: 1.5703\n",
            "Epoch [11/200], Batch [41/123], Loss: 3.5414\n",
            "Epoch [11/200], Batch [42/123], Loss: 2.7538\n",
            "Epoch [11/200], Batch [43/123], Loss: 2.4050\n",
            "Epoch [11/200], Batch [44/123], Loss: 3.1065\n",
            "Epoch [11/200], Batch [45/123], Loss: 2.1010\n",
            "Epoch [11/200], Batch [46/123], Loss: 2.6449\n",
            "Epoch [11/200], Batch [47/123], Loss: 2.7938\n",
            "Epoch [11/200], Batch [48/123], Loss: 2.9064\n",
            "Epoch [11/200], Batch [49/123], Loss: 2.7134\n",
            "Epoch [11/200], Batch [50/123], Loss: 3.3563\n",
            "Epoch [11/200], Batch [51/123], Loss: 1.8342\n",
            "Epoch [11/200], Batch [52/123], Loss: 1.4412\n",
            "Epoch [11/200], Batch [53/123], Loss: 2.2007\n",
            "Epoch [11/200], Batch [54/123], Loss: 3.1221\n",
            "Epoch [11/200], Batch [55/123], Loss: 2.5684\n",
            "Epoch [11/200], Batch [56/123], Loss: 2.8398\n",
            "Epoch [11/200], Batch [57/123], Loss: 2.8811\n",
            "Epoch [11/200], Batch [58/123], Loss: 1.6563\n",
            "Epoch [11/200], Batch [59/123], Loss: 2.0124\n",
            "Epoch [11/200], Batch [60/123], Loss: 3.1848\n",
            "Epoch [11/200], Batch [61/123], Loss: 2.8290\n",
            "Epoch [11/200], Batch [62/123], Loss: 2.1550\n",
            "Epoch [11/200], Batch [63/123], Loss: 2.1285\n",
            "Epoch [11/200], Batch [64/123], Loss: 2.2719\n",
            "Epoch [11/200], Batch [65/123], Loss: 2.6025\n",
            "Epoch [11/200], Batch [66/123], Loss: 2.3671\n",
            "Epoch [11/200], Batch [67/123], Loss: 2.0794\n",
            "Epoch [11/200], Batch [68/123], Loss: 3.0556\n",
            "Epoch [11/200], Batch [69/123], Loss: 2.5058\n",
            "Epoch [11/200], Batch [70/123], Loss: 1.7401\n",
            "Epoch [11/200], Batch [71/123], Loss: 1.9031\n",
            "Epoch [11/200], Batch [72/123], Loss: 2.7231\n",
            "Epoch [11/200], Batch [73/123], Loss: 3.5648\n",
            "Epoch [11/200], Batch [74/123], Loss: 2.9814\n",
            "Epoch [11/200], Batch [75/123], Loss: 3.6023\n",
            "Epoch [11/200], Batch [76/123], Loss: 3.5588\n",
            "Epoch [11/200], Batch [77/123], Loss: 2.5110\n",
            "Epoch [11/200], Batch [78/123], Loss: 1.7604\n",
            "Epoch [11/200], Batch [79/123], Loss: 2.3215\n",
            "Epoch [11/200], Batch [80/123], Loss: 4.6832\n",
            "Epoch [11/200], Batch [81/123], Loss: 2.0169\n",
            "Epoch [11/200], Batch [82/123], Loss: 2.5270\n",
            "Epoch [11/200], Batch [83/123], Loss: 2.1787\n",
            "Epoch [11/200], Batch [84/123], Loss: 3.2494\n",
            "Epoch [11/200], Batch [85/123], Loss: 1.5781\n",
            "Epoch [11/200], Batch [86/123], Loss: 2.3797\n",
            "Epoch [11/200], Batch [87/123], Loss: 2.2287\n",
            "Epoch [11/200], Batch [88/123], Loss: 2.8729\n",
            "Epoch [11/200], Batch [89/123], Loss: 3.1942\n",
            "Epoch [11/200], Batch [90/123], Loss: 1.9877\n",
            "Epoch [11/200], Batch [91/123], Loss: 1.5476\n",
            "Epoch [11/200], Batch [92/123], Loss: 2.1255\n",
            "Epoch [11/200], Batch [93/123], Loss: 1.9868\n",
            "Epoch [11/200], Batch [94/123], Loss: 2.0665\n",
            "Epoch [11/200], Batch [95/123], Loss: 2.3876\n",
            "Epoch [11/200], Batch [96/123], Loss: 2.5972\n",
            "Epoch [11/200], Batch [97/123], Loss: 2.9541\n",
            "Epoch [11/200], Batch [98/123], Loss: 3.1044\n",
            "Epoch [11/200], Batch [99/123], Loss: 2.2356\n",
            "Epoch [11/200], Batch [100/123], Loss: 2.0503\n",
            "Epoch [11/200], Batch [101/123], Loss: 2.4028\n",
            "Epoch [11/200], Batch [102/123], Loss: 2.2036\n",
            "Epoch [11/200], Batch [103/123], Loss: 2.2995\n",
            "Epoch [11/200], Batch [104/123], Loss: 1.7266\n",
            "Epoch [11/200], Batch [105/123], Loss: 1.6098\n",
            "Epoch [11/200], Batch [106/123], Loss: 2.3895\n",
            "Epoch [11/200], Batch [107/123], Loss: 2.0654\n",
            "Epoch [11/200], Batch [108/123], Loss: 2.1000\n",
            "Epoch [11/200], Batch [109/123], Loss: 1.6094\n",
            "Epoch [11/200], Batch [110/123], Loss: 2.4090\n",
            "Epoch [11/200], Batch [111/123], Loss: 2.0018\n",
            "Epoch [11/200], Batch [112/123], Loss: 1.6558\n",
            "Epoch [11/200], Batch [113/123], Loss: 2.2128\n",
            "Epoch [11/200], Batch [114/123], Loss: 1.6791\n",
            "Epoch [11/200], Batch [115/123], Loss: 2.1123\n",
            "Epoch [11/200], Batch [116/123], Loss: 2.0153\n",
            "Epoch [11/200], Batch [117/123], Loss: 2.3628\n",
            "Epoch [11/200], Batch [118/123], Loss: 1.8539\n",
            "Epoch [11/200], Batch [119/123], Loss: 2.2861\n",
            "Epoch [11/200], Batch [120/123], Loss: 1.9576\n",
            "Epoch [11/200], Batch [121/123], Loss: 2.1516\n",
            "Epoch [11/200], Batch [122/123], Loss: 1.5232\n",
            "Epoch [11/200], Batch [123/123], Loss: 1.7203\n",
            "Epoch [12/200], Batch [1/123], Loss: 3.2299\n",
            "Epoch [12/200], Batch [2/123], Loss: 1.8552\n",
            "Epoch [12/200], Batch [3/123], Loss: 2.2238\n",
            "Epoch [12/200], Batch [4/123], Loss: 1.9840\n",
            "Epoch [12/200], Batch [5/123], Loss: 1.8910\n",
            "Epoch [12/200], Batch [6/123], Loss: 2.4113\n",
            "Epoch [12/200], Batch [7/123], Loss: 1.9576\n",
            "Epoch [12/200], Batch [8/123], Loss: 1.5559\n",
            "Epoch [12/200], Batch [9/123], Loss: 1.9383\n",
            "Epoch [12/200], Batch [10/123], Loss: 1.9258\n",
            "Epoch [12/200], Batch [11/123], Loss: 2.4734\n",
            "Epoch [12/200], Batch [12/123], Loss: 1.9914\n",
            "Epoch [12/200], Batch [13/123], Loss: 1.7583\n",
            "Epoch [12/200], Batch [14/123], Loss: 1.8521\n",
            "Epoch [12/200], Batch [15/123], Loss: 1.2334\n",
            "Epoch [12/200], Batch [16/123], Loss: 1.5717\n",
            "Epoch [12/200], Batch [17/123], Loss: 1.7117\n",
            "Epoch [12/200], Batch [18/123], Loss: 4.5619\n",
            "Epoch [12/200], Batch [19/123], Loss: 2.6644\n",
            "Epoch [12/200], Batch [20/123], Loss: 2.4220\n",
            "Epoch [12/200], Batch [21/123], Loss: 2.7308\n",
            "Epoch [12/200], Batch [22/123], Loss: 1.7532\n",
            "Epoch [12/200], Batch [23/123], Loss: 1.8890\n",
            "Epoch [12/200], Batch [24/123], Loss: 1.8005\n",
            "Epoch [12/200], Batch [25/123], Loss: 1.7986\n",
            "Epoch [12/200], Batch [26/123], Loss: 1.5067\n",
            "Epoch [12/200], Batch [27/123], Loss: 2.5525\n",
            "Epoch [12/200], Batch [28/123], Loss: 3.1113\n",
            "Epoch [12/200], Batch [29/123], Loss: 1.8793\n",
            "Epoch [12/200], Batch [30/123], Loss: 1.8761\n",
            "Epoch [12/200], Batch [31/123], Loss: 2.6876\n",
            "Epoch [12/200], Batch [32/123], Loss: 2.3781\n",
            "Epoch [12/200], Batch [33/123], Loss: 2.3571\n",
            "Epoch [12/200], Batch [34/123], Loss: 1.9785\n",
            "Epoch [12/200], Batch [35/123], Loss: 2.1167\n",
            "Epoch [12/200], Batch [36/123], Loss: 1.9980\n",
            "Epoch [12/200], Batch [37/123], Loss: 1.9525\n",
            "Epoch [12/200], Batch [38/123], Loss: 2.1193\n",
            "Epoch [12/200], Batch [39/123], Loss: 1.9694\n",
            "Epoch [12/200], Batch [40/123], Loss: 2.2513\n",
            "Epoch [12/200], Batch [41/123], Loss: 1.8239\n",
            "Epoch [12/200], Batch [42/123], Loss: 1.7103\n",
            "Epoch [12/200], Batch [43/123], Loss: 2.0727\n",
            "Epoch [12/200], Batch [44/123], Loss: 2.2213\n",
            "Epoch [12/200], Batch [45/123], Loss: 1.9057\n",
            "Epoch [12/200], Batch [46/123], Loss: 2.0132\n",
            "Epoch [12/200], Batch [47/123], Loss: 1.1897\n",
            "Epoch [12/200], Batch [48/123], Loss: 2.1246\n",
            "Epoch [12/200], Batch [49/123], Loss: 2.0551\n",
            "Epoch [12/200], Batch [50/123], Loss: 2.9902\n",
            "Epoch [12/200], Batch [51/123], Loss: 1.6860\n",
            "Epoch [12/200], Batch [52/123], Loss: 1.8597\n",
            "Epoch [12/200], Batch [53/123], Loss: 2.6493\n",
            "Epoch [12/200], Batch [54/123], Loss: 2.0361\n",
            "Epoch [12/200], Batch [55/123], Loss: 2.2327\n",
            "Epoch [12/200], Batch [56/123], Loss: 1.7734\n",
            "Epoch [12/200], Batch [57/123], Loss: 1.9228\n",
            "Epoch [12/200], Batch [58/123], Loss: 1.9524\n",
            "Epoch [12/200], Batch [59/123], Loss: 2.5771\n",
            "Epoch [12/200], Batch [60/123], Loss: 3.0701\n",
            "Epoch [12/200], Batch [61/123], Loss: 2.6172\n",
            "Epoch [12/200], Batch [62/123], Loss: 1.4551\n",
            "Epoch [12/200], Batch [63/123], Loss: 2.2744\n",
            "Epoch [12/200], Batch [64/123], Loss: 2.3638\n",
            "Epoch [12/200], Batch [65/123], Loss: 3.0277\n",
            "Epoch [12/200], Batch [66/123], Loss: 2.4120\n",
            "Epoch [12/200], Batch [67/123], Loss: 1.9770\n",
            "Epoch [12/200], Batch [68/123], Loss: 2.3349\n",
            "Epoch [12/200], Batch [69/123], Loss: 3.1581\n",
            "Epoch [12/200], Batch [70/123], Loss: 2.3003\n",
            "Epoch [12/200], Batch [71/123], Loss: 2.7020\n",
            "Epoch [12/200], Batch [72/123], Loss: 2.0915\n",
            "Epoch [12/200], Batch [73/123], Loss: 1.8462\n",
            "Epoch [12/200], Batch [74/123], Loss: 2.5273\n",
            "Epoch [12/200], Batch [75/123], Loss: 2.4594\n",
            "Epoch [12/200], Batch [76/123], Loss: 1.5869\n",
            "Epoch [12/200], Batch [77/123], Loss: 1.9936\n",
            "Epoch [12/200], Batch [78/123], Loss: 2.3830\n",
            "Epoch [12/200], Batch [79/123], Loss: 1.3865\n",
            "Epoch [12/200], Batch [80/123], Loss: 2.1537\n",
            "Epoch [12/200], Batch [81/123], Loss: 2.7336\n",
            "Epoch [12/200], Batch [82/123], Loss: 1.7248\n",
            "Epoch [12/200], Batch [83/123], Loss: 2.3244\n",
            "Epoch [12/200], Batch [84/123], Loss: 3.1291\n",
            "Epoch [12/200], Batch [85/123], Loss: 2.2075\n",
            "Epoch [12/200], Batch [86/123], Loss: 2.6432\n",
            "Epoch [12/200], Batch [87/123], Loss: 1.7305\n",
            "Epoch [12/200], Batch [88/123], Loss: 1.5788\n",
            "Epoch [12/200], Batch [89/123], Loss: 2.1865\n",
            "Epoch [12/200], Batch [90/123], Loss: 1.7960\n",
            "Epoch [12/200], Batch [91/123], Loss: 2.8529\n",
            "Epoch [12/200], Batch [92/123], Loss: 1.4789\n",
            "Epoch [12/200], Batch [93/123], Loss: 2.4285\n",
            "Epoch [12/200], Batch [94/123], Loss: 1.5339\n",
            "Epoch [12/200], Batch [95/123], Loss: 2.2817\n",
            "Epoch [12/200], Batch [96/123], Loss: 1.8080\n",
            "Epoch [12/200], Batch [97/123], Loss: 2.0389\n",
            "Epoch [12/200], Batch [98/123], Loss: 1.4702\n",
            "Epoch [12/200], Batch [99/123], Loss: 2.0961\n",
            "Epoch [12/200], Batch [100/123], Loss: 1.7342\n",
            "Epoch [12/200], Batch [101/123], Loss: 2.1293\n",
            "Epoch [12/200], Batch [102/123], Loss: 3.4293\n",
            "Epoch [12/200], Batch [103/123], Loss: 2.6018\n",
            "Epoch [12/200], Batch [104/123], Loss: 2.5487\n",
            "Epoch [12/200], Batch [105/123], Loss: 1.8367\n",
            "Epoch [12/200], Batch [106/123], Loss: 1.6407\n",
            "Epoch [12/200], Batch [107/123], Loss: 2.5441\n",
            "Epoch [12/200], Batch [108/123], Loss: 2.6144\n",
            "Epoch [12/200], Batch [109/123], Loss: 2.2237\n",
            "Epoch [12/200], Batch [110/123], Loss: 2.4461\n",
            "Epoch [12/200], Batch [111/123], Loss: 2.3377\n",
            "Epoch [12/200], Batch [112/123], Loss: 2.5712\n",
            "Epoch [12/200], Batch [113/123], Loss: 1.4331\n",
            "Epoch [12/200], Batch [114/123], Loss: 2.0060\n",
            "Epoch [12/200], Batch [115/123], Loss: 1.9771\n",
            "Epoch [12/200], Batch [116/123], Loss: 2.6269\n",
            "Epoch [12/200], Batch [117/123], Loss: 1.3845\n",
            "Epoch [12/200], Batch [118/123], Loss: 2.0318\n",
            "Epoch [12/200], Batch [119/123], Loss: 2.0643\n",
            "Epoch [12/200], Batch [120/123], Loss: 3.4469\n",
            "Epoch [12/200], Batch [121/123], Loss: 2.3418\n",
            "Epoch [12/200], Batch [122/123], Loss: 1.6721\n",
            "Epoch [12/200], Batch [123/123], Loss: 2.6971\n",
            "Epoch [13/200], Batch [1/123], Loss: 1.6915\n",
            "Epoch [13/200], Batch [2/123], Loss: 2.2950\n",
            "Epoch [13/200], Batch [3/123], Loss: 2.3222\n",
            "Epoch [13/200], Batch [4/123], Loss: 2.2560\n",
            "Epoch [13/200], Batch [5/123], Loss: 3.1460\n",
            "Epoch [13/200], Batch [6/123], Loss: 1.3914\n",
            "Epoch [13/200], Batch [7/123], Loss: 1.7891\n",
            "Epoch [13/200], Batch [8/123], Loss: 1.9299\n",
            "Epoch [13/200], Batch [9/123], Loss: 2.2554\n",
            "Epoch [13/200], Batch [10/123], Loss: 1.6890\n",
            "Epoch [13/200], Batch [11/123], Loss: 2.5120\n",
            "Epoch [13/200], Batch [12/123], Loss: 2.3458\n",
            "Epoch [13/200], Batch [13/123], Loss: 2.2714\n",
            "Epoch [13/200], Batch [14/123], Loss: 1.5295\n",
            "Epoch [13/200], Batch [15/123], Loss: 1.6504\n",
            "Epoch [13/200], Batch [16/123], Loss: 1.7495\n",
            "Epoch [13/200], Batch [17/123], Loss: 2.0421\n",
            "Epoch [13/200], Batch [18/123], Loss: 1.9774\n",
            "Epoch [13/200], Batch [19/123], Loss: 2.3395\n",
            "Epoch [13/200], Batch [20/123], Loss: 1.9571\n",
            "Epoch [13/200], Batch [21/123], Loss: 2.0473\n",
            "Epoch [13/200], Batch [22/123], Loss: 1.8856\n",
            "Epoch [13/200], Batch [23/123], Loss: 2.5316\n",
            "Epoch [13/200], Batch [24/123], Loss: 1.9820\n",
            "Epoch [13/200], Batch [25/123], Loss: 2.8634\n",
            "Epoch [13/200], Batch [26/123], Loss: 1.7327\n",
            "Epoch [13/200], Batch [27/123], Loss: 2.0843\n",
            "Epoch [13/200], Batch [28/123], Loss: 2.0400\n",
            "Epoch [13/200], Batch [29/123], Loss: 2.9595\n",
            "Epoch [13/200], Batch [30/123], Loss: 1.7599\n",
            "Epoch [13/200], Batch [31/123], Loss: 1.9959\n",
            "Epoch [13/200], Batch [32/123], Loss: 2.3768\n",
            "Epoch [13/200], Batch [33/123], Loss: 2.0650\n",
            "Epoch [13/200], Batch [34/123], Loss: 1.8226\n",
            "Epoch [13/200], Batch [35/123], Loss: 2.0056\n",
            "Epoch [13/200], Batch [36/123], Loss: 1.7681\n",
            "Epoch [13/200], Batch [37/123], Loss: 3.0859\n",
            "Epoch [13/200], Batch [38/123], Loss: 1.7388\n",
            "Epoch [13/200], Batch [39/123], Loss: 1.6368\n",
            "Epoch [13/200], Batch [40/123], Loss: 2.1734\n",
            "Epoch [13/200], Batch [41/123], Loss: 1.6793\n",
            "Epoch [13/200], Batch [42/123], Loss: 1.7332\n",
            "Epoch [13/200], Batch [43/123], Loss: 1.7278\n",
            "Epoch [13/200], Batch [44/123], Loss: 1.9130\n",
            "Epoch [13/200], Batch [45/123], Loss: 1.9073\n",
            "Epoch [13/200], Batch [46/123], Loss: 1.4621\n",
            "Epoch [13/200], Batch [47/123], Loss: 1.5114\n",
            "Epoch [13/200], Batch [48/123], Loss: 1.3212\n",
            "Epoch [13/200], Batch [49/123], Loss: 1.9161\n",
            "Epoch [13/200], Batch [50/123], Loss: 1.9564\n",
            "Epoch [13/200], Batch [51/123], Loss: 1.5793\n",
            "Epoch [13/200], Batch [52/123], Loss: 1.9780\n",
            "Epoch [13/200], Batch [53/123], Loss: 2.1132\n",
            "Epoch [13/200], Batch [54/123], Loss: 2.1462\n",
            "Epoch [13/200], Batch [55/123], Loss: 2.5810\n",
            "Epoch [13/200], Batch [56/123], Loss: 1.5868\n",
            "Epoch [13/200], Batch [57/123], Loss: 2.1571\n",
            "Epoch [13/200], Batch [58/123], Loss: 2.1870\n",
            "Epoch [13/200], Batch [59/123], Loss: 1.6302\n",
            "Epoch [13/200], Batch [60/123], Loss: 2.2505\n",
            "Epoch [13/200], Batch [61/123], Loss: 1.8390\n",
            "Epoch [13/200], Batch [62/123], Loss: 2.0518\n",
            "Epoch [13/200], Batch [63/123], Loss: 1.9228\n",
            "Epoch [13/200], Batch [64/123], Loss: 2.0125\n",
            "Epoch [13/200], Batch [65/123], Loss: 1.9311\n",
            "Epoch [13/200], Batch [66/123], Loss: 1.8016\n",
            "Epoch [13/200], Batch [67/123], Loss: 2.5221\n",
            "Epoch [13/200], Batch [68/123], Loss: 2.5625\n",
            "Epoch [13/200], Batch [69/123], Loss: 1.9246\n",
            "Epoch [13/200], Batch [70/123], Loss: 1.9462\n",
            "Epoch [13/200], Batch [71/123], Loss: 1.5715\n",
            "Epoch [13/200], Batch [72/123], Loss: 2.3475\n",
            "Epoch [13/200], Batch [73/123], Loss: 1.2040\n",
            "Epoch [13/200], Batch [74/123], Loss: 1.9613\n",
            "Epoch [13/200], Batch [75/123], Loss: 1.3591\n",
            "Epoch [13/200], Batch [76/123], Loss: 1.6443\n",
            "Epoch [13/200], Batch [77/123], Loss: 1.5298\n",
            "Epoch [13/200], Batch [78/123], Loss: 1.7090\n",
            "Epoch [13/200], Batch [79/123], Loss: 3.5028\n",
            "Epoch [13/200], Batch [80/123], Loss: 1.8262\n",
            "Epoch [13/200], Batch [81/123], Loss: 1.8152\n",
            "Epoch [13/200], Batch [82/123], Loss: 1.9759\n",
            "Epoch [13/200], Batch [83/123], Loss: 1.7389\n",
            "Epoch [13/200], Batch [84/123], Loss: 2.3873\n",
            "Epoch [13/200], Batch [85/123], Loss: 1.8097\n",
            "Epoch [13/200], Batch [86/123], Loss: 1.8435\n",
            "Epoch [13/200], Batch [87/123], Loss: 1.7873\n",
            "Epoch [13/200], Batch [88/123], Loss: 1.8098\n",
            "Epoch [13/200], Batch [89/123], Loss: 2.0978\n",
            "Epoch [13/200], Batch [90/123], Loss: 1.7299\n",
            "Epoch [13/200], Batch [91/123], Loss: 1.3665\n",
            "Epoch [13/200], Batch [92/123], Loss: 2.6552\n",
            "Epoch [13/200], Batch [93/123], Loss: 1.6055\n",
            "Epoch [13/200], Batch [94/123], Loss: 2.2348\n",
            "Epoch [13/200], Batch [95/123], Loss: 2.7821\n",
            "Epoch [13/200], Batch [96/123], Loss: 2.0420\n",
            "Epoch [13/200], Batch [97/123], Loss: 1.8139\n",
            "Epoch [13/200], Batch [98/123], Loss: 2.0511\n",
            "Epoch [13/200], Batch [99/123], Loss: 3.3337\n",
            "Epoch [13/200], Batch [100/123], Loss: 2.1802\n",
            "Epoch [13/200], Batch [101/123], Loss: 3.3625\n",
            "Epoch [13/200], Batch [102/123], Loss: 1.7299\n",
            "Epoch [13/200], Batch [103/123], Loss: 2.1512\n",
            "Epoch [13/200], Batch [104/123], Loss: 2.3707\n",
            "Epoch [13/200], Batch [105/123], Loss: 2.0379\n",
            "Epoch [13/200], Batch [106/123], Loss: 2.0520\n",
            "Epoch [13/200], Batch [107/123], Loss: 1.5106\n",
            "Epoch [13/200], Batch [108/123], Loss: 1.9473\n",
            "Epoch [13/200], Batch [109/123], Loss: 2.1309\n",
            "Epoch [13/200], Batch [110/123], Loss: 2.2142\n",
            "Epoch [13/200], Batch [111/123], Loss: 3.3496\n",
            "Epoch [13/200], Batch [112/123], Loss: 2.6993\n",
            "Epoch [13/200], Batch [113/123], Loss: 1.8202\n",
            "Epoch [13/200], Batch [114/123], Loss: 2.2267\n",
            "Epoch [13/200], Batch [115/123], Loss: 3.5369\n",
            "Epoch [13/200], Batch [116/123], Loss: 2.6748\n",
            "Epoch [13/200], Batch [117/123], Loss: 2.6391\n",
            "Epoch [13/200], Batch [118/123], Loss: 1.7800\n",
            "Epoch [13/200], Batch [119/123], Loss: 1.9560\n",
            "Epoch [13/200], Batch [120/123], Loss: 1.8492\n",
            "Epoch [13/200], Batch [121/123], Loss: 2.7960\n",
            "Epoch [13/200], Batch [122/123], Loss: 2.2720\n",
            "Epoch [13/200], Batch [123/123], Loss: 3.1035\n",
            "Epoch [14/200], Batch [1/123], Loss: 2.3652\n",
            "Epoch [14/200], Batch [2/123], Loss: 1.9682\n",
            "Epoch [14/200], Batch [3/123], Loss: 2.1312\n",
            "Epoch [14/200], Batch [4/123], Loss: 1.5579\n",
            "Epoch [14/200], Batch [5/123], Loss: 2.4994\n",
            "Epoch [14/200], Batch [6/123], Loss: 1.8534\n",
            "Epoch [14/200], Batch [7/123], Loss: 1.7079\n",
            "Epoch [14/200], Batch [8/123], Loss: 2.3140\n",
            "Epoch [14/200], Batch [9/123], Loss: 1.6171\n",
            "Epoch [14/200], Batch [10/123], Loss: 1.8186\n",
            "Epoch [14/200], Batch [11/123], Loss: 2.2398\n",
            "Epoch [14/200], Batch [12/123], Loss: 1.4336\n",
            "Epoch [14/200], Batch [13/123], Loss: 1.7042\n",
            "Epoch [14/200], Batch [14/123], Loss: 2.3593\n",
            "Epoch [14/200], Batch [15/123], Loss: 1.7359\n",
            "Epoch [14/200], Batch [16/123], Loss: 2.0301\n",
            "Epoch [14/200], Batch [17/123], Loss: 1.7403\n",
            "Epoch [14/200], Batch [18/123], Loss: 1.7975\n",
            "Epoch [14/200], Batch [19/123], Loss: 1.8810\n",
            "Epoch [14/200], Batch [20/123], Loss: 1.4229\n",
            "Epoch [14/200], Batch [21/123], Loss: 1.7446\n",
            "Epoch [14/200], Batch [22/123], Loss: 1.6531\n",
            "Epoch [14/200], Batch [23/123], Loss: 1.6789\n",
            "Epoch [14/200], Batch [24/123], Loss: 2.6463\n",
            "Epoch [14/200], Batch [25/123], Loss: 2.5644\n",
            "Epoch [14/200], Batch [26/123], Loss: 2.3225\n",
            "Epoch [14/200], Batch [27/123], Loss: 1.4882\n",
            "Epoch [14/200], Batch [28/123], Loss: 1.8109\n",
            "Epoch [14/200], Batch [29/123], Loss: 1.3808\n",
            "Epoch [14/200], Batch [30/123], Loss: 1.5825\n",
            "Epoch [14/200], Batch [31/123], Loss: 1.7246\n",
            "Epoch [14/200], Batch [32/123], Loss: 1.8379\n",
            "Epoch [14/200], Batch [33/123], Loss: 1.8491\n",
            "Epoch [14/200], Batch [34/123], Loss: 1.9594\n",
            "Epoch [14/200], Batch [35/123], Loss: 1.6747\n",
            "Epoch [14/200], Batch [36/123], Loss: 1.6038\n",
            "Epoch [14/200], Batch [37/123], Loss: 1.8595\n",
            "Epoch [14/200], Batch [38/123], Loss: 1.5193\n",
            "Epoch [14/200], Batch [39/123], Loss: 2.5493\n",
            "Epoch [14/200], Batch [40/123], Loss: 1.1255\n",
            "Epoch [14/200], Batch [41/123], Loss: 1.9503\n",
            "Epoch [14/200], Batch [42/123], Loss: 2.2576\n",
            "Epoch [14/200], Batch [43/123], Loss: 2.2695\n",
            "Epoch [14/200], Batch [44/123], Loss: 1.9885\n",
            "Epoch [14/200], Batch [45/123], Loss: 1.5378\n",
            "Epoch [14/200], Batch [46/123], Loss: 1.6775\n",
            "Epoch [14/200], Batch [47/123], Loss: 1.9871\n",
            "Epoch [14/200], Batch [48/123], Loss: 2.1294\n",
            "Epoch [14/200], Batch [49/123], Loss: 1.6237\n",
            "Epoch [14/200], Batch [50/123], Loss: 1.9484\n",
            "Epoch [14/200], Batch [51/123], Loss: 1.6830\n",
            "Epoch [14/200], Batch [52/123], Loss: 2.4189\n",
            "Epoch [14/200], Batch [53/123], Loss: 1.4092\n",
            "Epoch [14/200], Batch [54/123], Loss: 1.8759\n",
            "Epoch [14/200], Batch [55/123], Loss: 1.6982\n",
            "Epoch [14/200], Batch [56/123], Loss: 1.8553\n",
            "Epoch [14/200], Batch [57/123], Loss: 1.9256\n",
            "Epoch [14/200], Batch [58/123], Loss: 2.2139\n",
            "Epoch [14/200], Batch [59/123], Loss: 1.3381\n",
            "Epoch [14/200], Batch [60/123], Loss: 1.6995\n",
            "Epoch [14/200], Batch [61/123], Loss: 2.1513\n",
            "Epoch [14/200], Batch [62/123], Loss: 1.7769\n",
            "Epoch [14/200], Batch [63/123], Loss: 1.9722\n",
            "Epoch [14/200], Batch [64/123], Loss: 1.4395\n",
            "Epoch [14/200], Batch [65/123], Loss: 2.1401\n",
            "Epoch [14/200], Batch [66/123], Loss: 2.0076\n",
            "Epoch [14/200], Batch [67/123], Loss: 1.3286\n",
            "Epoch [14/200], Batch [68/123], Loss: 1.4540\n",
            "Epoch [14/200], Batch [69/123], Loss: 1.9740\n",
            "Epoch [14/200], Batch [70/123], Loss: 1.8124\n",
            "Epoch [14/200], Batch [71/123], Loss: 2.0347\n",
            "Epoch [14/200], Batch [72/123], Loss: 1.2413\n",
            "Epoch [14/200], Batch [73/123], Loss: 1.9196\n",
            "Epoch [14/200], Batch [74/123], Loss: 1.4359\n",
            "Epoch [14/200], Batch [75/123], Loss: 1.5781\n",
            "Epoch [14/200], Batch [76/123], Loss: 2.2756\n",
            "Epoch [14/200], Batch [77/123], Loss: 3.8034\n",
            "Epoch [14/200], Batch [78/123], Loss: 2.2478\n",
            "Epoch [14/200], Batch [79/123], Loss: 1.7993\n",
            "Epoch [14/200], Batch [80/123], Loss: 1.8807\n",
            "Epoch [14/200], Batch [81/123], Loss: 1.6349\n",
            "Epoch [14/200], Batch [82/123], Loss: 1.9065\n",
            "Epoch [14/200], Batch [83/123], Loss: 2.9119\n",
            "Epoch [14/200], Batch [84/123], Loss: 2.0894\n",
            "Epoch [14/200], Batch [85/123], Loss: 2.0190\n",
            "Epoch [14/200], Batch [86/123], Loss: 1.6053\n",
            "Epoch [14/200], Batch [87/123], Loss: 3.2036\n",
            "Epoch [14/200], Batch [88/123], Loss: 1.8622\n",
            "Epoch [14/200], Batch [89/123], Loss: 1.6506\n",
            "Epoch [14/200], Batch [90/123], Loss: 2.9543\n",
            "Epoch [14/200], Batch [91/123], Loss: 1.1031\n",
            "Epoch [14/200], Batch [92/123], Loss: 1.5831\n",
            "Epoch [14/200], Batch [93/123], Loss: 3.2024\n",
            "Epoch [14/200], Batch [94/123], Loss: 2.1041\n",
            "Epoch [14/200], Batch [95/123], Loss: 1.5723\n",
            "Epoch [14/200], Batch [96/123], Loss: 1.8403\n",
            "Epoch [14/200], Batch [97/123], Loss: 2.0356\n",
            "Epoch [14/200], Batch [98/123], Loss: 2.1527\n",
            "Epoch [14/200], Batch [99/123], Loss: 1.9848\n",
            "Epoch [14/200], Batch [100/123], Loss: 3.2879\n",
            "Epoch [14/200], Batch [101/123], Loss: 1.8917\n",
            "Epoch [14/200], Batch [102/123], Loss: 1.7674\n",
            "Epoch [14/200], Batch [103/123], Loss: 2.2825\n",
            "Epoch [14/200], Batch [104/123], Loss: 2.1282\n",
            "Epoch [14/200], Batch [105/123], Loss: 1.7895\n",
            "Epoch [14/200], Batch [106/123], Loss: 3.4906\n",
            "Epoch [14/200], Batch [107/123], Loss: 1.3886\n",
            "Epoch [14/200], Batch [108/123], Loss: 1.5801\n",
            "Epoch [14/200], Batch [109/123], Loss: 1.5387\n",
            "Epoch [14/200], Batch [110/123], Loss: 2.5822\n",
            "Epoch [14/200], Batch [111/123], Loss: 2.4376\n",
            "Epoch [14/200], Batch [112/123], Loss: 1.7926\n",
            "Epoch [14/200], Batch [113/123], Loss: 1.7773\n",
            "Epoch [14/200], Batch [114/123], Loss: 1.5439\n",
            "Epoch [14/200], Batch [115/123], Loss: 1.9848\n",
            "Epoch [14/200], Batch [116/123], Loss: 1.8311\n",
            "Epoch [14/200], Batch [117/123], Loss: 2.4224\n",
            "Epoch [14/200], Batch [118/123], Loss: 1.6219\n",
            "Epoch [14/200], Batch [119/123], Loss: 1.6472\n",
            "Epoch [14/200], Batch [120/123], Loss: 1.4531\n",
            "Epoch [14/200], Batch [121/123], Loss: 2.3278\n",
            "Epoch [14/200], Batch [122/123], Loss: 2.4163\n",
            "Epoch [14/200], Batch [123/123], Loss: 1.6956\n",
            "Epoch [15/200], Batch [1/123], Loss: 2.0213\n",
            "Epoch [15/200], Batch [2/123], Loss: 2.2633\n",
            "Epoch [15/200], Batch [3/123], Loss: 1.7377\n",
            "Epoch [15/200], Batch [4/123], Loss: 1.5823\n",
            "Epoch [15/200], Batch [5/123], Loss: 1.2738\n",
            "Epoch [15/200], Batch [6/123], Loss: 2.6452\n",
            "Epoch [15/200], Batch [7/123], Loss: 2.1130\n",
            "Epoch [15/200], Batch [8/123], Loss: 1.8661\n",
            "Epoch [15/200], Batch [9/123], Loss: 1.5289\n",
            "Epoch [15/200], Batch [10/123], Loss: 2.1955\n",
            "Epoch [15/200], Batch [11/123], Loss: 1.6200\n",
            "Epoch [15/200], Batch [12/123], Loss: 2.0864\n",
            "Epoch [15/200], Batch [13/123], Loss: 2.1191\n",
            "Epoch [15/200], Batch [14/123], Loss: 1.4508\n",
            "Epoch [15/200], Batch [15/123], Loss: 2.1663\n",
            "Epoch [15/200], Batch [16/123], Loss: 1.6869\n",
            "Epoch [15/200], Batch [17/123], Loss: 2.5286\n",
            "Epoch [15/200], Batch [18/123], Loss: 1.5434\n",
            "Epoch [15/200], Batch [19/123], Loss: 1.4304\n",
            "Epoch [15/200], Batch [20/123], Loss: 1.6301\n",
            "Epoch [15/200], Batch [21/123], Loss: 2.9696\n",
            "Epoch [15/200], Batch [22/123], Loss: 1.7065\n",
            "Epoch [15/200], Batch [23/123], Loss: 2.2440\n",
            "Epoch [15/200], Batch [24/123], Loss: 1.5643\n",
            "Epoch [15/200], Batch [25/123], Loss: 1.6180\n",
            "Epoch [15/200], Batch [26/123], Loss: 1.6089\n",
            "Epoch [15/200], Batch [27/123], Loss: 1.8341\n",
            "Epoch [15/200], Batch [28/123], Loss: 1.3209\n",
            "Epoch [15/200], Batch [29/123], Loss: 2.2419\n",
            "Epoch [15/200], Batch [30/123], Loss: 1.4793\n",
            "Epoch [15/200], Batch [31/123], Loss: 1.1705\n",
            "Epoch [15/200], Batch [32/123], Loss: 1.5956\n",
            "Epoch [15/200], Batch [33/123], Loss: 2.3814\n",
            "Epoch [15/200], Batch [34/123], Loss: 1.9412\n",
            "Epoch [15/200], Batch [35/123], Loss: 2.6291\n",
            "Epoch [15/200], Batch [36/123], Loss: 1.6003\n",
            "Epoch [15/200], Batch [37/123], Loss: 1.7182\n",
            "Epoch [15/200], Batch [38/123], Loss: 1.4608\n",
            "Epoch [15/200], Batch [39/123], Loss: 1.7157\n",
            "Epoch [15/200], Batch [40/123], Loss: 2.0989\n",
            "Epoch [15/200], Batch [41/123], Loss: 1.4521\n",
            "Epoch [15/200], Batch [42/123], Loss: 1.8745\n",
            "Epoch [15/200], Batch [43/123], Loss: 1.5718\n",
            "Epoch [15/200], Batch [44/123], Loss: 1.7608\n",
            "Epoch [15/200], Batch [45/123], Loss: 1.4953\n",
            "Epoch [15/200], Batch [46/123], Loss: 2.1313\n",
            "Epoch [15/200], Batch [47/123], Loss: 2.8440\n",
            "Epoch [15/200], Batch [48/123], Loss: 2.0728\n",
            "Epoch [15/200], Batch [49/123], Loss: 2.1743\n",
            "Epoch [15/200], Batch [50/123], Loss: 1.5690\n",
            "Epoch [15/200], Batch [51/123], Loss: 1.8548\n",
            "Epoch [15/200], Batch [52/123], Loss: 1.4308\n",
            "Epoch [15/200], Batch [53/123], Loss: 2.0127\n",
            "Epoch [15/200], Batch [54/123], Loss: 2.4492\n",
            "Epoch [15/200], Batch [55/123], Loss: 1.8315\n",
            "Epoch [15/200], Batch [56/123], Loss: 4.0605\n",
            "Epoch [15/200], Batch [57/123], Loss: 1.9867\n",
            "Epoch [15/200], Batch [58/123], Loss: 2.4337\n",
            "Epoch [15/200], Batch [59/123], Loss: 2.3982\n",
            "Epoch [15/200], Batch [60/123], Loss: 2.0148\n",
            "Epoch [15/200], Batch [61/123], Loss: 2.5664\n",
            "Epoch [15/200], Batch [62/123], Loss: 2.0111\n",
            "Epoch [15/200], Batch [63/123], Loss: 2.6074\n",
            "Epoch [15/200], Batch [64/123], Loss: 1.4404\n",
            "Epoch [15/200], Batch [65/123], Loss: 1.8191\n",
            "Epoch [15/200], Batch [66/123], Loss: 1.9840\n",
            "Epoch [15/200], Batch [67/123], Loss: 1.8005\n",
            "Epoch [15/200], Batch [68/123], Loss: 1.6392\n",
            "Epoch [15/200], Batch [69/123], Loss: 2.0324\n",
            "Epoch [15/200], Batch [70/123], Loss: 1.2948\n",
            "Epoch [15/200], Batch [71/123], Loss: 1.6003\n",
            "Epoch [15/200], Batch [72/123], Loss: 2.4961\n",
            "Epoch [15/200], Batch [73/123], Loss: 2.0551\n",
            "Epoch [15/200], Batch [74/123], Loss: 1.9037\n",
            "Epoch [15/200], Batch [75/123], Loss: 2.3441\n",
            "Epoch [15/200], Batch [76/123], Loss: 2.0518\n",
            "Epoch [15/200], Batch [77/123], Loss: 1.4196\n",
            "Epoch [15/200], Batch [78/123], Loss: 1.5964\n",
            "Epoch [15/200], Batch [79/123], Loss: 1.8221\n",
            "Epoch [15/200], Batch [80/123], Loss: 2.2237\n",
            "Epoch [15/200], Batch [81/123], Loss: 1.3924\n",
            "Epoch [15/200], Batch [82/123], Loss: 1.5042\n",
            "Epoch [15/200], Batch [83/123], Loss: 1.1884\n",
            "Epoch [15/200], Batch [84/123], Loss: 1.3658\n",
            "Epoch [15/200], Batch [85/123], Loss: 2.1353\n",
            "Epoch [15/200], Batch [86/123], Loss: 2.3891\n",
            "Epoch [15/200], Batch [87/123], Loss: 1.2408\n",
            "Epoch [15/200], Batch [88/123], Loss: 1.2608\n",
            "Epoch [15/200], Batch [89/123], Loss: 2.0112\n",
            "Epoch [15/200], Batch [90/123], Loss: 1.7805\n",
            "Epoch [15/200], Batch [91/123], Loss: 1.7233\n",
            "Epoch [15/200], Batch [92/123], Loss: 1.6593\n",
            "Epoch [15/200], Batch [93/123], Loss: 1.6300\n",
            "Epoch [15/200], Batch [94/123], Loss: 1.7168\n",
            "Epoch [15/200], Batch [95/123], Loss: 1.6024\n",
            "Epoch [15/200], Batch [96/123], Loss: 1.6763\n",
            "Epoch [15/200], Batch [97/123], Loss: 1.5184\n",
            "Epoch [15/200], Batch [98/123], Loss: 1.8345\n",
            "Epoch [15/200], Batch [99/123], Loss: 1.4663\n",
            "Epoch [15/200], Batch [100/123], Loss: 3.0478\n",
            "Epoch [15/200], Batch [101/123], Loss: 2.0239\n",
            "Epoch [15/200], Batch [102/123], Loss: 1.6464\n",
            "Epoch [15/200], Batch [103/123], Loss: 1.6272\n",
            "Epoch [15/200], Batch [104/123], Loss: 2.2338\n",
            "Epoch [15/200], Batch [105/123], Loss: 1.6970\n",
            "Epoch [15/200], Batch [106/123], Loss: 1.4568\n",
            "Epoch [15/200], Batch [107/123], Loss: 1.9379\n",
            "Epoch [15/200], Batch [108/123], Loss: 2.1945\n",
            "Epoch [15/200], Batch [109/123], Loss: 1.2893\n",
            "Epoch [15/200], Batch [110/123], Loss: 2.6683\n",
            "Epoch [15/200], Batch [111/123], Loss: 1.3019\n",
            "Epoch [15/200], Batch [112/123], Loss: 1.3783\n",
            "Epoch [15/200], Batch [113/123], Loss: 1.7838\n",
            "Epoch [15/200], Batch [114/123], Loss: 1.3824\n",
            "Epoch [15/200], Batch [115/123], Loss: 1.5791\n",
            "Epoch [15/200], Batch [116/123], Loss: 1.1081\n",
            "Epoch [15/200], Batch [117/123], Loss: 2.3822\n",
            "Epoch [15/200], Batch [118/123], Loss: 1.4421\n",
            "Epoch [15/200], Batch [119/123], Loss: 1.6056\n",
            "Epoch [15/200], Batch [120/123], Loss: 1.6757\n",
            "Epoch [15/200], Batch [121/123], Loss: 1.7685\n",
            "Epoch [15/200], Batch [122/123], Loss: 1.5192\n",
            "Epoch [15/200], Batch [123/123], Loss: 1.4054\n",
            "Epoch [16/200], Batch [1/123], Loss: 2.2884\n",
            "Epoch [16/200], Batch [2/123], Loss: 1.9450\n",
            "Epoch [16/200], Batch [3/123], Loss: 1.8435\n",
            "Epoch [16/200], Batch [4/123], Loss: 1.2136\n",
            "Epoch [16/200], Batch [5/123], Loss: 1.5808\n",
            "Epoch [16/200], Batch [6/123], Loss: 1.4342\n",
            "Epoch [16/200], Batch [7/123], Loss: 2.1899\n",
            "Epoch [16/200], Batch [8/123], Loss: 1.7928\n",
            "Epoch [16/200], Batch [9/123], Loss: 2.2849\n",
            "Epoch [16/200], Batch [10/123], Loss: 1.8183\n",
            "Epoch [16/200], Batch [11/123], Loss: 1.2451\n",
            "Epoch [16/200], Batch [12/123], Loss: 2.2683\n",
            "Epoch [16/200], Batch [13/123], Loss: 1.6267\n",
            "Epoch [16/200], Batch [14/123], Loss: 1.4555\n",
            "Epoch [16/200], Batch [15/123], Loss: 1.6951\n",
            "Epoch [16/200], Batch [16/123], Loss: 2.0800\n",
            "Epoch [16/200], Batch [17/123], Loss: 2.0105\n",
            "Epoch [16/200], Batch [18/123], Loss: 1.3303\n",
            "Epoch [16/200], Batch [19/123], Loss: 1.5782\n",
            "Epoch [16/200], Batch [20/123], Loss: 1.4291\n",
            "Epoch [16/200], Batch [21/123], Loss: 1.5739\n",
            "Epoch [16/200], Batch [22/123], Loss: 1.9831\n",
            "Epoch [16/200], Batch [23/123], Loss: 2.1029\n",
            "Epoch [16/200], Batch [24/123], Loss: 2.0342\n",
            "Epoch [16/200], Batch [25/123], Loss: 1.9451\n",
            "Epoch [16/200], Batch [26/123], Loss: 1.6088\n",
            "Epoch [16/200], Batch [27/123], Loss: 3.1237\n",
            "Epoch [16/200], Batch [28/123], Loss: 3.5078\n",
            "Epoch [16/200], Batch [29/123], Loss: 2.7203\n",
            "Epoch [16/200], Batch [30/123], Loss: 1.8355\n",
            "Epoch [16/200], Batch [31/123], Loss: 2.6443\n",
            "Epoch [16/200], Batch [32/123], Loss: 1.5642\n",
            "Epoch [16/200], Batch [33/123], Loss: 1.5969\n",
            "Epoch [16/200], Batch [34/123], Loss: 0.9206\n",
            "Epoch [16/200], Batch [35/123], Loss: 1.8388\n",
            "Epoch [16/200], Batch [36/123], Loss: 1.4690\n",
            "Epoch [16/200], Batch [37/123], Loss: 1.4822\n",
            "Epoch [16/200], Batch [38/123], Loss: 1.8821\n",
            "Epoch [16/200], Batch [39/123], Loss: 1.5164\n",
            "Epoch [16/200], Batch [40/123], Loss: 2.4334\n",
            "Epoch [16/200], Batch [41/123], Loss: 1.1507\n",
            "Epoch [16/200], Batch [42/123], Loss: 1.7460\n",
            "Epoch [16/200], Batch [43/123], Loss: 1.5305\n",
            "Epoch [16/200], Batch [44/123], Loss: 1.7475\n",
            "Epoch [16/200], Batch [45/123], Loss: 1.6371\n",
            "Epoch [16/200], Batch [46/123], Loss: 1.4829\n",
            "Epoch [16/200], Batch [47/123], Loss: 2.0123\n",
            "Epoch [16/200], Batch [48/123], Loss: 1.2564\n",
            "Epoch [16/200], Batch [49/123], Loss: 1.7835\n",
            "Epoch [16/200], Batch [50/123], Loss: 1.4522\n",
            "Epoch [16/200], Batch [51/123], Loss: 1.7854\n",
            "Epoch [16/200], Batch [52/123], Loss: 1.6118\n",
            "Epoch [16/200], Batch [53/123], Loss: 1.8782\n",
            "Epoch [16/200], Batch [54/123], Loss: 1.6230\n",
            "Epoch [16/200], Batch [55/123], Loss: 1.7416\n",
            "Epoch [16/200], Batch [56/123], Loss: 1.9262\n",
            "Epoch [16/200], Batch [57/123], Loss: 2.4059\n",
            "Epoch [16/200], Batch [58/123], Loss: 1.2902\n",
            "Epoch [16/200], Batch [59/123], Loss: 2.7184\n",
            "Epoch [16/200], Batch [60/123], Loss: 1.6383\n",
            "Epoch [16/200], Batch [61/123], Loss: 1.7166\n",
            "Epoch [16/200], Batch [62/123], Loss: 1.7536\n",
            "Epoch [16/200], Batch [63/123], Loss: 1.7101\n",
            "Epoch [16/200], Batch [64/123], Loss: 1.0498\n",
            "Epoch [16/200], Batch [65/123], Loss: 1.7537\n",
            "Epoch [16/200], Batch [66/123], Loss: 2.0009\n",
            "Epoch [16/200], Batch [67/123], Loss: 2.0343\n",
            "Epoch [16/200], Batch [68/123], Loss: 1.7788\n",
            "Epoch [16/200], Batch [69/123], Loss: 1.2661\n",
            "Epoch [16/200], Batch [70/123], Loss: 1.5319\n",
            "Epoch [16/200], Batch [71/123], Loss: 1.1807\n",
            "Epoch [16/200], Batch [72/123], Loss: 1.7041\n",
            "Epoch [16/200], Batch [73/123], Loss: 1.2167\n",
            "Epoch [16/200], Batch [74/123], Loss: 1.5249\n",
            "Epoch [16/200], Batch [75/123], Loss: 1.8728\n",
            "Epoch [16/200], Batch [76/123], Loss: 1.6226\n",
            "Epoch [16/200], Batch [77/123], Loss: 1.3742\n",
            "Epoch [16/200], Batch [78/123], Loss: 1.4394\n",
            "Epoch [16/200], Batch [79/123], Loss: 1.2269\n",
            "Epoch [16/200], Batch [80/123], Loss: 1.1981\n",
            "Epoch [16/200], Batch [81/123], Loss: 1.2007\n",
            "Epoch [16/200], Batch [82/123], Loss: 1.5322\n",
            "Epoch [16/200], Batch [83/123], Loss: 1.8371\n",
            "Epoch [16/200], Batch [84/123], Loss: 1.7336\n",
            "Epoch [16/200], Batch [85/123], Loss: 2.4298\n",
            "Epoch [16/200], Batch [86/123], Loss: 1.4771\n",
            "Epoch [16/200], Batch [87/123], Loss: 1.9898\n",
            "Epoch [16/200], Batch [88/123], Loss: 2.3120\n",
            "Epoch [16/200], Batch [89/123], Loss: 1.2660\n",
            "Epoch [16/200], Batch [90/123], Loss: 1.7816\n",
            "Epoch [16/200], Batch [91/123], Loss: 2.4738\n",
            "Epoch [16/200], Batch [92/123], Loss: 1.2362\n",
            "Epoch [16/200], Batch [93/123], Loss: 1.9821\n",
            "Epoch [16/200], Batch [94/123], Loss: 1.4696\n",
            "Epoch [16/200], Batch [95/123], Loss: 2.0235\n",
            "Epoch [16/200], Batch [96/123], Loss: 2.4164\n",
            "Epoch [16/200], Batch [97/123], Loss: 1.6707\n",
            "Epoch [16/200], Batch [98/123], Loss: 1.7749\n",
            "Epoch [16/200], Batch [99/123], Loss: 1.9170\n",
            "Epoch [16/200], Batch [100/123], Loss: 1.0898\n",
            "Epoch [16/200], Batch [101/123], Loss: 1.5546\n",
            "Epoch [16/200], Batch [102/123], Loss: 1.6642\n",
            "Epoch [16/200], Batch [103/123], Loss: 1.5169\n",
            "Epoch [16/200], Batch [104/123], Loss: 2.0555\n",
            "Epoch [16/200], Batch [105/123], Loss: 1.9007\n",
            "Epoch [16/200], Batch [106/123], Loss: 2.1083\n",
            "Epoch [16/200], Batch [107/123], Loss: 2.1597\n",
            "Epoch [16/200], Batch [108/123], Loss: 1.5904\n",
            "Epoch [16/200], Batch [109/123], Loss: 1.7629\n",
            "Epoch [16/200], Batch [110/123], Loss: 1.0438\n",
            "Epoch [16/200], Batch [111/123], Loss: 1.8759\n",
            "Epoch [16/200], Batch [112/123], Loss: 1.5127\n",
            "Epoch [16/200], Batch [113/123], Loss: 1.6413\n",
            "Epoch [16/200], Batch [114/123], Loss: 1.3050\n",
            "Epoch [16/200], Batch [115/123], Loss: 1.5193\n",
            "Epoch [16/200], Batch [116/123], Loss: 2.3623\n",
            "Epoch [16/200], Batch [117/123], Loss: 1.6072\n",
            "Epoch [16/200], Batch [118/123], Loss: 2.4402\n",
            "Epoch [16/200], Batch [119/123], Loss: 1.8256\n",
            "Epoch [16/200], Batch [120/123], Loss: 1.6399\n",
            "Epoch [16/200], Batch [121/123], Loss: 2.6314\n",
            "Epoch [16/200], Batch [122/123], Loss: 1.8404\n",
            "Epoch [16/200], Batch [123/123], Loss: 2.9889\n",
            "Epoch [17/200], Batch [1/123], Loss: 1.4540\n",
            "Epoch [17/200], Batch [2/123], Loss: 1.3762\n",
            "Epoch [17/200], Batch [3/123], Loss: 1.9756\n",
            "Epoch [17/200], Batch [4/123], Loss: 1.6896\n",
            "Epoch [17/200], Batch [5/123], Loss: 2.5492\n",
            "Epoch [17/200], Batch [6/123], Loss: 2.1746\n",
            "Epoch [17/200], Batch [7/123], Loss: 1.5889\n",
            "Epoch [17/200], Batch [8/123], Loss: 2.2314\n",
            "Epoch [17/200], Batch [9/123], Loss: 1.2365\n",
            "Epoch [17/200], Batch [10/123], Loss: 2.0776\n",
            "Epoch [17/200], Batch [11/123], Loss: 2.4948\n",
            "Epoch [17/200], Batch [12/123], Loss: 1.9465\n",
            "Epoch [17/200], Batch [13/123], Loss: 1.1991\n",
            "Epoch [17/200], Batch [14/123], Loss: 1.7193\n",
            "Epoch [17/200], Batch [15/123], Loss: 1.3892\n",
            "Epoch [17/200], Batch [16/123], Loss: 1.2951\n",
            "Epoch [17/200], Batch [17/123], Loss: 2.1459\n",
            "Epoch [17/200], Batch [18/123], Loss: 1.8022\n",
            "Epoch [17/200], Batch [19/123], Loss: 1.7287\n",
            "Epoch [17/200], Batch [20/123], Loss: 1.5856\n",
            "Epoch [17/200], Batch [21/123], Loss: 1.5784\n",
            "Epoch [17/200], Batch [22/123], Loss: 1.8338\n",
            "Epoch [17/200], Batch [23/123], Loss: 1.5209\n",
            "Epoch [17/200], Batch [24/123], Loss: 1.8660\n",
            "Epoch [17/200], Batch [25/123], Loss: 1.4557\n",
            "Epoch [17/200], Batch [26/123], Loss: 2.4386\n",
            "Epoch [17/200], Batch [27/123], Loss: 1.9359\n",
            "Epoch [17/200], Batch [28/123], Loss: 1.8280\n",
            "Epoch [17/200], Batch [29/123], Loss: 1.9426\n",
            "Epoch [17/200], Batch [30/123], Loss: 1.8888\n",
            "Epoch [17/200], Batch [31/123], Loss: 1.8604\n",
            "Epoch [17/200], Batch [32/123], Loss: 2.1131\n",
            "Epoch [17/200], Batch [33/123], Loss: 1.9898\n",
            "Epoch [17/200], Batch [34/123], Loss: 2.1087\n",
            "Epoch [17/200], Batch [35/123], Loss: 1.8218\n",
            "Epoch [17/200], Batch [36/123], Loss: 2.0157\n",
            "Epoch [17/200], Batch [37/123], Loss: 1.7997\n",
            "Epoch [17/200], Batch [38/123], Loss: 1.5193\n",
            "Epoch [17/200], Batch [39/123], Loss: 1.6886\n",
            "Epoch [17/200], Batch [40/123], Loss: 1.3887\n",
            "Epoch [17/200], Batch [41/123], Loss: 1.9817\n",
            "Epoch [17/200], Batch [42/123], Loss: 2.0056\n",
            "Epoch [17/200], Batch [43/123], Loss: 1.7379\n",
            "Epoch [17/200], Batch [44/123], Loss: 2.1143\n",
            "Epoch [17/200], Batch [45/123], Loss: 2.1300\n",
            "Epoch [17/200], Batch [46/123], Loss: 1.5778\n",
            "Epoch [17/200], Batch [47/123], Loss: 1.9677\n",
            "Epoch [17/200], Batch [48/123], Loss: 2.0994\n",
            "Epoch [17/200], Batch [49/123], Loss: 1.7086\n",
            "Epoch [17/200], Batch [50/123], Loss: 1.8937\n",
            "Epoch [17/200], Batch [51/123], Loss: 1.9058\n",
            "Epoch [17/200], Batch [52/123], Loss: 2.3800\n",
            "Epoch [17/200], Batch [53/123], Loss: 1.2833\n",
            "Epoch [17/200], Batch [54/123], Loss: 1.3007\n",
            "Epoch [17/200], Batch [55/123], Loss: 2.4174\n",
            "Epoch [17/200], Batch [56/123], Loss: 1.7166\n",
            "Epoch [17/200], Batch [57/123], Loss: 1.5298\n",
            "Epoch [17/200], Batch [58/123], Loss: 1.8450\n",
            "Epoch [17/200], Batch [59/123], Loss: 2.0550\n",
            "Epoch [17/200], Batch [60/123], Loss: 1.3433\n",
            "Epoch [17/200], Batch [61/123], Loss: 1.8214\n",
            "Epoch [17/200], Batch [62/123], Loss: 1.7775\n",
            "Epoch [17/200], Batch [63/123], Loss: 1.4847\n",
            "Epoch [17/200], Batch [64/123], Loss: 1.9483\n",
            "Epoch [17/200], Batch [65/123], Loss: 2.5819\n",
            "Epoch [17/200], Batch [66/123], Loss: 1.6926\n",
            "Epoch [17/200], Batch [67/123], Loss: 1.3797\n",
            "Epoch [17/200], Batch [68/123], Loss: 2.2312\n",
            "Epoch [17/200], Batch [69/123], Loss: 1.6421\n",
            "Epoch [17/200], Batch [70/123], Loss: 1.6310\n",
            "Epoch [17/200], Batch [71/123], Loss: 1.9888\n",
            "Epoch [17/200], Batch [72/123], Loss: 1.6790\n",
            "Epoch [17/200], Batch [73/123], Loss: 1.9952\n",
            "Epoch [17/200], Batch [74/123], Loss: 2.0940\n",
            "Epoch [17/200], Batch [75/123], Loss: 1.4332\n",
            "Epoch [17/200], Batch [76/123], Loss: 1.9823\n",
            "Epoch [17/200], Batch [77/123], Loss: 1.5276\n",
            "Epoch [17/200], Batch [78/123], Loss: 2.4306\n",
            "Epoch [17/200], Batch [79/123], Loss: 1.8686\n",
            "Epoch [17/200], Batch [80/123], Loss: 2.1574\n",
            "Epoch [17/200], Batch [81/123], Loss: 1.5734\n",
            "Epoch [17/200], Batch [82/123], Loss: 2.0199\n",
            "Epoch [17/200], Batch [83/123], Loss: 1.8863\n",
            "Epoch [17/200], Batch [84/123], Loss: 1.6111\n",
            "Epoch [17/200], Batch [85/123], Loss: 1.6285\n",
            "Epoch [17/200], Batch [86/123], Loss: 1.1066\n",
            "Epoch [17/200], Batch [87/123], Loss: 2.7732\n",
            "Epoch [17/200], Batch [88/123], Loss: 2.1369\n",
            "Epoch [17/200], Batch [89/123], Loss: 2.4701\n",
            "Epoch [17/200], Batch [90/123], Loss: 1.5699\n",
            "Epoch [17/200], Batch [91/123], Loss: 3.2435\n",
            "Epoch [17/200], Batch [92/123], Loss: 1.9699\n",
            "Epoch [17/200], Batch [93/123], Loss: 2.4665\n",
            "Epoch [17/200], Batch [94/123], Loss: 2.0121\n",
            "Epoch [17/200], Batch [95/123], Loss: 2.3658\n",
            "Epoch [17/200], Batch [96/123], Loss: 2.1366\n",
            "Epoch [17/200], Batch [97/123], Loss: 1.6279\n",
            "Epoch [17/200], Batch [98/123], Loss: 2.7487\n",
            "Epoch [17/200], Batch [99/123], Loss: 1.2709\n",
            "Epoch [17/200], Batch [100/123], Loss: 1.9735\n",
            "Epoch [17/200], Batch [101/123], Loss: 1.7386\n",
            "Epoch [17/200], Batch [102/123], Loss: 1.8372\n",
            "Epoch [17/200], Batch [103/123], Loss: 1.6685\n",
            "Epoch [17/200], Batch [104/123], Loss: 1.6611\n",
            "Epoch [17/200], Batch [105/123], Loss: 1.6893\n",
            "Epoch [17/200], Batch [106/123], Loss: 1.6621\n",
            "Epoch [17/200], Batch [107/123], Loss: 1.9243\n",
            "Epoch [17/200], Batch [108/123], Loss: 1.8239\n",
            "Epoch [17/200], Batch [109/123], Loss: 1.3214\n",
            "Epoch [17/200], Batch [110/123], Loss: 1.5675\n",
            "Epoch [17/200], Batch [111/123], Loss: 1.6698\n",
            "Epoch [17/200], Batch [112/123], Loss: 1.7722\n",
            "Epoch [17/200], Batch [113/123], Loss: 1.5565\n",
            "Epoch [17/200], Batch [114/123], Loss: 1.9566\n",
            "Epoch [17/200], Batch [115/123], Loss: 1.4265\n",
            "Epoch [17/200], Batch [116/123], Loss: 1.2469\n",
            "Epoch [17/200], Batch [117/123], Loss: 1.6080\n",
            "Epoch [17/200], Batch [118/123], Loss: 1.3142\n",
            "Epoch [17/200], Batch [119/123], Loss: 1.6974\n",
            "Epoch [17/200], Batch [120/123], Loss: 1.2790\n",
            "Epoch [17/200], Batch [121/123], Loss: 1.8906\n",
            "Epoch [17/200], Batch [122/123], Loss: 1.3810\n",
            "Epoch [17/200], Batch [123/123], Loss: 1.4516\n",
            "Epoch [18/200], Batch [1/123], Loss: 1.6694\n",
            "Epoch [18/200], Batch [2/123], Loss: 1.5400\n",
            "Epoch [18/200], Batch [3/123], Loss: 2.5929\n",
            "Epoch [18/200], Batch [4/123], Loss: 1.5567\n",
            "Epoch [18/200], Batch [5/123], Loss: 1.3713\n",
            "Epoch [18/200], Batch [6/123], Loss: 2.2659\n",
            "Epoch [18/200], Batch [7/123], Loss: 1.7804\n",
            "Epoch [18/200], Batch [8/123], Loss: 1.6935\n",
            "Epoch [18/200], Batch [9/123], Loss: 1.9627\n",
            "Epoch [18/200], Batch [10/123], Loss: 1.6886\n",
            "Epoch [18/200], Batch [11/123], Loss: 0.8979\n",
            "Epoch [18/200], Batch [12/123], Loss: 1.5506\n",
            "Epoch [18/200], Batch [13/123], Loss: 1.5228\n",
            "Epoch [18/200], Batch [14/123], Loss: 2.1632\n",
            "Epoch [18/200], Batch [15/123], Loss: 1.4180\n",
            "Epoch [18/200], Batch [16/123], Loss: 1.1043\n",
            "Epoch [18/200], Batch [17/123], Loss: 1.9135\n",
            "Epoch [18/200], Batch [18/123], Loss: 1.5749\n",
            "Epoch [18/200], Batch [19/123], Loss: 1.4727\n",
            "Epoch [18/200], Batch [20/123], Loss: 1.5251\n",
            "Epoch [18/200], Batch [21/123], Loss: 2.2419\n",
            "Epoch [18/200], Batch [22/123], Loss: 2.0046\n",
            "Epoch [18/200], Batch [23/123], Loss: 1.4499\n",
            "Epoch [18/200], Batch [24/123], Loss: 1.9825\n",
            "Epoch [18/200], Batch [25/123], Loss: 1.4261\n",
            "Epoch [18/200], Batch [26/123], Loss: 2.2851\n",
            "Epoch [18/200], Batch [27/123], Loss: 1.8663\n",
            "Epoch [18/200], Batch [28/123], Loss: 1.9012\n",
            "Epoch [18/200], Batch [29/123], Loss: 1.4854\n",
            "Epoch [18/200], Batch [30/123], Loss: 1.5357\n",
            "Epoch [18/200], Batch [31/123], Loss: 1.3069\n",
            "Epoch [18/200], Batch [32/123], Loss: 2.0177\n",
            "Epoch [18/200], Batch [33/123], Loss: 1.3716\n",
            "Epoch [18/200], Batch [34/123], Loss: 1.3742\n",
            "Epoch [18/200], Batch [35/123], Loss: 1.6538\n",
            "Epoch [18/200], Batch [36/123], Loss: 2.0876\n",
            "Epoch [18/200], Batch [37/123], Loss: 1.1340\n",
            "Epoch [18/200], Batch [38/123], Loss: 1.4515\n",
            "Epoch [18/200], Batch [39/123], Loss: 1.5413\n",
            "Epoch [18/200], Batch [40/123], Loss: 1.6255\n",
            "Epoch [18/200], Batch [41/123], Loss: 1.5318\n",
            "Epoch [18/200], Batch [42/123], Loss: 1.2408\n",
            "Epoch [18/200], Batch [43/123], Loss: 2.3951\n",
            "Epoch [18/200], Batch [44/123], Loss: 1.2486\n",
            "Epoch [18/200], Batch [45/123], Loss: 1.3548\n",
            "Epoch [18/200], Batch [46/123], Loss: 1.7851\n",
            "Epoch [18/200], Batch [47/123], Loss: 1.8419\n",
            "Epoch [18/200], Batch [48/123], Loss: 1.9383\n",
            "Epoch [18/200], Batch [49/123], Loss: 1.2955\n",
            "Epoch [18/200], Batch [50/123], Loss: 1.2051\n",
            "Epoch [18/200], Batch [51/123], Loss: 1.5768\n",
            "Epoch [18/200], Batch [52/123], Loss: 1.4321\n",
            "Epoch [18/200], Batch [53/123], Loss: 1.9444\n",
            "Epoch [18/200], Batch [54/123], Loss: 1.4626\n",
            "Epoch [18/200], Batch [55/123], Loss: 1.9242\n",
            "Epoch [18/200], Batch [56/123], Loss: 1.6564\n",
            "Epoch [18/200], Batch [57/123], Loss: 1.5557\n",
            "Epoch [18/200], Batch [58/123], Loss: 1.4761\n",
            "Epoch [18/200], Batch [59/123], Loss: 0.8568\n",
            "Epoch [18/200], Batch [60/123], Loss: 1.5384\n",
            "Epoch [18/200], Batch [61/123], Loss: 2.0058\n",
            "Epoch [18/200], Batch [62/123], Loss: 1.7939\n",
            "Epoch [18/200], Batch [63/123], Loss: 1.9998\n",
            "Epoch [18/200], Batch [64/123], Loss: 1.5351\n",
            "Epoch [18/200], Batch [65/123], Loss: 1.4137\n",
            "Epoch [18/200], Batch [66/123], Loss: 1.7825\n",
            "Epoch [18/200], Batch [67/123], Loss: 1.8522\n",
            "Epoch [18/200], Batch [68/123], Loss: 1.4393\n",
            "Epoch [18/200], Batch [69/123], Loss: 1.5449\n",
            "Epoch [18/200], Batch [70/123], Loss: 1.3644\n",
            "Epoch [18/200], Batch [71/123], Loss: 1.5193\n",
            "Epoch [18/200], Batch [72/123], Loss: 1.4940\n",
            "Epoch [18/200], Batch [73/123], Loss: 1.9978\n",
            "Epoch [18/200], Batch [74/123], Loss: 1.4681\n",
            "Epoch [18/200], Batch [75/123], Loss: 1.2095\n",
            "Epoch [18/200], Batch [76/123], Loss: 2.3160\n",
            "Epoch [18/200], Batch [77/123], Loss: 1.8179\n",
            "Epoch [18/200], Batch [78/123], Loss: 1.4158\n",
            "Epoch [18/200], Batch [79/123], Loss: 1.7901\n",
            "Epoch [18/200], Batch [80/123], Loss: 1.5337\n",
            "Epoch [18/200], Batch [81/123], Loss: 1.6405\n",
            "Epoch [18/200], Batch [82/123], Loss: 1.2686\n",
            "Epoch [18/200], Batch [83/123], Loss: 2.0488\n",
            "Epoch [18/200], Batch [84/123], Loss: 1.8884\n",
            "Epoch [18/200], Batch [85/123], Loss: 1.4199\n",
            "Epoch [18/200], Batch [86/123], Loss: 1.4897\n",
            "Epoch [18/200], Batch [87/123], Loss: 2.0079\n",
            "Epoch [18/200], Batch [88/123], Loss: 1.3733\n",
            "Epoch [18/200], Batch [89/123], Loss: 1.6152\n",
            "Epoch [18/200], Batch [90/123], Loss: 1.9697\n",
            "Epoch [18/200], Batch [91/123], Loss: 1.9348\n",
            "Epoch [18/200], Batch [92/123], Loss: 1.7284\n",
            "Epoch [18/200], Batch [93/123], Loss: 1.1979\n",
            "Epoch [18/200], Batch [94/123], Loss: 1.1843\n",
            "Epoch [18/200], Batch [95/123], Loss: 2.1680\n",
            "Epoch [18/200], Batch [96/123], Loss: 1.3251\n",
            "Epoch [18/200], Batch [97/123], Loss: 1.6046\n",
            "Epoch [18/200], Batch [98/123], Loss: 1.9926\n",
            "Epoch [18/200], Batch [99/123], Loss: 1.4974\n",
            "Epoch [18/200], Batch [100/123], Loss: 1.5174\n",
            "Epoch [18/200], Batch [101/123], Loss: 1.3543\n",
            "Epoch [18/200], Batch [102/123], Loss: 1.4969\n",
            "Epoch [18/200], Batch [103/123], Loss: 1.3752\n",
            "Epoch [18/200], Batch [104/123], Loss: 1.5315\n",
            "Epoch [18/200], Batch [105/123], Loss: 2.3431\n",
            "Epoch [18/200], Batch [106/123], Loss: 1.4856\n",
            "Epoch [18/200], Batch [107/123], Loss: 1.3357\n",
            "Epoch [18/200], Batch [108/123], Loss: 1.4140\n",
            "Epoch [18/200], Batch [109/123], Loss: 1.7263\n",
            "Epoch [18/200], Batch [110/123], Loss: 1.9725\n",
            "Epoch [18/200], Batch [111/123], Loss: 1.2979\n",
            "Epoch [18/200], Batch [112/123], Loss: 1.5359\n",
            "Epoch [18/200], Batch [113/123], Loss: 2.5509\n",
            "Epoch [18/200], Batch [114/123], Loss: 1.7886\n",
            "Epoch [18/200], Batch [115/123], Loss: 1.5862\n",
            "Epoch [18/200], Batch [116/123], Loss: 1.5798\n",
            "Epoch [18/200], Batch [117/123], Loss: 2.8620\n",
            "Epoch [18/200], Batch [118/123], Loss: 2.1498\n",
            "Epoch [18/200], Batch [119/123], Loss: 1.3788\n",
            "Epoch [18/200], Batch [120/123], Loss: 1.5616\n",
            "Epoch [18/200], Batch [121/123], Loss: 1.9757\n",
            "Epoch [18/200], Batch [122/123], Loss: 2.2828\n",
            "Epoch [18/200], Batch [123/123], Loss: 1.4947\n",
            "Epoch [19/200], Batch [1/123], Loss: 1.7817\n",
            "Epoch [19/200], Batch [2/123], Loss: 1.3940\n",
            "Epoch [19/200], Batch [3/123], Loss: 1.7299\n",
            "Epoch [19/200], Batch [4/123], Loss: 1.7484\n",
            "Epoch [19/200], Batch [5/123], Loss: 1.4724\n",
            "Epoch [19/200], Batch [6/123], Loss: 1.4895\n",
            "Epoch [19/200], Batch [7/123], Loss: 1.4231\n",
            "Epoch [19/200], Batch [8/123], Loss: 1.4876\n",
            "Epoch [19/200], Batch [9/123], Loss: 1.3606\n",
            "Epoch [19/200], Batch [10/123], Loss: 1.3711\n",
            "Epoch [19/200], Batch [11/123], Loss: 3.0274\n",
            "Epoch [19/200], Batch [12/123], Loss: 1.5405\n",
            "Epoch [19/200], Batch [13/123], Loss: 1.0658\n",
            "Epoch [19/200], Batch [14/123], Loss: 1.1678\n",
            "Epoch [19/200], Batch [15/123], Loss: 1.4945\n",
            "Epoch [19/200], Batch [16/123], Loss: 2.5243\n",
            "Epoch [19/200], Batch [17/123], Loss: 2.0157\n",
            "Epoch [19/200], Batch [18/123], Loss: 0.9362\n",
            "Epoch [19/200], Batch [19/123], Loss: 1.1410\n",
            "Epoch [19/200], Batch [20/123], Loss: 1.6826\n",
            "Epoch [19/200], Batch [21/123], Loss: 2.1297\n",
            "Epoch [19/200], Batch [22/123], Loss: 1.4206\n",
            "Epoch [19/200], Batch [23/123], Loss: 2.0122\n",
            "Epoch [19/200], Batch [24/123], Loss: 1.8035\n",
            "Epoch [19/200], Batch [25/123], Loss: 1.0808\n",
            "Epoch [19/200], Batch [26/123], Loss: 2.6328\n",
            "Epoch [19/200], Batch [27/123], Loss: 1.8107\n",
            "Epoch [19/200], Batch [28/123], Loss: 1.2163\n",
            "Epoch [19/200], Batch [29/123], Loss: 1.9130\n",
            "Epoch [19/200], Batch [30/123], Loss: 1.7430\n",
            "Epoch [19/200], Batch [31/123], Loss: 1.4855\n",
            "Epoch [19/200], Batch [32/123], Loss: 1.6830\n",
            "Epoch [19/200], Batch [33/123], Loss: 1.2586\n",
            "Epoch [19/200], Batch [34/123], Loss: 1.9180\n",
            "Epoch [19/200], Batch [35/123], Loss: 1.2933\n",
            "Epoch [19/200], Batch [36/123], Loss: 1.5631\n",
            "Epoch [19/200], Batch [37/123], Loss: 0.9548\n",
            "Epoch [19/200], Batch [38/123], Loss: 1.7287\n",
            "Epoch [19/200], Batch [39/123], Loss: 2.3940\n",
            "Epoch [19/200], Batch [40/123], Loss: 1.4949\n",
            "Epoch [19/200], Batch [41/123], Loss: 1.3830\n",
            "Epoch [19/200], Batch [42/123], Loss: 2.0079\n",
            "Epoch [19/200], Batch [43/123], Loss: 3.2340\n",
            "Epoch [19/200], Batch [44/123], Loss: 1.8452\n",
            "Epoch [19/200], Batch [45/123], Loss: 1.8355\n",
            "Epoch [19/200], Batch [46/123], Loss: 1.5535\n",
            "Epoch [19/200], Batch [47/123], Loss: 1.0790\n",
            "Epoch [19/200], Batch [48/123], Loss: 1.3710\n",
            "Epoch [19/200], Batch [49/123], Loss: 1.4302\n",
            "Epoch [19/200], Batch [50/123], Loss: 1.5360\n",
            "Epoch [19/200], Batch [51/123], Loss: 1.4186\n",
            "Epoch [19/200], Batch [52/123], Loss: 1.9610\n",
            "Epoch [19/200], Batch [53/123], Loss: 1.4862\n",
            "Epoch [19/200], Batch [54/123], Loss: 1.5125\n",
            "Epoch [19/200], Batch [55/123], Loss: 2.1602\n",
            "Epoch [19/200], Batch [56/123], Loss: 1.9731\n",
            "Epoch [19/200], Batch [57/123], Loss: 1.5636\n",
            "Epoch [19/200], Batch [58/123], Loss: 1.5394\n",
            "Epoch [19/200], Batch [59/123], Loss: 2.5607\n",
            "Epoch [19/200], Batch [60/123], Loss: 1.4021\n",
            "Epoch [19/200], Batch [61/123], Loss: 1.7499\n",
            "Epoch [19/200], Batch [62/123], Loss: 1.3432\n",
            "Epoch [19/200], Batch [63/123], Loss: 1.4004\n",
            "Epoch [19/200], Batch [64/123], Loss: 2.1999\n",
            "Epoch [19/200], Batch [65/123], Loss: 1.8995\n",
            "Epoch [19/200], Batch [66/123], Loss: 1.5712\n",
            "Epoch [19/200], Batch [67/123], Loss: 1.5226\n",
            "Epoch [19/200], Batch [68/123], Loss: 2.1119\n",
            "Epoch [19/200], Batch [69/123], Loss: 1.4157\n",
            "Epoch [19/200], Batch [70/123], Loss: 1.6308\n",
            "Epoch [19/200], Batch [71/123], Loss: 1.4431\n",
            "Epoch [19/200], Batch [72/123], Loss: 1.6379\n",
            "Epoch [19/200], Batch [73/123], Loss: 1.3406\n",
            "Epoch [19/200], Batch [74/123], Loss: 1.2838\n",
            "Epoch [19/200], Batch [75/123], Loss: 1.4400\n",
            "Epoch [19/200], Batch [76/123], Loss: 1.7095\n",
            "Epoch [19/200], Batch [77/123], Loss: 1.8732\n",
            "Epoch [19/200], Batch [78/123], Loss: 1.0623\n",
            "Epoch [19/200], Batch [79/123], Loss: 1.4032\n",
            "Epoch [19/200], Batch [80/123], Loss: 1.3536\n",
            "Epoch [19/200], Batch [81/123], Loss: 2.3075\n",
            "Epoch [19/200], Batch [82/123], Loss: 2.2696\n",
            "Epoch [19/200], Batch [83/123], Loss: 1.3360\n",
            "Epoch [19/200], Batch [84/123], Loss: 1.9136\n",
            "Epoch [19/200], Batch [85/123], Loss: 1.6388\n",
            "Epoch [19/200], Batch [86/123], Loss: 1.6082\n",
            "Epoch [19/200], Batch [87/123], Loss: 1.7272\n",
            "Epoch [19/200], Batch [88/123], Loss: 1.2594\n",
            "Epoch [19/200], Batch [89/123], Loss: 1.4264\n",
            "Epoch [19/200], Batch [90/123], Loss: 1.8980\n",
            "Epoch [19/200], Batch [91/123], Loss: 1.4893\n",
            "Epoch [19/200], Batch [92/123], Loss: 1.6325\n",
            "Epoch [19/200], Batch [93/123], Loss: 1.6926\n",
            "Epoch [19/200], Batch [94/123], Loss: 1.5308\n",
            "Epoch [19/200], Batch [95/123], Loss: 1.7647\n",
            "Epoch [19/200], Batch [96/123], Loss: 1.8291\n",
            "Epoch [19/200], Batch [97/123], Loss: 1.8452\n",
            "Epoch [19/200], Batch [98/123], Loss: 1.4778\n",
            "Epoch [19/200], Batch [99/123], Loss: 1.8795\n",
            "Epoch [19/200], Batch [100/123], Loss: 1.7430\n",
            "Epoch [19/200], Batch [101/123], Loss: 1.4920\n",
            "Epoch [19/200], Batch [102/123], Loss: 1.4256\n",
            "Epoch [19/200], Batch [103/123], Loss: 1.4379\n",
            "Epoch [19/200], Batch [104/123], Loss: 1.4764\n",
            "Epoch [19/200], Batch [105/123], Loss: 1.6539\n",
            "Epoch [19/200], Batch [106/123], Loss: 1.1297\n",
            "Epoch [19/200], Batch [107/123], Loss: 1.2043\n",
            "Epoch [19/200], Batch [108/123], Loss: 1.5231\n",
            "Epoch [19/200], Batch [109/123], Loss: 1.3073\n",
            "Epoch [19/200], Batch [110/123], Loss: 1.5240\n",
            "Epoch [19/200], Batch [111/123], Loss: 1.4303\n",
            "Epoch [19/200], Batch [112/123], Loss: 1.0993\n",
            "Epoch [19/200], Batch [113/123], Loss: 1.1955\n",
            "Epoch [19/200], Batch [114/123], Loss: 1.7085\n",
            "Epoch [19/200], Batch [115/123], Loss: 1.9498\n",
            "Epoch [19/200], Batch [116/123], Loss: 1.4553\n",
            "Epoch [19/200], Batch [117/123], Loss: 1.5705\n",
            "Epoch [19/200], Batch [118/123], Loss: 1.6485\n",
            "Epoch [19/200], Batch [119/123], Loss: 1.6621\n",
            "Epoch [19/200], Batch [120/123], Loss: 1.4239\n",
            "Epoch [19/200], Batch [121/123], Loss: 1.4220\n",
            "Epoch [19/200], Batch [122/123], Loss: 1.6871\n",
            "Epoch [19/200], Batch [123/123], Loss: 1.8417\n",
            "Epoch [20/200], Batch [1/123], Loss: 0.9972\n",
            "Epoch [20/200], Batch [2/123], Loss: 1.7413\n",
            "Epoch [20/200], Batch [3/123], Loss: 1.3654\n",
            "Epoch [20/200], Batch [4/123], Loss: 1.5826\n",
            "Epoch [20/200], Batch [5/123], Loss: 1.4355\n",
            "Epoch [20/200], Batch [6/123], Loss: 1.3033\n",
            "Epoch [20/200], Batch [7/123], Loss: 1.6126\n",
            "Epoch [20/200], Batch [8/123], Loss: 1.7264\n",
            "Epoch [20/200], Batch [9/123], Loss: 1.2061\n",
            "Epoch [20/200], Batch [10/123], Loss: 1.0754\n",
            "Epoch [20/200], Batch [11/123], Loss: 1.5189\n",
            "Epoch [20/200], Batch [12/123], Loss: 1.2485\n",
            "Epoch [20/200], Batch [13/123], Loss: 1.2848\n",
            "Epoch [20/200], Batch [14/123], Loss: 1.8669\n",
            "Epoch [20/200], Batch [15/123], Loss: 1.1083\n",
            "Epoch [20/200], Batch [16/123], Loss: 1.5583\n",
            "Epoch [20/200], Batch [17/123], Loss: 1.7020\n",
            "Epoch [20/200], Batch [18/123], Loss: 1.6810\n",
            "Epoch [20/200], Batch [19/123], Loss: 1.3673\n",
            "Epoch [20/200], Batch [20/123], Loss: 1.6253\n",
            "Epoch [20/200], Batch [21/123], Loss: 1.6392\n",
            "Epoch [20/200], Batch [22/123], Loss: 1.6327\n",
            "Epoch [20/200], Batch [23/123], Loss: 2.0403\n",
            "Epoch [20/200], Batch [24/123], Loss: 1.1241\n",
            "Epoch [20/200], Batch [25/123], Loss: 1.5319\n",
            "Epoch [20/200], Batch [26/123], Loss: 1.5080\n",
            "Epoch [20/200], Batch [27/123], Loss: 1.4812\n",
            "Epoch [20/200], Batch [28/123], Loss: 2.1401\n",
            "Epoch [20/200], Batch [29/123], Loss: 1.1618\n",
            "Epoch [20/200], Batch [30/123], Loss: 1.3498\n",
            "Epoch [20/200], Batch [31/123], Loss: 1.5420\n",
            "Epoch [20/200], Batch [32/123], Loss: 1.1531\n",
            "Epoch [20/200], Batch [33/123], Loss: 1.7935\n",
            "Epoch [20/200], Batch [34/123], Loss: 2.0577\n",
            "Epoch [20/200], Batch [35/123], Loss: 1.2096\n",
            "Epoch [20/200], Batch [36/123], Loss: 1.6916\n",
            "Epoch [20/200], Batch [37/123], Loss: 1.5448\n",
            "Epoch [20/200], Batch [38/123], Loss: 0.8863\n",
            "Epoch [20/200], Batch [39/123], Loss: 1.4776\n",
            "Epoch [20/200], Batch [40/123], Loss: 1.6201\n",
            "Epoch [20/200], Batch [41/123], Loss: 1.9765\n",
            "Epoch [20/200], Batch [42/123], Loss: 1.4885\n",
            "Epoch [20/200], Batch [43/123], Loss: 1.0542\n",
            "Epoch [20/200], Batch [44/123], Loss: 1.7013\n",
            "Epoch [20/200], Batch [45/123], Loss: 1.2689\n",
            "Epoch [20/200], Batch [46/123], Loss: 1.7704\n",
            "Epoch [20/200], Batch [47/123], Loss: 1.3483\n",
            "Epoch [20/200], Batch [48/123], Loss: 2.4293\n",
            "Epoch [20/200], Batch [49/123], Loss: 1.4369\n",
            "Epoch [20/200], Batch [50/123], Loss: 1.7784\n",
            "Epoch [20/200], Batch [51/123], Loss: 2.0334\n",
            "Epoch [20/200], Batch [52/123], Loss: 1.3711\n",
            "Epoch [20/200], Batch [53/123], Loss: 1.3824\n",
            "Epoch [20/200], Batch [54/123], Loss: 2.0694\n",
            "Epoch [20/200], Batch [55/123], Loss: 1.6793\n",
            "Epoch [20/200], Batch [56/123], Loss: 1.8610\n",
            "Epoch [20/200], Batch [57/123], Loss: 2.4200\n",
            "Epoch [20/200], Batch [58/123], Loss: 1.4778\n",
            "Epoch [20/200], Batch [59/123], Loss: 1.5200\n",
            "Epoch [20/200], Batch [60/123], Loss: 1.3413\n",
            "Epoch [20/200], Batch [61/123], Loss: 1.6404\n",
            "Epoch [20/200], Batch [62/123], Loss: 3.7879\n",
            "Epoch [20/200], Batch [63/123], Loss: 2.3305\n",
            "Epoch [20/200], Batch [64/123], Loss: 1.7238\n",
            "Epoch [20/200], Batch [65/123], Loss: 1.5628\n",
            "Epoch [20/200], Batch [66/123], Loss: 1.8184\n",
            "Epoch [20/200], Batch [67/123], Loss: 1.6085\n",
            "Epoch [20/200], Batch [68/123], Loss: 2.2600\n",
            "Epoch [20/200], Batch [69/123], Loss: 2.3421\n",
            "Epoch [20/200], Batch [70/123], Loss: 1.4510\n",
            "Epoch [20/200], Batch [71/123], Loss: 1.0669\n",
            "Epoch [20/200], Batch [72/123], Loss: 1.8416\n",
            "Epoch [20/200], Batch [73/123], Loss: 1.9822\n",
            "Epoch [20/200], Batch [74/123], Loss: 2.0699\n",
            "Epoch [20/200], Batch [75/123], Loss: 1.5860\n",
            "Epoch [20/200], Batch [76/123], Loss: 1.1794\n",
            "Epoch [20/200], Batch [77/123], Loss: 2.2971\n",
            "Epoch [20/200], Batch [78/123], Loss: 1.7424\n",
            "Epoch [20/200], Batch [79/123], Loss: 1.5237\n",
            "Epoch [20/200], Batch [80/123], Loss: 1.8627\n",
            "Epoch [20/200], Batch [81/123], Loss: 1.2389\n",
            "Epoch [20/200], Batch [82/123], Loss: 1.6676\n",
            "Epoch [20/200], Batch [83/123], Loss: 1.7301\n",
            "Epoch [20/200], Batch [84/123], Loss: 2.0002\n",
            "Epoch [20/200], Batch [85/123], Loss: 1.4060\n",
            "Epoch [20/200], Batch [86/123], Loss: 1.4997\n",
            "Epoch [20/200], Batch [87/123], Loss: 1.6529\n",
            "Epoch [20/200], Batch [88/123], Loss: 1.5565\n",
            "Epoch [20/200], Batch [89/123], Loss: 1.7173\n",
            "Epoch [20/200], Batch [90/123], Loss: 1.6191\n",
            "Epoch [20/200], Batch [91/123], Loss: 1.9040\n",
            "Epoch [20/200], Batch [92/123], Loss: 1.7698\n",
            "Epoch [20/200], Batch [93/123], Loss: 1.7866\n",
            "Epoch [20/200], Batch [94/123], Loss: 2.3617\n",
            "Epoch [20/200], Batch [95/123], Loss: 1.2501\n",
            "Epoch [20/200], Batch [96/123], Loss: 1.7849\n",
            "Epoch [20/200], Batch [97/123], Loss: 2.2565\n",
            "Epoch [20/200], Batch [98/123], Loss: 2.4889\n",
            "Epoch [20/200], Batch [99/123], Loss: 1.4468\n",
            "Epoch [20/200], Batch [100/123], Loss: 1.7605\n",
            "Epoch [20/200], Batch [101/123], Loss: 1.6675\n",
            "Epoch [20/200], Batch [102/123], Loss: 1.2701\n",
            "Epoch [20/200], Batch [103/123], Loss: 1.4966\n",
            "Epoch [20/200], Batch [104/123], Loss: 1.7456\n",
            "Epoch [20/200], Batch [105/123], Loss: 1.6134\n",
            "Epoch [20/200], Batch [106/123], Loss: 1.9301\n",
            "Epoch [20/200], Batch [107/123], Loss: 1.6745\n",
            "Epoch [20/200], Batch [108/123], Loss: 2.4198\n",
            "Epoch [20/200], Batch [109/123], Loss: 1.3756\n",
            "Epoch [20/200], Batch [110/123], Loss: 1.1705\n",
            "Epoch [20/200], Batch [111/123], Loss: 1.6822\n",
            "Epoch [20/200], Batch [112/123], Loss: 1.3908\n",
            "Epoch [20/200], Batch [113/123], Loss: 1.5456\n",
            "Epoch [20/200], Batch [114/123], Loss: 1.3409\n",
            "Epoch [20/200], Batch [115/123], Loss: 1.6494\n",
            "Epoch [20/200], Batch [116/123], Loss: 1.5127\n",
            "Epoch [20/200], Batch [117/123], Loss: 1.2101\n",
            "Epoch [20/200], Batch [118/123], Loss: 1.3549\n",
            "Epoch [20/200], Batch [119/123], Loss: 1.6012\n",
            "Epoch [20/200], Batch [120/123], Loss: 1.7378\n",
            "Epoch [20/200], Batch [121/123], Loss: 1.4492\n",
            "Epoch [20/200], Batch [122/123], Loss: 1.5102\n",
            "Epoch [20/200], Batch [123/123], Loss: 1.1869\n",
            "Epoch [21/200], Batch [1/123], Loss: 1.5106\n",
            "Epoch [21/200], Batch [2/123], Loss: 1.3875\n",
            "Epoch [21/200], Batch [3/123], Loss: 1.6145\n",
            "Epoch [21/200], Batch [4/123], Loss: 1.4804\n",
            "Epoch [21/200], Batch [5/123], Loss: 1.8958\n",
            "Epoch [21/200], Batch [6/123], Loss: 2.1977\n",
            "Epoch [21/200], Batch [7/123], Loss: 1.1632\n",
            "Epoch [21/200], Batch [8/123], Loss: 1.6513\n",
            "Epoch [21/200], Batch [9/123], Loss: 2.1083\n",
            "Epoch [21/200], Batch [10/123], Loss: 1.7416\n",
            "Epoch [21/200], Batch [11/123], Loss: 1.6060\n",
            "Epoch [21/200], Batch [12/123], Loss: 1.4810\n",
            "Epoch [21/200], Batch [13/123], Loss: 1.3429\n",
            "Epoch [21/200], Batch [14/123], Loss: 1.2840\n",
            "Epoch [21/200], Batch [15/123], Loss: 1.8050\n",
            "Epoch [21/200], Batch [16/123], Loss: 2.7278\n",
            "Epoch [21/200], Batch [17/123], Loss: 1.6691\n",
            "Epoch [21/200], Batch [18/123], Loss: 1.3726\n",
            "Epoch [21/200], Batch [19/123], Loss: 2.2934\n",
            "Epoch [21/200], Batch [20/123], Loss: 1.5360\n",
            "Epoch [21/200], Batch [21/123], Loss: 1.5073\n",
            "Epoch [21/200], Batch [22/123], Loss: 1.4279\n",
            "Epoch [21/200], Batch [23/123], Loss: 1.3265\n",
            "Epoch [21/200], Batch [24/123], Loss: 1.2396\n",
            "Epoch [21/200], Batch [25/123], Loss: 0.8162\n",
            "Epoch [21/200], Batch [26/123], Loss: 2.2766\n",
            "Epoch [21/200], Batch [27/123], Loss: 1.4842\n",
            "Epoch [21/200], Batch [28/123], Loss: 1.7554\n",
            "Epoch [21/200], Batch [29/123], Loss: 1.5145\n",
            "Epoch [21/200], Batch [30/123], Loss: 1.6945\n",
            "Epoch [21/200], Batch [31/123], Loss: 1.8225\n",
            "Epoch [21/200], Batch [32/123], Loss: 1.4095\n",
            "Epoch [21/200], Batch [33/123], Loss: 1.8396\n",
            "Epoch [21/200], Batch [34/123], Loss: 1.6970\n",
            "Epoch [21/200], Batch [35/123], Loss: 1.1717\n",
            "Epoch [21/200], Batch [36/123], Loss: 1.3310\n",
            "Epoch [21/200], Batch [37/123], Loss: 1.0878\n",
            "Epoch [21/200], Batch [38/123], Loss: 1.6707\n",
            "Epoch [21/200], Batch [39/123], Loss: 1.3332\n",
            "Epoch [21/200], Batch [40/123], Loss: 1.6101\n",
            "Epoch [21/200], Batch [41/123], Loss: 1.4753\n",
            "Epoch [21/200], Batch [42/123], Loss: 1.2594\n",
            "Epoch [21/200], Batch [43/123], Loss: 1.5380\n",
            "Epoch [21/200], Batch [44/123], Loss: 1.4917\n",
            "Epoch [21/200], Batch [45/123], Loss: 1.6982\n",
            "Epoch [21/200], Batch [46/123], Loss: 1.4647\n",
            "Epoch [21/200], Batch [47/123], Loss: 1.7786\n",
            "Epoch [21/200], Batch [48/123], Loss: 1.5336\n",
            "Epoch [21/200], Batch [49/123], Loss: 1.2982\n",
            "Epoch [21/200], Batch [50/123], Loss: 1.0417\n",
            "Epoch [21/200], Batch [51/123], Loss: 1.2602\n",
            "Epoch [21/200], Batch [52/123], Loss: 1.7206\n",
            "Epoch [21/200], Batch [53/123], Loss: 1.3239\n",
            "Epoch [21/200], Batch [54/123], Loss: 1.7223\n",
            "Epoch [21/200], Batch [55/123], Loss: 1.3741\n",
            "Epoch [21/200], Batch [56/123], Loss: 1.6485\n",
            "Epoch [21/200], Batch [57/123], Loss: 1.1508\n",
            "Epoch [21/200], Batch [58/123], Loss: 1.3983\n",
            "Epoch [21/200], Batch [59/123], Loss: 1.1080\n",
            "Epoch [21/200], Batch [60/123], Loss: 1.6254\n",
            "Epoch [21/200], Batch [61/123], Loss: 1.8893\n",
            "Epoch [21/200], Batch [62/123], Loss: 1.9818\n",
            "Epoch [21/200], Batch [63/123], Loss: 1.0653\n",
            "Epoch [21/200], Batch [64/123], Loss: 1.3074\n",
            "Epoch [21/200], Batch [65/123], Loss: 1.8411\n",
            "Epoch [21/200], Batch [66/123], Loss: 1.3011\n",
            "Epoch [21/200], Batch [67/123], Loss: 1.3602\n",
            "Epoch [21/200], Batch [68/123], Loss: 2.0937\n",
            "Epoch [21/200], Batch [69/123], Loss: 1.3803\n",
            "Epoch [21/200], Batch [70/123], Loss: 2.1196\n",
            "Epoch [21/200], Batch [71/123], Loss: 1.6149\n",
            "Epoch [21/200], Batch [72/123], Loss: 1.7568\n",
            "Epoch [21/200], Batch [73/123], Loss: 1.2212\n",
            "Epoch [21/200], Batch [74/123], Loss: 1.2307\n",
            "Epoch [21/200], Batch [75/123], Loss: 1.5885\n",
            "Epoch [21/200], Batch [76/123], Loss: 1.9931\n",
            "Epoch [21/200], Batch [77/123], Loss: 1.5183\n",
            "Epoch [21/200], Batch [78/123], Loss: 1.9870\n",
            "Epoch [21/200], Batch [79/123], Loss: 1.1698\n",
            "Epoch [21/200], Batch [80/123], Loss: 1.7185\n",
            "Epoch [21/200], Batch [81/123], Loss: 1.0528\n",
            "Epoch [21/200], Batch [82/123], Loss: 1.1411\n",
            "Epoch [21/200], Batch [83/123], Loss: 1.3564\n",
            "Epoch [21/200], Batch [84/123], Loss: 1.4213\n",
            "Epoch [21/200], Batch [85/123], Loss: 2.4809\n",
            "Epoch [21/200], Batch [86/123], Loss: 1.2909\n",
            "Epoch [21/200], Batch [87/123], Loss: 1.3843\n",
            "Epoch [21/200], Batch [88/123], Loss: 1.6210\n",
            "Epoch [21/200], Batch [89/123], Loss: 2.1346\n",
            "Epoch [21/200], Batch [90/123], Loss: 3.1561\n",
            "Epoch [21/200], Batch [91/123], Loss: 1.8172\n",
            "Epoch [21/200], Batch [92/123], Loss: 1.6161\n",
            "Epoch [21/200], Batch [93/123], Loss: 1.5146\n",
            "Epoch [21/200], Batch [94/123], Loss: 1.6043\n",
            "Epoch [21/200], Batch [95/123], Loss: 1.9645\n",
            "Epoch [21/200], Batch [96/123], Loss: 1.2019\n",
            "Epoch [21/200], Batch [97/123], Loss: 1.4718\n",
            "Epoch [21/200], Batch [98/123], Loss: 1.4457\n",
            "Epoch [21/200], Batch [99/123], Loss: 1.5419\n",
            "Epoch [21/200], Batch [100/123], Loss: 1.6232\n",
            "Epoch [21/200], Batch [101/123], Loss: 1.3395\n",
            "Epoch [21/200], Batch [102/123], Loss: 1.9571\n",
            "Epoch [21/200], Batch [103/123], Loss: 1.3083\n",
            "Epoch [21/200], Batch [104/123], Loss: 1.7373\n",
            "Epoch [21/200], Batch [105/123], Loss: 1.3569\n",
            "Epoch [21/200], Batch [106/123], Loss: 1.7880\n",
            "Epoch [21/200], Batch [107/123], Loss: 2.4500\n",
            "Epoch [21/200], Batch [108/123], Loss: 1.4543\n",
            "Epoch [21/200], Batch [109/123], Loss: 1.4654\n",
            "Epoch [21/200], Batch [110/123], Loss: 1.4278\n",
            "Epoch [21/200], Batch [111/123], Loss: 0.9700\n",
            "Epoch [21/200], Batch [112/123], Loss: 1.7000\n",
            "Epoch [21/200], Batch [113/123], Loss: 1.6236\n",
            "Epoch [21/200], Batch [114/123], Loss: 1.3363\n",
            "Epoch [21/200], Batch [115/123], Loss: 1.1784\n",
            "Epoch [21/200], Batch [116/123], Loss: 1.3892\n",
            "Epoch [21/200], Batch [117/123], Loss: 2.3495\n",
            "Epoch [21/200], Batch [118/123], Loss: 1.5049\n",
            "Epoch [21/200], Batch [119/123], Loss: 1.2949\n",
            "Epoch [21/200], Batch [120/123], Loss: 1.5209\n",
            "Epoch [21/200], Batch [121/123], Loss: 2.7773\n",
            "Epoch [21/200], Batch [122/123], Loss: 1.8159\n",
            "Epoch [21/200], Batch [123/123], Loss: 1.1590\n",
            "Epoch [22/200], Batch [1/123], Loss: 1.9116\n",
            "Epoch [22/200], Batch [2/123], Loss: 1.1984\n",
            "Epoch [22/200], Batch [3/123], Loss: 2.5822\n",
            "Epoch [22/200], Batch [4/123], Loss: 2.2219\n",
            "Epoch [22/200], Batch [5/123], Loss: 2.3558\n",
            "Epoch [22/200], Batch [6/123], Loss: 1.1956\n",
            "Epoch [22/200], Batch [7/123], Loss: 1.4219\n",
            "Epoch [22/200], Batch [8/123], Loss: 1.9686\n",
            "Epoch [22/200], Batch [9/123], Loss: 3.2026\n",
            "Epoch [22/200], Batch [10/123], Loss: 1.9084\n",
            "Epoch [22/200], Batch [11/123], Loss: 1.5764\n",
            "Epoch [22/200], Batch [12/123], Loss: 2.8252\n",
            "Epoch [22/200], Batch [13/123], Loss: 2.0860\n",
            "Epoch [22/200], Batch [14/123], Loss: 1.6031\n",
            "Epoch [22/200], Batch [15/123], Loss: 1.9970\n",
            "Epoch [22/200], Batch [16/123], Loss: 1.4339\n",
            "Epoch [22/200], Batch [17/123], Loss: 1.3140\n",
            "Epoch [22/200], Batch [18/123], Loss: 1.4393\n",
            "Epoch [22/200], Batch [19/123], Loss: 1.4934\n",
            "Epoch [22/200], Batch [20/123], Loss: 1.8759\n",
            "Epoch [22/200], Batch [21/123], Loss: 1.4410\n",
            "Epoch [22/200], Batch [22/123], Loss: 1.7461\n",
            "Epoch [22/200], Batch [23/123], Loss: 1.3626\n",
            "Epoch [22/200], Batch [24/123], Loss: 1.6122\n",
            "Epoch [22/200], Batch [25/123], Loss: 2.2078\n",
            "Epoch [22/200], Batch [26/123], Loss: 1.8650\n",
            "Epoch [22/200], Batch [27/123], Loss: 1.7954\n",
            "Epoch [22/200], Batch [28/123], Loss: 1.5332\n",
            "Epoch [22/200], Batch [29/123], Loss: 1.4950\n",
            "Epoch [22/200], Batch [30/123], Loss: 1.3149\n",
            "Epoch [22/200], Batch [31/123], Loss: 2.0172\n",
            "Epoch [22/200], Batch [32/123], Loss: 1.3742\n",
            "Epoch [22/200], Batch [33/123], Loss: 1.7669\n",
            "Epoch [22/200], Batch [34/123], Loss: 0.8804\n",
            "Epoch [22/200], Batch [35/123], Loss: 1.0821\n",
            "Epoch [22/200], Batch [36/123], Loss: 1.2843\n",
            "Epoch [22/200], Batch [37/123], Loss: 1.2267\n",
            "Epoch [22/200], Batch [38/123], Loss: 1.0018\n",
            "Epoch [22/200], Batch [39/123], Loss: 2.0316\n",
            "Epoch [22/200], Batch [40/123], Loss: 1.6055\n",
            "Epoch [22/200], Batch [41/123], Loss: 1.4618\n",
            "Epoch [22/200], Batch [42/123], Loss: 1.2827\n",
            "Epoch [22/200], Batch [43/123], Loss: 1.2297\n",
            "Epoch [22/200], Batch [44/123], Loss: 1.4231\n",
            "Epoch [22/200], Batch [45/123], Loss: 1.2617\n",
            "Epoch [22/200], Batch [46/123], Loss: 1.3841\n",
            "Epoch [22/200], Batch [47/123], Loss: 1.3557\n",
            "Epoch [22/200], Batch [48/123], Loss: 1.3004\n",
            "Epoch [22/200], Batch [49/123], Loss: 1.5414\n",
            "Epoch [22/200], Batch [50/123], Loss: 1.3264\n",
            "Epoch [22/200], Batch [51/123], Loss: 1.4068\n",
            "Epoch [22/200], Batch [52/123], Loss: 1.7393\n",
            "Epoch [22/200], Batch [53/123], Loss: 2.0442\n",
            "Epoch [22/200], Batch [54/123], Loss: 1.5646\n",
            "Epoch [22/200], Batch [55/123], Loss: 1.5875\n",
            "Epoch [22/200], Batch [56/123], Loss: 1.7890\n",
            "Epoch [22/200], Batch [57/123], Loss: 1.8264\n",
            "Epoch [22/200], Batch [58/123], Loss: 1.5387\n",
            "Epoch [22/200], Batch [59/123], Loss: 1.7993\n",
            "Epoch [22/200], Batch [60/123], Loss: 1.4979\n",
            "Epoch [22/200], Batch [61/123], Loss: 1.5269\n",
            "Epoch [22/200], Batch [62/123], Loss: 1.6020\n",
            "Epoch [22/200], Batch [63/123], Loss: 1.9423\n",
            "Epoch [22/200], Batch [64/123], Loss: 1.0789\n",
            "Epoch [22/200], Batch [65/123], Loss: 1.0521\n",
            "Epoch [22/200], Batch [66/123], Loss: 1.1997\n",
            "Epoch [22/200], Batch [67/123], Loss: 1.6623\n",
            "Epoch [22/200], Batch [68/123], Loss: 1.8356\n",
            "Epoch [22/200], Batch [69/123], Loss: 1.5734\n",
            "Epoch [22/200], Batch [70/123], Loss: 1.4639\n",
            "Epoch [22/200], Batch [71/123], Loss: 1.1154\n",
            "Epoch [22/200], Batch [72/123], Loss: 1.8132\n",
            "Epoch [22/200], Batch [73/123], Loss: 1.5711\n",
            "Epoch [22/200], Batch [74/123], Loss: 1.7410\n",
            "Epoch [22/200], Batch [75/123], Loss: 1.4799\n",
            "Epoch [22/200], Batch [76/123], Loss: 1.5785\n",
            "Epoch [22/200], Batch [77/123], Loss: 1.7073\n",
            "Epoch [22/200], Batch [78/123], Loss: 2.0863\n",
            "Epoch [22/200], Batch [79/123], Loss: 1.9520\n",
            "Epoch [22/200], Batch [80/123], Loss: 1.9267\n",
            "Epoch [22/200], Batch [81/123], Loss: 1.3903\n",
            "Epoch [22/200], Batch [82/123], Loss: 1.5316\n",
            "Epoch [22/200], Batch [83/123], Loss: 2.3219\n",
            "Epoch [22/200], Batch [84/123], Loss: 2.0121\n",
            "Epoch [22/200], Batch [85/123], Loss: 1.5191\n",
            "Epoch [22/200], Batch [86/123], Loss: 1.4863\n",
            "Epoch [22/200], Batch [87/123], Loss: 1.9209\n",
            "Epoch [22/200], Batch [88/123], Loss: 1.1439\n",
            "Epoch [22/200], Batch [89/123], Loss: 2.1867\n",
            "Epoch [22/200], Batch [90/123], Loss: 2.9341\n",
            "Epoch [22/200], Batch [91/123], Loss: 1.7027\n",
            "Epoch [22/200], Batch [92/123], Loss: 1.2359\n",
            "Epoch [22/200], Batch [93/123], Loss: 1.0488\n",
            "Epoch [22/200], Batch [94/123], Loss: 1.8939\n",
            "Epoch [22/200], Batch [95/123], Loss: 1.9720\n",
            "Epoch [22/200], Batch [96/123], Loss: 1.4982\n",
            "Epoch [22/200], Batch [97/123], Loss: 1.2818\n",
            "Epoch [22/200], Batch [98/123], Loss: 1.8175\n",
            "Epoch [22/200], Batch [99/123], Loss: 1.6732\n",
            "Epoch [22/200], Batch [100/123], Loss: 1.4559\n",
            "Epoch [22/200], Batch [101/123], Loss: 1.5566\n",
            "Epoch [22/200], Batch [102/123], Loss: 1.4984\n",
            "Epoch [22/200], Batch [103/123], Loss: 1.4220\n",
            "Epoch [22/200], Batch [104/123], Loss: 1.4834\n",
            "Epoch [22/200], Batch [105/123], Loss: 1.3215\n",
            "Epoch [22/200], Batch [106/123], Loss: 1.5343\n",
            "Epoch [22/200], Batch [107/123], Loss: 1.3024\n",
            "Epoch [22/200], Batch [108/123], Loss: 1.3590\n",
            "Epoch [22/200], Batch [109/123], Loss: 2.6301\n",
            "Epoch [22/200], Batch [110/123], Loss: 1.4148\n",
            "Epoch [22/200], Batch [111/123], Loss: 1.5401\n",
            "Epoch [22/200], Batch [112/123], Loss: 1.6754\n",
            "Epoch [22/200], Batch [113/123], Loss: 1.6518\n",
            "Epoch [22/200], Batch [114/123], Loss: 1.6078\n",
            "Epoch [22/200], Batch [115/123], Loss: 1.4955\n",
            "Epoch [22/200], Batch [116/123], Loss: 1.3159\n",
            "Epoch [22/200], Batch [117/123], Loss: 1.7126\n",
            "Epoch [22/200], Batch [118/123], Loss: 1.3380\n",
            "Epoch [22/200], Batch [119/123], Loss: 1.2256\n",
            "Epoch [22/200], Batch [120/123], Loss: 1.2929\n",
            "Epoch [22/200], Batch [121/123], Loss: 1.4387\n",
            "Epoch [22/200], Batch [122/123], Loss: 1.0745\n",
            "Epoch [22/200], Batch [123/123], Loss: 1.5767\n",
            "Epoch [23/200], Batch [1/123], Loss: 1.2129\n",
            "Epoch [23/200], Batch [2/123], Loss: 1.3483\n",
            "Epoch [23/200], Batch [3/123], Loss: 1.4438\n",
            "Epoch [23/200], Batch [4/123], Loss: 1.5957\n",
            "Epoch [23/200], Batch [5/123], Loss: 1.6265\n",
            "Epoch [23/200], Batch [6/123], Loss: 1.4122\n",
            "Epoch [23/200], Batch [7/123], Loss: 1.3398\n",
            "Epoch [23/200], Batch [8/123], Loss: 1.2465\n",
            "Epoch [23/200], Batch [9/123], Loss: 1.1016\n",
            "Epoch [23/200], Batch [10/123], Loss: 1.3754\n",
            "Epoch [23/200], Batch [11/123], Loss: 1.3799\n",
            "Epoch [23/200], Batch [12/123], Loss: 2.3745\n",
            "Epoch [23/200], Batch [13/123], Loss: 1.2622\n",
            "Epoch [23/200], Batch [14/123], Loss: 1.2526\n",
            "Epoch [23/200], Batch [15/123], Loss: 1.9556\n",
            "Epoch [23/200], Batch [16/123], Loss: 1.4019\n",
            "Epoch [23/200], Batch [17/123], Loss: 1.3657\n",
            "Epoch [23/200], Batch [18/123], Loss: 1.6550\n",
            "Epoch [23/200], Batch [19/123], Loss: 0.9179\n",
            "Epoch [23/200], Batch [20/123], Loss: 1.1872\n",
            "Epoch [23/200], Batch [21/123], Loss: 1.6456\n",
            "Epoch [23/200], Batch [22/123], Loss: 1.7410\n",
            "Epoch [23/200], Batch [23/123], Loss: 1.7804\n",
            "Epoch [23/200], Batch [24/123], Loss: 1.6490\n",
            "Epoch [23/200], Batch [25/123], Loss: 1.0628\n",
            "Epoch [23/200], Batch [26/123], Loss: 1.0533\n",
            "Epoch [23/200], Batch [27/123], Loss: 1.3894\n",
            "Epoch [23/200], Batch [28/123], Loss: 1.4592\n",
            "Epoch [23/200], Batch [29/123], Loss: 1.2676\n",
            "Epoch [23/200], Batch [30/123], Loss: 1.2702\n",
            "Epoch [23/200], Batch [31/123], Loss: 1.3698\n",
            "Epoch [23/200], Batch [32/123], Loss: 1.5110\n",
            "Epoch [23/200], Batch [33/123], Loss: 1.7022\n",
            "Epoch [23/200], Batch [34/123], Loss: 1.2938\n",
            "Epoch [23/200], Batch [35/123], Loss: 1.6379\n",
            "Epoch [23/200], Batch [36/123], Loss: 1.5595\n",
            "Epoch [23/200], Batch [37/123], Loss: 1.1762\n",
            "Epoch [23/200], Batch [38/123], Loss: 1.4257\n",
            "Epoch [23/200], Batch [39/123], Loss: 2.0038\n",
            "Epoch [23/200], Batch [40/123], Loss: 2.3191\n",
            "Epoch [23/200], Batch [41/123], Loss: 1.7816\n",
            "Epoch [23/200], Batch [42/123], Loss: 1.5595\n",
            "Epoch [23/200], Batch [43/123], Loss: 1.4397\n",
            "Epoch [23/200], Batch [44/123], Loss: 1.6481\n",
            "Epoch [23/200], Batch [45/123], Loss: 1.6079\n",
            "Epoch [23/200], Batch [46/123], Loss: 1.1968\n",
            "Epoch [23/200], Batch [47/123], Loss: 1.2249\n",
            "Epoch [23/200], Batch [48/123], Loss: 1.1280\n",
            "Epoch [23/200], Batch [49/123], Loss: 1.7391\n",
            "Epoch [23/200], Batch [50/123], Loss: 1.0195\n",
            "Epoch [23/200], Batch [51/123], Loss: 1.6167\n",
            "Epoch [23/200], Batch [52/123], Loss: 1.4017\n",
            "Epoch [23/200], Batch [53/123], Loss: 1.2719\n",
            "Epoch [23/200], Batch [54/123], Loss: 1.5396\n",
            "Epoch [23/200], Batch [55/123], Loss: 1.1258\n",
            "Epoch [23/200], Batch [56/123], Loss: 1.4901\n",
            "Epoch [23/200], Batch [57/123], Loss: 1.6407\n",
            "Epoch [23/200], Batch [58/123], Loss: 1.4230\n",
            "Epoch [23/200], Batch [59/123], Loss: 1.2428\n",
            "Epoch [23/200], Batch [60/123], Loss: 1.0568\n",
            "Epoch [23/200], Batch [61/123], Loss: 0.9260\n",
            "Epoch [23/200], Batch [62/123], Loss: 1.7839\n",
            "Epoch [23/200], Batch [63/123], Loss: 2.1260\n",
            "Epoch [23/200], Batch [64/123], Loss: 1.1025\n",
            "Epoch [23/200], Batch [65/123], Loss: 1.2976\n",
            "Epoch [23/200], Batch [66/123], Loss: 1.7454\n",
            "Epoch [23/200], Batch [67/123], Loss: 1.6144\n",
            "Epoch [23/200], Batch [68/123], Loss: 1.1640\n",
            "Epoch [23/200], Batch [69/123], Loss: 1.5275\n",
            "Epoch [23/200], Batch [70/123], Loss: 1.3856\n",
            "Epoch [23/200], Batch [71/123], Loss: 2.1846\n",
            "Epoch [23/200], Batch [72/123], Loss: 1.7815\n",
            "Epoch [23/200], Batch [73/123], Loss: 0.9460\n",
            "Epoch [23/200], Batch [74/123], Loss: 1.5289\n",
            "Epoch [23/200], Batch [75/123], Loss: 1.5583\n",
            "Epoch [23/200], Batch [76/123], Loss: 1.2505\n",
            "Epoch [23/200], Batch [77/123], Loss: 1.2898\n",
            "Epoch [23/200], Batch [78/123], Loss: 1.6056\n",
            "Epoch [23/200], Batch [79/123], Loss: 1.4876\n",
            "Epoch [23/200], Batch [80/123], Loss: 1.1548\n",
            "Epoch [23/200], Batch [81/123], Loss: 1.4269\n",
            "Epoch [23/200], Batch [82/123], Loss: 1.9017\n",
            "Epoch [23/200], Batch [83/123], Loss: 1.6013\n",
            "Epoch [23/200], Batch [84/123], Loss: 1.1622\n",
            "Epoch [23/200], Batch [85/123], Loss: 1.4985\n",
            "Epoch [23/200], Batch [86/123], Loss: 1.3035\n",
            "Epoch [23/200], Batch [87/123], Loss: 2.2349\n",
            "Epoch [23/200], Batch [88/123], Loss: 1.9691\n",
            "Epoch [23/200], Batch [89/123], Loss: 1.4159\n",
            "Epoch [23/200], Batch [90/123], Loss: 2.2716\n",
            "Epoch [23/200], Batch [91/123], Loss: 1.7259\n",
            "Epoch [23/200], Batch [92/123], Loss: 1.5536\n",
            "Epoch [23/200], Batch [93/123], Loss: 1.8036\n",
            "Epoch [23/200], Batch [94/123], Loss: 1.0389\n",
            "Epoch [23/200], Batch [95/123], Loss: 1.6461\n",
            "Epoch [23/200], Batch [96/123], Loss: 1.0343\n",
            "Epoch [23/200], Batch [97/123], Loss: 1.2110\n",
            "Epoch [23/200], Batch [98/123], Loss: 1.5807\n",
            "Epoch [23/200], Batch [99/123], Loss: 1.6701\n",
            "Epoch [23/200], Batch [100/123], Loss: 1.1850\n",
            "Epoch [23/200], Batch [101/123], Loss: 1.6821\n",
            "Epoch [23/200], Batch [102/123], Loss: 1.1553\n",
            "Epoch [23/200], Batch [103/123], Loss: 1.7978\n",
            "Epoch [23/200], Batch [104/123], Loss: 1.4684\n",
            "Epoch [23/200], Batch [105/123], Loss: 2.1187\n",
            "Epoch [23/200], Batch [106/123], Loss: 1.6322\n",
            "Epoch [23/200], Batch [107/123], Loss: 1.7502\n",
            "Epoch [23/200], Batch [108/123], Loss: 2.0207\n",
            "Epoch [23/200], Batch [109/123], Loss: 1.8225\n",
            "Epoch [23/200], Batch [110/123], Loss: 1.4707\n",
            "Epoch [23/200], Batch [111/123], Loss: 1.3006\n",
            "Epoch [23/200], Batch [112/123], Loss: 1.6300\n",
            "Epoch [23/200], Batch [113/123], Loss: 1.8636\n",
            "Epoch [23/200], Batch [114/123], Loss: 2.0289\n",
            "Epoch [23/200], Batch [115/123], Loss: 0.9920\n",
            "Epoch [23/200], Batch [116/123], Loss: 1.1612\n",
            "Epoch [23/200], Batch [117/123], Loss: 1.5923\n",
            "Epoch [23/200], Batch [118/123], Loss: 1.6693\n",
            "Epoch [23/200], Batch [119/123], Loss: 1.6604\n",
            "Epoch [23/200], Batch [120/123], Loss: 1.5616\n",
            "Epoch [23/200], Batch [121/123], Loss: 1.4962\n",
            "Epoch [23/200], Batch [122/123], Loss: 2.2832\n",
            "Epoch [23/200], Batch [123/123], Loss: 1.4352\n",
            "Epoch [24/200], Batch [1/123], Loss: 1.3507\n",
            "Epoch [24/200], Batch [2/123], Loss: 1.6896\n",
            "Epoch [24/200], Batch [3/123], Loss: 1.4029\n",
            "Epoch [24/200], Batch [4/123], Loss: 1.3497\n",
            "Epoch [24/200], Batch [5/123], Loss: 1.6514\n",
            "Epoch [24/200], Batch [6/123], Loss: 1.6548\n",
            "Epoch [24/200], Batch [7/123], Loss: 1.2786\n",
            "Epoch [24/200], Batch [8/123], Loss: 0.9601\n",
            "Epoch [24/200], Batch [9/123], Loss: 1.4687\n",
            "Epoch [24/200], Batch [10/123], Loss: 1.0310\n",
            "Epoch [24/200], Batch [11/123], Loss: 1.1720\n",
            "Epoch [24/200], Batch [12/123], Loss: 1.6136\n",
            "Epoch [24/200], Batch [13/123], Loss: 1.1079\n",
            "Epoch [24/200], Batch [14/123], Loss: 0.9869\n",
            "Epoch [24/200], Batch [15/123], Loss: 1.7719\n",
            "Epoch [24/200], Batch [16/123], Loss: 1.0629\n",
            "Epoch [24/200], Batch [17/123], Loss: 2.2896\n",
            "Epoch [24/200], Batch [18/123], Loss: 1.9107\n",
            "Epoch [24/200], Batch [19/123], Loss: 1.4874\n",
            "Epoch [24/200], Batch [20/123], Loss: 1.7780\n",
            "Epoch [24/200], Batch [21/123], Loss: 0.9795\n",
            "Epoch [24/200], Batch [22/123], Loss: 1.2162\n",
            "Epoch [24/200], Batch [23/123], Loss: 1.3555\n",
            "Epoch [24/200], Batch [24/123], Loss: 1.4370\n",
            "Epoch [24/200], Batch [25/123], Loss: 1.0684\n",
            "Epoch [24/200], Batch [26/123], Loss: 1.3891\n",
            "Epoch [24/200], Batch [27/123], Loss: 2.0818\n",
            "Epoch [24/200], Batch [28/123], Loss: 1.4726\n",
            "Epoch [24/200], Batch [29/123], Loss: 1.3611\n",
            "Epoch [24/200], Batch [30/123], Loss: 1.5571\n",
            "Epoch [24/200], Batch [31/123], Loss: 1.9509\n",
            "Epoch [24/200], Batch [32/123], Loss: 1.7613\n",
            "Epoch [24/200], Batch [33/123], Loss: 0.9382\n",
            "Epoch [24/200], Batch [34/123], Loss: 1.3420\n",
            "Epoch [24/200], Batch [35/123], Loss: 1.5726\n",
            "Epoch [24/200], Batch [36/123], Loss: 1.6287\n",
            "Epoch [24/200], Batch [37/123], Loss: 1.2784\n",
            "Epoch [24/200], Batch [38/123], Loss: 1.4530\n",
            "Epoch [24/200], Batch [39/123], Loss: 1.1792\n",
            "Epoch [24/200], Batch [40/123], Loss: 1.2815\n",
            "Epoch [24/200], Batch [41/123], Loss: 1.0424\n",
            "Epoch [24/200], Batch [42/123], Loss: 2.3226\n",
            "Epoch [24/200], Batch [43/123], Loss: 1.3226\n",
            "Epoch [24/200], Batch [44/123], Loss: 1.2329\n",
            "Epoch [24/200], Batch [45/123], Loss: 1.2763\n",
            "Epoch [24/200], Batch [46/123], Loss: 1.0649\n",
            "Epoch [24/200], Batch [47/123], Loss: 1.2616\n",
            "Epoch [24/200], Batch [48/123], Loss: 1.1836\n",
            "Epoch [24/200], Batch [49/123], Loss: 1.2379\n",
            "Epoch [24/200], Batch [50/123], Loss: 1.7555\n",
            "Epoch [24/200], Batch [51/123], Loss: 1.9465\n",
            "Epoch [24/200], Batch [52/123], Loss: 1.1119\n",
            "Epoch [24/200], Batch [53/123], Loss: 1.7696\n",
            "Epoch [24/200], Batch [54/123], Loss: 1.9699\n",
            "Epoch [24/200], Batch [55/123], Loss: 1.7050\n",
            "Epoch [24/200], Batch [56/123], Loss: 1.3475\n",
            "Epoch [24/200], Batch [57/123], Loss: 1.7216\n",
            "Epoch [24/200], Batch [58/123], Loss: 2.0138\n",
            "Epoch [24/200], Batch [59/123], Loss: 1.7910\n",
            "Epoch [24/200], Batch [60/123], Loss: 1.6800\n",
            "Epoch [24/200], Batch [61/123], Loss: 1.7673\n",
            "Epoch [24/200], Batch [62/123], Loss: 1.5136\n",
            "Epoch [24/200], Batch [63/123], Loss: 1.4670\n",
            "Epoch [24/200], Batch [64/123], Loss: 1.5595\n",
            "Epoch [24/200], Batch [65/123], Loss: 0.7945\n",
            "Epoch [24/200], Batch [66/123], Loss: 1.3538\n",
            "Epoch [24/200], Batch [67/123], Loss: 1.8446\n",
            "Epoch [24/200], Batch [68/123], Loss: 1.0593\n",
            "Epoch [24/200], Batch [69/123], Loss: 2.0219\n",
            "Epoch [24/200], Batch [70/123], Loss: 1.5081\n",
            "Epoch [24/200], Batch [71/123], Loss: 0.9810\n",
            "Epoch [24/200], Batch [72/123], Loss: 1.2484\n",
            "Epoch [24/200], Batch [73/123], Loss: 1.7753\n",
            "Epoch [24/200], Batch [74/123], Loss: 1.3836\n",
            "Epoch [24/200], Batch [75/123], Loss: 1.0809\n",
            "Epoch [24/200], Batch [76/123], Loss: 1.1784\n",
            "Epoch [24/200], Batch [77/123], Loss: 1.7876\n",
            "Epoch [24/200], Batch [78/123], Loss: 1.2833\n",
            "Epoch [24/200], Batch [79/123], Loss: 1.4092\n",
            "Epoch [24/200], Batch [80/123], Loss: 2.0067\n",
            "Epoch [24/200], Batch [81/123], Loss: 1.8800\n",
            "Epoch [24/200], Batch [82/123], Loss: 1.5734\n",
            "Epoch [24/200], Batch [83/123], Loss: 1.5336\n",
            "Epoch [24/200], Batch [84/123], Loss: 1.3221\n",
            "Epoch [24/200], Batch [85/123], Loss: 1.0758\n",
            "Epoch [24/200], Batch [86/123], Loss: 1.3155\n",
            "Epoch [24/200], Batch [87/123], Loss: 1.2885\n",
            "Epoch [24/200], Batch [88/123], Loss: 1.0692\n",
            "Epoch [24/200], Batch [89/123], Loss: 1.1365\n",
            "Epoch [24/200], Batch [90/123], Loss: 1.0337\n",
            "Epoch [24/200], Batch [91/123], Loss: 1.7289\n",
            "Epoch [24/200], Batch [92/123], Loss: 1.5524\n",
            "Epoch [24/200], Batch [93/123], Loss: 1.2908\n",
            "Epoch [24/200], Batch [94/123], Loss: 1.3260\n",
            "Epoch [24/200], Batch [95/123], Loss: 1.2862\n",
            "Epoch [24/200], Batch [96/123], Loss: 1.6044\n",
            "Epoch [24/200], Batch [97/123], Loss: 1.0966\n",
            "Epoch [24/200], Batch [98/123], Loss: 1.1893\n",
            "Epoch [24/200], Batch [99/123], Loss: 1.7041\n",
            "Epoch [24/200], Batch [100/123], Loss: 1.4219\n",
            "Epoch [24/200], Batch [101/123], Loss: 1.0253\n",
            "Epoch [24/200], Batch [102/123], Loss: 1.3582\n",
            "Epoch [24/200], Batch [103/123], Loss: 1.7036\n",
            "Epoch [24/200], Batch [104/123], Loss: 1.2040\n",
            "Epoch [24/200], Batch [105/123], Loss: 1.5812\n",
            "Epoch [24/200], Batch [106/123], Loss: 1.4797\n",
            "Epoch [24/200], Batch [107/123], Loss: 1.5111\n",
            "Epoch [24/200], Batch [108/123], Loss: 1.4532\n",
            "Epoch [24/200], Batch [109/123], Loss: 1.8044\n",
            "Epoch [24/200], Batch [110/123], Loss: 1.5928\n",
            "Epoch [24/200], Batch [111/123], Loss: 1.7251\n",
            "Epoch [24/200], Batch [112/123], Loss: 1.3727\n",
            "Epoch [24/200], Batch [113/123], Loss: 0.9485\n",
            "Epoch [24/200], Batch [114/123], Loss: 1.5861\n",
            "Epoch [24/200], Batch [115/123], Loss: 1.1580\n",
            "Epoch [24/200], Batch [116/123], Loss: 1.0500\n",
            "Epoch [24/200], Batch [117/123], Loss: 1.1687\n",
            "Epoch [24/200], Batch [118/123], Loss: 2.8762\n",
            "Epoch [24/200], Batch [119/123], Loss: 2.1481\n",
            "Epoch [24/200], Batch [120/123], Loss: 1.0459\n",
            "Epoch [24/200], Batch [121/123], Loss: 0.8416\n",
            "Epoch [24/200], Batch [122/123], Loss: 0.9871\n",
            "Epoch [24/200], Batch [123/123], Loss: 1.3751\n",
            "Epoch [25/200], Batch [1/123], Loss: 1.2098\n",
            "Epoch [25/200], Batch [2/123], Loss: 1.7579\n",
            "Epoch [25/200], Batch [3/123], Loss: 1.0173\n",
            "Epoch [25/200], Batch [4/123], Loss: 1.3747\n",
            "Epoch [25/200], Batch [5/123], Loss: 1.6187\n",
            "Epoch [25/200], Batch [6/123], Loss: 1.0265\n",
            "Epoch [25/200], Batch [7/123], Loss: 0.9007\n",
            "Epoch [25/200], Batch [8/123], Loss: 1.3298\n",
            "Epoch [25/200], Batch [9/123], Loss: 1.2372\n",
            "Epoch [25/200], Batch [10/123], Loss: 1.6347\n",
            "Epoch [25/200], Batch [11/123], Loss: 1.1811\n",
            "Epoch [25/200], Batch [12/123], Loss: 1.1706\n",
            "Epoch [25/200], Batch [13/123], Loss: 1.4479\n",
            "Epoch [25/200], Batch [14/123], Loss: 1.4387\n",
            "Epoch [25/200], Batch [15/123], Loss: 1.8840\n",
            "Epoch [25/200], Batch [16/123], Loss: 1.4229\n",
            "Epoch [25/200], Batch [17/123], Loss: 1.2896\n",
            "Epoch [25/200], Batch [18/123], Loss: 1.1975\n",
            "Epoch [25/200], Batch [19/123], Loss: 1.2455\n",
            "Epoch [25/200], Batch [20/123], Loss: 1.2319\n",
            "Epoch [25/200], Batch [21/123], Loss: 1.3141\n",
            "Epoch [25/200], Batch [22/123], Loss: 0.9201\n",
            "Epoch [25/200], Batch [23/123], Loss: 2.1914\n",
            "Epoch [25/200], Batch [24/123], Loss: 0.9926\n",
            "Epoch [25/200], Batch [25/123], Loss: 1.5995\n",
            "Epoch [25/200], Batch [26/123], Loss: 1.6882\n",
            "Epoch [25/200], Batch [27/123], Loss: 1.3511\n",
            "Epoch [25/200], Batch [28/123], Loss: 1.3194\n",
            "Epoch [25/200], Batch [29/123], Loss: 1.0452\n",
            "Epoch [25/200], Batch [30/123], Loss: 1.1666\n",
            "Epoch [25/200], Batch [31/123], Loss: 1.9395\n",
            "Epoch [25/200], Batch [32/123], Loss: 2.0800\n",
            "Epoch [25/200], Batch [33/123], Loss: 0.7545\n",
            "Epoch [25/200], Batch [34/123], Loss: 1.6187\n",
            "Epoch [25/200], Batch [35/123], Loss: 0.9934\n",
            "Epoch [25/200], Batch [36/123], Loss: 1.5578\n",
            "Epoch [25/200], Batch [37/123], Loss: 1.6999\n",
            "Epoch [25/200], Batch [38/123], Loss: 2.3201\n",
            "Epoch [25/200], Batch [39/123], Loss: 1.3135\n",
            "Epoch [25/200], Batch [40/123], Loss: 1.7698\n",
            "Epoch [25/200], Batch [41/123], Loss: 1.6572\n",
            "Epoch [25/200], Batch [42/123], Loss: 1.9228\n",
            "Epoch [25/200], Batch [43/123], Loss: 1.3997\n",
            "Epoch [25/200], Batch [44/123], Loss: 1.3600\n",
            "Epoch [25/200], Batch [45/123], Loss: 1.4347\n",
            "Epoch [25/200], Batch [46/123], Loss: 1.6157\n",
            "Epoch [25/200], Batch [47/123], Loss: 1.1859\n",
            "Epoch [25/200], Batch [48/123], Loss: 2.1303\n",
            "Epoch [25/200], Batch [49/123], Loss: 1.1708\n",
            "Epoch [25/200], Batch [50/123], Loss: 2.0051\n",
            "Epoch [25/200], Batch [51/123], Loss: 1.2535\n",
            "Epoch [25/200], Batch [52/123], Loss: 1.5202\n",
            "Epoch [25/200], Batch [53/123], Loss: 1.4626\n",
            "Epoch [25/200], Batch [54/123], Loss: 1.2304\n",
            "Epoch [25/200], Batch [55/123], Loss: 1.3940\n",
            "Epoch [25/200], Batch [56/123], Loss: 1.4430\n",
            "Epoch [25/200], Batch [57/123], Loss: 1.7630\n",
            "Epoch [25/200], Batch [58/123], Loss: 1.3072\n",
            "Epoch [25/200], Batch [59/123], Loss: 1.4216\n",
            "Epoch [25/200], Batch [60/123], Loss: 0.7727\n",
            "Epoch [25/200], Batch [61/123], Loss: 1.8293\n",
            "Epoch [25/200], Batch [62/123], Loss: 1.4709\n",
            "Epoch [25/200], Batch [63/123], Loss: 1.2677\n",
            "Epoch [25/200], Batch [64/123], Loss: 1.2057\n",
            "Epoch [25/200], Batch [65/123], Loss: 2.5613\n",
            "Epoch [25/200], Batch [66/123], Loss: 1.3933\n",
            "Epoch [25/200], Batch [67/123], Loss: 1.2932\n",
            "Epoch [25/200], Batch [68/123], Loss: 2.3573\n",
            "Epoch [25/200], Batch [69/123], Loss: 1.4669\n",
            "Epoch [25/200], Batch [70/123], Loss: 1.7286\n",
            "Epoch [25/200], Batch [71/123], Loss: 1.3653\n",
            "Epoch [25/200], Batch [72/123], Loss: 1.3108\n",
            "Epoch [25/200], Batch [73/123], Loss: 1.3277\n",
            "Epoch [25/200], Batch [74/123], Loss: 1.1641\n",
            "Epoch [25/200], Batch [75/123], Loss: 1.7574\n",
            "Epoch [25/200], Batch [76/123], Loss: 1.3750\n",
            "Epoch [25/200], Batch [77/123], Loss: 1.4136\n",
            "Epoch [25/200], Batch [78/123], Loss: 0.9917\n",
            "Epoch [25/200], Batch [79/123], Loss: 1.2822\n",
            "Epoch [25/200], Batch [80/123], Loss: 1.1465\n",
            "Epoch [25/200], Batch [81/123], Loss: 1.5370\n",
            "Epoch [25/200], Batch [82/123], Loss: 1.4135\n",
            "Epoch [25/200], Batch [83/123], Loss: 1.4336\n",
            "Epoch [25/200], Batch [84/123], Loss: 1.3091\n",
            "Epoch [25/200], Batch [85/123], Loss: 1.1822\n",
            "Epoch [25/200], Batch [86/123], Loss: 1.2520\n",
            "Epoch [25/200], Batch [87/123], Loss: 1.5920\n",
            "Epoch [25/200], Batch [88/123], Loss: 1.4229\n",
            "Epoch [25/200], Batch [89/123], Loss: 1.0080\n",
            "Epoch [25/200], Batch [90/123], Loss: 1.3366\n",
            "Epoch [25/200], Batch [91/123], Loss: 1.4102\n",
            "Epoch [25/200], Batch [92/123], Loss: 1.4131\n",
            "Epoch [25/200], Batch [93/123], Loss: 1.4757\n",
            "Epoch [25/200], Batch [94/123], Loss: 1.8864\n",
            "Epoch [25/200], Batch [95/123], Loss: 1.1268\n",
            "Epoch [25/200], Batch [96/123], Loss: 1.3691\n",
            "Epoch [25/200], Batch [97/123], Loss: 1.4327\n",
            "Epoch [25/200], Batch [98/123], Loss: 1.3793\n",
            "Epoch [25/200], Batch [99/123], Loss: 1.5401\n",
            "Epoch [25/200], Batch [100/123], Loss: 1.3424\n",
            "Epoch [25/200], Batch [101/123], Loss: 1.2275\n",
            "Epoch [25/200], Batch [102/123], Loss: 1.5247\n",
            "Epoch [25/200], Batch [103/123], Loss: 1.6225\n",
            "Epoch [25/200], Batch [104/123], Loss: 1.6313\n",
            "Epoch [25/200], Batch [105/123], Loss: 1.4240\n",
            "Epoch [25/200], Batch [106/123], Loss: 1.9397\n",
            "Epoch [25/200], Batch [107/123], Loss: 1.3695\n",
            "Epoch [25/200], Batch [108/123], Loss: 1.1476\n",
            "Epoch [25/200], Batch [109/123], Loss: 1.3283\n",
            "Epoch [25/200], Batch [110/123], Loss: 1.4691\n",
            "Epoch [25/200], Batch [111/123], Loss: 1.1492\n",
            "Epoch [25/200], Batch [112/123], Loss: 1.4445\n",
            "Epoch [25/200], Batch [113/123], Loss: 1.2070\n",
            "Epoch [25/200], Batch [114/123], Loss: 1.5436\n",
            "Epoch [25/200], Batch [115/123], Loss: 1.6376\n",
            "Epoch [25/200], Batch [116/123], Loss: 1.2685\n",
            "Epoch [25/200], Batch [117/123], Loss: 1.2107\n",
            "Epoch [25/200], Batch [118/123], Loss: 2.7090\n",
            "Epoch [25/200], Batch [119/123], Loss: 1.2397\n",
            "Epoch [25/200], Batch [120/123], Loss: 1.6459\n",
            "Epoch [25/200], Batch [121/123], Loss: 0.8135\n",
            "Epoch [25/200], Batch [122/123], Loss: 1.3960\n",
            "Epoch [25/200], Batch [123/123], Loss: 1.8060\n",
            "Epoch [26/200], Batch [1/123], Loss: 0.9453\n",
            "Epoch [26/200], Batch [2/123], Loss: 1.3841\n",
            "Epoch [26/200], Batch [3/123], Loss: 1.4793\n",
            "Epoch [26/200], Batch [4/123], Loss: 0.9599\n",
            "Epoch [26/200], Batch [5/123], Loss: 1.6027\n",
            "Epoch [26/200], Batch [6/123], Loss: 1.2295\n",
            "Epoch [26/200], Batch [7/123], Loss: 1.6639\n",
            "Epoch [26/200], Batch [8/123], Loss: 0.9804\n",
            "Epoch [26/200], Batch [9/123], Loss: 1.9071\n",
            "Epoch [26/200], Batch [10/123], Loss: 1.5704\n",
            "Epoch [26/200], Batch [11/123], Loss: 1.1894\n",
            "Epoch [26/200], Batch [12/123], Loss: 1.5584\n",
            "Epoch [26/200], Batch [13/123], Loss: 1.1791\n",
            "Epoch [26/200], Batch [14/123], Loss: 1.0631\n",
            "Epoch [26/200], Batch [15/123], Loss: 1.2158\n",
            "Epoch [26/200], Batch [16/123], Loss: 1.3162\n",
            "Epoch [26/200], Batch [17/123], Loss: 1.6681\n",
            "Epoch [26/200], Batch [18/123], Loss: 1.2105\n",
            "Epoch [26/200], Batch [19/123], Loss: 1.1279\n",
            "Epoch [26/200], Batch [20/123], Loss: 1.1598\n",
            "Epoch [26/200], Batch [21/123], Loss: 1.5272\n",
            "Epoch [26/200], Batch [22/123], Loss: 2.3294\n",
            "Epoch [26/200], Batch [23/123], Loss: 1.2935\n",
            "Epoch [26/200], Batch [24/123], Loss: 1.2725\n",
            "Epoch [26/200], Batch [25/123], Loss: 1.0155\n",
            "Epoch [26/200], Batch [26/123], Loss: 1.5773\n",
            "Epoch [26/200], Batch [27/123], Loss: 0.8669\n",
            "Epoch [26/200], Batch [28/123], Loss: 1.4127\n",
            "Epoch [26/200], Batch [29/123], Loss: 1.2222\n",
            "Epoch [26/200], Batch [30/123], Loss: 2.1678\n",
            "Epoch [26/200], Batch [31/123], Loss: 0.8735\n",
            "Epoch [26/200], Batch [32/123], Loss: 1.1970\n",
            "Epoch [26/200], Batch [33/123], Loss: 1.1834\n",
            "Epoch [26/200], Batch [34/123], Loss: 1.2352\n",
            "Epoch [26/200], Batch [35/123], Loss: 1.5636\n",
            "Epoch [26/200], Batch [36/123], Loss: 1.8365\n",
            "Epoch [26/200], Batch [37/123], Loss: 1.1970\n",
            "Epoch [26/200], Batch [38/123], Loss: 2.0433\n",
            "Epoch [26/200], Batch [39/123], Loss: 1.1631\n",
            "Epoch [26/200], Batch [40/123], Loss: 1.4786\n",
            "Epoch [26/200], Batch [41/123], Loss: 1.2850\n",
            "Epoch [26/200], Batch [42/123], Loss: 1.2709\n",
            "Epoch [26/200], Batch [43/123], Loss: 0.9757\n",
            "Epoch [26/200], Batch [44/123], Loss: 1.4693\n",
            "Epoch [26/200], Batch [45/123], Loss: 1.6963\n",
            "Epoch [26/200], Batch [46/123], Loss: 1.1182\n",
            "Epoch [26/200], Batch [47/123], Loss: 1.7192\n",
            "Epoch [26/200], Batch [48/123], Loss: 1.4418\n",
            "Epoch [26/200], Batch [49/123], Loss: 1.1361\n",
            "Epoch [26/200], Batch [50/123], Loss: 1.4278\n",
            "Epoch [26/200], Batch [51/123], Loss: 1.6617\n",
            "Epoch [26/200], Batch [52/123], Loss: 0.9956\n",
            "Epoch [26/200], Batch [53/123], Loss: 0.9835\n",
            "Epoch [26/200], Batch [54/123], Loss: 1.3734\n",
            "Epoch [26/200], Batch [55/123], Loss: 1.4055\n",
            "Epoch [26/200], Batch [56/123], Loss: 1.0504\n",
            "Epoch [26/200], Batch [57/123], Loss: 1.7631\n",
            "Epoch [26/200], Batch [58/123], Loss: 1.1822\n",
            "Epoch [26/200], Batch [59/123], Loss: 1.1248\n",
            "Epoch [26/200], Batch [60/123], Loss: 1.3102\n",
            "Epoch [26/200], Batch [61/123], Loss: 1.1361\n",
            "Epoch [26/200], Batch [62/123], Loss: 1.3574\n",
            "Epoch [26/200], Batch [63/123], Loss: 2.1132\n",
            "Epoch [26/200], Batch [64/123], Loss: 1.2604\n",
            "Epoch [26/200], Batch [65/123], Loss: 2.0012\n",
            "Epoch [26/200], Batch [66/123], Loss: 1.4648\n",
            "Epoch [26/200], Batch [67/123], Loss: 1.3372\n",
            "Epoch [26/200], Batch [68/123], Loss: 1.2945\n",
            "Epoch [26/200], Batch [69/123], Loss: 1.6526\n",
            "Epoch [26/200], Batch [70/123], Loss: 2.1087\n",
            "Epoch [26/200], Batch [71/123], Loss: 1.2602\n",
            "Epoch [26/200], Batch [72/123], Loss: 1.2493\n",
            "Epoch [26/200], Batch [73/123], Loss: 1.2921\n",
            "Epoch [26/200], Batch [74/123], Loss: 1.5537\n",
            "Epoch [26/200], Batch [75/123], Loss: 1.7221\n",
            "Epoch [26/200], Batch [76/123], Loss: 2.4895\n",
            "Epoch [26/200], Batch [77/123], Loss: 1.5114\n",
            "Epoch [26/200], Batch [78/123], Loss: 1.2602\n",
            "Epoch [26/200], Batch [79/123], Loss: 1.6976\n",
            "Epoch [26/200], Batch [80/123], Loss: 2.0478\n",
            "Epoch [26/200], Batch [81/123], Loss: 2.4381\n",
            "Epoch [26/200], Batch [82/123], Loss: 1.7138\n",
            "Epoch [26/200], Batch [83/123], Loss: 1.2212\n",
            "Epoch [26/200], Batch [84/123], Loss: 1.6864\n",
            "Epoch [26/200], Batch [85/123], Loss: 1.2332\n",
            "Epoch [26/200], Batch [86/123], Loss: 1.4077\n",
            "Epoch [26/200], Batch [87/123], Loss: 2.0887\n",
            "Epoch [26/200], Batch [88/123], Loss: 1.5212\n",
            "Epoch [26/200], Batch [89/123], Loss: 1.7725\n",
            "Epoch [26/200], Batch [90/123], Loss: 1.5378\n",
            "Epoch [26/200], Batch [91/123], Loss: 1.7200\n",
            "Epoch [26/200], Batch [92/123], Loss: 1.0457\n",
            "Epoch [26/200], Batch [93/123], Loss: 2.0542\n",
            "Epoch [26/200], Batch [94/123], Loss: 1.7440\n",
            "Epoch [26/200], Batch [95/123], Loss: 1.8174\n",
            "Epoch [26/200], Batch [96/123], Loss: 1.1916\n",
            "Epoch [26/200], Batch [97/123], Loss: 1.1390\n",
            "Epoch [26/200], Batch [98/123], Loss: 1.5311\n",
            "Epoch [26/200], Batch [99/123], Loss: 1.5851\n",
            "Epoch [26/200], Batch [100/123], Loss: 2.1201\n",
            "Epoch [26/200], Batch [101/123], Loss: 1.9195\n",
            "Epoch [26/200], Batch [102/123], Loss: 1.1841\n",
            "Epoch [26/200], Batch [103/123], Loss: 1.2202\n",
            "Epoch [26/200], Batch [104/123], Loss: 1.2226\n",
            "Epoch [26/200], Batch [105/123], Loss: 1.1362\n",
            "Epoch [26/200], Batch [106/123], Loss: 1.2606\n",
            "Epoch [26/200], Batch [107/123], Loss: 1.4459\n",
            "Epoch [26/200], Batch [108/123], Loss: 1.2792\n",
            "Epoch [26/200], Batch [109/123], Loss: 1.4581\n",
            "Epoch [26/200], Batch [110/123], Loss: 0.9614\n",
            "Epoch [26/200], Batch [111/123], Loss: 1.2257\n",
            "Epoch [26/200], Batch [112/123], Loss: 1.0532\n",
            "Epoch [26/200], Batch [113/123], Loss: 1.1257\n",
            "Epoch [26/200], Batch [114/123], Loss: 1.7141\n",
            "Epoch [26/200], Batch [115/123], Loss: 1.7290\n",
            "Epoch [26/200], Batch [116/123], Loss: 1.2123\n",
            "Epoch [26/200], Batch [117/123], Loss: 2.0109\n",
            "Epoch [26/200], Batch [118/123], Loss: 1.1214\n",
            "Epoch [26/200], Batch [119/123], Loss: 1.3184\n",
            "Epoch [26/200], Batch [120/123], Loss: 1.4282\n",
            "Epoch [26/200], Batch [121/123], Loss: 1.4901\n",
            "Epoch [26/200], Batch [122/123], Loss: 1.3151\n",
            "Epoch [26/200], Batch [123/123], Loss: 1.3880\n",
            "Epoch [27/200], Batch [1/123], Loss: 1.2736\n",
            "Epoch [27/200], Batch [2/123], Loss: 1.6223\n",
            "Epoch [27/200], Batch [3/123], Loss: 1.6363\n",
            "Epoch [27/200], Batch [4/123], Loss: 1.2461\n",
            "Epoch [27/200], Batch [5/123], Loss: 1.3883\n",
            "Epoch [27/200], Batch [6/123], Loss: 2.6072\n",
            "Epoch [27/200], Batch [7/123], Loss: 2.1795\n",
            "Epoch [27/200], Batch [8/123], Loss: 1.2262\n",
            "Epoch [27/200], Batch [9/123], Loss: 1.0268\n",
            "Epoch [27/200], Batch [10/123], Loss: 1.4551\n",
            "Epoch [27/200], Batch [11/123], Loss: 1.6051\n",
            "Epoch [27/200], Batch [12/123], Loss: 1.2465\n",
            "Epoch [27/200], Batch [13/123], Loss: 1.4343\n",
            "Epoch [27/200], Batch [14/123], Loss: 1.2435\n",
            "Epoch [27/200], Batch [15/123], Loss: 1.7704\n",
            "Epoch [27/200], Batch [16/123], Loss: 0.8822\n",
            "Epoch [27/200], Batch [17/123], Loss: 1.4978\n",
            "Epoch [27/200], Batch [18/123], Loss: 1.2543\n",
            "Epoch [27/200], Batch [19/123], Loss: 1.1704\n",
            "Epoch [27/200], Batch [20/123], Loss: 1.3334\n",
            "Epoch [27/200], Batch [21/123], Loss: 1.2771\n",
            "Epoch [27/200], Batch [22/123], Loss: 1.1853\n",
            "Epoch [27/200], Batch [23/123], Loss: 1.8810\n",
            "Epoch [27/200], Batch [24/123], Loss: 1.3716\n",
            "Epoch [27/200], Batch [25/123], Loss: 1.2945\n",
            "Epoch [27/200], Batch [26/123], Loss: 1.4627\n",
            "Epoch [27/200], Batch [27/123], Loss: 2.1770\n",
            "Epoch [27/200], Batch [28/123], Loss: 2.0062\n",
            "Epoch [27/200], Batch [29/123], Loss: 2.2616\n",
            "Epoch [27/200], Batch [30/123], Loss: 1.4088\n",
            "Epoch [27/200], Batch [31/123], Loss: 1.3400\n",
            "Epoch [27/200], Batch [32/123], Loss: 1.6430\n",
            "Epoch [27/200], Batch [33/123], Loss: 1.7779\n",
            "Epoch [27/200], Batch [34/123], Loss: 1.4384\n",
            "Epoch [27/200], Batch [35/123], Loss: 1.3655\n",
            "Epoch [27/200], Batch [36/123], Loss: 1.1196\n",
            "Epoch [27/200], Batch [37/123], Loss: 0.9845\n",
            "Epoch [27/200], Batch [38/123], Loss: 0.8883\n",
            "Epoch [27/200], Batch [39/123], Loss: 1.4064\n",
            "Epoch [27/200], Batch [40/123], Loss: 1.2091\n",
            "Epoch [27/200], Batch [41/123], Loss: 1.8406\n",
            "Epoch [27/200], Batch [42/123], Loss: 1.9435\n",
            "Epoch [27/200], Batch [43/123], Loss: 1.5117\n",
            "Epoch [27/200], Batch [44/123], Loss: 1.3846\n",
            "Epoch [27/200], Batch [45/123], Loss: 1.4639\n",
            "Epoch [27/200], Batch [46/123], Loss: 1.8688\n",
            "Epoch [27/200], Batch [47/123], Loss: 1.2423\n",
            "Epoch [27/200], Batch [48/123], Loss: 1.1726\n",
            "Epoch [27/200], Batch [49/123], Loss: 1.3962\n",
            "Epoch [27/200], Batch [50/123], Loss: 0.8556\n",
            "Epoch [27/200], Batch [51/123], Loss: 1.4775\n",
            "Epoch [27/200], Batch [52/123], Loss: 1.7235\n",
            "Epoch [27/200], Batch [53/123], Loss: 1.1137\n",
            "Epoch [27/200], Batch [54/123], Loss: 1.2906\n",
            "Epoch [27/200], Batch [55/123], Loss: 1.5853\n",
            "Epoch [27/200], Batch [56/123], Loss: 1.1814\n",
            "Epoch [27/200], Batch [57/123], Loss: 1.0008\n",
            "Epoch [27/200], Batch [58/123], Loss: 1.3413\n",
            "Epoch [27/200], Batch [59/123], Loss: 1.2070\n",
            "Epoch [27/200], Batch [60/123], Loss: 1.1959\n",
            "Epoch [27/200], Batch [61/123], Loss: 1.4697\n",
            "Epoch [27/200], Batch [62/123], Loss: 1.2716\n",
            "Epoch [27/200], Batch [63/123], Loss: 1.1989\n",
            "Epoch [27/200], Batch [64/123], Loss: 1.2340\n",
            "Epoch [27/200], Batch [65/123], Loss: 0.9128\n",
            "Epoch [27/200], Batch [66/123], Loss: 1.2807\n",
            "Epoch [27/200], Batch [67/123], Loss: 1.8176\n",
            "Epoch [27/200], Batch [68/123], Loss: 1.0072\n",
            "Epoch [27/200], Batch [69/123], Loss: 0.9002\n",
            "Epoch [27/200], Batch [70/123], Loss: 1.0265\n",
            "Epoch [27/200], Batch [71/123], Loss: 1.3930\n",
            "Epoch [27/200], Batch [72/123], Loss: 1.2171\n",
            "Epoch [27/200], Batch [73/123], Loss: 1.9239\n",
            "Epoch [27/200], Batch [74/123], Loss: 1.3992\n",
            "Epoch [27/200], Batch [75/123], Loss: 1.1950\n",
            "Epoch [27/200], Batch [76/123], Loss: 1.0209\n",
            "Epoch [27/200], Batch [77/123], Loss: 1.3397\n",
            "Epoch [27/200], Batch [78/123], Loss: 1.0431\n",
            "Epoch [27/200], Batch [79/123], Loss: 0.9500\n",
            "Epoch [27/200], Batch [80/123], Loss: 1.0864\n",
            "Epoch [27/200], Batch [81/123], Loss: 0.7398\n",
            "Epoch [27/200], Batch [82/123], Loss: 1.6254\n",
            "Epoch [27/200], Batch [83/123], Loss: 1.7395\n",
            "Epoch [27/200], Batch [84/123], Loss: 1.1459\n",
            "Epoch [27/200], Batch [85/123], Loss: 1.5733\n",
            "Epoch [27/200], Batch [86/123], Loss: 1.5434\n",
            "Epoch [27/200], Batch [87/123], Loss: 1.1810\n",
            "Epoch [27/200], Batch [88/123], Loss: 1.0509\n",
            "Epoch [27/200], Batch [89/123], Loss: 1.2739\n",
            "Epoch [27/200], Batch [90/123], Loss: 1.1618\n",
            "Epoch [27/200], Batch [91/123], Loss: 1.4746\n",
            "Epoch [27/200], Batch [92/123], Loss: 1.7407\n",
            "Epoch [27/200], Batch [93/123], Loss: 1.3162\n",
            "Epoch [27/200], Batch [94/123], Loss: 1.8887\n",
            "Epoch [27/200], Batch [95/123], Loss: 1.1422\n",
            "Epoch [27/200], Batch [96/123], Loss: 1.5383\n",
            "Epoch [27/200], Batch [97/123], Loss: 1.3935\n",
            "Epoch [27/200], Batch [98/123], Loss: 2.4737\n",
            "Epoch [27/200], Batch [99/123], Loss: 1.0013\n",
            "Epoch [27/200], Batch [100/123], Loss: 1.3087\n",
            "Epoch [27/200], Batch [101/123], Loss: 1.4676\n",
            "Epoch [27/200], Batch [102/123], Loss: 1.1659\n",
            "Epoch [27/200], Batch [103/123], Loss: 1.2029\n",
            "Epoch [27/200], Batch [104/123], Loss: 1.1066\n",
            "Epoch [27/200], Batch [105/123], Loss: 1.5184\n",
            "Epoch [27/200], Batch [106/123], Loss: 1.5491\n",
            "Epoch [27/200], Batch [107/123], Loss: 1.2903\n",
            "Epoch [27/200], Batch [108/123], Loss: 1.3055\n",
            "Epoch [27/200], Batch [109/123], Loss: 1.4620\n",
            "Epoch [27/200], Batch [110/123], Loss: 1.7496\n",
            "Epoch [27/200], Batch [111/123], Loss: 1.4206\n",
            "Epoch [27/200], Batch [112/123], Loss: 1.3787\n",
            "Epoch [27/200], Batch [113/123], Loss: 1.3300\n",
            "Epoch [27/200], Batch [114/123], Loss: 1.9900\n",
            "Epoch [27/200], Batch [115/123], Loss: 2.1922\n",
            "Epoch [27/200], Batch [116/123], Loss: 1.3055\n",
            "Epoch [27/200], Batch [117/123], Loss: 1.2878\n",
            "Epoch [27/200], Batch [118/123], Loss: 1.8924\n",
            "Epoch [27/200], Batch [119/123], Loss: 1.6678\n",
            "Epoch [27/200], Batch [120/123], Loss: 1.4221\n",
            "Epoch [27/200], Batch [121/123], Loss: 1.0353\n",
            "Epoch [27/200], Batch [122/123], Loss: 1.3974\n",
            "Epoch [27/200], Batch [123/123], Loss: 1.8100\n",
            "Epoch [28/200], Batch [1/123], Loss: 1.8488\n",
            "Epoch [28/200], Batch [2/123], Loss: 1.2324\n",
            "Epoch [28/200], Batch [3/123], Loss: 1.6407\n",
            "Epoch [28/200], Batch [4/123], Loss: 0.9435\n",
            "Epoch [28/200], Batch [5/123], Loss: 0.9565\n",
            "Epoch [28/200], Batch [6/123], Loss: 1.2504\n",
            "Epoch [28/200], Batch [7/123], Loss: 1.4216\n",
            "Epoch [28/200], Batch [8/123], Loss: 1.2496\n",
            "Epoch [28/200], Batch [9/123], Loss: 1.7402\n",
            "Epoch [28/200], Batch [10/123], Loss: 1.2522\n",
            "Epoch [28/200], Batch [11/123], Loss: 1.0122\n",
            "Epoch [28/200], Batch [12/123], Loss: 1.5149\n",
            "Epoch [28/200], Batch [13/123], Loss: 1.2846\n",
            "Epoch [28/200], Batch [14/123], Loss: 1.3989\n",
            "Epoch [28/200], Batch [15/123], Loss: 1.2422\n",
            "Epoch [28/200], Batch [16/123], Loss: 1.1355\n",
            "Epoch [28/200], Batch [17/123], Loss: 1.5845\n",
            "Epoch [28/200], Batch [18/123], Loss: 1.9083\n",
            "Epoch [28/200], Batch [19/123], Loss: 1.3896\n",
            "Epoch [28/200], Batch [20/123], Loss: 1.9996\n",
            "Epoch [28/200], Batch [21/123], Loss: 1.6838\n",
            "Epoch [28/200], Batch [22/123], Loss: 1.7235\n",
            "Epoch [28/200], Batch [23/123], Loss: 1.2667\n",
            "Epoch [28/200], Batch [24/123], Loss: 1.4388\n",
            "Epoch [28/200], Batch [25/123], Loss: 1.0809\n",
            "Epoch [28/200], Batch [26/123], Loss: 1.1860\n",
            "Epoch [28/200], Batch [27/123], Loss: 1.6564\n",
            "Epoch [28/200], Batch [28/123], Loss: 1.1243\n",
            "Epoch [28/200], Batch [29/123], Loss: 1.4624\n",
            "Epoch [28/200], Batch [30/123], Loss: 1.0223\n",
            "Epoch [28/200], Batch [31/123], Loss: 1.2986\n",
            "Epoch [28/200], Batch [32/123], Loss: 1.4888\n",
            "Epoch [28/200], Batch [33/123], Loss: 1.7456\n",
            "Epoch [28/200], Batch [34/123], Loss: 1.6469\n",
            "Epoch [28/200], Batch [35/123], Loss: 1.5973\n",
            "Epoch [28/200], Batch [36/123], Loss: 1.6409\n",
            "Epoch [28/200], Batch [37/123], Loss: 1.2350\n",
            "Epoch [28/200], Batch [38/123], Loss: 1.4589\n",
            "Epoch [28/200], Batch [39/123], Loss: 1.2230\n",
            "Epoch [28/200], Batch [40/123], Loss: 0.9344\n",
            "Epoch [28/200], Batch [41/123], Loss: 1.5497\n",
            "Epoch [28/200], Batch [42/123], Loss: 2.1954\n",
            "Epoch [28/200], Batch [43/123], Loss: 1.1459\n",
            "Epoch [28/200], Batch [44/123], Loss: 1.2453\n",
            "Epoch [28/200], Batch [45/123], Loss: 2.1847\n",
            "Epoch [28/200], Batch [46/123], Loss: 2.0454\n",
            "Epoch [28/200], Batch [47/123], Loss: 1.5298\n",
            "Epoch [28/200], Batch [48/123], Loss: 1.8599\n",
            "Epoch [28/200], Batch [49/123], Loss: 1.1064\n",
            "Epoch [28/200], Batch [50/123], Loss: 1.5858\n",
            "Epoch [28/200], Batch [51/123], Loss: 1.4851\n",
            "Epoch [28/200], Batch [52/123], Loss: 1.2794\n",
            "Epoch [28/200], Batch [53/123], Loss: 1.1824\n",
            "Epoch [28/200], Batch [54/123], Loss: 1.1965\n",
            "Epoch [28/200], Batch [55/123], Loss: 1.1917\n",
            "Epoch [28/200], Batch [56/123], Loss: 0.8919\n",
            "Epoch [28/200], Batch [57/123], Loss: 1.3761\n",
            "Epoch [28/200], Batch [58/123], Loss: 1.3598\n",
            "Epoch [28/200], Batch [59/123], Loss: 1.3573\n",
            "Epoch [28/200], Batch [60/123], Loss: 2.0450\n",
            "Epoch [28/200], Batch [61/123], Loss: 0.9578\n",
            "Epoch [28/200], Batch [62/123], Loss: 1.0712\n",
            "Epoch [28/200], Batch [63/123], Loss: 1.2365\n",
            "Epoch [28/200], Batch [64/123], Loss: 1.4399\n",
            "Epoch [28/200], Batch [65/123], Loss: 1.2531\n",
            "Epoch [28/200], Batch [66/123], Loss: 1.3897\n",
            "Epoch [28/200], Batch [67/123], Loss: 1.4849\n",
            "Epoch [28/200], Batch [68/123], Loss: 1.4208\n",
            "Epoch [28/200], Batch [69/123], Loss: 1.4411\n",
            "Epoch [28/200], Batch [70/123], Loss: 1.6619\n",
            "Epoch [28/200], Batch [71/123], Loss: 1.9035\n",
            "Epoch [28/200], Batch [72/123], Loss: 1.5145\n",
            "Epoch [28/200], Batch [73/123], Loss: 1.2825\n",
            "Epoch [28/200], Batch [74/123], Loss: 0.9816\n",
            "Epoch [28/200], Batch [75/123], Loss: 1.4025\n",
            "Epoch [28/200], Batch [76/123], Loss: 1.1856\n",
            "Epoch [28/200], Batch [77/123], Loss: 1.0628\n",
            "Epoch [28/200], Batch [78/123], Loss: 1.5808\n",
            "Epoch [28/200], Batch [79/123], Loss: 1.2893\n",
            "Epoch [28/200], Batch [80/123], Loss: 1.2714\n",
            "Epoch [28/200], Batch [81/123], Loss: 1.7033\n",
            "Epoch [28/200], Batch [82/123], Loss: 1.3300\n",
            "Epoch [28/200], Batch [83/123], Loss: 1.3384\n",
            "Epoch [28/200], Batch [84/123], Loss: 1.2524\n",
            "Epoch [28/200], Batch [85/123], Loss: 1.6445\n",
            "Epoch [28/200], Batch [86/123], Loss: 1.7879\n",
            "Epoch [28/200], Batch [87/123], Loss: 0.9955\n",
            "Epoch [28/200], Batch [88/123], Loss: 1.2678\n",
            "Epoch [28/200], Batch [89/123], Loss: 1.5669\n",
            "Epoch [28/200], Batch [90/123], Loss: 1.5388\n",
            "Epoch [28/200], Batch [91/123], Loss: 0.9322\n",
            "Epoch [28/200], Batch [92/123], Loss: 2.3027\n",
            "Epoch [28/200], Batch [93/123], Loss: 1.8775\n",
            "Epoch [28/200], Batch [94/123], Loss: 2.3313\n",
            "Epoch [28/200], Batch [95/123], Loss: 1.3965\n",
            "Epoch [28/200], Batch [96/123], Loss: 0.9821\n",
            "Epoch [28/200], Batch [97/123], Loss: 1.8818\n",
            "Epoch [28/200], Batch [98/123], Loss: 1.4163\n",
            "Epoch [28/200], Batch [99/123], Loss: 1.3264\n",
            "Epoch [28/200], Batch [100/123], Loss: 0.9998\n",
            "Epoch [28/200], Batch [101/123], Loss: 1.7513\n",
            "Epoch [28/200], Batch [102/123], Loss: 1.2680\n",
            "Epoch [28/200], Batch [103/123], Loss: 1.0915\n",
            "Epoch [28/200], Batch [104/123], Loss: 1.4213\n",
            "Epoch [28/200], Batch [105/123], Loss: 1.6258\n",
            "Epoch [28/200], Batch [106/123], Loss: 1.3193\n",
            "Epoch [28/200], Batch [107/123], Loss: 1.0999\n",
            "Epoch [28/200], Batch [108/123], Loss: 1.1297\n",
            "Epoch [28/200], Batch [109/123], Loss: 1.5514\n",
            "Epoch [28/200], Batch [110/123], Loss: 1.8162\n",
            "Epoch [28/200], Batch [111/123], Loss: 1.4019\n",
            "Epoch [28/200], Batch [112/123], Loss: 1.0110\n",
            "Epoch [28/200], Batch [113/123], Loss: 1.3491\n",
            "Epoch [28/200], Batch [114/123], Loss: 1.3408\n",
            "Epoch [28/200], Batch [115/123], Loss: 1.1205\n",
            "Epoch [28/200], Batch [116/123], Loss: 1.5352\n",
            "Epoch [28/200], Batch [117/123], Loss: 1.2628\n",
            "Epoch [28/200], Batch [118/123], Loss: 1.3726\n",
            "Epoch [28/200], Batch [119/123], Loss: 1.0905\n",
            "Epoch [28/200], Batch [120/123], Loss: 1.4310\n",
            "Epoch [28/200], Batch [121/123], Loss: 1.2013\n",
            "Epoch [28/200], Batch [122/123], Loss: 1.2094\n",
            "Epoch [28/200], Batch [123/123], Loss: 1.6182\n",
            "Epoch [29/200], Batch [1/123], Loss: 1.8110\n",
            "Epoch [29/200], Batch [2/123], Loss: 1.3330\n",
            "Epoch [29/200], Batch [3/123], Loss: 1.4083\n",
            "Epoch [29/200], Batch [4/123], Loss: 1.9666\n",
            "Epoch [29/200], Batch [5/123], Loss: 1.5734\n",
            "Epoch [29/200], Batch [6/123], Loss: 1.0953\n",
            "Epoch [29/200], Batch [7/123], Loss: 1.2660\n",
            "Epoch [29/200], Batch [8/123], Loss: 1.4659\n",
            "Epoch [29/200], Batch [9/123], Loss: 1.1148\n",
            "Epoch [29/200], Batch [10/123], Loss: 1.8033\n",
            "Epoch [29/200], Batch [11/123], Loss: 1.2246\n",
            "Epoch [29/200], Batch [12/123], Loss: 1.0307\n",
            "Epoch [29/200], Batch [13/123], Loss: 1.2504\n",
            "Epoch [29/200], Batch [14/123], Loss: 1.5166\n",
            "Epoch [29/200], Batch [15/123], Loss: 1.2057\n",
            "Epoch [29/200], Batch [16/123], Loss: 1.2124\n",
            "Epoch [29/200], Batch [17/123], Loss: 1.1021\n",
            "Epoch [29/200], Batch [18/123], Loss: 1.2549\n",
            "Epoch [29/200], Batch [19/123], Loss: 1.1728\n",
            "Epoch [29/200], Batch [20/123], Loss: 1.3942\n",
            "Epoch [29/200], Batch [21/123], Loss: 0.8713\n",
            "Epoch [29/200], Batch [22/123], Loss: 1.3186\n",
            "Epoch [29/200], Batch [23/123], Loss: 0.9975\n",
            "Epoch [29/200], Batch [24/123], Loss: 1.5694\n",
            "Epoch [29/200], Batch [25/123], Loss: 1.4512\n",
            "Epoch [29/200], Batch [26/123], Loss: 1.2149\n",
            "Epoch [29/200], Batch [27/123], Loss: 2.0780\n",
            "Epoch [29/200], Batch [28/123], Loss: 1.4984\n",
            "Epoch [29/200], Batch [29/123], Loss: 1.2393\n",
            "Epoch [29/200], Batch [30/123], Loss: 1.2190\n",
            "Epoch [29/200], Batch [31/123], Loss: 1.4480\n",
            "Epoch [29/200], Batch [32/123], Loss: 1.1242\n",
            "Epoch [29/200], Batch [33/123], Loss: 1.1427\n",
            "Epoch [29/200], Batch [34/123], Loss: 0.9336\n",
            "Epoch [29/200], Batch [35/123], Loss: 1.6053\n",
            "Epoch [29/200], Batch [36/123], Loss: 1.6422\n",
            "Epoch [29/200], Batch [37/123], Loss: 1.4270\n",
            "Epoch [29/200], Batch [38/123], Loss: 1.7439\n",
            "Epoch [29/200], Batch [39/123], Loss: 1.3117\n",
            "Epoch [29/200], Batch [40/123], Loss: 1.0184\n",
            "Epoch [29/200], Batch [41/123], Loss: 1.2617\n",
            "Epoch [29/200], Batch [42/123], Loss: 1.1212\n",
            "Epoch [29/200], Batch [43/123], Loss: 1.1813\n",
            "Epoch [29/200], Batch [44/123], Loss: 1.1984\n",
            "Epoch [29/200], Batch [45/123], Loss: 1.0149\n",
            "Epoch [29/200], Batch [46/123], Loss: 1.2880\n",
            "Epoch [29/200], Batch [47/123], Loss: 1.2195\n",
            "Epoch [29/200], Batch [48/123], Loss: 1.2431\n",
            "Epoch [29/200], Batch [49/123], Loss: 0.9660\n",
            "Epoch [29/200], Batch [50/123], Loss: 1.3559\n",
            "Epoch [29/200], Batch [51/123], Loss: 1.4788\n",
            "Epoch [29/200], Batch [52/123], Loss: 1.4677\n",
            "Epoch [29/200], Batch [53/123], Loss: 1.2815\n",
            "Epoch [29/200], Batch [54/123], Loss: 1.1671\n",
            "Epoch [29/200], Batch [55/123], Loss: 1.3170\n",
            "Epoch [29/200], Batch [56/123], Loss: 1.4424\n",
            "Epoch [29/200], Batch [57/123], Loss: 1.7630\n",
            "Epoch [29/200], Batch [58/123], Loss: 1.7276\n",
            "Epoch [29/200], Batch [59/123], Loss: 1.0917\n",
            "Epoch [29/200], Batch [60/123], Loss: 0.9949\n",
            "Epoch [29/200], Batch [61/123], Loss: 1.4999\n",
            "Epoch [29/200], Batch [62/123], Loss: 1.2996\n",
            "Epoch [29/200], Batch [63/123], Loss: 1.7446\n",
            "Epoch [29/200], Batch [64/123], Loss: 1.1146\n",
            "Epoch [29/200], Batch [65/123], Loss: 2.2690\n",
            "Epoch [29/200], Batch [66/123], Loss: 1.7331\n",
            "Epoch [29/200], Batch [67/123], Loss: 1.0808\n",
            "Epoch [29/200], Batch [68/123], Loss: 2.3476\n",
            "Epoch [29/200], Batch [69/123], Loss: 1.4778\n",
            "Epoch [29/200], Batch [70/123], Loss: 1.2050\n",
            "Epoch [29/200], Batch [71/123], Loss: 1.3859\n",
            "Epoch [29/200], Batch [72/123], Loss: 1.0135\n",
            "Epoch [29/200], Batch [73/123], Loss: 1.2855\n",
            "Epoch [29/200], Batch [74/123], Loss: 1.3669\n",
            "Epoch [29/200], Batch [75/123], Loss: 1.0699\n",
            "Epoch [29/200], Batch [76/123], Loss: 0.8439\n",
            "Epoch [29/200], Batch [77/123], Loss: 1.2618\n",
            "Epoch [29/200], Batch [78/123], Loss: 1.2967\n",
            "Epoch [29/200], Batch [79/123], Loss: 1.4395\n",
            "Epoch [29/200], Batch [80/123], Loss: 1.3259\n",
            "Epoch [29/200], Batch [81/123], Loss: 0.9546\n",
            "Epoch [29/200], Batch [82/123], Loss: 1.5278\n",
            "Epoch [29/200], Batch [83/123], Loss: 1.1697\n",
            "Epoch [29/200], Batch [84/123], Loss: 0.9957\n",
            "Epoch [29/200], Batch [85/123], Loss: 1.1822\n",
            "Epoch [29/200], Batch [86/123], Loss: 1.7833\n",
            "Epoch [29/200], Batch [87/123], Loss: 1.2426\n",
            "Epoch [29/200], Batch [88/123], Loss: 1.3739\n",
            "Epoch [29/200], Batch [89/123], Loss: 1.1221\n",
            "Epoch [29/200], Batch [90/123], Loss: 1.4244\n",
            "Epoch [29/200], Batch [91/123], Loss: 1.9252\n",
            "Epoch [29/200], Batch [92/123], Loss: 1.4317\n",
            "Epoch [29/200], Batch [93/123], Loss: 0.9000\n",
            "Epoch [29/200], Batch [94/123], Loss: 1.3797\n",
            "Epoch [29/200], Batch [95/123], Loss: 1.1380\n",
            "Epoch [29/200], Batch [96/123], Loss: 1.1848\n",
            "Epoch [29/200], Batch [97/123], Loss: 1.2949\n",
            "Epoch [29/200], Batch [98/123], Loss: 1.2336\n",
            "Epoch [29/200], Batch [99/123], Loss: 0.9356\n",
            "Epoch [29/200], Batch [100/123], Loss: 2.1982\n",
            "Epoch [29/200], Batch [101/123], Loss: 1.4704\n",
            "Epoch [29/200], Batch [102/123], Loss: 1.4609\n",
            "Epoch [29/200], Batch [103/123], Loss: 1.1277\n",
            "Epoch [29/200], Batch [104/123], Loss: 0.9237\n",
            "Epoch [29/200], Batch [105/123], Loss: 0.9379\n",
            "Epoch [29/200], Batch [106/123], Loss: 1.6707\n",
            "Epoch [29/200], Batch [107/123], Loss: 1.3967\n",
            "Epoch [29/200], Batch [108/123], Loss: 1.8008\n",
            "Epoch [29/200], Batch [109/123], Loss: 1.6723\n",
            "Epoch [29/200], Batch [110/123], Loss: 1.1153\n",
            "Epoch [29/200], Batch [111/123], Loss: 1.5387\n",
            "Epoch [29/200], Batch [112/123], Loss: 1.0672\n",
            "Epoch [29/200], Batch [113/123], Loss: 1.2754\n",
            "Epoch [29/200], Batch [114/123], Loss: 1.1870\n",
            "Epoch [29/200], Batch [115/123], Loss: 1.1514\n",
            "Epoch [29/200], Batch [116/123], Loss: 1.2208\n",
            "Epoch [29/200], Batch [117/123], Loss: 2.0051\n",
            "Epoch [29/200], Batch [118/123], Loss: 1.1346\n",
            "Epoch [29/200], Batch [119/123], Loss: 1.2719\n",
            "Epoch [29/200], Batch [120/123], Loss: 1.6469\n",
            "Epoch [29/200], Batch [121/123], Loss: 1.0525\n",
            "Epoch [29/200], Batch [122/123], Loss: 1.3656\n",
            "Epoch [29/200], Batch [123/123], Loss: 1.2130\n",
            "Epoch [30/200], Batch [1/123], Loss: 1.3867\n",
            "Epoch [30/200], Batch [2/123], Loss: 1.8160\n",
            "Epoch [30/200], Batch [3/123], Loss: 1.3859\n",
            "Epoch [30/200], Batch [4/123], Loss: 1.3449\n",
            "Epoch [30/200], Batch [5/123], Loss: 1.3671\n",
            "Epoch [30/200], Batch [6/123], Loss: 1.4125\n",
            "Epoch [30/200], Batch [7/123], Loss: 1.0682\n",
            "Epoch [30/200], Batch [8/123], Loss: 1.3066\n",
            "Epoch [30/200], Batch [9/123], Loss: 1.3411\n",
            "Epoch [30/200], Batch [10/123], Loss: 1.3685\n",
            "Epoch [30/200], Batch [11/123], Loss: 1.0777\n",
            "Epoch [30/200], Batch [12/123], Loss: 1.0358\n",
            "Epoch [30/200], Batch [13/123], Loss: 1.5415\n",
            "Epoch [30/200], Batch [14/123], Loss: 0.9512\n",
            "Epoch [30/200], Batch [15/123], Loss: 1.3265\n",
            "Epoch [30/200], Batch [16/123], Loss: 1.0375\n",
            "Epoch [30/200], Batch [17/123], Loss: 1.6489\n",
            "Epoch [30/200], Batch [18/123], Loss: 1.6391\n",
            "Epoch [30/200], Batch [19/123], Loss: 1.8065\n",
            "Epoch [30/200], Batch [20/123], Loss: 1.3026\n",
            "Epoch [30/200], Batch [21/123], Loss: 1.2636\n",
            "Epoch [30/200], Batch [22/123], Loss: 1.6132\n",
            "Epoch [30/200], Batch [23/123], Loss: 1.0633\n",
            "Epoch [30/200], Batch [24/123], Loss: 1.0855\n",
            "Epoch [30/200], Batch [25/123], Loss: 1.5538\n",
            "Epoch [30/200], Batch [26/123], Loss: 1.0875\n",
            "Epoch [30/200], Batch [27/123], Loss: 1.4328\n",
            "Epoch [30/200], Batch [28/123], Loss: 1.1839\n",
            "Epoch [30/200], Batch [29/123], Loss: 1.0751\n",
            "Epoch [30/200], Batch [30/123], Loss: 1.3265\n",
            "Epoch [30/200], Batch [31/123], Loss: 1.0269\n",
            "Epoch [30/200], Batch [32/123], Loss: 0.8046\n",
            "Epoch [30/200], Batch [33/123], Loss: 0.7557\n",
            "Epoch [30/200], Batch [34/123], Loss: 1.0390\n",
            "Epoch [30/200], Batch [35/123], Loss: 1.2067\n",
            "Epoch [30/200], Batch [36/123], Loss: 1.3544\n",
            "Epoch [30/200], Batch [37/123], Loss: 1.3930\n",
            "Epoch [30/200], Batch [38/123], Loss: 1.3132\n",
            "Epoch [30/200], Batch [39/123], Loss: 1.6588\n",
            "Epoch [30/200], Batch [40/123], Loss: 1.0262\n",
            "Epoch [30/200], Batch [41/123], Loss: 1.2291\n",
            "Epoch [30/200], Batch [42/123], Loss: 1.7215\n",
            "Epoch [30/200], Batch [43/123], Loss: 1.6700\n",
            "Epoch [30/200], Batch [44/123], Loss: 1.6727\n",
            "Epoch [30/200], Batch [45/123], Loss: 1.3200\n",
            "Epoch [30/200], Batch [46/123], Loss: 1.6831\n",
            "Epoch [30/200], Batch [47/123], Loss: 0.9524\n",
            "Epoch [30/200], Batch [48/123], Loss: 1.2337\n",
            "Epoch [30/200], Batch [49/123], Loss: 0.9630\n",
            "Epoch [30/200], Batch [50/123], Loss: 1.2725\n",
            "Epoch [30/200], Batch [51/123], Loss: 1.1459\n",
            "Epoch [30/200], Batch [52/123], Loss: 1.2759\n",
            "Epoch [30/200], Batch [53/123], Loss: 1.0141\n",
            "Epoch [30/200], Batch [54/123], Loss: 0.8410\n",
            "Epoch [30/200], Batch [55/123], Loss: 1.0849\n",
            "Epoch [30/200], Batch [56/123], Loss: 0.9359\n",
            "Epoch [30/200], Batch [57/123], Loss: 1.0899\n",
            "Epoch [30/200], Batch [58/123], Loss: 1.3126\n",
            "Epoch [30/200], Batch [59/123], Loss: 1.2155\n",
            "Epoch [30/200], Batch [60/123], Loss: 1.3183\n",
            "Epoch [30/200], Batch [61/123], Loss: 1.8421\n",
            "Epoch [30/200], Batch [62/123], Loss: 0.8593\n",
            "Epoch [30/200], Batch [63/123], Loss: 1.9863\n",
            "Epoch [30/200], Batch [64/123], Loss: 1.6337\n",
            "Epoch [30/200], Batch [65/123], Loss: 1.3370\n",
            "Epoch [30/200], Batch [66/123], Loss: 1.2175\n",
            "Epoch [30/200], Batch [67/123], Loss: 1.5162\n",
            "Epoch [30/200], Batch [68/123], Loss: 1.4312\n",
            "Epoch [30/200], Batch [69/123], Loss: 1.7225\n",
            "Epoch [30/200], Batch [70/123], Loss: 1.3913\n",
            "Epoch [30/200], Batch [71/123], Loss: 1.0546\n",
            "Epoch [30/200], Batch [72/123], Loss: 1.3440\n",
            "Epoch [30/200], Batch [73/123], Loss: 1.1642\n",
            "Epoch [30/200], Batch [74/123], Loss: 1.7817\n",
            "Epoch [30/200], Batch [75/123], Loss: 2.1508\n",
            "Epoch [30/200], Batch [76/123], Loss: 1.1893\n",
            "Epoch [30/200], Batch [77/123], Loss: 1.3041\n",
            "Epoch [30/200], Batch [78/123], Loss: 1.3730\n",
            "Epoch [30/200], Batch [79/123], Loss: 1.1867\n",
            "Epoch [30/200], Batch [80/123], Loss: 1.2350\n",
            "Epoch [30/200], Batch [81/123], Loss: 1.6930\n",
            "Epoch [30/200], Batch [82/123], Loss: 1.0008\n",
            "Epoch [30/200], Batch [83/123], Loss: 1.1945\n",
            "Epoch [30/200], Batch [84/123], Loss: 1.6940\n",
            "Epoch [30/200], Batch [85/123], Loss: 2.0653\n",
            "Epoch [30/200], Batch [86/123], Loss: 0.9127\n",
            "Epoch [30/200], Batch [87/123], Loss: 1.2408\n",
            "Epoch [30/200], Batch [88/123], Loss: 1.0223\n",
            "Epoch [30/200], Batch [89/123], Loss: 0.6822\n",
            "Epoch [30/200], Batch [90/123], Loss: 1.1714\n",
            "Epoch [30/200], Batch [91/123], Loss: 1.1845\n",
            "Epoch [30/200], Batch [92/123], Loss: 1.1137\n",
            "Epoch [30/200], Batch [93/123], Loss: 1.2988\n",
            "Epoch [30/200], Batch [94/123], Loss: 1.6058\n",
            "Epoch [30/200], Batch [95/123], Loss: 1.2191\n",
            "Epoch [30/200], Batch [96/123], Loss: 1.0829\n",
            "Epoch [30/200], Batch [97/123], Loss: 1.2644\n",
            "Epoch [30/200], Batch [98/123], Loss: 1.8214\n",
            "Epoch [30/200], Batch [99/123], Loss: 1.2206\n",
            "Epoch [30/200], Batch [100/123], Loss: 1.5144\n",
            "Epoch [30/200], Batch [101/123], Loss: 1.1311\n",
            "Epoch [30/200], Batch [102/123], Loss: 1.1459\n",
            "Epoch [30/200], Batch [103/123], Loss: 0.9561\n",
            "Epoch [30/200], Batch [104/123], Loss: 1.0874\n",
            "Epoch [30/200], Batch [105/123], Loss: 1.3112\n",
            "Epoch [30/200], Batch [106/123], Loss: 0.9627\n",
            "Epoch [30/200], Batch [107/123], Loss: 1.6162\n",
            "Epoch [30/200], Batch [108/123], Loss: 1.7288\n",
            "Epoch [30/200], Batch [109/123], Loss: 1.6272\n",
            "Epoch [30/200], Batch [110/123], Loss: 0.9679\n",
            "Epoch [30/200], Batch [111/123], Loss: 1.0660\n",
            "Epoch [30/200], Batch [112/123], Loss: 1.3089\n",
            "Epoch [30/200], Batch [113/123], Loss: 1.5677\n",
            "Epoch [30/200], Batch [114/123], Loss: 1.2181\n",
            "Epoch [30/200], Batch [115/123], Loss: 0.8612\n",
            "Epoch [30/200], Batch [116/123], Loss: 1.1927\n",
            "Epoch [30/200], Batch [117/123], Loss: 1.2757\n",
            "Epoch [30/200], Batch [118/123], Loss: 0.8497\n",
            "Epoch [30/200], Batch [119/123], Loss: 1.0897\n",
            "Epoch [30/200], Batch [120/123], Loss: 0.9478\n",
            "Epoch [30/200], Batch [121/123], Loss: 1.0008\n",
            "Epoch [30/200], Batch [122/123], Loss: 1.6105\n",
            "Epoch [30/200], Batch [123/123], Loss: 0.8569\n",
            "Epoch [31/200], Batch [1/123], Loss: 1.2773\n",
            "Epoch [31/200], Batch [2/123], Loss: 1.0622\n",
            "Epoch [31/200], Batch [3/123], Loss: 1.6061\n",
            "Epoch [31/200], Batch [4/123], Loss: 1.5400\n",
            "Epoch [31/200], Batch [5/123], Loss: 1.1271\n",
            "Epoch [31/200], Batch [6/123], Loss: 1.3002\n",
            "Epoch [31/200], Batch [7/123], Loss: 0.8801\n",
            "Epoch [31/200], Batch [8/123], Loss: 1.3943\n",
            "Epoch [31/200], Batch [9/123], Loss: 1.8243\n",
            "Epoch [31/200], Batch [10/123], Loss: 0.8756\n",
            "Epoch [31/200], Batch [11/123], Loss: 1.0768\n",
            "Epoch [31/200], Batch [12/123], Loss: 1.7007\n",
            "Epoch [31/200], Batch [13/123], Loss: 1.0558\n",
            "Epoch [31/200], Batch [14/123], Loss: 1.2449\n",
            "Epoch [31/200], Batch [15/123], Loss: 1.8865\n",
            "Epoch [31/200], Batch [16/123], Loss: 1.1698\n",
            "Epoch [31/200], Batch [17/123], Loss: 1.4150\n",
            "Epoch [31/200], Batch [18/123], Loss: 1.0736\n",
            "Epoch [31/200], Batch [19/123], Loss: 1.3267\n",
            "Epoch [31/200], Batch [20/123], Loss: 1.5992\n",
            "Epoch [31/200], Batch [21/123], Loss: 1.1088\n",
            "Epoch [31/200], Batch [22/123], Loss: 0.7667\n",
            "Epoch [31/200], Batch [23/123], Loss: 0.9953\n",
            "Epoch [31/200], Batch [24/123], Loss: 1.0610\n",
            "Epoch [31/200], Batch [25/123], Loss: 1.1943\n",
            "Epoch [31/200], Batch [26/123], Loss: 1.3605\n",
            "Epoch [31/200], Batch [27/123], Loss: 1.6989\n",
            "Epoch [31/200], Batch [28/123], Loss: 1.4830\n",
            "Epoch [31/200], Batch [29/123], Loss: 1.3688\n",
            "Epoch [31/200], Batch [30/123], Loss: 1.5451\n",
            "Epoch [31/200], Batch [31/123], Loss: 1.3481\n",
            "Epoch [31/200], Batch [32/123], Loss: 1.5666\n",
            "Epoch [31/200], Batch [33/123], Loss: 1.2600\n",
            "Epoch [31/200], Batch [34/123], Loss: 1.2770\n",
            "Epoch [31/200], Batch [35/123], Loss: 1.0763\n",
            "Epoch [31/200], Batch [36/123], Loss: 1.3085\n",
            "Epoch [31/200], Batch [37/123], Loss: 1.1404\n",
            "Epoch [31/200], Batch [38/123], Loss: 2.1723\n",
            "Epoch [31/200], Batch [39/123], Loss: 1.3835\n",
            "Epoch [31/200], Batch [40/123], Loss: 0.9080\n",
            "Epoch [31/200], Batch [41/123], Loss: 1.1140\n",
            "Epoch [31/200], Batch [42/123], Loss: 1.1902\n",
            "Epoch [31/200], Batch [43/123], Loss: 1.1927\n",
            "Epoch [31/200], Batch [44/123], Loss: 1.1438\n",
            "Epoch [31/200], Batch [45/123], Loss: 1.0924\n",
            "Epoch [31/200], Batch [46/123], Loss: 0.9913\n",
            "Epoch [31/200], Batch [47/123], Loss: 1.2562\n",
            "Epoch [31/200], Batch [48/123], Loss: 1.0236\n",
            "Epoch [31/200], Batch [49/123], Loss: 1.5309\n",
            "Epoch [31/200], Batch [50/123], Loss: 1.2381\n",
            "Epoch [31/200], Batch [51/123], Loss: 1.1512\n",
            "Epoch [31/200], Batch [52/123], Loss: 1.6928\n",
            "Epoch [31/200], Batch [53/123], Loss: 0.9139\n",
            "Epoch [31/200], Batch [54/123], Loss: 1.5771\n",
            "Epoch [31/200], Batch [55/123], Loss: 1.9611\n",
            "Epoch [31/200], Batch [56/123], Loss: 1.0977\n",
            "Epoch [31/200], Batch [57/123], Loss: 0.9664\n",
            "Epoch [31/200], Batch [58/123], Loss: 1.0066\n",
            "Epoch [31/200], Batch [59/123], Loss: 1.8025\n",
            "Epoch [31/200], Batch [60/123], Loss: 1.6333\n",
            "Epoch [31/200], Batch [61/123], Loss: 1.8708\n",
            "Epoch [31/200], Batch [62/123], Loss: 1.2133\n",
            "Epoch [31/200], Batch [63/123], Loss: 0.9463\n",
            "Epoch [31/200], Batch [64/123], Loss: 1.4731\n",
            "Epoch [31/200], Batch [65/123], Loss: 1.8449\n",
            "Epoch [31/200], Batch [66/123], Loss: 1.2964\n",
            "Epoch [31/200], Batch [67/123], Loss: 0.9270\n",
            "Epoch [31/200], Batch [68/123], Loss: 1.0657\n",
            "Epoch [31/200], Batch [69/123], Loss: 1.3623\n",
            "Epoch [31/200], Batch [70/123], Loss: 1.3395\n",
            "Epoch [31/200], Batch [71/123], Loss: 1.4236\n",
            "Epoch [31/200], Batch [72/123], Loss: 1.3299\n",
            "Epoch [31/200], Batch [73/123], Loss: 1.3712\n",
            "Epoch [31/200], Batch [74/123], Loss: 1.1413\n",
            "Epoch [31/200], Batch [75/123], Loss: 1.4727\n",
            "Epoch [31/200], Batch [76/123], Loss: 1.6236\n",
            "Epoch [31/200], Batch [77/123], Loss: 0.9571\n",
            "Epoch [31/200], Batch [78/123], Loss: 1.6330\n",
            "Epoch [31/200], Batch [79/123], Loss: 1.1609\n",
            "Epoch [31/200], Batch [80/123], Loss: 1.0765\n",
            "Epoch [31/200], Batch [81/123], Loss: 1.4363\n",
            "Epoch [31/200], Batch [82/123], Loss: 1.3102\n",
            "Epoch [31/200], Batch [83/123], Loss: 1.7472\n",
            "Epoch [31/200], Batch [84/123], Loss: 1.3678\n",
            "Epoch [31/200], Batch [85/123], Loss: 1.5738\n",
            "Epoch [31/200], Batch [86/123], Loss: 1.4505\n",
            "Epoch [31/200], Batch [87/123], Loss: 1.2020\n",
            "Epoch [31/200], Batch [88/123], Loss: 1.4497\n",
            "Epoch [31/200], Batch [89/123], Loss: 1.0140\n",
            "Epoch [31/200], Batch [90/123], Loss: 1.1802\n",
            "Epoch [31/200], Batch [91/123], Loss: 1.0501\n",
            "Epoch [31/200], Batch [92/123], Loss: 1.3639\n",
            "Epoch [31/200], Batch [93/123], Loss: 1.5069\n",
            "Epoch [31/200], Batch [94/123], Loss: 1.3542\n",
            "Epoch [31/200], Batch [95/123], Loss: 1.0740\n",
            "Epoch [31/200], Batch [96/123], Loss: 0.9362\n",
            "Epoch [31/200], Batch [97/123], Loss: 1.6017\n",
            "Epoch [31/200], Batch [98/123], Loss: 1.2372\n",
            "Epoch [31/200], Batch [99/123], Loss: 1.1574\n",
            "Epoch [31/200], Batch [100/123], Loss: 1.2576\n",
            "Epoch [31/200], Batch [101/123], Loss: 1.4633\n",
            "Epoch [31/200], Batch [102/123], Loss: 1.2299\n",
            "Epoch [31/200], Batch [103/123], Loss: 1.4612\n",
            "Epoch [31/200], Batch [104/123], Loss: 1.4014\n",
            "Epoch [31/200], Batch [105/123], Loss: 1.0102\n",
            "Epoch [31/200], Batch [106/123], Loss: 1.4762\n",
            "Epoch [31/200], Batch [107/123], Loss: 1.6753\n",
            "Epoch [31/200], Batch [108/123], Loss: 1.2307\n",
            "Epoch [31/200], Batch [109/123], Loss: 1.2430\n",
            "Epoch [31/200], Batch [110/123], Loss: 0.9291\n",
            "Epoch [31/200], Batch [111/123], Loss: 1.3674\n",
            "Epoch [31/200], Batch [112/123], Loss: 1.0957\n",
            "Epoch [31/200], Batch [113/123], Loss: 1.5363\n",
            "Epoch [31/200], Batch [114/123], Loss: 1.4669\n",
            "Epoch [31/200], Batch [115/123], Loss: 1.0756\n",
            "Epoch [31/200], Batch [116/123], Loss: 1.1832\n",
            "Epoch [31/200], Batch [117/123], Loss: 1.4857\n",
            "Epoch [31/200], Batch [118/123], Loss: 1.4460\n",
            "Epoch [31/200], Batch [119/123], Loss: 1.3987\n",
            "Epoch [31/200], Batch [120/123], Loss: 0.9253\n",
            "Epoch [31/200], Batch [121/123], Loss: 1.1409\n",
            "Epoch [31/200], Batch [122/123], Loss: 1.1798\n",
            "Epoch [31/200], Batch [123/123], Loss: 0.6328\n",
            "Epoch [32/200], Batch [1/123], Loss: 0.9057\n",
            "Epoch [32/200], Batch [2/123], Loss: 1.4095\n",
            "Epoch [32/200], Batch [3/123], Loss: 1.2328\n",
            "Epoch [32/200], Batch [4/123], Loss: 1.3796\n",
            "Epoch [32/200], Batch [5/123], Loss: 1.8286\n",
            "Epoch [32/200], Batch [6/123], Loss: 1.0935\n",
            "Epoch [32/200], Batch [7/123], Loss: 1.1681\n",
            "Epoch [32/200], Batch [8/123], Loss: 1.5768\n",
            "Epoch [32/200], Batch [9/123], Loss: 2.1944\n",
            "Epoch [32/200], Batch [10/123], Loss: 1.6364\n",
            "Epoch [32/200], Batch [11/123], Loss: 1.8078\n",
            "Epoch [32/200], Batch [12/123], Loss: 0.8830\n",
            "Epoch [32/200], Batch [13/123], Loss: 1.3250\n",
            "Epoch [32/200], Batch [14/123], Loss: 1.3201\n",
            "Epoch [32/200], Batch [15/123], Loss: 1.1818\n",
            "Epoch [32/200], Batch [16/123], Loss: 0.8122\n",
            "Epoch [32/200], Batch [17/123], Loss: 1.5383\n",
            "Epoch [32/200], Batch [18/123], Loss: 1.2809\n",
            "Epoch [32/200], Batch [19/123], Loss: 1.6631\n",
            "Epoch [32/200], Batch [20/123], Loss: 1.1469\n",
            "Epoch [32/200], Batch [21/123], Loss: 0.9409\n",
            "Epoch [32/200], Batch [22/123], Loss: 1.2181\n",
            "Epoch [32/200], Batch [23/123], Loss: 1.3926\n",
            "Epoch [32/200], Batch [24/123], Loss: 1.1425\n",
            "Epoch [32/200], Batch [25/123], Loss: 1.1742\n",
            "Epoch [32/200], Batch [26/123], Loss: 1.4373\n",
            "Epoch [32/200], Batch [27/123], Loss: 1.0710\n",
            "Epoch [32/200], Batch [28/123], Loss: 1.2967\n",
            "Epoch [32/200], Batch [29/123], Loss: 1.4148\n",
            "Epoch [32/200], Batch [30/123], Loss: 1.4874\n",
            "Epoch [32/200], Batch [31/123], Loss: 1.0413\n",
            "Epoch [32/200], Batch [32/123], Loss: 1.3338\n",
            "Epoch [32/200], Batch [33/123], Loss: 1.5695\n",
            "Epoch [32/200], Batch [34/123], Loss: 1.4403\n",
            "Epoch [32/200], Batch [35/123], Loss: 1.4864\n",
            "Epoch [32/200], Batch [36/123], Loss: 1.4126\n",
            "Epoch [32/200], Batch [37/123], Loss: 0.8805\n",
            "Epoch [32/200], Batch [38/123], Loss: 1.0392\n",
            "Epoch [32/200], Batch [39/123], Loss: 1.3012\n",
            "Epoch [32/200], Batch [40/123], Loss: 1.2800\n",
            "Epoch [32/200], Batch [41/123], Loss: 1.4053\n",
            "Epoch [32/200], Batch [42/123], Loss: 1.0489\n",
            "Epoch [32/200], Batch [43/123], Loss: 1.4849\n",
            "Epoch [32/200], Batch [44/123], Loss: 2.6602\n",
            "Epoch [32/200], Batch [45/123], Loss: 1.3134\n",
            "Epoch [32/200], Batch [46/123], Loss: 1.1722\n",
            "Epoch [32/200], Batch [47/123], Loss: 1.1107\n",
            "Epoch [32/200], Batch [48/123], Loss: 1.1227\n",
            "Epoch [32/200], Batch [49/123], Loss: 1.2656\n",
            "Epoch [32/200], Batch [50/123], Loss: 1.2111\n",
            "Epoch [32/200], Batch [51/123], Loss: 1.6054\n",
            "Epoch [32/200], Batch [52/123], Loss: 1.3972\n",
            "Epoch [32/200], Batch [53/123], Loss: 1.5667\n",
            "Epoch [32/200], Batch [54/123], Loss: 1.4655\n",
            "Epoch [32/200], Batch [55/123], Loss: 1.1659\n",
            "Epoch [32/200], Batch [56/123], Loss: 1.4447\n",
            "Epoch [32/200], Batch [57/123], Loss: 1.8720\n",
            "Epoch [32/200], Batch [58/123], Loss: 1.2020\n",
            "Epoch [32/200], Batch [59/123], Loss: 1.3106\n",
            "Epoch [32/200], Batch [60/123], Loss: 1.4068\n",
            "Epoch [32/200], Batch [61/123], Loss: 1.0176\n",
            "Epoch [32/200], Batch [62/123], Loss: 0.8671\n",
            "Epoch [32/200], Batch [63/123], Loss: 1.8860\n",
            "Epoch [32/200], Batch [64/123], Loss: 1.1369\n",
            "Epoch [32/200], Batch [65/123], Loss: 1.5371\n",
            "Epoch [32/200], Batch [66/123], Loss: 1.3021\n",
            "Epoch [32/200], Batch [67/123], Loss: 1.0246\n",
            "Epoch [32/200], Batch [68/123], Loss: 1.7920\n",
            "Epoch [32/200], Batch [69/123], Loss: 1.5663\n",
            "Epoch [32/200], Batch [70/123], Loss: 1.4910\n",
            "Epoch [32/200], Batch [71/123], Loss: 1.5745\n",
            "Epoch [32/200], Batch [72/123], Loss: 1.3600\n",
            "Epoch [32/200], Batch [73/123], Loss: 1.0317\n",
            "Epoch [32/200], Batch [74/123], Loss: 1.3244\n",
            "Epoch [32/200], Batch [75/123], Loss: 2.1568\n",
            "Epoch [32/200], Batch [76/123], Loss: 1.6428\n",
            "Epoch [32/200], Batch [77/123], Loss: 2.5257\n",
            "Epoch [32/200], Batch [78/123], Loss: 1.3901\n",
            "Epoch [32/200], Batch [79/123], Loss: 2.1093\n",
            "Epoch [32/200], Batch [80/123], Loss: 1.0743\n",
            "Epoch [32/200], Batch [81/123], Loss: 1.1397\n",
            "Epoch [32/200], Batch [82/123], Loss: 1.5374\n",
            "Epoch [32/200], Batch [83/123], Loss: 1.1986\n",
            "Epoch [32/200], Batch [84/123], Loss: 1.1768\n",
            "Epoch [32/200], Batch [85/123], Loss: 1.3027\n",
            "Epoch [32/200], Batch [86/123], Loss: 1.3122\n",
            "Epoch [32/200], Batch [87/123], Loss: 1.5532\n",
            "Epoch [32/200], Batch [88/123], Loss: 1.1320\n",
            "Epoch [32/200], Batch [89/123], Loss: 1.2263\n",
            "Epoch [32/200], Batch [90/123], Loss: 1.4705\n",
            "Epoch [32/200], Batch [91/123], Loss: 1.1540\n",
            "Epoch [32/200], Batch [92/123], Loss: 1.0339\n",
            "Epoch [32/200], Batch [93/123], Loss: 1.2760\n",
            "Epoch [32/200], Batch [94/123], Loss: 1.3463\n",
            "Epoch [32/200], Batch [95/123], Loss: 0.8992\n",
            "Epoch [32/200], Batch [96/123], Loss: 1.2695\n",
            "Epoch [32/200], Batch [97/123], Loss: 1.6596\n",
            "Epoch [32/200], Batch [98/123], Loss: 1.4306\n",
            "Epoch [32/200], Batch [99/123], Loss: 0.8566\n",
            "Epoch [32/200], Batch [100/123], Loss: 0.7074\n",
            "Epoch [32/200], Batch [101/123], Loss: 0.9215\n",
            "Epoch [32/200], Batch [102/123], Loss: 1.6468\n",
            "Epoch [32/200], Batch [103/123], Loss: 1.1720\n",
            "Epoch [32/200], Batch [104/123], Loss: 1.1513\n",
            "Epoch [32/200], Batch [105/123], Loss: 1.0707\n",
            "Epoch [32/200], Batch [106/123], Loss: 0.8360\n",
            "Epoch [32/200], Batch [107/123], Loss: 0.8515\n",
            "Epoch [32/200], Batch [108/123], Loss: 1.2258\n",
            "Epoch [32/200], Batch [109/123], Loss: 0.9850\n",
            "Epoch [32/200], Batch [110/123], Loss: 1.2747\n",
            "Epoch [32/200], Batch [111/123], Loss: 1.1869\n",
            "Epoch [32/200], Batch [112/123], Loss: 1.0195\n",
            "Epoch [32/200], Batch [113/123], Loss: 1.0769\n",
            "Epoch [32/200], Batch [114/123], Loss: 1.1840\n",
            "Epoch [32/200], Batch [115/123], Loss: 1.1315\n",
            "Epoch [32/200], Batch [116/123], Loss: 1.1311\n",
            "Epoch [32/200], Batch [117/123], Loss: 0.7716\n",
            "Epoch [32/200], Batch [118/123], Loss: 1.2110\n",
            "Epoch [32/200], Batch [119/123], Loss: 1.0524\n",
            "Epoch [32/200], Batch [120/123], Loss: 0.9037\n",
            "Epoch [32/200], Batch [121/123], Loss: 1.7796\n",
            "Epoch [32/200], Batch [122/123], Loss: 1.5511\n",
            "Epoch [32/200], Batch [123/123], Loss: 1.5383\n",
            "Epoch [33/200], Batch [1/123], Loss: 1.2134\n",
            "Epoch [33/200], Batch [2/123], Loss: 1.0879\n",
            "Epoch [33/200], Batch [3/123], Loss: 1.0059\n",
            "Epoch [33/200], Batch [4/123], Loss: 1.0956\n",
            "Epoch [33/200], Batch [5/123], Loss: 1.4258\n",
            "Epoch [33/200], Batch [6/123], Loss: 1.3111\n",
            "Epoch [33/200], Batch [7/123], Loss: 1.7030\n",
            "Epoch [33/200], Batch [8/123], Loss: 0.7021\n",
            "Epoch [33/200], Batch [9/123], Loss: 1.6014\n",
            "Epoch [33/200], Batch [10/123], Loss: 1.2078\n",
            "Epoch [33/200], Batch [11/123], Loss: 1.6464\n",
            "Epoch [33/200], Batch [12/123], Loss: 1.1834\n",
            "Epoch [33/200], Batch [13/123], Loss: 1.1309\n",
            "Epoch [33/200], Batch [14/123], Loss: 1.2979\n",
            "Epoch [33/200], Batch [15/123], Loss: 1.0693\n",
            "Epoch [33/200], Batch [16/123], Loss: 2.2505\n",
            "Epoch [33/200], Batch [17/123], Loss: 1.1355\n",
            "Epoch [33/200], Batch [18/123], Loss: 0.7111\n",
            "Epoch [33/200], Batch [19/123], Loss: 2.6887\n",
            "Epoch [33/200], Batch [20/123], Loss: 1.6971\n",
            "Epoch [33/200], Batch [21/123], Loss: 1.4131\n",
            "Epoch [33/200], Batch [22/123], Loss: 1.1831\n",
            "Epoch [33/200], Batch [23/123], Loss: 1.4603\n",
            "Epoch [33/200], Batch [24/123], Loss: 1.2200\n",
            "Epoch [33/200], Batch [25/123], Loss: 0.9968\n",
            "Epoch [33/200], Batch [26/123], Loss: 1.5567\n",
            "Epoch [33/200], Batch [27/123], Loss: 1.0818\n",
            "Epoch [33/200], Batch [28/123], Loss: 1.4895\n",
            "Epoch [33/200], Batch [29/123], Loss: 1.4362\n",
            "Epoch [33/200], Batch [30/123], Loss: 1.0951\n",
            "Epoch [33/200], Batch [31/123], Loss: 0.9815\n",
            "Epoch [33/200], Batch [32/123], Loss: 1.4827\n",
            "Epoch [33/200], Batch [33/123], Loss: 1.3363\n",
            "Epoch [33/200], Batch [34/123], Loss: 1.0767\n",
            "Epoch [33/200], Batch [35/123], Loss: 0.9370\n",
            "Epoch [33/200], Batch [36/123], Loss: 1.4600\n",
            "Epoch [33/200], Batch [37/123], Loss: 0.8549\n",
            "Epoch [33/200], Batch [38/123], Loss: 1.4979\n",
            "Epoch [33/200], Batch [39/123], Loss: 1.5255\n",
            "Epoch [33/200], Batch [40/123], Loss: 1.4516\n",
            "Epoch [33/200], Batch [41/123], Loss: 1.1364\n",
            "Epoch [33/200], Batch [42/123], Loss: 1.1260\n",
            "Epoch [33/200], Batch [43/123], Loss: 0.9295\n",
            "Epoch [33/200], Batch [44/123], Loss: 1.2317\n",
            "Epoch [33/200], Batch [45/123], Loss: 1.0940\n",
            "Epoch [33/200], Batch [46/123], Loss: 1.2194\n",
            "Epoch [33/200], Batch [47/123], Loss: 1.1207\n",
            "Epoch [33/200], Batch [48/123], Loss: 2.5925\n",
            "Epoch [33/200], Batch [49/123], Loss: 0.8108\n",
            "Epoch [33/200], Batch [50/123], Loss: 1.1300\n",
            "Epoch [33/200], Batch [51/123], Loss: 1.1514\n",
            "Epoch [33/200], Batch [52/123], Loss: 1.3952\n",
            "Epoch [33/200], Batch [53/123], Loss: 1.2972\n",
            "Epoch [33/200], Batch [54/123], Loss: 1.0397\n",
            "Epoch [33/200], Batch [55/123], Loss: 1.3739\n",
            "Epoch [33/200], Batch [56/123], Loss: 1.1900\n",
            "Epoch [33/200], Batch [57/123], Loss: 1.2567\n",
            "Epoch [33/200], Batch [58/123], Loss: 1.1911\n",
            "Epoch [33/200], Batch [59/123], Loss: 1.5085\n",
            "Epoch [33/200], Batch [60/123], Loss: 1.5311\n",
            "Epoch [33/200], Batch [61/123], Loss: 0.9463\n",
            "Epoch [33/200], Batch [62/123], Loss: 1.3333\n",
            "Epoch [33/200], Batch [63/123], Loss: 1.5202\n",
            "Epoch [33/200], Batch [64/123], Loss: 1.4395\n",
            "Epoch [33/200], Batch [65/123], Loss: 1.0377\n",
            "Epoch [33/200], Batch [66/123], Loss: 0.9715\n",
            "Epoch [33/200], Batch [67/123], Loss: 1.4475\n",
            "Epoch [33/200], Batch [68/123], Loss: 1.8348\n",
            "Epoch [33/200], Batch [69/123], Loss: 1.7171\n",
            "Epoch [33/200], Batch [70/123], Loss: 1.2205\n",
            "Epoch [33/200], Batch [71/123], Loss: 1.6011\n",
            "Epoch [33/200], Batch [72/123], Loss: 1.5094\n",
            "Epoch [33/200], Batch [73/123], Loss: 1.2090\n",
            "Epoch [33/200], Batch [74/123], Loss: 1.4360\n",
            "Epoch [33/200], Batch [75/123], Loss: 1.0899\n",
            "Epoch [33/200], Batch [76/123], Loss: 1.0816\n",
            "Epoch [33/200], Batch [77/123], Loss: 2.4652\n",
            "Epoch [33/200], Batch [78/123], Loss: 1.5787\n",
            "Epoch [33/200], Batch [79/123], Loss: 1.4566\n",
            "Epoch [33/200], Batch [80/123], Loss: 1.1678\n",
            "Epoch [33/200], Batch [81/123], Loss: 1.2262\n",
            "Epoch [33/200], Batch [82/123], Loss: 1.6558\n",
            "Epoch [33/200], Batch [83/123], Loss: 1.1176\n",
            "Epoch [33/200], Batch [84/123], Loss: 1.2156\n",
            "Epoch [33/200], Batch [85/123], Loss: 1.1253\n",
            "Epoch [33/200], Batch [86/123], Loss: 1.4236\n",
            "Epoch [33/200], Batch [87/123], Loss: 1.4447\n",
            "Epoch [33/200], Batch [88/123], Loss: 1.0492\n",
            "Epoch [33/200], Batch [89/123], Loss: 1.8035\n",
            "Epoch [33/200], Batch [90/123], Loss: 0.9848\n",
            "Epoch [33/200], Batch [91/123], Loss: 1.3089\n",
            "Epoch [33/200], Batch [92/123], Loss: 1.5689\n",
            "Epoch [33/200], Batch [93/123], Loss: 0.9207\n",
            "Epoch [33/200], Batch [94/123], Loss: 1.1176\n",
            "Epoch [33/200], Batch [95/123], Loss: 2.3671\n",
            "Epoch [33/200], Batch [96/123], Loss: 1.4036\n",
            "Epoch [33/200], Batch [97/123], Loss: 1.8958\n",
            "Epoch [33/200], Batch [98/123], Loss: 1.7590\n",
            "Epoch [33/200], Batch [99/123], Loss: 1.3523\n",
            "Epoch [33/200], Batch [100/123], Loss: 1.7600\n",
            "Epoch [33/200], Batch [101/123], Loss: 1.1517\n",
            "Epoch [33/200], Batch [102/123], Loss: 1.0593\n",
            "Epoch [33/200], Batch [103/123], Loss: 1.2555\n",
            "Epoch [33/200], Batch [104/123], Loss: 0.9029\n",
            "Epoch [33/200], Batch [105/123], Loss: 1.0959\n",
            "Epoch [33/200], Batch [106/123], Loss: 0.8225\n",
            "Epoch [33/200], Batch [107/123], Loss: 1.2849\n",
            "Epoch [33/200], Batch [108/123], Loss: 1.3843\n",
            "Epoch [33/200], Batch [109/123], Loss: 0.9643\n",
            "Epoch [33/200], Batch [110/123], Loss: 1.4676\n",
            "Epoch [33/200], Batch [111/123], Loss: 0.9142\n",
            "Epoch [33/200], Batch [112/123], Loss: 1.0947\n",
            "Epoch [33/200], Batch [113/123], Loss: 1.2487\n",
            "Epoch [33/200], Batch [114/123], Loss: 0.9501\n",
            "Epoch [33/200], Batch [115/123], Loss: 1.2781\n",
            "Epoch [33/200], Batch [116/123], Loss: 1.1711\n",
            "Epoch [33/200], Batch [117/123], Loss: 0.9841\n",
            "Epoch [33/200], Batch [118/123], Loss: 0.8410\n",
            "Epoch [33/200], Batch [119/123], Loss: 0.9210\n",
            "Epoch [33/200], Batch [120/123], Loss: 1.2860\n",
            "Epoch [33/200], Batch [121/123], Loss: 0.9739\n",
            "Epoch [33/200], Batch [122/123], Loss: 1.1820\n",
            "Epoch [33/200], Batch [123/123], Loss: 1.1202\n",
            "Epoch [34/200], Batch [1/123], Loss: 1.6136\n",
            "Epoch [34/200], Batch [2/123], Loss: 1.2084\n",
            "Epoch [34/200], Batch [3/123], Loss: 1.1531\n",
            "Epoch [34/200], Batch [4/123], Loss: 1.3687\n",
            "Epoch [34/200], Batch [5/123], Loss: 1.3666\n",
            "Epoch [34/200], Batch [6/123], Loss: 1.0654\n",
            "Epoch [34/200], Batch [7/123], Loss: 0.8175\n",
            "Epoch [34/200], Batch [8/123], Loss: 1.1398\n",
            "Epoch [34/200], Batch [9/123], Loss: 1.2913\n",
            "Epoch [34/200], Batch [10/123], Loss: 1.3009\n",
            "Epoch [34/200], Batch [11/123], Loss: 1.4754\n",
            "Epoch [34/200], Batch [12/123], Loss: 1.9783\n",
            "Epoch [34/200], Batch [13/123], Loss: 1.4758\n",
            "Epoch [34/200], Batch [14/123], Loss: 1.2825\n",
            "Epoch [34/200], Batch [15/123], Loss: 0.8157\n",
            "Epoch [34/200], Batch [16/123], Loss: 1.2787\n",
            "Epoch [34/200], Batch [17/123], Loss: 0.7612\n",
            "Epoch [34/200], Batch [18/123], Loss: 1.2899\n",
            "Epoch [34/200], Batch [19/123], Loss: 1.4846\n",
            "Epoch [34/200], Batch [20/123], Loss: 1.4452\n",
            "Epoch [34/200], Batch [21/123], Loss: 1.1133\n",
            "Epoch [34/200], Batch [22/123], Loss: 1.3062\n",
            "Epoch [34/200], Batch [23/123], Loss: 1.0856\n",
            "Epoch [34/200], Batch [24/123], Loss: 1.2168\n",
            "Epoch [34/200], Batch [25/123], Loss: 1.1612\n",
            "Epoch [34/200], Batch [26/123], Loss: 1.0829\n",
            "Epoch [34/200], Batch [27/123], Loss: 2.2579\n",
            "Epoch [34/200], Batch [28/123], Loss: 1.0962\n",
            "Epoch [34/200], Batch [29/123], Loss: 1.2225\n",
            "Epoch [34/200], Batch [30/123], Loss: 1.4573\n",
            "Epoch [34/200], Batch [31/123], Loss: 0.9973\n",
            "Epoch [34/200], Batch [32/123], Loss: 1.6503\n",
            "Epoch [34/200], Batch [33/123], Loss: 1.3315\n",
            "Epoch [34/200], Batch [34/123], Loss: 1.1790\n",
            "Epoch [34/200], Batch [35/123], Loss: 1.2426\n",
            "Epoch [34/200], Batch [36/123], Loss: 0.9711\n",
            "Epoch [34/200], Batch [37/123], Loss: 0.8850\n",
            "Epoch [34/200], Batch [38/123], Loss: 1.3604\n",
            "Epoch [34/200], Batch [39/123], Loss: 1.2545\n",
            "Epoch [34/200], Batch [40/123], Loss: 1.8701\n",
            "Epoch [34/200], Batch [41/123], Loss: 1.1666\n",
            "Epoch [34/200], Batch [42/123], Loss: 1.1900\n",
            "Epoch [34/200], Batch [43/123], Loss: 1.0640\n",
            "Epoch [34/200], Batch [44/123], Loss: 1.1482\n",
            "Epoch [34/200], Batch [45/123], Loss: 0.9670\n",
            "Epoch [34/200], Batch [46/123], Loss: 1.4050\n",
            "Epoch [34/200], Batch [47/123], Loss: 1.9406\n",
            "Epoch [34/200], Batch [48/123], Loss: 1.3559\n",
            "Epoch [34/200], Batch [49/123], Loss: 0.9212\n",
            "Epoch [34/200], Batch [50/123], Loss: 1.1044\n",
            "Epoch [34/200], Batch [51/123], Loss: 1.0650\n",
            "Epoch [34/200], Batch [52/123], Loss: 1.0825\n",
            "Epoch [34/200], Batch [53/123], Loss: 0.9812\n",
            "Epoch [34/200], Batch [54/123], Loss: 1.3941\n",
            "Epoch [34/200], Batch [55/123], Loss: 2.0522\n",
            "Epoch [34/200], Batch [56/123], Loss: 1.2152\n",
            "Epoch [34/200], Batch [57/123], Loss: 1.0306\n",
            "Epoch [34/200], Batch [58/123], Loss: 1.2832\n",
            "Epoch [34/200], Batch [59/123], Loss: 1.6252\n",
            "Epoch [34/200], Batch [60/123], Loss: 1.2569\n",
            "Epoch [34/200], Batch [61/123], Loss: 1.0384\n",
            "Epoch [34/200], Batch [62/123], Loss: 1.3578\n",
            "Epoch [34/200], Batch [63/123], Loss: 1.0362\n",
            "Epoch [34/200], Batch [64/123], Loss: 1.5959\n",
            "Epoch [34/200], Batch [65/123], Loss: 0.9556\n",
            "Epoch [34/200], Batch [66/123], Loss: 1.1687\n",
            "Epoch [34/200], Batch [67/123], Loss: 1.1281\n",
            "Epoch [34/200], Batch [68/123], Loss: 0.9997\n",
            "Epoch [34/200], Batch [69/123], Loss: 1.1416\n",
            "Epoch [34/200], Batch [70/123], Loss: 1.5907\n",
            "Epoch [34/200], Batch [71/123], Loss: 1.0360\n",
            "Epoch [34/200], Batch [72/123], Loss: 1.4661\n",
            "Epoch [34/200], Batch [73/123], Loss: 1.1804\n",
            "Epoch [34/200], Batch [74/123], Loss: 1.0806\n",
            "Epoch [34/200], Batch [75/123], Loss: 1.3462\n",
            "Epoch [34/200], Batch [76/123], Loss: 1.1985\n",
            "Epoch [34/200], Batch [77/123], Loss: 0.9825\n",
            "Epoch [34/200], Batch [78/123], Loss: 1.3373\n",
            "Epoch [34/200], Batch [79/123], Loss: 1.5966\n",
            "Epoch [34/200], Batch [80/123], Loss: 1.1396\n",
            "Epoch [34/200], Batch [81/123], Loss: 1.1540\n",
            "Epoch [34/200], Batch [82/123], Loss: 1.2571\n",
            "Epoch [34/200], Batch [83/123], Loss: 1.3195\n",
            "Epoch [34/200], Batch [84/123], Loss: 1.4334\n",
            "Epoch [34/200], Batch [85/123], Loss: 1.3038\n",
            "Epoch [34/200], Batch [86/123], Loss: 0.8821\n",
            "Epoch [34/200], Batch [87/123], Loss: 1.0944\n",
            "Epoch [34/200], Batch [88/123], Loss: 0.9089\n",
            "Epoch [34/200], Batch [89/123], Loss: 1.2940\n",
            "Epoch [34/200], Batch [90/123], Loss: 1.8664\n",
            "Epoch [34/200], Batch [91/123], Loss: 1.2686\n",
            "Epoch [34/200], Batch [92/123], Loss: 1.8494\n",
            "Epoch [34/200], Batch [93/123], Loss: 1.4482\n",
            "Epoch [34/200], Batch [94/123], Loss: 1.1566\n",
            "Epoch [34/200], Batch [95/123], Loss: 1.1803\n",
            "Epoch [34/200], Batch [96/123], Loss: 1.2325\n",
            "Epoch [34/200], Batch [97/123], Loss: 1.1708\n",
            "Epoch [34/200], Batch [98/123], Loss: 1.2037\n",
            "Epoch [34/200], Batch [99/123], Loss: 1.1973\n",
            "Epoch [34/200], Batch [100/123], Loss: 1.2229\n",
            "Epoch [34/200], Batch [101/123], Loss: 1.2627\n",
            "Epoch [34/200], Batch [102/123], Loss: 1.2260\n",
            "Epoch [34/200], Batch [103/123], Loss: 0.9924\n",
            "Epoch [34/200], Batch [104/123], Loss: 1.1246\n",
            "Epoch [34/200], Batch [105/123], Loss: 0.9817\n",
            "Epoch [34/200], Batch [106/123], Loss: 1.4883\n",
            "Epoch [34/200], Batch [107/123], Loss: 1.3058\n",
            "Epoch [34/200], Batch [108/123], Loss: 1.0869\n",
            "Epoch [34/200], Batch [109/123], Loss: 0.9951\n",
            "Epoch [34/200], Batch [110/123], Loss: 1.5763\n",
            "Epoch [34/200], Batch [111/123], Loss: 1.1165\n",
            "Epoch [34/200], Batch [112/123], Loss: 0.7856\n",
            "Epoch [34/200], Batch [113/123], Loss: 1.0702\n",
            "Epoch [34/200], Batch [114/123], Loss: 1.3669\n",
            "Epoch [34/200], Batch [115/123], Loss: 1.1332\n",
            "Epoch [34/200], Batch [116/123], Loss: 1.4607\n",
            "Epoch [34/200], Batch [117/123], Loss: 1.2267\n",
            "Epoch [34/200], Batch [118/123], Loss: 1.3888\n",
            "Epoch [34/200], Batch [119/123], Loss: 1.6391\n",
            "Epoch [34/200], Batch [120/123], Loss: 1.0575\n",
            "Epoch [34/200], Batch [121/123], Loss: 1.3267\n",
            "Epoch [34/200], Batch [122/123], Loss: 0.9874\n",
            "Epoch [34/200], Batch [123/123], Loss: 1.1092\n",
            "Epoch [35/200], Batch [1/123], Loss: 0.9352\n",
            "Epoch [35/200], Batch [2/123], Loss: 1.1414\n",
            "Epoch [35/200], Batch [3/123], Loss: 1.0380\n",
            "Epoch [35/200], Batch [4/123], Loss: 1.2613\n",
            "Epoch [35/200], Batch [5/123], Loss: 0.9250\n",
            "Epoch [35/200], Batch [6/123], Loss: 1.4442\n",
            "Epoch [35/200], Batch [7/123], Loss: 1.5813\n",
            "Epoch [35/200], Batch [8/123], Loss: 0.9871\n",
            "Epoch [35/200], Batch [9/123], Loss: 1.1344\n",
            "Epoch [35/200], Batch [10/123], Loss: 1.5637\n",
            "Epoch [35/200], Batch [11/123], Loss: 1.0142\n",
            "Epoch [35/200], Batch [12/123], Loss: 1.2442\n",
            "Epoch [35/200], Batch [13/123], Loss: 1.4799\n",
            "Epoch [35/200], Batch [14/123], Loss: 0.8378\n",
            "Epoch [35/200], Batch [15/123], Loss: 1.3169\n",
            "Epoch [35/200], Batch [16/123], Loss: 1.6119\n",
            "Epoch [35/200], Batch [17/123], Loss: 1.5976\n",
            "Epoch [35/200], Batch [18/123], Loss: 1.3105\n",
            "Epoch [35/200], Batch [19/123], Loss: 0.9452\n",
            "Epoch [35/200], Batch [20/123], Loss: 1.1768\n",
            "Epoch [35/200], Batch [21/123], Loss: 1.1790\n",
            "Epoch [35/200], Batch [22/123], Loss: 1.1922\n",
            "Epoch [35/200], Batch [23/123], Loss: 1.3193\n",
            "Epoch [35/200], Batch [24/123], Loss: 1.5870\n",
            "Epoch [35/200], Batch [25/123], Loss: 1.3676\n",
            "Epoch [35/200], Batch [26/123], Loss: 1.2927\n",
            "Epoch [35/200], Batch [27/123], Loss: 1.3042\n",
            "Epoch [35/200], Batch [28/123], Loss: 1.2550\n",
            "Epoch [35/200], Batch [29/123], Loss: 1.0135\n",
            "Epoch [35/200], Batch [30/123], Loss: 1.0463\n",
            "Epoch [35/200], Batch [31/123], Loss: 1.4392\n",
            "Epoch [35/200], Batch [32/123], Loss: 1.2003\n",
            "Epoch [35/200], Batch [33/123], Loss: 1.1117\n",
            "Epoch [35/200], Batch [34/123], Loss: 1.3871\n",
            "Epoch [35/200], Batch [35/123], Loss: 1.0303\n",
            "Epoch [35/200], Batch [36/123], Loss: 1.0735\n",
            "Epoch [35/200], Batch [37/123], Loss: 1.1216\n",
            "Epoch [35/200], Batch [38/123], Loss: 1.0361\n",
            "Epoch [35/200], Batch [39/123], Loss: 1.0569\n",
            "Epoch [35/200], Batch [40/123], Loss: 1.2153\n",
            "Epoch [35/200], Batch [41/123], Loss: 0.9330\n",
            "Epoch [35/200], Batch [42/123], Loss: 1.3225\n",
            "Epoch [35/200], Batch [43/123], Loss: 1.4817\n",
            "Epoch [35/200], Batch [44/123], Loss: 1.4378\n",
            "Epoch [35/200], Batch [45/123], Loss: 1.4818\n",
            "Epoch [35/200], Batch [46/123], Loss: 0.9373\n",
            "Epoch [35/200], Batch [47/123], Loss: 1.0524\n",
            "Epoch [35/200], Batch [48/123], Loss: 1.0903\n",
            "Epoch [35/200], Batch [49/123], Loss: 1.2511\n",
            "Epoch [35/200], Batch [50/123], Loss: 1.4001\n",
            "Epoch [35/200], Batch [51/123], Loss: 0.8179\n",
            "Epoch [35/200], Batch [52/123], Loss: 1.1243\n",
            "Epoch [35/200], Batch [53/123], Loss: 0.9331\n",
            "Epoch [35/200], Batch [54/123], Loss: 0.9484\n",
            "Epoch [35/200], Batch [55/123], Loss: 1.3950\n",
            "Epoch [35/200], Batch [56/123], Loss: 0.9137\n",
            "Epoch [35/200], Batch [57/123], Loss: 1.0823\n",
            "Epoch [35/200], Batch [58/123], Loss: 1.3343\n",
            "Epoch [35/200], Batch [59/123], Loss: 1.0469\n",
            "Epoch [35/200], Batch [60/123], Loss: 1.1503\n",
            "Epoch [35/200], Batch [61/123], Loss: 1.0373\n",
            "Epoch [35/200], Batch [62/123], Loss: 0.8650\n",
            "Epoch [35/200], Batch [63/123], Loss: 1.3574\n",
            "Epoch [35/200], Batch [64/123], Loss: 1.2455\n",
            "Epoch [35/200], Batch [65/123], Loss: 1.0828\n",
            "Epoch [35/200], Batch [66/123], Loss: 1.1342\n",
            "Epoch [35/200], Batch [67/123], Loss: 1.1372\n",
            "Epoch [35/200], Batch [68/123], Loss: 1.0440\n",
            "Epoch [35/200], Batch [69/123], Loss: 1.0074\n",
            "Epoch [35/200], Batch [70/123], Loss: 1.4548\n",
            "Epoch [35/200], Batch [71/123], Loss: 1.5183\n",
            "Epoch [35/200], Batch [72/123], Loss: 0.9156\n",
            "Epoch [35/200], Batch [73/123], Loss: 2.4895\n",
            "Epoch [35/200], Batch [74/123], Loss: 1.0683\n",
            "Epoch [35/200], Batch [75/123], Loss: 1.2535\n",
            "Epoch [35/200], Batch [76/123], Loss: 1.3533\n",
            "Epoch [35/200], Batch [77/123], Loss: 1.3930\n",
            "Epoch [35/200], Batch [78/123], Loss: 1.6439\n",
            "Epoch [35/200], Batch [79/123], Loss: 1.1947\n",
            "Epoch [35/200], Batch [80/123], Loss: 1.8458\n",
            "Epoch [35/200], Batch [81/123], Loss: 0.9485\n",
            "Epoch [35/200], Batch [82/123], Loss: 2.1130\n",
            "Epoch [35/200], Batch [83/123], Loss: 1.3486\n",
            "Epoch [35/200], Batch [84/123], Loss: 0.9092\n",
            "Epoch [35/200], Batch [85/123], Loss: 0.9256\n",
            "Epoch [35/200], Batch [86/123], Loss: 0.9826\n",
            "Epoch [35/200], Batch [87/123], Loss: 1.0418\n",
            "Epoch [35/200], Batch [88/123], Loss: 1.2687\n",
            "Epoch [35/200], Batch [89/123], Loss: 0.9816\n",
            "Epoch [35/200], Batch [90/123], Loss: 0.8438\n",
            "Epoch [35/200], Batch [91/123], Loss: 0.9875\n",
            "Epoch [35/200], Batch [92/123], Loss: 1.0055\n",
            "Epoch [35/200], Batch [93/123], Loss: 1.0988\n",
            "Epoch [35/200], Batch [94/123], Loss: 1.0085\n",
            "Epoch [35/200], Batch [95/123], Loss: 1.0507\n",
            "Epoch [35/200], Batch [96/123], Loss: 0.7402\n",
            "Epoch [35/200], Batch [97/123], Loss: 1.2769\n",
            "Epoch [35/200], Batch [98/123], Loss: 1.3115\n",
            "Epoch [35/200], Batch [99/123], Loss: 1.2138\n",
            "Epoch [35/200], Batch [100/123], Loss: 1.3047\n",
            "Epoch [35/200], Batch [101/123], Loss: 0.9850\n",
            "Epoch [35/200], Batch [102/123], Loss: 1.4133\n",
            "Epoch [35/200], Batch [103/123], Loss: 0.8358\n",
            "Epoch [35/200], Batch [104/123], Loss: 1.2686\n",
            "Epoch [35/200], Batch [105/123], Loss: 1.4334\n",
            "Epoch [35/200], Batch [106/123], Loss: 1.3637\n",
            "Epoch [35/200], Batch [107/123], Loss: 0.8764\n",
            "Epoch [35/200], Batch [108/123], Loss: 1.0005\n",
            "Epoch [35/200], Batch [109/123], Loss: 0.8870\n",
            "Epoch [35/200], Batch [110/123], Loss: 1.1437\n",
            "Epoch [35/200], Batch [111/123], Loss: 1.2319\n",
            "Epoch [35/200], Batch [112/123], Loss: 1.5851\n",
            "Epoch [35/200], Batch [113/123], Loss: 1.2796\n",
            "Epoch [35/200], Batch [114/123], Loss: 1.0119\n",
            "Epoch [35/200], Batch [115/123], Loss: 1.3470\n",
            "Epoch [35/200], Batch [116/123], Loss: 1.2884\n",
            "Epoch [35/200], Batch [117/123], Loss: 1.1461\n",
            "Epoch [35/200], Batch [118/123], Loss: 1.8450\n",
            "Epoch [35/200], Batch [119/123], Loss: 1.2516\n",
            "Epoch [35/200], Batch [120/123], Loss: 1.2485\n",
            "Epoch [35/200], Batch [121/123], Loss: 1.1002\n",
            "Epoch [35/200], Batch [122/123], Loss: 0.9795\n",
            "Epoch [35/200], Batch [123/123], Loss: 1.2365\n",
            "Epoch [36/200], Batch [1/123], Loss: 1.1809\n",
            "Epoch [36/200], Batch [2/123], Loss: 0.9978\n",
            "Epoch [36/200], Batch [3/123], Loss: 1.3487\n",
            "Epoch [36/200], Batch [4/123], Loss: 1.1619\n",
            "Epoch [36/200], Batch [5/123], Loss: 0.9930\n",
            "Epoch [36/200], Batch [6/123], Loss: 1.1285\n",
            "Epoch [36/200], Batch [7/123], Loss: 1.1296\n",
            "Epoch [36/200], Batch [8/123], Loss: 1.0335\n",
            "Epoch [36/200], Batch [9/123], Loss: 1.6534\n",
            "Epoch [36/200], Batch [10/123], Loss: 1.1279\n",
            "Epoch [36/200], Batch [11/123], Loss: 1.1334\n",
            "Epoch [36/200], Batch [12/123], Loss: 1.4138\n",
            "Epoch [36/200], Batch [13/123], Loss: 1.3572\n",
            "Epoch [36/200], Batch [14/123], Loss: 1.0951\n",
            "Epoch [36/200], Batch [15/123], Loss: 0.8619\n",
            "Epoch [36/200], Batch [16/123], Loss: 0.7626\n",
            "Epoch [36/200], Batch [17/123], Loss: 1.4095\n",
            "Epoch [36/200], Batch [18/123], Loss: 1.0398\n",
            "Epoch [36/200], Batch [19/123], Loss: 1.1800\n",
            "Epoch [36/200], Batch [20/123], Loss: 0.9992\n",
            "Epoch [36/200], Batch [21/123], Loss: 1.2084\n",
            "Epoch [36/200], Batch [22/123], Loss: 1.1790\n",
            "Epoch [36/200], Batch [23/123], Loss: 1.1788\n",
            "Epoch [36/200], Batch [24/123], Loss: 1.2426\n",
            "Epoch [36/200], Batch [25/123], Loss: 1.1923\n",
            "Epoch [36/200], Batch [26/123], Loss: 1.0181\n",
            "Epoch [36/200], Batch [27/123], Loss: 2.7555\n",
            "Epoch [36/200], Batch [28/123], Loss: 0.9967\n",
            "Epoch [36/200], Batch [29/123], Loss: 1.8121\n",
            "Epoch [36/200], Batch [30/123], Loss: 1.9136\n",
            "Epoch [36/200], Batch [31/123], Loss: 1.1231\n",
            "Epoch [36/200], Batch [32/123], Loss: 1.8119\n",
            "Epoch [36/200], Batch [33/123], Loss: 1.2100\n",
            "Epoch [36/200], Batch [34/123], Loss: 1.1337\n",
            "Epoch [36/200], Batch [35/123], Loss: 1.1162\n",
            "Epoch [36/200], Batch [36/123], Loss: 1.2596\n",
            "Epoch [36/200], Batch [37/123], Loss: 1.2342\n",
            "Epoch [36/200], Batch [38/123], Loss: 1.4122\n",
            "Epoch [36/200], Batch [39/123], Loss: 1.0924\n",
            "Epoch [36/200], Batch [40/123], Loss: 0.8738\n",
            "Epoch [36/200], Batch [41/123], Loss: 1.0609\n",
            "Epoch [36/200], Batch [42/123], Loss: 0.8536\n",
            "Epoch [36/200], Batch [43/123], Loss: 1.6934\n",
            "Epoch [36/200], Batch [44/123], Loss: 1.6422\n",
            "Epoch [36/200], Batch [45/123], Loss: 1.7667\n",
            "Epoch [36/200], Batch [46/123], Loss: 1.1537\n",
            "Epoch [36/200], Batch [47/123], Loss: 1.3701\n",
            "Epoch [36/200], Batch [48/123], Loss: 2.2457\n",
            "Epoch [36/200], Batch [49/123], Loss: 1.1748\n",
            "Epoch [36/200], Batch [50/123], Loss: 1.1299\n",
            "Epoch [36/200], Batch [51/123], Loss: 1.0001\n",
            "Epoch [36/200], Batch [52/123], Loss: 2.4289\n",
            "Epoch [36/200], Batch [53/123], Loss: 0.9892\n",
            "Epoch [36/200], Batch [54/123], Loss: 1.4023\n",
            "Epoch [36/200], Batch [55/123], Loss: 1.1037\n",
            "Epoch [36/200], Batch [56/123], Loss: 1.5174\n",
            "Epoch [36/200], Batch [57/123], Loss: 1.1637\n",
            "Epoch [36/200], Batch [58/123], Loss: 0.8244\n",
            "Epoch [36/200], Batch [59/123], Loss: 1.3867\n",
            "Epoch [36/200], Batch [60/123], Loss: 1.0514\n",
            "Epoch [36/200], Batch [61/123], Loss: 0.8503\n",
            "Epoch [36/200], Batch [62/123], Loss: 1.1055\n",
            "Epoch [36/200], Batch [63/123], Loss: 1.0652\n",
            "Epoch [36/200], Batch [64/123], Loss: 1.1455\n",
            "Epoch [36/200], Batch [65/123], Loss: 0.8402\n",
            "Epoch [36/200], Batch [66/123], Loss: 1.6205\n",
            "Epoch [36/200], Batch [67/123], Loss: 1.0493\n",
            "Epoch [36/200], Batch [68/123], Loss: 0.9681\n",
            "Epoch [36/200], Batch [69/123], Loss: 1.5659\n",
            "Epoch [36/200], Batch [70/123], Loss: 1.9541\n",
            "Epoch [36/200], Batch [71/123], Loss: 0.8721\n",
            "Epoch [36/200], Batch [72/123], Loss: 0.9495\n",
            "Epoch [36/200], Batch [73/123], Loss: 0.9823\n",
            "Epoch [36/200], Batch [74/123], Loss: 1.0373\n",
            "Epoch [36/200], Batch [75/123], Loss: 1.2494\n",
            "Epoch [36/200], Batch [76/123], Loss: 1.2030\n",
            "Epoch [36/200], Batch [77/123], Loss: 0.8958\n",
            "Epoch [36/200], Batch [78/123], Loss: 1.0059\n",
            "Epoch [36/200], Batch [79/123], Loss: 1.1911\n",
            "Epoch [36/200], Batch [80/123], Loss: 1.4589\n",
            "Epoch [36/200], Batch [81/123], Loss: 1.3745\n",
            "Epoch [36/200], Batch [82/123], Loss: 1.3933\n",
            "Epoch [36/200], Batch [83/123], Loss: 0.8049\n",
            "Epoch [36/200], Batch [84/123], Loss: 1.2879\n",
            "Epoch [36/200], Batch [85/123], Loss: 1.4966\n",
            "Epoch [36/200], Batch [86/123], Loss: 1.5425\n",
            "Epoch [36/200], Batch [87/123], Loss: 1.2360\n",
            "Epoch [36/200], Batch [88/123], Loss: 1.0640\n",
            "Epoch [36/200], Batch [89/123], Loss: 0.9453\n",
            "Epoch [36/200], Batch [90/123], Loss: 1.4949\n",
            "Epoch [36/200], Batch [91/123], Loss: 1.8327\n",
            "Epoch [36/200], Batch [92/123], Loss: 0.9236\n",
            "Epoch [36/200], Batch [93/123], Loss: 1.0908\n",
            "Epoch [36/200], Batch [94/123], Loss: 0.9522\n",
            "Epoch [36/200], Batch [95/123], Loss: 1.0708\n",
            "Epoch [36/200], Batch [96/123], Loss: 1.4011\n",
            "Epoch [36/200], Batch [97/123], Loss: 1.5307\n",
            "Epoch [36/200], Batch [98/123], Loss: 1.4411\n",
            "Epoch [36/200], Batch [99/123], Loss: 1.1837\n",
            "Epoch [36/200], Batch [100/123], Loss: 1.1341\n",
            "Epoch [36/200], Batch [101/123], Loss: 1.0827\n",
            "Epoch [36/200], Batch [102/123], Loss: 0.7508\n",
            "Epoch [36/200], Batch [103/123], Loss: 1.0043\n",
            "Epoch [36/200], Batch [104/123], Loss: 1.2029\n",
            "Epoch [36/200], Batch [105/123], Loss: 0.7613\n",
            "Epoch [36/200], Batch [106/123], Loss: 1.0880\n",
            "Epoch [36/200], Batch [107/123], Loss: 1.0498\n",
            "Epoch [36/200], Batch [108/123], Loss: 1.0575\n",
            "Epoch [36/200], Batch [109/123], Loss: 1.3456\n",
            "Epoch [36/200], Batch [110/123], Loss: 1.5036\n",
            "Epoch [36/200], Batch [111/123], Loss: 1.3196\n",
            "Epoch [36/200], Batch [112/123], Loss: 0.7645\n",
            "Epoch [36/200], Batch [113/123], Loss: 1.0731\n",
            "Epoch [36/200], Batch [114/123], Loss: 1.3568\n",
            "Epoch [36/200], Batch [115/123], Loss: 1.1922\n",
            "Epoch [36/200], Batch [116/123], Loss: 1.1706\n",
            "Epoch [36/200], Batch [117/123], Loss: 1.5562\n",
            "Epoch [36/200], Batch [118/123], Loss: 1.0820\n",
            "Epoch [36/200], Batch [119/123], Loss: 1.2793\n",
            "Epoch [36/200], Batch [120/123], Loss: 1.3238\n",
            "Epoch [36/200], Batch [121/123], Loss: 1.1486\n",
            "Epoch [36/200], Batch [122/123], Loss: 0.9705\n",
            "Epoch [36/200], Batch [123/123], Loss: 1.3145\n",
            "Epoch [37/200], Batch [1/123], Loss: 1.1429\n",
            "Epoch [37/200], Batch [2/123], Loss: 0.8673\n",
            "Epoch [37/200], Batch [3/123], Loss: 1.4316\n",
            "Epoch [37/200], Batch [4/123], Loss: 2.2421\n",
            "Epoch [37/200], Batch [5/123], Loss: 1.0182\n",
            "Epoch [37/200], Batch [6/123], Loss: 1.1408\n",
            "Epoch [37/200], Batch [7/123], Loss: 1.2378\n",
            "Epoch [37/200], Batch [8/123], Loss: 1.0531\n",
            "Epoch [37/200], Batch [9/123], Loss: 1.2341\n",
            "Epoch [37/200], Batch [10/123], Loss: 1.4643\n",
            "Epoch [37/200], Batch [11/123], Loss: 1.1524\n",
            "Epoch [37/200], Batch [12/123], Loss: 0.8612\n",
            "Epoch [37/200], Batch [13/123], Loss: 1.1005\n",
            "Epoch [37/200], Batch [14/123], Loss: 1.4581\n",
            "Epoch [37/200], Batch [15/123], Loss: 1.1681\n",
            "Epoch [37/200], Batch [16/123], Loss: 0.8968\n",
            "Epoch [37/200], Batch [17/123], Loss: 0.8452\n",
            "Epoch [37/200], Batch [18/123], Loss: 0.8532\n",
            "Epoch [37/200], Batch [19/123], Loss: 1.0491\n",
            "Epoch [37/200], Batch [20/123], Loss: 0.9984\n",
            "Epoch [37/200], Batch [21/123], Loss: 1.1669\n",
            "Epoch [37/200], Batch [22/123], Loss: 1.3516\n",
            "Epoch [37/200], Batch [23/123], Loss: 1.3903\n",
            "Epoch [37/200], Batch [24/123], Loss: 1.1683\n",
            "Epoch [37/200], Batch [25/123], Loss: 1.0168\n",
            "Epoch [37/200], Batch [26/123], Loss: 1.4539\n",
            "Epoch [37/200], Batch [27/123], Loss: 0.7621\n",
            "Epoch [37/200], Batch [28/123], Loss: 1.2818\n",
            "Epoch [37/200], Batch [29/123], Loss: 0.9277\n",
            "Epoch [37/200], Batch [30/123], Loss: 0.7658\n",
            "Epoch [37/200], Batch [31/123], Loss: 1.1588\n",
            "Epoch [37/200], Batch [32/123], Loss: 0.6821\n",
            "Epoch [37/200], Batch [33/123], Loss: 1.2725\n",
            "Epoch [37/200], Batch [34/123], Loss: 0.8744\n",
            "Epoch [37/200], Batch [35/123], Loss: 0.9456\n",
            "Epoch [37/200], Batch [36/123], Loss: 1.0786\n",
            "Epoch [37/200], Batch [37/123], Loss: 1.2512\n",
            "Epoch [37/200], Batch [38/123], Loss: 1.1753\n",
            "Epoch [37/200], Batch [39/123], Loss: 1.0279\n",
            "Epoch [37/200], Batch [40/123], Loss: 1.0096\n",
            "Epoch [37/200], Batch [41/123], Loss: 1.6684\n",
            "Epoch [37/200], Batch [42/123], Loss: 0.9491\n",
            "Epoch [37/200], Batch [43/123], Loss: 1.4062\n",
            "Epoch [37/200], Batch [44/123], Loss: 1.2220\n",
            "Epoch [37/200], Batch [45/123], Loss: 1.3075\n",
            "Epoch [37/200], Batch [46/123], Loss: 1.0633\n",
            "Epoch [37/200], Batch [47/123], Loss: 1.2672\n",
            "Epoch [37/200], Batch [48/123], Loss: 1.0694\n",
            "Epoch [37/200], Batch [49/123], Loss: 1.1283\n",
            "Epoch [37/200], Batch [50/123], Loss: 1.0051\n",
            "Epoch [37/200], Batch [51/123], Loss: 1.6139\n",
            "Epoch [37/200], Batch [52/123], Loss: 1.0755\n",
            "Epoch [37/200], Batch [53/123], Loss: 1.0758\n",
            "Epoch [37/200], Batch [54/123], Loss: 1.0722\n",
            "Epoch [37/200], Batch [55/123], Loss: 0.8822\n",
            "Epoch [37/200], Batch [56/123], Loss: 1.2247\n",
            "Epoch [37/200], Batch [57/123], Loss: 1.0614\n",
            "Epoch [37/200], Batch [58/123], Loss: 1.4080\n",
            "Epoch [37/200], Batch [59/123], Loss: 1.2744\n",
            "Epoch [37/200], Batch [60/123], Loss: 1.8743\n",
            "Epoch [37/200], Batch [61/123], Loss: 1.6436\n",
            "Epoch [37/200], Batch [62/123], Loss: 0.7725\n",
            "Epoch [37/200], Batch [63/123], Loss: 1.3038\n",
            "Epoch [37/200], Batch [64/123], Loss: 1.2038\n",
            "Epoch [37/200], Batch [65/123], Loss: 1.3666\n",
            "Epoch [37/200], Batch [66/123], Loss: 1.7724\n",
            "Epoch [37/200], Batch [67/123], Loss: 1.1718\n",
            "Epoch [37/200], Batch [68/123], Loss: 0.9563\n",
            "Epoch [37/200], Batch [69/123], Loss: 1.0941\n",
            "Epoch [37/200], Batch [70/123], Loss: 1.1358\n",
            "Epoch [37/200], Batch [71/123], Loss: 0.9543\n",
            "Epoch [37/200], Batch [72/123], Loss: 1.2858\n",
            "Epoch [37/200], Batch [73/123], Loss: 1.0512\n",
            "Epoch [37/200], Batch [74/123], Loss: 1.3510\n",
            "Epoch [37/200], Batch [75/123], Loss: 1.0238\n",
            "Epoch [37/200], Batch [76/123], Loss: 0.9579\n",
            "Epoch [37/200], Batch [77/123], Loss: 1.0513\n",
            "Epoch [37/200], Batch [78/123], Loss: 1.0860\n",
            "Epoch [37/200], Batch [79/123], Loss: 1.0684\n",
            "Epoch [37/200], Batch [80/123], Loss: 0.9350\n",
            "Epoch [37/200], Batch [81/123], Loss: 0.7947\n",
            "Epoch [37/200], Batch [82/123], Loss: 1.4042\n",
            "Epoch [37/200], Batch [83/123], Loss: 1.2511\n",
            "Epoch [37/200], Batch [84/123], Loss: 1.0699\n",
            "Epoch [37/200], Batch [85/123], Loss: 0.8976\n",
            "Epoch [37/200], Batch [86/123], Loss: 1.3014\n",
            "Epoch [37/200], Batch [87/123], Loss: 1.2821\n",
            "Epoch [37/200], Batch [88/123], Loss: 1.3548\n",
            "Epoch [37/200], Batch [89/123], Loss: 0.7746\n",
            "Epoch [37/200], Batch [90/123], Loss: 1.3729\n",
            "Epoch [37/200], Batch [91/123], Loss: 1.3492\n",
            "Epoch [37/200], Batch [92/123], Loss: 1.5598\n",
            "Epoch [37/200], Batch [93/123], Loss: 1.7940\n",
            "Epoch [37/200], Batch [94/123], Loss: 1.8340\n",
            "Epoch [37/200], Batch [95/123], Loss: 1.1020\n",
            "Epoch [37/200], Batch [96/123], Loss: 1.6596\n",
            "Epoch [37/200], Batch [97/123], Loss: 0.9584\n",
            "Epoch [37/200], Batch [98/123], Loss: 1.5103\n",
            "Epoch [37/200], Batch [99/123], Loss: 1.2745\n",
            "Epoch [37/200], Batch [100/123], Loss: 1.0888\n",
            "Epoch [37/200], Batch [101/123], Loss: 1.1180\n",
            "Epoch [37/200], Batch [102/123], Loss: 1.1015\n",
            "Epoch [37/200], Batch [103/123], Loss: 1.2987\n",
            "Epoch [37/200], Batch [104/123], Loss: 0.9385\n",
            "Epoch [37/200], Batch [105/123], Loss: 1.0877\n",
            "Epoch [37/200], Batch [106/123], Loss: 1.2303\n",
            "Epoch [37/200], Batch [107/123], Loss: 1.5255\n",
            "Epoch [37/200], Batch [108/123], Loss: 1.3171\n",
            "Epoch [37/200], Batch [109/123], Loss: 1.0677\n",
            "Epoch [37/200], Batch [110/123], Loss: 1.0676\n",
            "Epoch [37/200], Batch [111/123], Loss: 1.3867\n",
            "Epoch [37/200], Batch [112/123], Loss: 1.0237\n",
            "Epoch [37/200], Batch [113/123], Loss: 1.3249\n",
            "Epoch [37/200], Batch [114/123], Loss: 1.2852\n",
            "Epoch [37/200], Batch [115/123], Loss: 1.1311\n",
            "Epoch [37/200], Batch [116/123], Loss: 1.3983\n",
            "Epoch [37/200], Batch [117/123], Loss: 1.1907\n",
            "Epoch [37/200], Batch [118/123], Loss: 1.0125\n",
            "Epoch [37/200], Batch [119/123], Loss: 1.3089\n",
            "Epoch [37/200], Batch [120/123], Loss: 2.0337\n",
            "Epoch [37/200], Batch [121/123], Loss: 1.5502\n",
            "Epoch [37/200], Batch [122/123], Loss: 1.4483\n",
            "Epoch [37/200], Batch [123/123], Loss: 1.1305\n",
            "Epoch [38/200], Batch [1/123], Loss: 0.8643\n",
            "Epoch [38/200], Batch [2/123], Loss: 0.9945\n",
            "Epoch [38/200], Batch [3/123], Loss: 1.2172\n",
            "Epoch [38/200], Batch [4/123], Loss: 1.5856\n",
            "Epoch [38/200], Batch [5/123], Loss: 0.9448\n",
            "Epoch [38/200], Batch [6/123], Loss: 1.0858\n",
            "Epoch [38/200], Batch [7/123], Loss: 1.0166\n",
            "Epoch [38/200], Batch [8/123], Loss: 1.8818\n",
            "Epoch [38/200], Batch [9/123], Loss: 1.0038\n",
            "Epoch [38/200], Batch [10/123], Loss: 1.5230\n",
            "Epoch [38/200], Batch [11/123], Loss: 1.2428\n",
            "Epoch [38/200], Batch [12/123], Loss: 1.1286\n",
            "Epoch [38/200], Batch [13/123], Loss: 1.4041\n",
            "Epoch [38/200], Batch [14/123], Loss: 0.9387\n",
            "Epoch [38/200], Batch [15/123], Loss: 1.1615\n",
            "Epoch [38/200], Batch [16/123], Loss: 1.3239\n",
            "Epoch [38/200], Batch [17/123], Loss: 1.3720\n",
            "Epoch [38/200], Batch [18/123], Loss: 1.4546\n",
            "Epoch [38/200], Batch [19/123], Loss: 1.0458\n",
            "Epoch [38/200], Batch [20/123], Loss: 0.8580\n",
            "Epoch [38/200], Batch [21/123], Loss: 1.2154\n",
            "Epoch [38/200], Batch [22/123], Loss: 1.1152\n",
            "Epoch [38/200], Batch [23/123], Loss: 1.6030\n",
            "Epoch [38/200], Batch [24/123], Loss: 0.9220\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create the data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create model instance\n",
        "model = CrossAttentionModel(hsi_config, lidar_config).to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=1.00) #loss 2.3866\n",
        "\n",
        "# Initialize the best validation loss and best_model_wts before the training loop\n",
        "best_val_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "# Instantiate the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize the lowest_loss and best_model_wts before the training loop\n",
        "lowest_loss = float('inf')  # Initialize with a very high value\n",
        "best_model_wts = copy.deepcopy(model.state_dict())  # Initialize with the model weights\n",
        "\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (hsi_batch, lidar_batch, label_batch) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        hsi_batch = hsi_batch.to(device)\n",
        "        lidar_batch = lidar_batch.to(device)\n",
        "        label_batch = label_batch.to(device)  # Reshape labels\n",
        "        # print('hsi_batch shape:', hsi_batch.shape)\n",
        "        # print('lidar_batch:', lidar_batch.shape)\n",
        "        # print('label_batch shape:', label_batch.shape)\n",
        "\n",
        "        # Forward pass\n",
        "        output,attn_scores  = model(lidar_batch, hsi_batch)\n",
        "        #print('output shape:', output.shape)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.transpose(1, 2), label_batch.squeeze(2))\n",
        "        #loss = criterion(output, label_batch.long())\n",
        "\n",
        "        # Print loss every 10 batches\n",
        "        #if (batch_idx + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "        # If the loss is lower than the current lowest, save the model's state\n",
        "        if loss.item() < lowest_loss:\n",
        "            lowest_loss = loss.item()\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "# Print finishing training\n",
        "print('Finishing training')\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_wts, 'best_model_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8G-EKMlepL2t",
      "metadata": {
        "id": "8G-EKMlepL2t"
      },
      "outputs": [],
      "source": [
        "# Save the best model state\n",
        "torch.save(best_model_wts, 'best_model_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jOc5jb92pL6V",
      "metadata": {
        "id": "jOc5jb92pL6V"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "best_model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "\n",
        "# Load the weights\n",
        "best_model.load_state_dict(torch.load('best_model_weights.pth'))\n",
        "\n",
        "# Move the model to the GPU\n",
        "best_model = best_model.to(device)\n",
        "\n",
        "# Now your model is ready for making predictions on GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oSKh2t4gJcGU",
      "metadata": {
        "id": "oSKh2t4gJcGU"
      },
      "source": [
        "# 5.0 Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IrRolVFaKZy_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrRolVFaKZy_",
        "outputId": "6dc95fa0-a1b7-4c43-b7ec-3719a1f0e270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "hsi_patch_embedding.pos_embedding \t torch.Size([1, 64, 128])\n",
            "hsi_patch_embedding.cls_token \t torch.Size([1, 1, 128])\n",
            "hsi_patch_embedding.proj.weight \t torch.Size([128, 1, 1, 1])\n",
            "hsi_patch_embedding.proj.bias \t torch.Size([128])\n",
            "lidar_patch_embedding.pos_embedding \t torch.Size([1, 2, 128])\n",
            "lidar_patch_embedding.cls_token \t torch.Size([1, 1, 128])\n",
            "lidar_patch_embedding.proj.weight \t torch.Size([128, 1, 1, 1])\n",
            "lidar_patch_embedding.proj.bias \t torch.Size([128])\n",
            "cross_attention.to_q.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_k.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_v.weight \t torch.Size([512, 128])\n",
            "cross_attention.to_out.weight \t torch.Size([63, 512])\n",
            "cross_attention.to_out.bias \t torch.Size([63])\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "model = CrossAttentionModel(hsi_config, lidar_config)\n",
        "model.load_state_dict(torch.load('best_model_weights.pth'))\n",
        "model.eval()  # set the model to evaluation mode\n",
        "\n",
        "# Move the model to the GPU\n",
        "#\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2OAS0mPPKi9O",
      "metadata": {
        "id": "2OAS0mPPKi9O",
        "outputId": "50695dbd-b1a4-4a0f-eb90-6c377637790a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "hsi_patch_embedding.pos_embedding \t tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
            "hsi_patch_embedding.cls_token \t tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "hsi_patch_embedding.proj.weight \t tensor([[[[-0.6715]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6349]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2062]]],\n",
            "\n",
            "\n",
            "        [[[-0.7504]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0371]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8272]]],\n",
            "\n",
            "\n",
            "        [[[-0.7578]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9350]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6925]]],\n",
            "\n",
            "\n",
            "        [[[-0.0401]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0512]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6608]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3546]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4503]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3555]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4990]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7067]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6914]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2294]]],\n",
            "\n",
            "\n",
            "        [[[-0.8851]]],\n",
            "\n",
            "\n",
            "        [[[-0.0470]]],\n",
            "\n",
            "\n",
            "        [[[-0.9130]]],\n",
            "\n",
            "\n",
            "        [[[-0.6152]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7772]]],\n",
            "\n",
            "\n",
            "        [[[-0.8640]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9594]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7948]]],\n",
            "\n",
            "\n",
            "        [[[-0.7706]]],\n",
            "\n",
            "\n",
            "        [[[-0.3892]]],\n",
            "\n",
            "\n",
            "        [[[-0.7652]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5324]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5078]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0827]]],\n",
            "\n",
            "\n",
            "        [[[-0.1169]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6996]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3592]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2713]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3116]]],\n",
            "\n",
            "\n",
            "        [[[-0.1281]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2336]]],\n",
            "\n",
            "\n",
            "        [[[-0.3683]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3131]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6852]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0586]]],\n",
            "\n",
            "\n",
            "        [[[-0.6160]]],\n",
            "\n",
            "\n",
            "        [[[-0.2724]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4142]]],\n",
            "\n",
            "\n",
            "        [[[-0.1744]]],\n",
            "\n",
            "\n",
            "        [[[-0.2966]]],\n",
            "\n",
            "\n",
            "        [[[-0.0226]]],\n",
            "\n",
            "\n",
            "        [[[-0.1934]]],\n",
            "\n",
            "\n",
            "        [[[-0.1299]]],\n",
            "\n",
            "\n",
            "        [[[-0.4171]]],\n",
            "\n",
            "\n",
            "        [[[-0.2975]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9202]]],\n",
            "\n",
            "\n",
            "        [[[-0.5458]]],\n",
            "\n",
            "\n",
            "        [[[-0.1875]]],\n",
            "\n",
            "\n",
            "        [[[-0.8889]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2950]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1569]]],\n",
            "\n",
            "\n",
            "        [[[-0.0727]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7289]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6684]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9500]]],\n",
            "\n",
            "\n",
            "        [[[-0.6784]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8603]]],\n",
            "\n",
            "\n",
            "        [[[-0.2876]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6203]]],\n",
            "\n",
            "\n",
            "        [[[-0.7228]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1684]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2620]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7530]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4896]]],\n",
            "\n",
            "\n",
            "        [[[-0.8073]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8724]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2479]]],\n",
            "\n",
            "\n",
            "        [[[-0.2547]]],\n",
            "\n",
            "\n",
            "        [[[-0.0172]]],\n",
            "\n",
            "\n",
            "        [[[-0.8224]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7463]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5701]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1969]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4194]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7411]]],\n",
            "\n",
            "\n",
            "        [[[-0.8803]]],\n",
            "\n",
            "\n",
            "        [[[-0.4367]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8356]]],\n",
            "\n",
            "\n",
            "        [[[-0.7747]]],\n",
            "\n",
            "\n",
            "        [[[-0.8025]]],\n",
            "\n",
            "\n",
            "        [[[-0.3762]]],\n",
            "\n",
            "\n",
            "        [[[-0.9167]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3525]]],\n",
            "\n",
            "\n",
            "        [[[-0.4526]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1201]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8810]]],\n",
            "\n",
            "\n",
            "        [[[-0.3988]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9666]]],\n",
            "\n",
            "\n",
            "        [[[-0.1570]]],\n",
            "\n",
            "\n",
            "        [[[-0.1210]]],\n",
            "\n",
            "\n",
            "        [[[-0.5877]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8817]]],\n",
            "\n",
            "\n",
            "        [[[-0.4532]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3323]]],\n",
            "\n",
            "\n",
            "        [[[-0.6337]]],\n",
            "\n",
            "\n",
            "        [[[-0.6521]]],\n",
            "\n",
            "\n",
            "        [[[-0.3152]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9177]]],\n",
            "\n",
            "\n",
            "        [[[-0.9931]]],\n",
            "\n",
            "\n",
            "        [[[-0.1233]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9113]]],\n",
            "\n",
            "\n",
            "        [[[-0.6289]]],\n",
            "\n",
            "\n",
            "        [[[-0.3407]]],\n",
            "\n",
            "\n",
            "        [[[-0.7626]]],\n",
            "\n",
            "\n",
            "        [[[-0.6728]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6198]]],\n",
            "\n",
            "\n",
            "        [[[-0.0146]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1399]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4101]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4731]]],\n",
            "\n",
            "\n",
            "        [[[-0.0298]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8480]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5816]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7687]]],\n",
            "\n",
            "\n",
            "        [[[-0.0488]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4334]]],\n",
            "\n",
            "\n",
            "        [[[-0.3502]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4490]]],\n",
            "\n",
            "\n",
            "        [[[-0.7171]]]])\n",
            "hsi_patch_embedding.proj.bias \t tensor([ 0.3858,  0.8457, -0.6462, -0.8374,  0.9089, -0.9562, -0.3117, -0.6422,\n",
            "         0.9778,  0.2209, -0.5516, -0.9739, -0.8740, -0.9702, -0.5851, -0.8208,\n",
            "         0.3334,  0.9478,  0.8796, -0.0377,  0.5379, -0.8810,  0.0241, -0.2021,\n",
            "        -0.9643,  0.4712,  0.9013,  0.6203, -0.3968,  0.1308, -0.9985, -0.9385,\n",
            "         0.0259,  0.7994, -0.1726,  0.5426,  0.5624,  0.1592, -0.5636, -0.0325,\n",
            "        -0.0509, -0.5509,  0.3207,  0.8125, -0.2667,  0.6825,  0.5699,  0.0967,\n",
            "        -0.6244,  0.5446, -0.3356,  0.6510, -0.7573,  0.3162,  0.4318, -0.8624,\n",
            "         0.6442,  0.4350,  0.4185,  0.6106, -0.6310, -0.4780,  0.3657, -0.9816,\n",
            "        -0.5756, -0.9890, -0.6805, -0.2946, -0.6082, -0.3833, -0.6657,  0.7672,\n",
            "        -0.3613,  0.4749, -0.0990, -0.6846,  0.0698, -0.6817,  0.7478, -0.8932,\n",
            "         0.2707, -0.8942, -0.5222, -0.8613,  0.8445,  0.6387,  0.6126,  0.5223,\n",
            "         0.7481,  0.6137,  0.8737,  0.0937,  0.6899, -0.6750,  0.1953, -0.1611,\n",
            "         0.0751, -0.5818,  0.5094,  0.3269, -0.3780,  0.5728, -0.9977, -0.6930,\n",
            "         0.4640, -0.0014,  0.1353,  0.6556,  0.2145, -0.1600, -0.9706,  0.6683,\n",
            "        -0.4844, -0.4307,  0.5347, -0.0880,  0.7176,  0.4843, -0.4886,  0.1400,\n",
            "        -0.6425, -0.2942, -0.6124, -0.3123,  0.0139,  0.0832,  0.2383,  0.8941])\n",
            "lidar_patch_embedding.pos_embedding \t tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.0000],\n",
            "         [ 0.0433,  0.0992, -0.0164, -0.1250,  0.0245,  0.0123, -0.0799,\n",
            "          -0.0775,  0.0506,  0.0267, -0.0298,  0.0593, -0.0766,  0.0190,\n",
            "           0.0065,  0.0423, -0.0078, -0.0497, -0.0964, -0.0115, -0.0840,\n",
            "           0.0600,  0.0911, -0.0832, -0.1114, -0.0400, -0.0910, -0.0695,\n",
            "          -0.1120, -0.0093, -0.0914,  0.0381,  0.0202,  0.0577,  0.1022,\n",
            "          -0.1098, -0.0897,  0.1063, -0.0799, -0.1144, -0.0842,  0.0706,\n",
            "          -0.0460,  0.1337, -0.0678, -0.0743,  0.1185, -0.0960,  0.1418,\n",
            "           0.0863, -0.0066,  0.0982, -0.0218,  0.0525,  0.0811,  0.1072,\n",
            "           0.0077,  0.1130,  0.1213, -0.0251, -0.0169,  0.1039,  0.0962,\n",
            "          -0.0887,  0.1044, -0.0411,  0.1026, -0.0278, -0.0509,  0.0817,\n",
            "          -0.0654, -0.0756,  0.0883,  0.0853,  0.0462, -0.0093, -0.0738,\n",
            "          -0.1117, -0.0764,  0.0504, -0.0873, -0.0358,  0.0640,  0.0115,\n",
            "          -0.0624, -0.0474, -0.0224, -0.0845, -0.0016, -0.0774,  0.0948,\n",
            "           0.0878,  0.0320, -0.1141,  0.0083, -0.0346, -0.0069, -0.0512,\n",
            "          -0.0870, -0.0472, -0.0489,  0.0643,  0.0929,  0.1134,  0.0220,\n",
            "           0.0038, -0.0257,  0.0345,  0.0174, -0.0345, -0.0168, -0.0419,\n",
            "           0.1283, -0.0837,  0.0116, -0.0676,  0.0163, -0.0967, -0.0591,\n",
            "           0.0062,  0.0123,  0.1070,  0.0060,  0.0747,  0.0020, -0.0022,\n",
            "           0.0879, -0.0814]]])\n",
            "lidar_patch_embedding.cls_token \t tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "lidar_patch_embedding.proj.weight \t tensor([[[[-0.5407]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5561]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1446]]],\n",
            "\n",
            "\n",
            "        [[[-0.6423]]],\n",
            "\n",
            "\n",
            "        [[[-0.8246]]],\n",
            "\n",
            "\n",
            "        [[[-0.6306]]],\n",
            "\n",
            "\n",
            "        [[[-0.6614]]],\n",
            "\n",
            "\n",
            "        [[[-0.4058]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1365]]],\n",
            "\n",
            "\n",
            "        [[[-0.1188]]],\n",
            "\n",
            "\n",
            "        [[[-0.3141]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0231]]],\n",
            "\n",
            "\n",
            "        [[[-0.5871]]],\n",
            "\n",
            "\n",
            "        [[[-0.1589]]],\n",
            "\n",
            "\n",
            "        [[[-0.0579]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9979]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4485]]],\n",
            "\n",
            "\n",
            "        [[[-0.7637]]],\n",
            "\n",
            "\n",
            "        [[[-0.5110]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7450]]],\n",
            "\n",
            "\n",
            "        [[[-0.2524]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9843]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4724]]],\n",
            "\n",
            "\n",
            "        [[[-0.2709]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7699]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7405]]],\n",
            "\n",
            "\n",
            "        [[[-0.6630]]],\n",
            "\n",
            "\n",
            "        [[[-0.2149]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7974]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1484]]],\n",
            "\n",
            "\n",
            "        [[[-0.2912]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2939]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4035]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9246]]],\n",
            "\n",
            "\n",
            "        [[[-0.7444]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9185]]],\n",
            "\n",
            "\n",
            "        [[[-0.3128]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1360]]],\n",
            "\n",
            "\n",
            "        [[[-0.6421]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1244]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4156]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7585]]],\n",
            "\n",
            "\n",
            "        [[[-0.9672]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0092]]],\n",
            "\n",
            "\n",
            "        [[[-0.2144]]],\n",
            "\n",
            "\n",
            "        [[[-0.7467]]],\n",
            "\n",
            "\n",
            "        [[[-0.4312]]],\n",
            "\n",
            "\n",
            "        [[[-0.4063]]],\n",
            "\n",
            "\n",
            "        [[[-0.0444]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2391]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8070]]],\n",
            "\n",
            "\n",
            "        [[[-0.0398]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0773]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9031]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3411]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9650]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7311]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7830]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1323]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0221]]],\n",
            "\n",
            "\n",
            "        [[[-0.7713]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3550]]],\n",
            "\n",
            "\n",
            "        [[[-0.8106]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1569]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6975]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9250]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4259]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4487]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3832]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9987]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9408]]],\n",
            "\n",
            "\n",
            "        [[[-0.7006]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5108]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0182]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9804]]],\n",
            "\n",
            "\n",
            "        [[[-0.1955]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8976]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1071]]],\n",
            "\n",
            "\n",
            "        [[[-0.1389]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6836]]],\n",
            "\n",
            "\n",
            "        [[[-0.6264]]],\n",
            "\n",
            "\n",
            "        [[[-0.6863]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8063]]],\n",
            "\n",
            "\n",
            "        [[[-0.7005]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5787]]],\n",
            "\n",
            "\n",
            "        [[[-0.1157]]],\n",
            "\n",
            "\n",
            "        [[[-0.4171]]],\n",
            "\n",
            "\n",
            "        [[[-0.5700]]],\n",
            "\n",
            "\n",
            "        [[[-0.1990]]],\n",
            "\n",
            "\n",
            "        [[[-0.9724]]],\n",
            "\n",
            "\n",
            "        [[[-0.8297]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0848]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9750]]],\n",
            "\n",
            "\n",
            "        [[[-0.3526]]],\n",
            "\n",
            "\n",
            "        [[[-0.8726]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2012]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4700]]],\n",
            "\n",
            "\n",
            "        [[[-0.7718]]],\n",
            "\n",
            "\n",
            "        [[[-0.5691]]],\n",
            "\n",
            "\n",
            "        [[[-0.4360]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3744]]],\n",
            "\n",
            "\n",
            "        [[[-0.3512]]],\n",
            "\n",
            "\n",
            "        [[[-0.7729]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0575]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7133]]],\n",
            "\n",
            "\n",
            "        [[[-0.8834]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8686]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4351]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7782]]],\n",
            "\n",
            "\n",
            "        [[[ 0.9029]]],\n",
            "\n",
            "\n",
            "        [[[-0.6468]]],\n",
            "\n",
            "\n",
            "        [[[-0.9549]]],\n",
            "\n",
            "\n",
            "        [[[-0.0311]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2126]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5859]]],\n",
            "\n",
            "\n",
            "        [[[-0.1204]]],\n",
            "\n",
            "\n",
            "        [[[-0.4433]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1774]]],\n",
            "\n",
            "\n",
            "        [[[ 0.8133]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3162]]],\n",
            "\n",
            "\n",
            "        [[[-0.5258]]],\n",
            "\n",
            "\n",
            "        [[[-0.4495]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2881]]],\n",
            "\n",
            "\n",
            "        [[[ 0.6154]]],\n",
            "\n",
            "\n",
            "        [[[ 0.7120]]],\n",
            "\n",
            "\n",
            "        [[[-0.1637]]],\n",
            "\n",
            "\n",
            "        [[[-0.6229]]],\n",
            "\n",
            "\n",
            "        [[[-0.5921]]]])\n",
            "lidar_patch_embedding.proj.bias \t tensor([ 0.4179, -0.4181, -0.7519, -0.4501,  0.6866,  0.5237, -0.7736, -0.2993,\n",
            "         0.5310,  0.9365,  0.4566,  0.6052, -0.6636,  0.0276,  0.0244,  0.4847,\n",
            "        -0.1307,  0.6994, -0.2232,  0.3848, -1.0782, -0.7708,  0.8993,  0.0920,\n",
            "        -0.6964, -0.7441, -0.0283, -0.2018, -0.3213, -0.4579, -0.9979,  0.1021,\n",
            "        -0.6037, -0.0963,  0.1339, -0.9785, -0.4113,  0.4714, -0.1553, -0.8787,\n",
            "        -0.8460, -0.0097, -0.4227,  0.7714, -0.9881, -0.6921,  0.6406, -0.0728,\n",
            "         0.6377,  0.3182,  0.9720,  1.0264, -0.0113,  0.8631,  0.7711,  0.5872,\n",
            "         0.0076, -0.0075,  0.9063,  0.3684,  0.6276,  1.0068,  0.8356, -0.2702,\n",
            "         0.0798, -0.3348,  0.9044, -0.8911,  0.3707, -0.3398, -0.7360,  0.1159,\n",
            "        -0.1795,  0.7692, -0.8032,  0.3680, -0.1619, -0.9203, -0.5593, -0.4399,\n",
            "        -0.0769,  0.6726, -0.7913, -0.6735, -0.6670, -0.1353,  0.5287, -0.1220,\n",
            "        -0.7798, -0.1481,  1.0573,  0.7097, -0.0779, -0.3610,  0.6100,  0.2982,\n",
            "         0.8688, -0.5495,  0.2501, -0.7979,  0.0177,  0.9214,  0.0933,  0.1946,\n",
            "         0.4881,  0.9387, -0.1914,  0.6775, -0.2640, -0.6839, -0.0927,  0.6829,\n",
            "         1.0948, -0.7219,  0.6821, -0.7019,  0.0867, -0.1668, -0.1167,  0.1401,\n",
            "         0.9783,  0.8794,  0.8113,  0.3754,  0.3657, -0.9117,  0.6940, -0.5520])\n",
            "cross_attention.to_q.weight \t tensor([[-0.0828,  0.0425, -0.0623,  ..., -0.0408, -0.0150, -0.0578],\n",
            "        [-0.0671, -0.0314, -0.0207,  ...,  0.0549,  0.0109,  0.0178],\n",
            "        [-0.0671, -0.0745,  0.0323,  ...,  0.0863, -0.0247, -0.0402],\n",
            "        ...,\n",
            "        [ 0.0273,  0.0476,  0.0241,  ...,  0.0161,  0.0518,  0.0257],\n",
            "        [ 0.0098,  0.0160,  0.0036,  ..., -0.0170, -0.0381,  0.0610],\n",
            "        [ 0.0246, -0.0509, -0.0222,  ...,  0.0344,  0.0721, -0.0689]])\n",
            "cross_attention.to_k.weight \t tensor([[-0.0861,  0.0862, -0.0867,  ..., -0.0705, -0.0441,  0.0562],\n",
            "        [ 0.0726,  0.0046,  0.0177,  ...,  0.0563,  0.0364, -0.0652],\n",
            "        [ 0.0551, -0.0615, -0.0137,  ...,  0.0131, -0.0181, -0.0833],\n",
            "        ...,\n",
            "        [ 0.0606,  0.0557, -0.0068,  ...,  0.0101,  0.0566,  0.0830],\n",
            "        [-0.0368,  0.0433,  0.0367,  ...,  0.0218,  0.0682, -0.0030],\n",
            "        [ 0.0704,  0.0671,  0.0236,  ..., -0.0011,  0.0356,  0.0006]])\n",
            "cross_attention.to_v.weight \t tensor([[-0.1378,  0.1548,  0.0912,  ...,  0.0008, -0.1220,  0.0498],\n",
            "        [ 0.0213,  0.0736, -0.0316,  ..., -0.1129,  0.0132, -0.0321],\n",
            "        [-0.0717, -0.0699,  0.0065,  ...,  0.0460,  0.0275,  0.0502],\n",
            "        ...,\n",
            "        [-0.0526,  0.0638,  0.1357,  ...,  0.1466, -0.1046,  0.1564],\n",
            "        [-0.0042,  0.0234,  0.0951,  ...,  0.1059, -0.0472,  0.0672],\n",
            "        [-0.1452,  0.0426,  0.0334,  ...,  0.1168, -0.0334,  0.0063]])\n",
            "cross_attention.to_out.weight \t tensor([[-4.3152e-02,  1.9099e-02, -8.0123e-03,  ..., -5.7773e-05,\n",
            "         -3.6563e-02, -5.8125e-03],\n",
            "        [ 3.3684e-02, -2.4315e-02,  3.3910e-02,  ...,  3.4128e-02,\n",
            "          2.3515e-02,  2.2009e-02],\n",
            "        [ 1.6929e-02,  9.8877e-04, -2.5182e-03,  ...,  3.5690e-02,\n",
            "          1.9155e-02,  2.3969e-02],\n",
            "        ...,\n",
            "        [-3.6399e-02,  4.2482e-02, -3.6793e-02,  ..., -2.2693e-02,\n",
            "          3.1941e-02,  1.9219e-02],\n",
            "        [-1.6208e-02,  4.2937e-02, -1.9642e-02,  ...,  2.7161e-03,\n",
            "          2.4236e-02,  2.2304e-02],\n",
            "        [-6.0214e-03, -1.6358e-02, -3.3477e-02,  ...,  3.0635e-02,\n",
            "          1.0305e-02, -1.7747e-02]])\n",
            "cross_attention.to_out.bias \t tensor([-0.0154,  0.0278, -0.0164,  0.0120, -0.0124,  0.0302,  0.0441, -0.0405,\n",
            "        -0.0047,  0.0190,  0.0370, -0.0272,  0.0393, -0.0135, -0.0116,  0.0389,\n",
            "         0.0393, -0.0106, -0.0034, -0.0024, -0.0218,  0.0440, -0.0053,  0.0079,\n",
            "         0.0201,  0.0020,  0.0398,  0.0294, -0.0322, -0.0236,  0.0099, -0.0101,\n",
            "         0.0399, -0.0151,  0.0025, -0.0385,  0.0111,  0.0044, -0.0067,  0.0018,\n",
            "         0.0361,  0.0349,  0.0117,  0.0124, -0.0221,  0.0285,  0.0098,  0.0180,\n",
            "         0.0338,  0.0417, -0.0359,  0.0390,  0.0184, -0.0388,  0.0365, -0.0176,\n",
            "        -0.0031, -0.0253, -0.0188,  0.0179, -0.0183,  0.0404, -0.0241])\n"
          ]
        }
      ],
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S2xqXWkOndVP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2xqXWkOndVP",
        "outputId": "68f0d128-583e-499a-e81b-ffb4b870522e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_predictions shape: (467, 63, 512)\n",
            "all_attn_scores shape: (467, 63, 63)\n"
          ]
        }
      ],
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "best_model.eval()\n",
        "\n",
        "# Initialize a list to hold all the predictions and attention scores\n",
        "all_predictions = []\n",
        "all_attn_scores = []\n",
        "\n",
        "# Loop over the validation set\n",
        "for hsi_batch, lidar_batch, _ in val_loader:  # we don't need the labels for predictions\n",
        "    # Move the batch to the desired device\n",
        "    hsi_batch = hsi_batch.to(device)\n",
        "    lidar_batch = lidar_batch.to(device)\n",
        "\n",
        "    # Pass the batch through the model\n",
        "    with torch.no_grad():\n",
        "        output, attn_scores = best_model(lidar_batch, hsi_batch)\n",
        "\n",
        "        # Add the predictions and attention scores to our lists\n",
        "        all_predictions.append(output.cpu().numpy())\n",
        "        all_attn_scores.append(attn_scores.cpu().numpy())\n",
        "\n",
        "# Concatenate all predictions and attention scores into a single numpy array\n",
        "all_predictions = np.concatenate(all_predictions)\n",
        "all_attn_scores = np.concatenate(all_attn_scores)\n",
        "\n",
        "# Now you can use the predictions and attention scores as needed\n",
        "print('all_predictions shape:', all_predictions.shape )\n",
        "print('all_attn_scores shape:', all_attn_scores.shape )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7o9FUM2ILtVK",
      "metadata": {
        "id": "7o9FUM2ILtVK"
      },
      "source": [
        "# Band Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oqc6nZNfXKEN",
      "metadata": {
        "id": "oqc6nZNfXKEN"
      },
      "source": [
        "Calculate the variance of the feature vectors for each band across all samples and then find the bands with the maximum variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4EH-ULRdlVVW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EH-ULRdlVVW",
        "outputId": "51b7877f-0448-4f06-ab05-0c2a464e71de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_attention: (63,)\n",
            "sorted_band_indices : (63,)\n",
            "sorted_attention_values : (63,)\n",
            "Top 70 bands based on the mean attention score:\n",
            "Band index: 54, Mean attention score: 65.69277954101562\n",
            "Band index: 61, Mean attention score: 58.85620880126953\n",
            "Band index: 9, Mean attention score: 56.95847702026367\n",
            "Band index: 39, Mean attention score: 54.948387145996094\n",
            "Band index: 35, Mean attention score: 52.40194320678711\n",
            "Band index: 59, Mean attention score: 48.99981689453125\n",
            "Band index: 58, Mean attention score: 48.081111907958984\n",
            "Band index: 16, Mean attention score: 43.53626251220703\n",
            "Band index: 47, Mean attention score: 42.674068450927734\n",
            "Band index: 32, Mean attention score: 42.53934097290039\n",
            "Band index: 0, Mean attention score: 38.19260025024414\n",
            "Band index: 31, Mean attention score: 36.499664306640625\n",
            "Band index: 60, Mean attention score: 33.035743713378906\n",
            "Band index: 56, Mean attention score: 31.52651596069336\n",
            "Band index: 17, Mean attention score: 30.98958396911621\n",
            "Band index: 21, Mean attention score: 29.970354080200195\n",
            "Band index: 8, Mean attention score: 26.89563751220703\n",
            "Band index: 23, Mean attention score: 26.300935745239258\n",
            "Band index: 49, Mean attention score: 23.694713592529297\n",
            "Band index: 19, Mean attention score: 23.366252899169922\n",
            "Band index: 36, Mean attention score: 22.092952728271484\n",
            "Band index: 25, Mean attention score: 21.180421829223633\n",
            "Band index: 15, Mean attention score: 17.4706974029541\n",
            "Band index: 38, Mean attention score: 16.964326858520508\n",
            "Band index: 29, Mean attention score: 16.75489616394043\n",
            "Band index: 6, Mean attention score: 15.79674243927002\n",
            "Band index: 43, Mean attention score: 14.955124855041504\n",
            "Band index: 27, Mean attention score: 14.643134117126465\n",
            "Band index: 52, Mean attention score: 13.890037536621094\n",
            "Band index: 13, Mean attention score: 13.634753227233887\n",
            "Band index: 42, Mean attention score: 12.879240989685059\n",
            "Band index: 1, Mean attention score: 11.814666748046875\n",
            "Band index: 45, Mean attention score: 11.688201904296875\n",
            "Band index: 12, Mean attention score: 11.443486213684082\n",
            "Band index: 5, Mean attention score: 6.4447455406188965\n",
            "Band index: 46, Mean attention score: 5.72416353225708\n",
            "Band index: 7, Mean attention score: 3.8716249465942383\n",
            "Band index: 33, Mean attention score: 2.4680662155151367\n",
            "Band index: 34, Mean attention score: 1.2471152544021606\n",
            "Band index: 50, Mean attention score: -0.6948674917221069\n",
            "Band index: 30, Mean attention score: -3.7320218086242676\n",
            "Band index: 51, Mean attention score: -4.599573135375977\n",
            "Band index: 20, Mean attention score: -7.13266658782959\n",
            "Band index: 18, Mean attention score: -10.849517822265625\n",
            "Band index: 14, Mean attention score: -13.295186996459961\n",
            "Band index: 53, Mean attention score: -14.141976356506348\n",
            "Band index: 26, Mean attention score: -16.015762329101562\n",
            "Band index: 22, Mean attention score: -16.1928653717041\n",
            "Band index: 28, Mean attention score: -18.298179626464844\n",
            "Band index: 11, Mean attention score: -18.34332847595215\n",
            "Band index: 24, Mean attention score: -19.225046157836914\n",
            "Band index: 3, Mean attention score: -20.858943939208984\n",
            "Band index: 57, Mean attention score: -23.704269409179688\n",
            "Band index: 37, Mean attention score: -24.015539169311523\n",
            "Band index: 10, Mean attention score: -27.970361709594727\n",
            "Band index: 2, Mean attention score: -29.008806228637695\n",
            "Band index: 4, Mean attention score: -31.069787979125977\n",
            "Band index: 48, Mean attention score: -34.22278594970703\n",
            "Band index: 41, Mean attention score: -35.08060073852539\n",
            "Band index: 40, Mean attention score: -39.79642868041992\n",
            "Band index: 44, Mean attention score: -46.306640625\n",
            "Band index: 62, Mean attention score: -63.8343505859375\n",
            "Band index: 55, Mean attention score: -72.67034912109375\n",
            "\n",
            "List of top 70 band indices:\n",
            "[54, 61, 9, 39, 35, 59, 58, 16, 47, 32, 0, 31, 60, 56, 17, 21, 8, 23, 49, 19, 36, 25, 15, 38, 29, 6, 43, 27, 52, 13, 42, 1, 45, 12, 5, 46, 7, 33, 34, 50, 30, 51, 20, 18, 14, 53, 26, 22, 28, 11, 24, 3, 57, 37, 10, 2, 4, 48, 41, 40, 44, 62, 55]\n"
          ]
        }
      ],
      "source": [
        "# First, take the mean across all samples and heads. This will result in a single 144-dim vector.\n",
        "mean_attention = np.mean(all_attn_scores, axis=(0, 1))\n",
        "print('mean_attention:',mean_attention.shape)\n",
        "\n",
        "# Then, sort the bands by attention. This will give you the band indices in descending order of attention.\n",
        "sorted_band_indices = np.argsort(mean_attention)[::-1]\n",
        "print('sorted_band_indices :',sorted_band_indices.shape)\n",
        "\n",
        "# If you want to see the attention values as well, you can sort the mean_attention array in the same order.\n",
        "sorted_attention_values = mean_attention[sorted_band_indices]\n",
        "print('sorted_attention_values :',sorted_attention_values.shape)\n",
        "\n",
        "N = 70  # Top N bands\n",
        "top_N_bands = sorted_band_indices[:N]\n",
        "top_N_attention_values = sorted_attention_values[:N]\n",
        "\n",
        "# Print the top N band indices with their attention scores\n",
        "print(f\"Top {N} bands based on the mean attention score:\")\n",
        "for band_index, attention_value in zip(top_N_bands, top_N_attention_values):\n",
        "    print(f\"Band index: {band_index}, Mean attention score: {attention_value}\")\n",
        "\n",
        "# Print the list of top N band indices\n",
        "print(\"\\nList of top {} band indices:\".format(N))\n",
        "print(top_N_bands.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711be420",
      "metadata": {
        "id": "711be420"
      },
      "outputs": [],
      "source": [
        "# Patch_size=1\n",
        "bands_p1=[54, 61, 9, 39, 35, 59, 58, 16, 47, 32, 0, 31, 60, 56, 17, 21, 8, 23, 49, 19, 36, 25, 15, 38, 29, 6, 43, 27, 52, 13, 42, 1, 45, 12, 5, 46, 7, 33, 34, 50, 30, 51, 20, 18, 14, 53, 26, 22, 28, 11, 24, 3, 57, 37, 10, 2, 4, 48, 41, 40, 44, 62, 55]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f156014",
      "metadata": {
        "id": "7f156014"
      },
      "outputs": [],
      "source": [
        "# Using Lidar spatial feature to sorting\n",
        "bands=[20, 53, 13, 3, 36, 19, 52, 62, 30, 21, 51, 4, 61, 48, 29, 40, 23, 33, 59, 56, 32, 14, 46, 50, 39, 5, 38, 9, 35, 8, 28, 42, 18, 60, 2, 11, 12, 54, 27, 57, 58, 41, 10, 7, 37, 22, 43, 49, 47, 55, 25, 1, 45, 34, 44, 26, 24, 31, 15, 0, 6, 16, 17]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2389b687",
      "metadata": {
        "id": "2389b687"
      },
      "outputs": [],
      "source": [
        "# patch_size=3 band selction\n",
        "bands=[34, 57, 33, 50, 20, 40, 35, 31, 39, 13, 26, 53, 51, 27, 47, 61, 6, 58, 9, 5, 46, 54, 4, 16, 43, 12, 19, 36, 1, 30, 2, 28, 7, 32, 42, 25, 8, 44, 17, 23, 38, 52, 15, 14, 21, 56, 48, 49, 0, 62, 10, 45, 59, 22, 37, 24, 60, 41, 18, 11, 3, 55, 29]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8TdumuTVo6PE",
      "metadata": {
        "id": "8TdumuTVo6PE"
      },
      "outputs": [],
      "source": [
        "# Patch_size= 5 band selection\n",
        "bands=[54, 15, 20, 22, 27, 48, 38, 60, 37, 33, 36, 14, 30, 56, 7, 47, 51, 23, 0, 50, 1, 12, 41, 43, 2, 13, 19, 45, 28, 31, 16, 34, 57, 21, 18, 6, 61, 24, 17, 49, 46, 62, 55, 44, 3, 35, 11, 58, 10, 9, 32, 29, 39, 8, 59, 53, 5, 42, 4, 52, 40, 25, 26]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yNBFu0zVo6Tg",
      "metadata": {
        "id": "yNBFu0zVo6Tg"
      },
      "outputs": [],
      "source": [
        "# Patch_size= 7 band selection\n",
        "bands=[18, 53, 24, 45, 31, 26, 9, 41, 12, 29, 36, 42, 5, 47, 6, 60, 59, 11, 22, 17, 58, 15, 14, 32, 44, 3, 16, 19, 13, 27, 50, 51, 46, 4, 25, 7, 30, 43, 37, 52, 21, 2, 39, 57, 56, 0, 49, 1, 62, 10, 38, 34, 8, 48, 23, 20, 28, 33, 40, 54, 61, 55, 35]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f295119",
      "metadata": {
        "id": "5f295119"
      },
      "outputs": [],
      "source": [
        "# Patch_size= 9 band selection\n",
        "bands=[37, 52, 39, 50, 46, 42, 31, 35, 18, 34, 43, 26, 14, 25, 59, 30, 44, 22, 57, 21, 5, 32, 6, 58, 27, 38, 36, 4, 49, 8, 56, 20, 12, 55, 0, 61, 11, 16, 54, 53, 3, 17, 51, 7, 33, 29, 41, 9, 24, 40, 1, 10, 23, 45, 47, 15, 28, 2, 13, 62, 48, 60, 19]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107cf5db",
      "metadata": {
        "id": "107cf5db"
      },
      "outputs": [],
      "source": [
        "## Patch_size= 11 band selection\n",
        "bands=[26, 52, 20, 14, 0, 10, 53, 35, 37, 36, 41, 9, 2, 6, 25, 28, 8, 19, 7, 45, 51, 46, 49, 44, 61, 60, 34, 5, 40, 30, 50, 12, 24, 42, 29, 18, 38, 39, 33, 21, 55, 56, 48, 17, 16, 59, 1, 23, 32, 57, 15, 13, 3, 43, 31, 58, 47, 62, 27, 54, 11, 4, 22]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}